I0920 12:04:59.380985      20 e2e.go:126] Starting e2e run "e878e702-44f2-4f20-b5dd-9ccb6fb27e1a" on Ginkgo node 1
Sep 20 12:04:59.390: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1695211499 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Sep 20 12:04:59.489: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
E0920 12:04:59.490750      20 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Sep 20 12:04:59.490: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Sep 20 12:04:59.624: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Sep 20 12:04:59.661: INFO: 29 / 29 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Sep 20 12:04:59.661: INFO: expected 7 pod replicas in namespace 'kube-system', 7 are Running and Ready.
Sep 20 12:04:59.662: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Sep 20 12:05:01.021: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'csi-cinder-nodeplugin' (1 seconds elapsed)
Sep 20 12:05:01.021: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'k8s-keystone-auth' (1 seconds elapsed)
Sep 20 12:05:01.021: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds' (1 seconds elapsed)
Sep 20 12:05:01.021: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'npd' (1 seconds elapsed)
Sep 20 12:05:01.021: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'openstack-cloud-controller-manager' (1 seconds elapsed)
Sep 20 12:05:01.021: INFO: e2e test version: v1.26.8
Sep 20 12:05:01.024: INFO: kube-apiserver version: v1.26.8
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Sep 20 12:05:01.025: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
Sep 20 12:05:01.042: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [1.553 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Sep 20 12:04:59.489: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    E0920 12:04:59.490750      20 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Sep 20 12:04:59.490: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Sep 20 12:04:59.624: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Sep 20 12:04:59.661: INFO: 29 / 29 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Sep 20 12:04:59.661: INFO: expected 7 pod replicas in namespace 'kube-system', 7 are Running and Ready.
    Sep 20 12:04:59.662: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Sep 20 12:05:01.021: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'csi-cinder-nodeplugin' (1 seconds elapsed)
    Sep 20 12:05:01.021: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'k8s-keystone-auth' (1 seconds elapsed)
    Sep 20 12:05:01.021: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds' (1 seconds elapsed)
    Sep 20 12:05:01.021: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'npd' (1 seconds elapsed)
    Sep 20 12:05:01.021: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'openstack-cloud-controller-manager' (1 seconds elapsed)
    Sep 20 12:05:01.021: INFO: e2e test version: v1.26.8
    Sep 20 12:05:01.024: INFO: kube-apiserver version: v1.26.8
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Sep 20 12:05:01.025: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    Sep 20 12:05:01.042: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:05:01.06
Sep 20 12:05:01.060: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename pods 09/20/23 12:05:01.061
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:05:02.454
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:05:02.457
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Sep 20 12:05:04.164: INFO: Waiting up to 5m0s for pod "server-envvars-9830417c-30de-4d00-862f-4d03eb347317" in namespace "pods-1097" to be "running and ready"
Sep 20 12:05:06.205: INFO: Pod "server-envvars-9830417c-30de-4d00-862f-4d03eb347317": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040842846s
Sep 20 12:05:06.205: INFO: The phase of Pod server-envvars-9830417c-30de-4d00-862f-4d03eb347317 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 12:05:08.213: INFO: Pod "server-envvars-9830417c-30de-4d00-862f-4d03eb347317": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049201151s
Sep 20 12:05:08.213: INFO: The phase of Pod server-envvars-9830417c-30de-4d00-862f-4d03eb347317 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 12:05:10.214: INFO: Pod "server-envvars-9830417c-30de-4d00-862f-4d03eb347317": Phase="Pending", Reason="", readiness=false. Elapsed: 6.050284669s
Sep 20 12:05:10.214: INFO: The phase of Pod server-envvars-9830417c-30de-4d00-862f-4d03eb347317 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 12:05:12.258: INFO: Pod "server-envvars-9830417c-30de-4d00-862f-4d03eb347317": Phase="Pending", Reason="", readiness=false. Elapsed: 8.09422938s
Sep 20 12:05:12.259: INFO: The phase of Pod server-envvars-9830417c-30de-4d00-862f-4d03eb347317 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 12:05:14.211: INFO: Pod "server-envvars-9830417c-30de-4d00-862f-4d03eb347317": Phase="Pending", Reason="", readiness=false. Elapsed: 10.047357979s
Sep 20 12:05:14.212: INFO: The phase of Pod server-envvars-9830417c-30de-4d00-862f-4d03eb347317 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 12:05:16.211: INFO: Pod "server-envvars-9830417c-30de-4d00-862f-4d03eb347317": Phase="Pending", Reason="", readiness=false. Elapsed: 12.047062059s
Sep 20 12:05:16.212: INFO: The phase of Pod server-envvars-9830417c-30de-4d00-862f-4d03eb347317 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 12:05:18.381: INFO: Pod "server-envvars-9830417c-30de-4d00-862f-4d03eb347317": Phase="Pending", Reason="", readiness=false. Elapsed: 14.217170488s
Sep 20 12:05:18.381: INFO: The phase of Pod server-envvars-9830417c-30de-4d00-862f-4d03eb347317 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 12:05:21.687: INFO: Pod "server-envvars-9830417c-30de-4d00-862f-4d03eb347317": Phase="Pending", Reason="", readiness=false. Elapsed: 17.523060954s
Sep 20 12:05:21.687: INFO: The phase of Pod server-envvars-9830417c-30de-4d00-862f-4d03eb347317 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 12:05:22.299: INFO: Pod "server-envvars-9830417c-30de-4d00-862f-4d03eb347317": Phase="Pending", Reason="", readiness=false. Elapsed: 18.134918746s
Sep 20 12:05:22.299: INFO: The phase of Pod server-envvars-9830417c-30de-4d00-862f-4d03eb347317 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 12:05:24.212: INFO: Pod "server-envvars-9830417c-30de-4d00-862f-4d03eb347317": Phase="Pending", Reason="", readiness=false. Elapsed: 20.047544533s
Sep 20 12:05:24.212: INFO: The phase of Pod server-envvars-9830417c-30de-4d00-862f-4d03eb347317 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 12:05:26.223: INFO: Pod "server-envvars-9830417c-30de-4d00-862f-4d03eb347317": Phase="Pending", Reason="", readiness=false. Elapsed: 22.05919344s
Sep 20 12:05:26.223: INFO: The phase of Pod server-envvars-9830417c-30de-4d00-862f-4d03eb347317 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 12:05:28.258: INFO: Pod "server-envvars-9830417c-30de-4d00-862f-4d03eb347317": Phase="Pending", Reason="", readiness=false. Elapsed: 24.093515932s
Sep 20 12:05:28.258: INFO: The phase of Pod server-envvars-9830417c-30de-4d00-862f-4d03eb347317 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 12:05:30.376: INFO: Pod "server-envvars-9830417c-30de-4d00-862f-4d03eb347317": Phase="Running", Reason="", readiness=true. Elapsed: 26.211702117s
Sep 20 12:05:30.376: INFO: The phase of Pod server-envvars-9830417c-30de-4d00-862f-4d03eb347317 is Running (Ready = true)
Sep 20 12:05:30.376: INFO: Pod "server-envvars-9830417c-30de-4d00-862f-4d03eb347317" satisfied condition "running and ready"
Sep 20 12:05:31.669: INFO: Waiting up to 5m0s for pod "client-envvars-b87cdb95-71d8-4256-994e-01120ce810e4" in namespace "pods-1097" to be "Succeeded or Failed"
Sep 20 12:05:31.863: INFO: Pod "client-envvars-b87cdb95-71d8-4256-994e-01120ce810e4": Phase="Pending", Reason="", readiness=false. Elapsed: 194.254968ms
Sep 20 12:05:33.868: INFO: Pod "client-envvars-b87cdb95-71d8-4256-994e-01120ce810e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.199656983s
Sep 20 12:05:35.877: INFO: Pod "client-envvars-b87cdb95-71d8-4256-994e-01120ce810e4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.208399071s
Sep 20 12:05:37.868: INFO: Pod "client-envvars-b87cdb95-71d8-4256-994e-01120ce810e4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.198933273s
Sep 20 12:05:39.868: INFO: Pod "client-envvars-b87cdb95-71d8-4256-994e-01120ce810e4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.199585026s
Sep 20 12:05:41.869: INFO: Pod "client-envvars-b87cdb95-71d8-4256-994e-01120ce810e4": Phase="Running", Reason="", readiness=false. Elapsed: 10.200546957s
Sep 20 12:05:44.453: INFO: Pod "client-envvars-b87cdb95-71d8-4256-994e-01120ce810e4": Phase="Running", Reason="", readiness=false. Elapsed: 12.783827816s
Sep 20 12:05:46.082: INFO: Pod "client-envvars-b87cdb95-71d8-4256-994e-01120ce810e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.412901856s
STEP: Saw pod success 09/20/23 12:05:46.082
Sep 20 12:05:46.082: INFO: Pod "client-envvars-b87cdb95-71d8-4256-994e-01120ce810e4" satisfied condition "Succeeded or Failed"
Sep 20 12:05:46.085: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod client-envvars-b87cdb95-71d8-4256-994e-01120ce810e4 container env3cont: <nil>
STEP: delete the pod 09/20/23 12:05:46.154
Sep 20 12:05:46.223: INFO: Waiting for pod client-envvars-b87cdb95-71d8-4256-994e-01120ce810e4 to disappear
Sep 20 12:05:46.226: INFO: Pod client-envvars-b87cdb95-71d8-4256-994e-01120ce810e4 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep 20 12:05:46.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1097" for this suite. 09/20/23 12:05:46.23
------------------------------
â€¢ [SLOW TEST] [45.176 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:05:01.06
    Sep 20 12:05:01.060: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename pods 09/20/23 12:05:01.061
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:05:02.454
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:05:02.457
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Sep 20 12:05:04.164: INFO: Waiting up to 5m0s for pod "server-envvars-9830417c-30de-4d00-862f-4d03eb347317" in namespace "pods-1097" to be "running and ready"
    Sep 20 12:05:06.205: INFO: Pod "server-envvars-9830417c-30de-4d00-862f-4d03eb347317": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040842846s
    Sep 20 12:05:06.205: INFO: The phase of Pod server-envvars-9830417c-30de-4d00-862f-4d03eb347317 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 12:05:08.213: INFO: Pod "server-envvars-9830417c-30de-4d00-862f-4d03eb347317": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049201151s
    Sep 20 12:05:08.213: INFO: The phase of Pod server-envvars-9830417c-30de-4d00-862f-4d03eb347317 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 12:05:10.214: INFO: Pod "server-envvars-9830417c-30de-4d00-862f-4d03eb347317": Phase="Pending", Reason="", readiness=false. Elapsed: 6.050284669s
    Sep 20 12:05:10.214: INFO: The phase of Pod server-envvars-9830417c-30de-4d00-862f-4d03eb347317 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 12:05:12.258: INFO: Pod "server-envvars-9830417c-30de-4d00-862f-4d03eb347317": Phase="Pending", Reason="", readiness=false. Elapsed: 8.09422938s
    Sep 20 12:05:12.259: INFO: The phase of Pod server-envvars-9830417c-30de-4d00-862f-4d03eb347317 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 12:05:14.211: INFO: Pod "server-envvars-9830417c-30de-4d00-862f-4d03eb347317": Phase="Pending", Reason="", readiness=false. Elapsed: 10.047357979s
    Sep 20 12:05:14.212: INFO: The phase of Pod server-envvars-9830417c-30de-4d00-862f-4d03eb347317 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 12:05:16.211: INFO: Pod "server-envvars-9830417c-30de-4d00-862f-4d03eb347317": Phase="Pending", Reason="", readiness=false. Elapsed: 12.047062059s
    Sep 20 12:05:16.212: INFO: The phase of Pod server-envvars-9830417c-30de-4d00-862f-4d03eb347317 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 12:05:18.381: INFO: Pod "server-envvars-9830417c-30de-4d00-862f-4d03eb347317": Phase="Pending", Reason="", readiness=false. Elapsed: 14.217170488s
    Sep 20 12:05:18.381: INFO: The phase of Pod server-envvars-9830417c-30de-4d00-862f-4d03eb347317 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 12:05:21.687: INFO: Pod "server-envvars-9830417c-30de-4d00-862f-4d03eb347317": Phase="Pending", Reason="", readiness=false. Elapsed: 17.523060954s
    Sep 20 12:05:21.687: INFO: The phase of Pod server-envvars-9830417c-30de-4d00-862f-4d03eb347317 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 12:05:22.299: INFO: Pod "server-envvars-9830417c-30de-4d00-862f-4d03eb347317": Phase="Pending", Reason="", readiness=false. Elapsed: 18.134918746s
    Sep 20 12:05:22.299: INFO: The phase of Pod server-envvars-9830417c-30de-4d00-862f-4d03eb347317 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 12:05:24.212: INFO: Pod "server-envvars-9830417c-30de-4d00-862f-4d03eb347317": Phase="Pending", Reason="", readiness=false. Elapsed: 20.047544533s
    Sep 20 12:05:24.212: INFO: The phase of Pod server-envvars-9830417c-30de-4d00-862f-4d03eb347317 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 12:05:26.223: INFO: Pod "server-envvars-9830417c-30de-4d00-862f-4d03eb347317": Phase="Pending", Reason="", readiness=false. Elapsed: 22.05919344s
    Sep 20 12:05:26.223: INFO: The phase of Pod server-envvars-9830417c-30de-4d00-862f-4d03eb347317 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 12:05:28.258: INFO: Pod "server-envvars-9830417c-30de-4d00-862f-4d03eb347317": Phase="Pending", Reason="", readiness=false. Elapsed: 24.093515932s
    Sep 20 12:05:28.258: INFO: The phase of Pod server-envvars-9830417c-30de-4d00-862f-4d03eb347317 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 12:05:30.376: INFO: Pod "server-envvars-9830417c-30de-4d00-862f-4d03eb347317": Phase="Running", Reason="", readiness=true. Elapsed: 26.211702117s
    Sep 20 12:05:30.376: INFO: The phase of Pod server-envvars-9830417c-30de-4d00-862f-4d03eb347317 is Running (Ready = true)
    Sep 20 12:05:30.376: INFO: Pod "server-envvars-9830417c-30de-4d00-862f-4d03eb347317" satisfied condition "running and ready"
    Sep 20 12:05:31.669: INFO: Waiting up to 5m0s for pod "client-envvars-b87cdb95-71d8-4256-994e-01120ce810e4" in namespace "pods-1097" to be "Succeeded or Failed"
    Sep 20 12:05:31.863: INFO: Pod "client-envvars-b87cdb95-71d8-4256-994e-01120ce810e4": Phase="Pending", Reason="", readiness=false. Elapsed: 194.254968ms
    Sep 20 12:05:33.868: INFO: Pod "client-envvars-b87cdb95-71d8-4256-994e-01120ce810e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.199656983s
    Sep 20 12:05:35.877: INFO: Pod "client-envvars-b87cdb95-71d8-4256-994e-01120ce810e4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.208399071s
    Sep 20 12:05:37.868: INFO: Pod "client-envvars-b87cdb95-71d8-4256-994e-01120ce810e4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.198933273s
    Sep 20 12:05:39.868: INFO: Pod "client-envvars-b87cdb95-71d8-4256-994e-01120ce810e4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.199585026s
    Sep 20 12:05:41.869: INFO: Pod "client-envvars-b87cdb95-71d8-4256-994e-01120ce810e4": Phase="Running", Reason="", readiness=false. Elapsed: 10.200546957s
    Sep 20 12:05:44.453: INFO: Pod "client-envvars-b87cdb95-71d8-4256-994e-01120ce810e4": Phase="Running", Reason="", readiness=false. Elapsed: 12.783827816s
    Sep 20 12:05:46.082: INFO: Pod "client-envvars-b87cdb95-71d8-4256-994e-01120ce810e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.412901856s
    STEP: Saw pod success 09/20/23 12:05:46.082
    Sep 20 12:05:46.082: INFO: Pod "client-envvars-b87cdb95-71d8-4256-994e-01120ce810e4" satisfied condition "Succeeded or Failed"
    Sep 20 12:05:46.085: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod client-envvars-b87cdb95-71d8-4256-994e-01120ce810e4 container env3cont: <nil>
    STEP: delete the pod 09/20/23 12:05:46.154
    Sep 20 12:05:46.223: INFO: Waiting for pod client-envvars-b87cdb95-71d8-4256-994e-01120ce810e4 to disappear
    Sep 20 12:05:46.226: INFO: Pod client-envvars-b87cdb95-71d8-4256-994e-01120ce810e4 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:05:46.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1097" for this suite. 09/20/23 12:05:46.23
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:05:46.242
Sep 20 12:05:46.242: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename daemonsets 09/20/23 12:05:46.243
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:05:46.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:05:46.52
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385
Sep 20 12:05:46.595: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 09/20/23 12:05:46.67
Sep 20 12:05:46.674: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:46.674: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:46.674: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:46.881: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 12:05:46.881: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 12:05:48.026: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:48.027: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:48.027: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:48.111: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 12:05:48.111: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 12:05:49.503: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:49.503: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:49.504: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:49.545: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 12:05:49.545: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 12:05:49.949: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:49.949: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:49.949: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:49.954: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 12:05:49.954: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 12:05:51.102: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:51.102: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:51.102: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:51.112: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 12:05:51.112: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 12:05:51.959: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:51.960: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:51.960: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:51.963: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 12:05:51.963: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 12:05:53.020: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:53.020: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:53.021: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:53.028: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 12:05:53.028: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 12:05:54.075: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:54.075: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:54.075: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:54.079: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 12:05:54.079: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 12:05:54.993: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:54.993: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:54.993: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:54.997: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 12:05:54.997: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 12:05:55.886: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:55.886: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:55.886: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:55.890: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 12:05:55.890: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 12:05:56.890: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:56.890: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:56.890: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:56.897: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 12:05:56.897: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 12:05:57.896: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:57.896: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:57.896: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:57.899: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 12:05:57.899: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 12:05:58.888: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:58.888: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:58.888: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:58.892: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep 20 12:05:58.892: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 12:05:59.987: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:59.987: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:05:59.987: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:00.265: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep 20 12:06:00.265: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 12:06:01.649: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:01.650: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:01.650: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:01.654: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep 20 12:06:01.654: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 12:06:02.134: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:02.134: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:02.134: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:02.198: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep 20 12:06:02.199: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 12:06:02.887: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:02.887: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:02.887: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:02.891: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep 20 12:06:02.891: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 12:06:03.888: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:03.888: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:03.888: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:03.892: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep 20 12:06:03.892: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 12:06:05.106: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:05.106: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:05.106: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:05.109: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep 20 12:06:05.109: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 12:06:05.889: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:05.889: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:05.889: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:06.069: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep 20 12:06:06.069: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 12:06:07.388: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:07.388: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:07.388: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:07.395: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep 20 12:06:07.395: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 12:06:07.887: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:07.887: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:07.887: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:07.891: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep 20 12:06:07.891: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 12:06:08.887: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:08.887: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:08.887: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:08.890: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep 20 12:06:08.890: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 12:06:10.060: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:10.060: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:10.060: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:10.345: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep 20 12:06:10.345: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 12:06:10.887: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:10.887: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:10.887: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:10.891: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep 20 12:06:10.891: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 12:06:13.203: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:13.204: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:13.204: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:13.326: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Sep 20 12:06:13.326: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 09/20/23 12:06:13.459
STEP: Check that daemon pods images are updated. 09/20/23 12:06:13.501
Sep 20 12:06:13.513: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:13.513: INFO: Wrong image for pod: daemon-set-xt5vq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:13.513: INFO: Wrong image for pod: daemon-set-zbbtc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:13.518: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:13.518: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:13.518: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:14.523: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:14.523: INFO: Wrong image for pod: daemon-set-zbbtc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:14.528: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:14.528: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:14.528: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:15.598: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:15.598: INFO: Wrong image for pod: daemon-set-zbbtc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:16.157: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:16.157: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:16.157: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:16.582: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:16.582: INFO: Pod daemon-set-dt4p4 is not available
Sep 20 12:06:16.582: INFO: Wrong image for pod: daemon-set-zbbtc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:17.224: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:17.224: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:17.224: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:17.537: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:17.537: INFO: Pod daemon-set-dt4p4 is not available
Sep 20 12:06:17.537: INFO: Wrong image for pod: daemon-set-zbbtc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:17.559: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:17.559: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:17.559: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:18.618: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:18.618: INFO: Pod daemon-set-dt4p4 is not available
Sep 20 12:06:18.618: INFO: Wrong image for pod: daemon-set-zbbtc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:19.175: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:19.175: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:19.175: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:19.530: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:19.530: INFO: Pod daemon-set-dt4p4 is not available
Sep 20 12:06:19.530: INFO: Wrong image for pod: daemon-set-zbbtc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:19.541: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:19.541: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:19.541: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:20.523: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:20.523: INFO: Pod daemon-set-dt4p4 is not available
Sep 20 12:06:20.523: INFO: Wrong image for pod: daemon-set-zbbtc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:20.527: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:20.527: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:20.527: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:21.555: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:21.555: INFO: Pod daemon-set-dt4p4 is not available
Sep 20 12:06:21.555: INFO: Wrong image for pod: daemon-set-zbbtc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:21.560: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:21.560: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:21.560: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:22.523: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:22.523: INFO: Pod daemon-set-dt4p4 is not available
Sep 20 12:06:22.523: INFO: Wrong image for pod: daemon-set-zbbtc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:22.530: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:22.530: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:22.530: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:23.654: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:23.654: INFO: Pod daemon-set-dt4p4 is not available
Sep 20 12:06:23.654: INFO: Wrong image for pod: daemon-set-zbbtc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:23.659: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:23.659: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:23.659: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:24.523: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:24.523: INFO: Pod daemon-set-dt4p4 is not available
Sep 20 12:06:24.523: INFO: Wrong image for pod: daemon-set-zbbtc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:24.528: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:24.528: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:24.528: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:25.522: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:25.522: INFO: Pod daemon-set-dt4p4 is not available
Sep 20 12:06:25.522: INFO: Wrong image for pod: daemon-set-zbbtc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:25.527: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:25.527: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:25.528: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:26.763: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:26.763: INFO: Pod daemon-set-dt4p4 is not available
Sep 20 12:06:26.763: INFO: Wrong image for pod: daemon-set-zbbtc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:26.768: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:26.768: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:26.768: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:27.974: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:27.974: INFO: Pod daemon-set-dt4p4 is not available
Sep 20 12:06:27.974: INFO: Wrong image for pod: daemon-set-zbbtc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:27.982: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:27.982: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:27.983: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:28.525: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:28.525: INFO: Pod daemon-set-dt4p4 is not available
Sep 20 12:06:28.525: INFO: Wrong image for pod: daemon-set-zbbtc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:28.531: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:28.531: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:28.531: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:29.549: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:29.554: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:29.555: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:29.555: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:30.523: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:30.528: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:30.528: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:30.528: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:31.732: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:31.737: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:31.737: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:31.737: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:32.862: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:32.869: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:32.869: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:32.870: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:33.984: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:33.984: INFO: Pod daemon-set-czqtm is not available
Sep 20 12:06:33.990: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:33.991: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:33.993: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:34.523: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:34.523: INFO: Pod daemon-set-czqtm is not available
Sep 20 12:06:34.528: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:34.528: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:34.528: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:35.525: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:35.525: INFO: Pod daemon-set-czqtm is not available
Sep 20 12:06:35.531: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:35.531: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:35.531: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:36.549: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:36.549: INFO: Pod daemon-set-czqtm is not available
Sep 20 12:06:36.555: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:36.555: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:36.555: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:37.524: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:37.524: INFO: Pod daemon-set-czqtm is not available
Sep 20 12:06:37.530: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:37.530: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:37.530: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:38.599: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:38.599: INFO: Pod daemon-set-czqtm is not available
Sep 20 12:06:38.604: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:38.604: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:38.604: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:39.530: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:39.530: INFO: Pod daemon-set-czqtm is not available
Sep 20 12:06:39.535: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:39.535: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:39.535: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:40.522: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:40.522: INFO: Pod daemon-set-czqtm is not available
Sep 20 12:06:40.526: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:40.526: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:40.526: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:41.522: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:41.523: INFO: Pod daemon-set-czqtm is not available
Sep 20 12:06:41.527: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:41.527: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:41.527: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:42.765: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:42.765: INFO: Pod daemon-set-czqtm is not available
Sep 20 12:06:42.919: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:42.919: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:42.919: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:43.521: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:43.522: INFO: Pod daemon-set-czqtm is not available
Sep 20 12:06:43.525: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:43.525: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:43.525: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:44.526: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:44.526: INFO: Pod daemon-set-czqtm is not available
Sep 20 12:06:44.533: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:44.533: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:44.533: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:45.522: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:45.522: INFO: Pod daemon-set-czqtm is not available
Sep 20 12:06:45.528: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:45.528: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:45.528: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:46.972: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:46.972: INFO: Pod daemon-set-czqtm is not available
Sep 20 12:06:46.987: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:46.987: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:46.987: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:47.717: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:47.718: INFO: Pod daemon-set-czqtm is not available
Sep 20 12:06:47.724: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:47.724: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:47.724: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:48.562: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:48.563: INFO: Pod daemon-set-czqtm is not available
Sep 20 12:06:48.567: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:48.567: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:48.567: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:49.523: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:49.523: INFO: Pod daemon-set-czqtm is not available
Sep 20 12:06:49.527: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:49.527: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:49.527: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:50.704: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:50.704: INFO: Pod daemon-set-czqtm is not available
Sep 20 12:06:50.708: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:50.708: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:50.708: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:51.585: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:51.585: INFO: Pod daemon-set-czqtm is not available
Sep 20 12:06:51.589: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:51.589: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:51.589: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:52.523: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:52.523: INFO: Pod daemon-set-czqtm is not available
Sep 20 12:06:52.529: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:52.529: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:52.529: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:53.784: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:53.784: INFO: Pod daemon-set-czqtm is not available
Sep 20 12:06:53.791: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:53.791: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:53.791: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:54.838: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:54.838: INFO: Pod daemon-set-czqtm is not available
Sep 20 12:06:54.843: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:54.843: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:54.843: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:55.724: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep 20 12:06:55.724: INFO: Pod daemon-set-czqtm is not available
Sep 20 12:06:55.776: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:55.776: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:55.776: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:56.790: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:56.790: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:56.790: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:57.950: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:57.950: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:57.950: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:58.524: INFO: Pod daemon-set-b9gz9 is not available
Sep 20 12:06:58.529: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:58.529: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:58.529: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 09/20/23 12:06:58.529
Sep 20 12:06:58.533: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:58.534: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:58.534: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:58.538: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep 20 12:06:58.538: INFO: Node mycluster-ww3cg64etuwi-node-2 is running 0 daemon pod, expected 1
Sep 20 12:06:59.546: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:59.547: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:59.547: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:06:59.551: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep 20 12:06:59.551: INFO: Node mycluster-ww3cg64etuwi-node-2 is running 0 daemon pod, expected 1
Sep 20 12:07:00.672: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:07:00.672: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:07:00.672: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:07:00.714: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep 20 12:07:00.714: INFO: Node mycluster-ww3cg64etuwi-node-2 is running 0 daemon pod, expected 1
Sep 20 12:07:01.889: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:07:01.889: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:07:01.889: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:07:02.040: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Sep 20 12:07:02.040: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 09/20/23 12:07:02.058
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5316, will wait for the garbage collector to delete the pods 09/20/23 12:07:02.058
Sep 20 12:07:02.154: INFO: Deleting DaemonSet.extensions daemon-set took: 43.13443ms
Sep 20 12:07:02.756: INFO: Terminating DaemonSet.extensions daemon-set pods took: 601.290066ms
Sep 20 12:07:05.751: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 12:07:05.751: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep 20 12:07:05.757: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5061"},"items":null}

Sep 20 12:07:05.761: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5061"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep 20 12:07:05.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5316" for this suite. 09/20/23 12:07:05.783
------------------------------
â€¢ [SLOW TEST] [79.577 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:05:46.242
    Sep 20 12:05:46.242: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename daemonsets 09/20/23 12:05:46.243
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:05:46.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:05:46.52
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:385
    Sep 20 12:05:46.595: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 09/20/23 12:05:46.67
    Sep 20 12:05:46.674: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:46.674: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:46.674: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:46.881: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 12:05:46.881: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 12:05:48.026: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:48.027: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:48.027: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:48.111: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 12:05:48.111: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 12:05:49.503: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:49.503: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:49.504: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:49.545: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 12:05:49.545: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 12:05:49.949: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:49.949: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:49.949: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:49.954: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 12:05:49.954: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 12:05:51.102: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:51.102: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:51.102: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:51.112: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 12:05:51.112: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 12:05:51.959: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:51.960: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:51.960: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:51.963: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 12:05:51.963: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 12:05:53.020: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:53.020: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:53.021: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:53.028: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 12:05:53.028: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 12:05:54.075: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:54.075: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:54.075: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:54.079: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 12:05:54.079: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 12:05:54.993: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:54.993: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:54.993: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:54.997: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 12:05:54.997: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 12:05:55.886: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:55.886: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:55.886: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:55.890: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 12:05:55.890: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 12:05:56.890: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:56.890: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:56.890: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:56.897: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 12:05:56.897: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 12:05:57.896: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:57.896: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:57.896: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:57.899: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 12:05:57.899: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 12:05:58.888: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:58.888: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:58.888: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:58.892: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep 20 12:05:58.892: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 12:05:59.987: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:59.987: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:05:59.987: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:00.265: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep 20 12:06:00.265: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 12:06:01.649: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:01.650: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:01.650: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:01.654: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep 20 12:06:01.654: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 12:06:02.134: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:02.134: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:02.134: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:02.198: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep 20 12:06:02.199: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 12:06:02.887: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:02.887: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:02.887: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:02.891: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep 20 12:06:02.891: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 12:06:03.888: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:03.888: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:03.888: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:03.892: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep 20 12:06:03.892: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 12:06:05.106: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:05.106: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:05.106: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:05.109: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep 20 12:06:05.109: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 12:06:05.889: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:05.889: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:05.889: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:06.069: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep 20 12:06:06.069: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 12:06:07.388: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:07.388: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:07.388: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:07.395: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep 20 12:06:07.395: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 12:06:07.887: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:07.887: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:07.887: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:07.891: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep 20 12:06:07.891: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 12:06:08.887: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:08.887: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:08.887: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:08.890: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep 20 12:06:08.890: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 12:06:10.060: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:10.060: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:10.060: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:10.345: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep 20 12:06:10.345: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 12:06:10.887: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:10.887: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:10.887: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:10.891: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep 20 12:06:10.891: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 12:06:13.203: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:13.204: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:13.204: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:13.326: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Sep 20 12:06:13.326: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 09/20/23 12:06:13.459
    STEP: Check that daemon pods images are updated. 09/20/23 12:06:13.501
    Sep 20 12:06:13.513: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:13.513: INFO: Wrong image for pod: daemon-set-xt5vq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:13.513: INFO: Wrong image for pod: daemon-set-zbbtc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:13.518: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:13.518: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:13.518: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:14.523: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:14.523: INFO: Wrong image for pod: daemon-set-zbbtc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:14.528: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:14.528: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:14.528: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:15.598: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:15.598: INFO: Wrong image for pod: daemon-set-zbbtc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:16.157: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:16.157: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:16.157: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:16.582: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:16.582: INFO: Pod daemon-set-dt4p4 is not available
    Sep 20 12:06:16.582: INFO: Wrong image for pod: daemon-set-zbbtc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:17.224: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:17.224: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:17.224: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:17.537: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:17.537: INFO: Pod daemon-set-dt4p4 is not available
    Sep 20 12:06:17.537: INFO: Wrong image for pod: daemon-set-zbbtc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:17.559: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:17.559: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:17.559: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:18.618: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:18.618: INFO: Pod daemon-set-dt4p4 is not available
    Sep 20 12:06:18.618: INFO: Wrong image for pod: daemon-set-zbbtc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:19.175: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:19.175: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:19.175: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:19.530: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:19.530: INFO: Pod daemon-set-dt4p4 is not available
    Sep 20 12:06:19.530: INFO: Wrong image for pod: daemon-set-zbbtc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:19.541: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:19.541: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:19.541: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:20.523: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:20.523: INFO: Pod daemon-set-dt4p4 is not available
    Sep 20 12:06:20.523: INFO: Wrong image for pod: daemon-set-zbbtc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:20.527: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:20.527: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:20.527: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:21.555: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:21.555: INFO: Pod daemon-set-dt4p4 is not available
    Sep 20 12:06:21.555: INFO: Wrong image for pod: daemon-set-zbbtc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:21.560: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:21.560: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:21.560: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:22.523: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:22.523: INFO: Pod daemon-set-dt4p4 is not available
    Sep 20 12:06:22.523: INFO: Wrong image for pod: daemon-set-zbbtc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:22.530: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:22.530: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:22.530: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:23.654: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:23.654: INFO: Pod daemon-set-dt4p4 is not available
    Sep 20 12:06:23.654: INFO: Wrong image for pod: daemon-set-zbbtc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:23.659: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:23.659: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:23.659: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:24.523: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:24.523: INFO: Pod daemon-set-dt4p4 is not available
    Sep 20 12:06:24.523: INFO: Wrong image for pod: daemon-set-zbbtc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:24.528: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:24.528: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:24.528: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:25.522: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:25.522: INFO: Pod daemon-set-dt4p4 is not available
    Sep 20 12:06:25.522: INFO: Wrong image for pod: daemon-set-zbbtc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:25.527: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:25.527: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:25.528: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:26.763: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:26.763: INFO: Pod daemon-set-dt4p4 is not available
    Sep 20 12:06:26.763: INFO: Wrong image for pod: daemon-set-zbbtc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:26.768: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:26.768: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:26.768: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:27.974: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:27.974: INFO: Pod daemon-set-dt4p4 is not available
    Sep 20 12:06:27.974: INFO: Wrong image for pod: daemon-set-zbbtc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:27.982: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:27.982: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:27.983: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:28.525: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:28.525: INFO: Pod daemon-set-dt4p4 is not available
    Sep 20 12:06:28.525: INFO: Wrong image for pod: daemon-set-zbbtc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:28.531: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:28.531: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:28.531: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:29.549: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:29.554: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:29.555: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:29.555: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:30.523: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:30.528: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:30.528: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:30.528: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:31.732: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:31.737: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:31.737: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:31.737: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:32.862: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:32.869: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:32.869: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:32.870: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:33.984: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:33.984: INFO: Pod daemon-set-czqtm is not available
    Sep 20 12:06:33.990: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:33.991: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:33.993: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:34.523: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:34.523: INFO: Pod daemon-set-czqtm is not available
    Sep 20 12:06:34.528: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:34.528: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:34.528: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:35.525: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:35.525: INFO: Pod daemon-set-czqtm is not available
    Sep 20 12:06:35.531: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:35.531: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:35.531: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:36.549: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:36.549: INFO: Pod daemon-set-czqtm is not available
    Sep 20 12:06:36.555: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:36.555: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:36.555: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:37.524: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:37.524: INFO: Pod daemon-set-czqtm is not available
    Sep 20 12:06:37.530: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:37.530: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:37.530: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:38.599: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:38.599: INFO: Pod daemon-set-czqtm is not available
    Sep 20 12:06:38.604: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:38.604: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:38.604: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:39.530: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:39.530: INFO: Pod daemon-set-czqtm is not available
    Sep 20 12:06:39.535: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:39.535: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:39.535: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:40.522: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:40.522: INFO: Pod daemon-set-czqtm is not available
    Sep 20 12:06:40.526: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:40.526: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:40.526: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:41.522: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:41.523: INFO: Pod daemon-set-czqtm is not available
    Sep 20 12:06:41.527: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:41.527: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:41.527: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:42.765: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:42.765: INFO: Pod daemon-set-czqtm is not available
    Sep 20 12:06:42.919: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:42.919: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:42.919: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:43.521: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:43.522: INFO: Pod daemon-set-czqtm is not available
    Sep 20 12:06:43.525: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:43.525: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:43.525: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:44.526: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:44.526: INFO: Pod daemon-set-czqtm is not available
    Sep 20 12:06:44.533: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:44.533: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:44.533: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:45.522: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:45.522: INFO: Pod daemon-set-czqtm is not available
    Sep 20 12:06:45.528: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:45.528: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:45.528: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:46.972: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:46.972: INFO: Pod daemon-set-czqtm is not available
    Sep 20 12:06:46.987: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:46.987: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:46.987: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:47.717: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:47.718: INFO: Pod daemon-set-czqtm is not available
    Sep 20 12:06:47.724: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:47.724: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:47.724: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:48.562: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:48.563: INFO: Pod daemon-set-czqtm is not available
    Sep 20 12:06:48.567: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:48.567: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:48.567: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:49.523: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:49.523: INFO: Pod daemon-set-czqtm is not available
    Sep 20 12:06:49.527: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:49.527: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:49.527: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:50.704: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:50.704: INFO: Pod daemon-set-czqtm is not available
    Sep 20 12:06:50.708: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:50.708: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:50.708: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:51.585: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:51.585: INFO: Pod daemon-set-czqtm is not available
    Sep 20 12:06:51.589: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:51.589: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:51.589: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:52.523: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:52.523: INFO: Pod daemon-set-czqtm is not available
    Sep 20 12:06:52.529: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:52.529: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:52.529: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:53.784: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:53.784: INFO: Pod daemon-set-czqtm is not available
    Sep 20 12:06:53.791: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:53.791: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:53.791: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:54.838: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:54.838: INFO: Pod daemon-set-czqtm is not available
    Sep 20 12:06:54.843: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:54.843: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:54.843: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:55.724: INFO: Wrong image for pod: daemon-set-5jqkq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep 20 12:06:55.724: INFO: Pod daemon-set-czqtm is not available
    Sep 20 12:06:55.776: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:55.776: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:55.776: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:56.790: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:56.790: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:56.790: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:57.950: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:57.950: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:57.950: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:58.524: INFO: Pod daemon-set-b9gz9 is not available
    Sep 20 12:06:58.529: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:58.529: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:58.529: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 09/20/23 12:06:58.529
    Sep 20 12:06:58.533: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:58.534: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:58.534: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:58.538: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep 20 12:06:58.538: INFO: Node mycluster-ww3cg64etuwi-node-2 is running 0 daemon pod, expected 1
    Sep 20 12:06:59.546: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:59.547: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:59.547: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:06:59.551: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep 20 12:06:59.551: INFO: Node mycluster-ww3cg64etuwi-node-2 is running 0 daemon pod, expected 1
    Sep 20 12:07:00.672: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:07:00.672: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:07:00.672: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:07:00.714: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep 20 12:07:00.714: INFO: Node mycluster-ww3cg64etuwi-node-2 is running 0 daemon pod, expected 1
    Sep 20 12:07:01.889: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:07:01.889: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:07:01.889: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:07:02.040: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Sep 20 12:07:02.040: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 09/20/23 12:07:02.058
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5316, will wait for the garbage collector to delete the pods 09/20/23 12:07:02.058
    Sep 20 12:07:02.154: INFO: Deleting DaemonSet.extensions daemon-set took: 43.13443ms
    Sep 20 12:07:02.756: INFO: Terminating DaemonSet.extensions daemon-set pods took: 601.290066ms
    Sep 20 12:07:05.751: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 12:07:05.751: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep 20 12:07:05.757: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5061"},"items":null}

    Sep 20 12:07:05.761: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5061"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:07:05.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5316" for this suite. 09/20/23 12:07:05.783
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:07:05.82
Sep 20 12:07:05.820: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename container-runtime 09/20/23 12:07:05.821
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:07:06.204
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:07:06.208
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 09/20/23 12:07:06.214
STEP: wait for the container to reach Succeeded 09/20/23 12:07:06.551
STEP: get the container status 09/20/23 12:07:23.073
STEP: the container should be terminated 09/20/23 12:07:23.077
STEP: the termination message should be set 09/20/23 12:07:23.078
Sep 20 12:07:23.078: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 09/20/23 12:07:23.078
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Sep 20 12:07:23.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-9633" for this suite. 09/20/23 12:07:23.388
------------------------------
â€¢ [SLOW TEST] [17.707 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:07:05.82
    Sep 20 12:07:05.820: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename container-runtime 09/20/23 12:07:05.821
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:07:06.204
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:07:06.208
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 09/20/23 12:07:06.214
    STEP: wait for the container to reach Succeeded 09/20/23 12:07:06.551
    STEP: get the container status 09/20/23 12:07:23.073
    STEP: the container should be terminated 09/20/23 12:07:23.077
    STEP: the termination message should be set 09/20/23 12:07:23.078
    Sep 20 12:07:23.078: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 09/20/23 12:07:23.078
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:07:23.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-9633" for this suite. 09/20/23 12:07:23.388
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:07:23.53
Sep 20 12:07:23.531: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename containers 09/20/23 12:07:23.531
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:07:23.909
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:07:23.913
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Sep 20 12:07:24.045: INFO: Waiting up to 5m0s for pod "client-containers-6829b9b6-83dd-4a04-9385-ad9d21c6766a" in namespace "containers-7439" to be "running"
Sep 20 12:07:24.051: INFO: Pod "client-containers-6829b9b6-83dd-4a04-9385-ad9d21c6766a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.105091ms
Sep 20 12:07:26.057: INFO: Pod "client-containers-6829b9b6-83dd-4a04-9385-ad9d21c6766a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011304067s
Sep 20 12:07:28.352: INFO: Pod "client-containers-6829b9b6-83dd-4a04-9385-ad9d21c6766a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.307114258s
Sep 20 12:07:30.326: INFO: Pod "client-containers-6829b9b6-83dd-4a04-9385-ad9d21c6766a": Phase="Running", Reason="", readiness=true. Elapsed: 6.281006707s
Sep 20 12:07:30.326: INFO: Pod "client-containers-6829b9b6-83dd-4a04-9385-ad9d21c6766a" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Sep 20 12:07:30.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-7439" for this suite. 09/20/23 12:07:31.392
------------------------------
â€¢ [SLOW TEST] [8.107 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:07:23.53
    Sep 20 12:07:23.531: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename containers 09/20/23 12:07:23.531
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:07:23.909
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:07:23.913
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Sep 20 12:07:24.045: INFO: Waiting up to 5m0s for pod "client-containers-6829b9b6-83dd-4a04-9385-ad9d21c6766a" in namespace "containers-7439" to be "running"
    Sep 20 12:07:24.051: INFO: Pod "client-containers-6829b9b6-83dd-4a04-9385-ad9d21c6766a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.105091ms
    Sep 20 12:07:26.057: INFO: Pod "client-containers-6829b9b6-83dd-4a04-9385-ad9d21c6766a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011304067s
    Sep 20 12:07:28.352: INFO: Pod "client-containers-6829b9b6-83dd-4a04-9385-ad9d21c6766a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.307114258s
    Sep 20 12:07:30.326: INFO: Pod "client-containers-6829b9b6-83dd-4a04-9385-ad9d21c6766a": Phase="Running", Reason="", readiness=true. Elapsed: 6.281006707s
    Sep 20 12:07:30.326: INFO: Pod "client-containers-6829b9b6-83dd-4a04-9385-ad9d21c6766a" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:07:30.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-7439" for this suite. 09/20/23 12:07:31.392
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:07:31.638
Sep 20 12:07:31.639: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename resourcequota 09/20/23 12:07:31.639
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:07:31.666
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:07:31.672
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 09/20/23 12:07:31.73
STEP: Getting a ResourceQuota 09/20/23 12:07:31.748
STEP: Listing all ResourceQuotas with LabelSelector 09/20/23 12:07:31.754
STEP: Patching the ResourceQuota 09/20/23 12:07:31.763
STEP: Deleting a Collection of ResourceQuotas 09/20/23 12:07:32.023
STEP: Verifying the deleted ResourceQuota 09/20/23 12:07:32.468
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep 20 12:07:32.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5195" for this suite. 09/20/23 12:07:32.479
------------------------------
â€¢ [0.943 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:07:31.638
    Sep 20 12:07:31.639: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename resourcequota 09/20/23 12:07:31.639
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:07:31.666
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:07:31.672
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 09/20/23 12:07:31.73
    STEP: Getting a ResourceQuota 09/20/23 12:07:31.748
    STEP: Listing all ResourceQuotas with LabelSelector 09/20/23 12:07:31.754
    STEP: Patching the ResourceQuota 09/20/23 12:07:31.763
    STEP: Deleting a Collection of ResourceQuotas 09/20/23 12:07:32.023
    STEP: Verifying the deleted ResourceQuota 09/20/23 12:07:32.468
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:07:32.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5195" for this suite. 09/20/23 12:07:32.479
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:07:32.583
Sep 20 12:07:32.583: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename projected 09/20/23 12:07:32.583
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:07:33.012
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:07:33.024
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-3183af68-330a-4225-b039-e8533ae5bd5e 09/20/23 12:07:33.039
STEP: Creating a pod to test consume secrets 09/20/23 12:07:33.092
Sep 20 12:07:33.190: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5201ebb0-8cc3-49b2-9766-922886fab194" in namespace "projected-8248" to be "Succeeded or Failed"
Sep 20 12:07:33.198: INFO: Pod "pod-projected-secrets-5201ebb0-8cc3-49b2-9766-922886fab194": Phase="Pending", Reason="", readiness=false. Elapsed: 7.815525ms
Sep 20 12:07:35.258: INFO: Pod "pod-projected-secrets-5201ebb0-8cc3-49b2-9766-922886fab194": Phase="Pending", Reason="", readiness=false. Elapsed: 2.067797457s
Sep 20 12:07:37.203: INFO: Pod "pod-projected-secrets-5201ebb0-8cc3-49b2-9766-922886fab194": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012922099s
Sep 20 12:07:39.208: INFO: Pod "pod-projected-secrets-5201ebb0-8cc3-49b2-9766-922886fab194": Phase="Running", Reason="", readiness=false. Elapsed: 6.018022149s
Sep 20 12:07:41.402: INFO: Pod "pod-projected-secrets-5201ebb0-8cc3-49b2-9766-922886fab194": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.212438203s
STEP: Saw pod success 09/20/23 12:07:41.402
Sep 20 12:07:41.402: INFO: Pod "pod-projected-secrets-5201ebb0-8cc3-49b2-9766-922886fab194" satisfied condition "Succeeded or Failed"
Sep 20 12:07:41.408: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-2 pod pod-projected-secrets-5201ebb0-8cc3-49b2-9766-922886fab194 container projected-secret-volume-test: <nil>
STEP: delete the pod 09/20/23 12:07:41.485
Sep 20 12:07:42.180: INFO: Waiting for pod pod-projected-secrets-5201ebb0-8cc3-49b2-9766-922886fab194 to disappear
Sep 20 12:07:42.187: INFO: Pod pod-projected-secrets-5201ebb0-8cc3-49b2-9766-922886fab194 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep 20 12:07:42.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8248" for this suite. 09/20/23 12:07:42.198
------------------------------
â€¢ [SLOW TEST] [10.032 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:07:32.583
    Sep 20 12:07:32.583: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename projected 09/20/23 12:07:32.583
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:07:33.012
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:07:33.024
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-3183af68-330a-4225-b039-e8533ae5bd5e 09/20/23 12:07:33.039
    STEP: Creating a pod to test consume secrets 09/20/23 12:07:33.092
    Sep 20 12:07:33.190: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5201ebb0-8cc3-49b2-9766-922886fab194" in namespace "projected-8248" to be "Succeeded or Failed"
    Sep 20 12:07:33.198: INFO: Pod "pod-projected-secrets-5201ebb0-8cc3-49b2-9766-922886fab194": Phase="Pending", Reason="", readiness=false. Elapsed: 7.815525ms
    Sep 20 12:07:35.258: INFO: Pod "pod-projected-secrets-5201ebb0-8cc3-49b2-9766-922886fab194": Phase="Pending", Reason="", readiness=false. Elapsed: 2.067797457s
    Sep 20 12:07:37.203: INFO: Pod "pod-projected-secrets-5201ebb0-8cc3-49b2-9766-922886fab194": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012922099s
    Sep 20 12:07:39.208: INFO: Pod "pod-projected-secrets-5201ebb0-8cc3-49b2-9766-922886fab194": Phase="Running", Reason="", readiness=false. Elapsed: 6.018022149s
    Sep 20 12:07:41.402: INFO: Pod "pod-projected-secrets-5201ebb0-8cc3-49b2-9766-922886fab194": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.212438203s
    STEP: Saw pod success 09/20/23 12:07:41.402
    Sep 20 12:07:41.402: INFO: Pod "pod-projected-secrets-5201ebb0-8cc3-49b2-9766-922886fab194" satisfied condition "Succeeded or Failed"
    Sep 20 12:07:41.408: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-2 pod pod-projected-secrets-5201ebb0-8cc3-49b2-9766-922886fab194 container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/20/23 12:07:41.485
    Sep 20 12:07:42.180: INFO: Waiting for pod pod-projected-secrets-5201ebb0-8cc3-49b2-9766-922886fab194 to disappear
    Sep 20 12:07:42.187: INFO: Pod pod-projected-secrets-5201ebb0-8cc3-49b2-9766-922886fab194 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:07:42.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8248" for this suite. 09/20/23 12:07:42.198
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:07:42.615
Sep 20 12:07:42.615: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename security-context-test 09/20/23 12:07:42.616
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:07:43.947
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:07:43.958
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Sep 20 12:07:44.430: INFO: Waiting up to 5m0s for pod "busybox-user-65534-9e22a02e-a964-4e77-b488-b912b67d8afe" in namespace "security-context-test-2149" to be "Succeeded or Failed"
Sep 20 12:07:44.638: INFO: Pod "busybox-user-65534-9e22a02e-a964-4e77-b488-b912b67d8afe": Phase="Pending", Reason="", readiness=false. Elapsed: 208.433139ms
Sep 20 12:07:46.645: INFO: Pod "busybox-user-65534-9e22a02e-a964-4e77-b488-b912b67d8afe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.214755501s
Sep 20 12:07:48.713: INFO: Pod "busybox-user-65534-9e22a02e-a964-4e77-b488-b912b67d8afe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.283033401s
Sep 20 12:07:50.644: INFO: Pod "busybox-user-65534-9e22a02e-a964-4e77-b488-b912b67d8afe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.213724142s
Sep 20 12:07:52.648: INFO: Pod "busybox-user-65534-9e22a02e-a964-4e77-b488-b912b67d8afe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.218073107s
Sep 20 12:07:52.648: INFO: Pod "busybox-user-65534-9e22a02e-a964-4e77-b488-b912b67d8afe" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep 20 12:07:52.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-2149" for this suite. 09/20/23 12:07:52.656
------------------------------
â€¢ [SLOW TEST] [10.083 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:07:42.615
    Sep 20 12:07:42.615: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename security-context-test 09/20/23 12:07:42.616
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:07:43.947
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:07:43.958
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Sep 20 12:07:44.430: INFO: Waiting up to 5m0s for pod "busybox-user-65534-9e22a02e-a964-4e77-b488-b912b67d8afe" in namespace "security-context-test-2149" to be "Succeeded or Failed"
    Sep 20 12:07:44.638: INFO: Pod "busybox-user-65534-9e22a02e-a964-4e77-b488-b912b67d8afe": Phase="Pending", Reason="", readiness=false. Elapsed: 208.433139ms
    Sep 20 12:07:46.645: INFO: Pod "busybox-user-65534-9e22a02e-a964-4e77-b488-b912b67d8afe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.214755501s
    Sep 20 12:07:48.713: INFO: Pod "busybox-user-65534-9e22a02e-a964-4e77-b488-b912b67d8afe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.283033401s
    Sep 20 12:07:50.644: INFO: Pod "busybox-user-65534-9e22a02e-a964-4e77-b488-b912b67d8afe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.213724142s
    Sep 20 12:07:52.648: INFO: Pod "busybox-user-65534-9e22a02e-a964-4e77-b488-b912b67d8afe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.218073107s
    Sep 20 12:07:52.648: INFO: Pod "busybox-user-65534-9e22a02e-a964-4e77-b488-b912b67d8afe" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:07:52.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-2149" for this suite. 09/20/23 12:07:52.656
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:07:52.702
Sep 20 12:07:52.702: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename tables 09/20/23 12:07:52.703
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:07:52.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:07:52.909
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Sep 20 12:07:52.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-3774" for this suite. 09/20/23 12:07:52.972
------------------------------
â€¢ [0.349 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:07:52.702
    Sep 20 12:07:52.702: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename tables 09/20/23 12:07:52.703
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:07:52.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:07:52.909
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:07:52.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-3774" for this suite. 09/20/23 12:07:52.972
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:07:53.056
Sep 20 12:07:53.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename webhook 09/20/23 12:07:53.057
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:07:53.622
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:07:53.627
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/20/23 12:07:53.652
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/20/23 12:07:54.085
STEP: Deploying the webhook pod 09/20/23 12:07:54.131
STEP: Wait for the deployment to be ready 09/20/23 12:07:54.167
Sep 20 12:07:54.193: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 12:07:56.209: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 7, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 7, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 7, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 7, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 12:07:58.297: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 7, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 7, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 7, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 7, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/20/23 12:08:00.281
STEP: Verifying the service has paired with the endpoint 09/20/23 12:08:00.502
Sep 20 12:08:01.503: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Sep 20 12:08:01.510: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1349-crds.webhook.example.com via the AdmissionRegistration API 09/20/23 12:08:02.274
STEP: Creating a custom resource while v1 is storage version 09/20/23 12:08:02.402
STEP: Patching Custom Resource Definition to set v2 as storage 09/20/23 12:08:04.514
STEP: Patching the custom resource while v2 is storage version 09/20/23 12:08:04.559
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep 20 12:08:05.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6351" for this suite. 09/20/23 12:08:05.864
STEP: Destroying namespace "webhook-6351-markers" for this suite. 09/20/23 12:08:05.886
------------------------------
â€¢ [SLOW TEST] [12.877 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:07:53.056
    Sep 20 12:07:53.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename webhook 09/20/23 12:07:53.057
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:07:53.622
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:07:53.627
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/20/23 12:07:53.652
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/20/23 12:07:54.085
    STEP: Deploying the webhook pod 09/20/23 12:07:54.131
    STEP: Wait for the deployment to be ready 09/20/23 12:07:54.167
    Sep 20 12:07:54.193: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Sep 20 12:07:56.209: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 7, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 7, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 7, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 7, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 12:07:58.297: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 7, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 7, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 7, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 7, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/20/23 12:08:00.281
    STEP: Verifying the service has paired with the endpoint 09/20/23 12:08:00.502
    Sep 20 12:08:01.503: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Sep 20 12:08:01.510: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1349-crds.webhook.example.com via the AdmissionRegistration API 09/20/23 12:08:02.274
    STEP: Creating a custom resource while v1 is storage version 09/20/23 12:08:02.402
    STEP: Patching Custom Resource Definition to set v2 as storage 09/20/23 12:08:04.514
    STEP: Patching the custom resource while v2 is storage version 09/20/23 12:08:04.559
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:08:05.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6351" for this suite. 09/20/23 12:08:05.864
    STEP: Destroying namespace "webhook-6351-markers" for this suite. 09/20/23 12:08:05.886
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:08:05.937
Sep 20 12:08:05.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename container-probe 09/20/23 12:08:05.937
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:08:06.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:08:06.209
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-15040cc8-ff63-4ded-9840-2e4812397fbf in namespace container-probe-5824 09/20/23 12:08:06.214
Sep 20 12:08:06.337: INFO: Waiting up to 5m0s for pod "busybox-15040cc8-ff63-4ded-9840-2e4812397fbf" in namespace "container-probe-5824" to be "not pending"
Sep 20 12:08:06.737: INFO: Pod "busybox-15040cc8-ff63-4ded-9840-2e4812397fbf": Phase="Pending", Reason="", readiness=false. Elapsed: 399.626308ms
Sep 20 12:08:08.742: INFO: Pod "busybox-15040cc8-ff63-4ded-9840-2e4812397fbf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.404668128s
Sep 20 12:08:10.741: INFO: Pod "busybox-15040cc8-ff63-4ded-9840-2e4812397fbf": Phase="Running", Reason="", readiness=true. Elapsed: 4.403771723s
Sep 20 12:08:10.741: INFO: Pod "busybox-15040cc8-ff63-4ded-9840-2e4812397fbf" satisfied condition "not pending"
Sep 20 12:08:10.741: INFO: Started pod busybox-15040cc8-ff63-4ded-9840-2e4812397fbf in namespace container-probe-5824
STEP: checking the pod's current state and verifying that restartCount is present 09/20/23 12:08:10.741
Sep 20 12:08:10.745: INFO: Initial restart count of pod busybox-15040cc8-ff63-4ded-9840-2e4812397fbf is 0
STEP: deleting the pod 09/20/23 12:12:11.737
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep 20 12:12:11.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-5824" for this suite. 09/20/23 12:12:11.901
------------------------------
â€¢ [SLOW TEST] [245.987 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:08:05.937
    Sep 20 12:08:05.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename container-probe 09/20/23 12:08:05.937
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:08:06.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:08:06.209
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-15040cc8-ff63-4ded-9840-2e4812397fbf in namespace container-probe-5824 09/20/23 12:08:06.214
    Sep 20 12:08:06.337: INFO: Waiting up to 5m0s for pod "busybox-15040cc8-ff63-4ded-9840-2e4812397fbf" in namespace "container-probe-5824" to be "not pending"
    Sep 20 12:08:06.737: INFO: Pod "busybox-15040cc8-ff63-4ded-9840-2e4812397fbf": Phase="Pending", Reason="", readiness=false. Elapsed: 399.626308ms
    Sep 20 12:08:08.742: INFO: Pod "busybox-15040cc8-ff63-4ded-9840-2e4812397fbf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.404668128s
    Sep 20 12:08:10.741: INFO: Pod "busybox-15040cc8-ff63-4ded-9840-2e4812397fbf": Phase="Running", Reason="", readiness=true. Elapsed: 4.403771723s
    Sep 20 12:08:10.741: INFO: Pod "busybox-15040cc8-ff63-4ded-9840-2e4812397fbf" satisfied condition "not pending"
    Sep 20 12:08:10.741: INFO: Started pod busybox-15040cc8-ff63-4ded-9840-2e4812397fbf in namespace container-probe-5824
    STEP: checking the pod's current state and verifying that restartCount is present 09/20/23 12:08:10.741
    Sep 20 12:08:10.745: INFO: Initial restart count of pod busybox-15040cc8-ff63-4ded-9840-2e4812397fbf is 0
    STEP: deleting the pod 09/20/23 12:12:11.737
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:12:11.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-5824" for this suite. 09/20/23 12:12:11.901
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:12:11.924
Sep 20 12:12:11.924: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename kubelet-test 09/20/23 12:12:11.925
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:12:11.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:12:11.961
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Sep 20 12:12:11.978: INFO: Waiting up to 5m0s for pod "busybox-scheduling-1fcc3c78-ef7b-448d-8d38-6b0ac56395c1" in namespace "kubelet-test-1101" to be "running and ready"
Sep 20 12:12:11.991: INFO: Pod "busybox-scheduling-1fcc3c78-ef7b-448d-8d38-6b0ac56395c1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.502878ms
Sep 20 12:12:11.991: INFO: The phase of Pod busybox-scheduling-1fcc3c78-ef7b-448d-8d38-6b0ac56395c1 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 12:12:13.996: INFO: Pod "busybox-scheduling-1fcc3c78-ef7b-448d-8d38-6b0ac56395c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018463745s
Sep 20 12:12:13.996: INFO: The phase of Pod busybox-scheduling-1fcc3c78-ef7b-448d-8d38-6b0ac56395c1 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 12:12:16.009: INFO: Pod "busybox-scheduling-1fcc3c78-ef7b-448d-8d38-6b0ac56395c1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031042212s
Sep 20 12:12:16.009: INFO: The phase of Pod busybox-scheduling-1fcc3c78-ef7b-448d-8d38-6b0ac56395c1 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 12:12:18.480: INFO: Pod "busybox-scheduling-1fcc3c78-ef7b-448d-8d38-6b0ac56395c1": Phase="Running", Reason="", readiness=true. Elapsed: 6.501856139s
Sep 20 12:12:18.480: INFO: The phase of Pod busybox-scheduling-1fcc3c78-ef7b-448d-8d38-6b0ac56395c1 is Running (Ready = true)
Sep 20 12:12:18.480: INFO: Pod "busybox-scheduling-1fcc3c78-ef7b-448d-8d38-6b0ac56395c1" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Sep 20 12:12:18.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-1101" for this suite. 09/20/23 12:12:18.837
------------------------------
â€¢ [SLOW TEST] [6.923 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:12:11.924
    Sep 20 12:12:11.924: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename kubelet-test 09/20/23 12:12:11.925
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:12:11.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:12:11.961
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Sep 20 12:12:11.978: INFO: Waiting up to 5m0s for pod "busybox-scheduling-1fcc3c78-ef7b-448d-8d38-6b0ac56395c1" in namespace "kubelet-test-1101" to be "running and ready"
    Sep 20 12:12:11.991: INFO: Pod "busybox-scheduling-1fcc3c78-ef7b-448d-8d38-6b0ac56395c1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.502878ms
    Sep 20 12:12:11.991: INFO: The phase of Pod busybox-scheduling-1fcc3c78-ef7b-448d-8d38-6b0ac56395c1 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 12:12:13.996: INFO: Pod "busybox-scheduling-1fcc3c78-ef7b-448d-8d38-6b0ac56395c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018463745s
    Sep 20 12:12:13.996: INFO: The phase of Pod busybox-scheduling-1fcc3c78-ef7b-448d-8d38-6b0ac56395c1 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 12:12:16.009: INFO: Pod "busybox-scheduling-1fcc3c78-ef7b-448d-8d38-6b0ac56395c1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031042212s
    Sep 20 12:12:16.009: INFO: The phase of Pod busybox-scheduling-1fcc3c78-ef7b-448d-8d38-6b0ac56395c1 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 12:12:18.480: INFO: Pod "busybox-scheduling-1fcc3c78-ef7b-448d-8d38-6b0ac56395c1": Phase="Running", Reason="", readiness=true. Elapsed: 6.501856139s
    Sep 20 12:12:18.480: INFO: The phase of Pod busybox-scheduling-1fcc3c78-ef7b-448d-8d38-6b0ac56395c1 is Running (Ready = true)
    Sep 20 12:12:18.480: INFO: Pod "busybox-scheduling-1fcc3c78-ef7b-448d-8d38-6b0ac56395c1" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:12:18.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-1101" for this suite. 09/20/23 12:12:18.837
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:12:18.852
Sep 20 12:12:18.852: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename job 09/20/23 12:12:18.853
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:12:19.733
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:12:19.739
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 09/20/23 12:12:19.745
STEP: Ensuring job reaches completions 09/20/23 12:12:19.796
STEP: Ensuring pods with index for job exist 09/20/23 12:12:35.8
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep 20 12:12:35.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-1170" for this suite. 09/20/23 12:12:35.809
------------------------------
â€¢ [SLOW TEST] [16.981 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:12:18.852
    Sep 20 12:12:18.852: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename job 09/20/23 12:12:18.853
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:12:19.733
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:12:19.739
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 09/20/23 12:12:19.745
    STEP: Ensuring job reaches completions 09/20/23 12:12:19.796
    STEP: Ensuring pods with index for job exist 09/20/23 12:12:35.8
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:12:35.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-1170" for this suite. 09/20/23 12:12:35.809
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:12:35.835
Sep 20 12:12:35.836: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename downward-api 09/20/23 12:12:35.836
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:12:35.853
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:12:35.858
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 09/20/23 12:12:35.863
Sep 20 12:12:36.169: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dd4cd2cb-43be-4042-ac7f-5955023b50d8" in namespace "downward-api-7384" to be "Succeeded or Failed"
Sep 20 12:12:36.201: INFO: Pod "downwardapi-volume-dd4cd2cb-43be-4042-ac7f-5955023b50d8": Phase="Pending", Reason="", readiness=false. Elapsed: 32.201634ms
Sep 20 12:12:38.208: INFO: Pod "downwardapi-volume-dd4cd2cb-43be-4042-ac7f-5955023b50d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039125298s
Sep 20 12:12:40.207: INFO: Pod "downwardapi-volume-dd4cd2cb-43be-4042-ac7f-5955023b50d8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038392248s
Sep 20 12:12:42.208: INFO: Pod "downwardapi-volume-dd4cd2cb-43be-4042-ac7f-5955023b50d8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.039638288s
Sep 20 12:12:44.617: INFO: Pod "downwardapi-volume-dd4cd2cb-43be-4042-ac7f-5955023b50d8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.448277906s
Sep 20 12:12:46.205: INFO: Pod "downwardapi-volume-dd4cd2cb-43be-4042-ac7f-5955023b50d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.035919443s
STEP: Saw pod success 09/20/23 12:12:46.205
Sep 20 12:12:46.205: INFO: Pod "downwardapi-volume-dd4cd2cb-43be-4042-ac7f-5955023b50d8" satisfied condition "Succeeded or Failed"
Sep 20 12:12:46.208: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-2 pod downwardapi-volume-dd4cd2cb-43be-4042-ac7f-5955023b50d8 container client-container: <nil>
STEP: delete the pod 09/20/23 12:12:46.277
Sep 20 12:12:46.300: INFO: Waiting for pod downwardapi-volume-dd4cd2cb-43be-4042-ac7f-5955023b50d8 to disappear
Sep 20 12:12:46.305: INFO: Pod downwardapi-volume-dd4cd2cb-43be-4042-ac7f-5955023b50d8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep 20 12:12:46.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7384" for this suite. 09/20/23 12:12:46.312
------------------------------
â€¢ [SLOW TEST] [10.487 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:12:35.835
    Sep 20 12:12:35.836: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename downward-api 09/20/23 12:12:35.836
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:12:35.853
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:12:35.858
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 09/20/23 12:12:35.863
    Sep 20 12:12:36.169: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dd4cd2cb-43be-4042-ac7f-5955023b50d8" in namespace "downward-api-7384" to be "Succeeded or Failed"
    Sep 20 12:12:36.201: INFO: Pod "downwardapi-volume-dd4cd2cb-43be-4042-ac7f-5955023b50d8": Phase="Pending", Reason="", readiness=false. Elapsed: 32.201634ms
    Sep 20 12:12:38.208: INFO: Pod "downwardapi-volume-dd4cd2cb-43be-4042-ac7f-5955023b50d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039125298s
    Sep 20 12:12:40.207: INFO: Pod "downwardapi-volume-dd4cd2cb-43be-4042-ac7f-5955023b50d8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038392248s
    Sep 20 12:12:42.208: INFO: Pod "downwardapi-volume-dd4cd2cb-43be-4042-ac7f-5955023b50d8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.039638288s
    Sep 20 12:12:44.617: INFO: Pod "downwardapi-volume-dd4cd2cb-43be-4042-ac7f-5955023b50d8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.448277906s
    Sep 20 12:12:46.205: INFO: Pod "downwardapi-volume-dd4cd2cb-43be-4042-ac7f-5955023b50d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.035919443s
    STEP: Saw pod success 09/20/23 12:12:46.205
    Sep 20 12:12:46.205: INFO: Pod "downwardapi-volume-dd4cd2cb-43be-4042-ac7f-5955023b50d8" satisfied condition "Succeeded or Failed"
    Sep 20 12:12:46.208: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-2 pod downwardapi-volume-dd4cd2cb-43be-4042-ac7f-5955023b50d8 container client-container: <nil>
    STEP: delete the pod 09/20/23 12:12:46.277
    Sep 20 12:12:46.300: INFO: Waiting for pod downwardapi-volume-dd4cd2cb-43be-4042-ac7f-5955023b50d8 to disappear
    Sep 20 12:12:46.305: INFO: Pod downwardapi-volume-dd4cd2cb-43be-4042-ac7f-5955023b50d8 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:12:46.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7384" for this suite. 09/20/23 12:12:46.312
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:12:46.324
Sep 20 12:12:46.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename kubectl 09/20/23 12:12:46.325
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:12:46.366
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:12:46.371
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 09/20/23 12:12:46.381
Sep 20 12:12:46.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8375 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Sep 20 12:12:46.493: INFO: stderr: ""
Sep 20 12:12:46.493: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 09/20/23 12:12:46.493
Sep 20 12:12:46.493: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Sep 20 12:12:46.493: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8375" to be "running and ready, or succeeded"
Sep 20 12:12:46.753: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 259.626015ms
Sep 20 12:12:46.753: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '' to be 'Running' but was 'Pending'
Sep 20 12:12:48.759: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.265508689s
Sep 20 12:12:48.759: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'mycluster-ww3cg64etuwi-node-2' to be 'Running' but was 'Pending'
Sep 20 12:12:50.763: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.269142997s
Sep 20 12:12:50.763: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'mycluster-ww3cg64etuwi-node-2' to be 'Running' but was 'Pending'
Sep 20 12:12:52.758: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 6.264434778s
Sep 20 12:12:52.758: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Sep 20 12:12:52.758: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 09/20/23 12:12:52.758
Sep 20 12:12:52.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8375 logs logs-generator logs-generator'
Sep 20 12:12:52.917: INFO: stderr: ""
Sep 20 12:12:52.917: INFO: stdout: "I0920 12:12:50.143967       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/n86z 333\nI0920 12:12:50.344330       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/hpf 372\nI0920 12:12:50.544641       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/4m8w 544\nI0920 12:12:50.744944       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/nz6r 526\nI0920 12:12:50.944395       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/g6c8 413\nI0920 12:12:51.144700       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/pg46 474\nI0920 12:12:51.344055       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/qgw 223\nI0920 12:12:51.544546       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/zc2v 297\nI0920 12:12:51.744860       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/cjrg 490\nI0920 12:12:51.944032       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/ppjl 402\nI0920 12:12:52.144385       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/2s72 343\nI0920 12:12:52.344731       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/cbrw 203\nI0920 12:12:52.544027       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/qhz5 264\nI0920 12:12:52.744325       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/f4p 210\n"
STEP: limiting log lines 09/20/23 12:12:52.917
Sep 20 12:12:52.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8375 logs logs-generator logs-generator --tail=1'
Sep 20 12:12:53.010: INFO: stderr: ""
Sep 20 12:12:53.010: INFO: stdout: "I0920 12:12:52.944622       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/r4nl 561\n"
Sep 20 12:12:53.010: INFO: got output "I0920 12:12:52.944622       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/r4nl 561\n"
STEP: limiting log bytes 09/20/23 12:12:53.01
Sep 20 12:12:53.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8375 logs logs-generator logs-generator --limit-bytes=1'
Sep 20 12:12:53.088: INFO: stderr: ""
Sep 20 12:12:53.088: INFO: stdout: "I"
Sep 20 12:12:53.088: INFO: got output "I"
STEP: exposing timestamps 09/20/23 12:12:53.088
Sep 20 12:12:53.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8375 logs logs-generator logs-generator --tail=1 --timestamps'
Sep 20 12:12:53.172: INFO: stderr: ""
Sep 20 12:12:53.172: INFO: stdout: "2023-09-20T12:12:53.145027263Z I0920 12:12:53.144918       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/x9lg 432\n"
Sep 20 12:12:53.172: INFO: got output "2023-09-20T12:12:53.145027263Z I0920 12:12:53.144918       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/x9lg 432\n"
STEP: restricting to a time range 09/20/23 12:12:53.172
Sep 20 12:12:55.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8375 logs logs-generator logs-generator --since=1s'
Sep 20 12:12:55.765: INFO: stderr: ""
Sep 20 12:12:55.765: INFO: stdout: "I0920 12:12:54.944028       1 logs_generator.go:76] 24 POST /api/v1/namespaces/kube-system/pods/5s5j 533\nI0920 12:12:55.144327       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/default/pods/d7mw 412\nI0920 12:12:55.344621       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/kube-system/pods/9w7 256\nI0920 12:12:55.544921       1 logs_generator.go:76] 27 GET /api/v1/namespaces/ns/pods/tdzr 470\nI0920 12:12:55.744029       1 logs_generator.go:76] 28 POST /api/v1/namespaces/kube-system/pods/w846 357\n"
Sep 20 12:12:55.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8375 logs logs-generator logs-generator --since=24h'
Sep 20 12:12:55.882: INFO: stderr: ""
Sep 20 12:12:55.882: INFO: stdout: "I0920 12:12:50.143967       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/n86z 333\nI0920 12:12:50.344330       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/hpf 372\nI0920 12:12:50.544641       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/4m8w 544\nI0920 12:12:50.744944       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/nz6r 526\nI0920 12:12:50.944395       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/g6c8 413\nI0920 12:12:51.144700       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/pg46 474\nI0920 12:12:51.344055       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/qgw 223\nI0920 12:12:51.544546       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/zc2v 297\nI0920 12:12:51.744860       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/cjrg 490\nI0920 12:12:51.944032       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/ppjl 402\nI0920 12:12:52.144385       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/2s72 343\nI0920 12:12:52.344731       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/cbrw 203\nI0920 12:12:52.544027       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/qhz5 264\nI0920 12:12:52.744325       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/f4p 210\nI0920 12:12:52.944622       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/r4nl 561\nI0920 12:12:53.144918       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/x9lg 432\nI0920 12:12:53.344027       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/4w5s 596\nI0920 12:12:53.544321       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/fng 389\nI0920 12:12:53.744610       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/fsj 318\nI0920 12:12:53.944903       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/crxt 463\nI0920 12:12:54.144030       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/j9tl 429\nI0920 12:12:54.344321       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/zflg 336\nI0920 12:12:54.544611       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/ns/pods/t59t 593\nI0920 12:12:54.744904       1 logs_generator.go:76] 23 POST /api/v1/namespaces/ns/pods/wds 305\nI0920 12:12:54.944028       1 logs_generator.go:76] 24 POST /api/v1/namespaces/kube-system/pods/5s5j 533\nI0920 12:12:55.144327       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/default/pods/d7mw 412\nI0920 12:12:55.344621       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/kube-system/pods/9w7 256\nI0920 12:12:55.544921       1 logs_generator.go:76] 27 GET /api/v1/namespaces/ns/pods/tdzr 470\nI0920 12:12:55.744029       1 logs_generator.go:76] 28 POST /api/v1/namespaces/kube-system/pods/w846 357\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Sep 20 12:12:55.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8375 delete pod logs-generator'
Sep 20 12:12:58.119: INFO: stderr: ""
Sep 20 12:12:58.119: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep 20 12:12:58.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8375" for this suite. 09/20/23 12:12:58.126
------------------------------
â€¢ [SLOW TEST] [11.812 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:12:46.324
    Sep 20 12:12:46.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename kubectl 09/20/23 12:12:46.325
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:12:46.366
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:12:46.371
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 09/20/23 12:12:46.381
    Sep 20 12:12:46.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8375 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Sep 20 12:12:46.493: INFO: stderr: ""
    Sep 20 12:12:46.493: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 09/20/23 12:12:46.493
    Sep 20 12:12:46.493: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Sep 20 12:12:46.493: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8375" to be "running and ready, or succeeded"
    Sep 20 12:12:46.753: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 259.626015ms
    Sep 20 12:12:46.753: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '' to be 'Running' but was 'Pending'
    Sep 20 12:12:48.759: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.265508689s
    Sep 20 12:12:48.759: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'mycluster-ww3cg64etuwi-node-2' to be 'Running' but was 'Pending'
    Sep 20 12:12:50.763: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.269142997s
    Sep 20 12:12:50.763: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'mycluster-ww3cg64etuwi-node-2' to be 'Running' but was 'Pending'
    Sep 20 12:12:52.758: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 6.264434778s
    Sep 20 12:12:52.758: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Sep 20 12:12:52.758: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 09/20/23 12:12:52.758
    Sep 20 12:12:52.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8375 logs logs-generator logs-generator'
    Sep 20 12:12:52.917: INFO: stderr: ""
    Sep 20 12:12:52.917: INFO: stdout: "I0920 12:12:50.143967       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/n86z 333\nI0920 12:12:50.344330       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/hpf 372\nI0920 12:12:50.544641       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/4m8w 544\nI0920 12:12:50.744944       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/nz6r 526\nI0920 12:12:50.944395       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/g6c8 413\nI0920 12:12:51.144700       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/pg46 474\nI0920 12:12:51.344055       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/qgw 223\nI0920 12:12:51.544546       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/zc2v 297\nI0920 12:12:51.744860       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/cjrg 490\nI0920 12:12:51.944032       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/ppjl 402\nI0920 12:12:52.144385       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/2s72 343\nI0920 12:12:52.344731       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/cbrw 203\nI0920 12:12:52.544027       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/qhz5 264\nI0920 12:12:52.744325       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/f4p 210\n"
    STEP: limiting log lines 09/20/23 12:12:52.917
    Sep 20 12:12:52.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8375 logs logs-generator logs-generator --tail=1'
    Sep 20 12:12:53.010: INFO: stderr: ""
    Sep 20 12:12:53.010: INFO: stdout: "I0920 12:12:52.944622       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/r4nl 561\n"
    Sep 20 12:12:53.010: INFO: got output "I0920 12:12:52.944622       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/r4nl 561\n"
    STEP: limiting log bytes 09/20/23 12:12:53.01
    Sep 20 12:12:53.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8375 logs logs-generator logs-generator --limit-bytes=1'
    Sep 20 12:12:53.088: INFO: stderr: ""
    Sep 20 12:12:53.088: INFO: stdout: "I"
    Sep 20 12:12:53.088: INFO: got output "I"
    STEP: exposing timestamps 09/20/23 12:12:53.088
    Sep 20 12:12:53.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8375 logs logs-generator logs-generator --tail=1 --timestamps'
    Sep 20 12:12:53.172: INFO: stderr: ""
    Sep 20 12:12:53.172: INFO: stdout: "2023-09-20T12:12:53.145027263Z I0920 12:12:53.144918       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/x9lg 432\n"
    Sep 20 12:12:53.172: INFO: got output "2023-09-20T12:12:53.145027263Z I0920 12:12:53.144918       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/x9lg 432\n"
    STEP: restricting to a time range 09/20/23 12:12:53.172
    Sep 20 12:12:55.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8375 logs logs-generator logs-generator --since=1s'
    Sep 20 12:12:55.765: INFO: stderr: ""
    Sep 20 12:12:55.765: INFO: stdout: "I0920 12:12:54.944028       1 logs_generator.go:76] 24 POST /api/v1/namespaces/kube-system/pods/5s5j 533\nI0920 12:12:55.144327       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/default/pods/d7mw 412\nI0920 12:12:55.344621       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/kube-system/pods/9w7 256\nI0920 12:12:55.544921       1 logs_generator.go:76] 27 GET /api/v1/namespaces/ns/pods/tdzr 470\nI0920 12:12:55.744029       1 logs_generator.go:76] 28 POST /api/v1/namespaces/kube-system/pods/w846 357\n"
    Sep 20 12:12:55.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8375 logs logs-generator logs-generator --since=24h'
    Sep 20 12:12:55.882: INFO: stderr: ""
    Sep 20 12:12:55.882: INFO: stdout: "I0920 12:12:50.143967       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/n86z 333\nI0920 12:12:50.344330       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/hpf 372\nI0920 12:12:50.544641       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/4m8w 544\nI0920 12:12:50.744944       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/nz6r 526\nI0920 12:12:50.944395       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/g6c8 413\nI0920 12:12:51.144700       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/pg46 474\nI0920 12:12:51.344055       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/qgw 223\nI0920 12:12:51.544546       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/zc2v 297\nI0920 12:12:51.744860       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/cjrg 490\nI0920 12:12:51.944032       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/ppjl 402\nI0920 12:12:52.144385       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/2s72 343\nI0920 12:12:52.344731       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/cbrw 203\nI0920 12:12:52.544027       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/qhz5 264\nI0920 12:12:52.744325       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/f4p 210\nI0920 12:12:52.944622       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/r4nl 561\nI0920 12:12:53.144918       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/x9lg 432\nI0920 12:12:53.344027       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/4w5s 596\nI0920 12:12:53.544321       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/fng 389\nI0920 12:12:53.744610       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/fsj 318\nI0920 12:12:53.944903       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/crxt 463\nI0920 12:12:54.144030       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/j9tl 429\nI0920 12:12:54.344321       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/zflg 336\nI0920 12:12:54.544611       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/ns/pods/t59t 593\nI0920 12:12:54.744904       1 logs_generator.go:76] 23 POST /api/v1/namespaces/ns/pods/wds 305\nI0920 12:12:54.944028       1 logs_generator.go:76] 24 POST /api/v1/namespaces/kube-system/pods/5s5j 533\nI0920 12:12:55.144327       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/default/pods/d7mw 412\nI0920 12:12:55.344621       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/kube-system/pods/9w7 256\nI0920 12:12:55.544921       1 logs_generator.go:76] 27 GET /api/v1/namespaces/ns/pods/tdzr 470\nI0920 12:12:55.744029       1 logs_generator.go:76] 28 POST /api/v1/namespaces/kube-system/pods/w846 357\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Sep 20 12:12:55.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8375 delete pod logs-generator'
    Sep 20 12:12:58.119: INFO: stderr: ""
    Sep 20 12:12:58.119: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:12:58.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8375" for this suite. 09/20/23 12:12:58.126
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:12:58.136
Sep 20 12:12:58.136: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename replicaset 09/20/23 12:12:58.137
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:12:58.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:12:58.163
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 09/20/23 12:12:58.173
Sep 20 12:12:58.294: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep 20 12:13:03.307: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/20/23 12:13:03.307
STEP: getting scale subresource 09/20/23 12:13:03.307
STEP: updating a scale subresource 09/20/23 12:13:03.319
STEP: verifying the replicaset Spec.Replicas was modified 09/20/23 12:13:03.427
STEP: Patch a scale subresource 09/20/23 12:13:03.509
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep 20 12:13:04.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-1701" for this suite. 09/20/23 12:13:04.319
------------------------------
â€¢ [SLOW TEST] [6.577 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:12:58.136
    Sep 20 12:12:58.136: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename replicaset 09/20/23 12:12:58.137
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:12:58.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:12:58.163
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 09/20/23 12:12:58.173
    Sep 20 12:12:58.294: INFO: Pod name sample-pod: Found 0 pods out of 1
    Sep 20 12:13:03.307: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/20/23 12:13:03.307
    STEP: getting scale subresource 09/20/23 12:13:03.307
    STEP: updating a scale subresource 09/20/23 12:13:03.319
    STEP: verifying the replicaset Spec.Replicas was modified 09/20/23 12:13:03.427
    STEP: Patch a scale subresource 09/20/23 12:13:03.509
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:13:04.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-1701" for this suite. 09/20/23 12:13:04.319
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:13:04.714
Sep 20 12:13:04.714: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename configmap 09/20/23 12:13:04.714
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:13:05.164
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:13:05.169
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-b564f567-4402-4a3e-b51d-aaf2f74c7baa 09/20/23 12:13:05.177
STEP: Creating a pod to test consume configMaps 09/20/23 12:13:05.194
Sep 20 12:13:05.298: INFO: Waiting up to 5m0s for pod "pod-configmaps-a123b13c-3415-4fd8-807d-a9b0f50c918b" in namespace "configmap-9879" to be "Succeeded or Failed"
Sep 20 12:13:05.307: INFO: Pod "pod-configmaps-a123b13c-3415-4fd8-807d-a9b0f50c918b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.814139ms
Sep 20 12:13:07.312: INFO: Pod "pod-configmaps-a123b13c-3415-4fd8-807d-a9b0f50c918b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013915688s
Sep 20 12:13:09.314: INFO: Pod "pod-configmaps-a123b13c-3415-4fd8-807d-a9b0f50c918b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01620986s
Sep 20 12:13:11.314: INFO: Pod "pod-configmaps-a123b13c-3415-4fd8-807d-a9b0f50c918b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016725601s
Sep 20 12:13:13.379: INFO: Pod "pod-configmaps-a123b13c-3415-4fd8-807d-a9b0f50c918b": Phase="Running", Reason="", readiness=false. Elapsed: 8.081773445s
Sep 20 12:13:15.327: INFO: Pod "pod-configmaps-a123b13c-3415-4fd8-807d-a9b0f50c918b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.029738788s
STEP: Saw pod success 09/20/23 12:13:15.327
Sep 20 12:13:15.328: INFO: Pod "pod-configmaps-a123b13c-3415-4fd8-807d-a9b0f50c918b" satisfied condition "Succeeded or Failed"
Sep 20 12:13:15.332: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-2 pod pod-configmaps-a123b13c-3415-4fd8-807d-a9b0f50c918b container agnhost-container: <nil>
STEP: delete the pod 09/20/23 12:13:15.339
Sep 20 12:13:15.513: INFO: Waiting for pod pod-configmaps-a123b13c-3415-4fd8-807d-a9b0f50c918b to disappear
Sep 20 12:13:15.519: INFO: Pod pod-configmaps-a123b13c-3415-4fd8-807d-a9b0f50c918b no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep 20 12:13:15.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9879" for this suite. 09/20/23 12:13:15.523
------------------------------
â€¢ [SLOW TEST] [10.817 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:13:04.714
    Sep 20 12:13:04.714: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename configmap 09/20/23 12:13:04.714
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:13:05.164
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:13:05.169
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-b564f567-4402-4a3e-b51d-aaf2f74c7baa 09/20/23 12:13:05.177
    STEP: Creating a pod to test consume configMaps 09/20/23 12:13:05.194
    Sep 20 12:13:05.298: INFO: Waiting up to 5m0s for pod "pod-configmaps-a123b13c-3415-4fd8-807d-a9b0f50c918b" in namespace "configmap-9879" to be "Succeeded or Failed"
    Sep 20 12:13:05.307: INFO: Pod "pod-configmaps-a123b13c-3415-4fd8-807d-a9b0f50c918b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.814139ms
    Sep 20 12:13:07.312: INFO: Pod "pod-configmaps-a123b13c-3415-4fd8-807d-a9b0f50c918b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013915688s
    Sep 20 12:13:09.314: INFO: Pod "pod-configmaps-a123b13c-3415-4fd8-807d-a9b0f50c918b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01620986s
    Sep 20 12:13:11.314: INFO: Pod "pod-configmaps-a123b13c-3415-4fd8-807d-a9b0f50c918b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016725601s
    Sep 20 12:13:13.379: INFO: Pod "pod-configmaps-a123b13c-3415-4fd8-807d-a9b0f50c918b": Phase="Running", Reason="", readiness=false. Elapsed: 8.081773445s
    Sep 20 12:13:15.327: INFO: Pod "pod-configmaps-a123b13c-3415-4fd8-807d-a9b0f50c918b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.029738788s
    STEP: Saw pod success 09/20/23 12:13:15.327
    Sep 20 12:13:15.328: INFO: Pod "pod-configmaps-a123b13c-3415-4fd8-807d-a9b0f50c918b" satisfied condition "Succeeded or Failed"
    Sep 20 12:13:15.332: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-2 pod pod-configmaps-a123b13c-3415-4fd8-807d-a9b0f50c918b container agnhost-container: <nil>
    STEP: delete the pod 09/20/23 12:13:15.339
    Sep 20 12:13:15.513: INFO: Waiting for pod pod-configmaps-a123b13c-3415-4fd8-807d-a9b0f50c918b to disappear
    Sep 20 12:13:15.519: INFO: Pod pod-configmaps-a123b13c-3415-4fd8-807d-a9b0f50c918b no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:13:15.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9879" for this suite. 09/20/23 12:13:15.523
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:13:15.532
Sep 20 12:13:15.532: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename projected 09/20/23 12:13:15.532
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:13:15.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:13:15.668
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-51e3c975-e1bd-4e01-a20e-c4479d1e4a9d 09/20/23 12:13:15.711
STEP: Creating secret with name s-test-opt-upd-97ae9905-cd8a-498c-bdc3-39d444553375 09/20/23 12:13:15.718
STEP: Creating the pod 09/20/23 12:13:15.754
Sep 20 12:13:15.770: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cee1869e-922a-4c14-bd2f-114b785014af" in namespace "projected-2339" to be "running and ready"
Sep 20 12:13:15.780: INFO: Pod "pod-projected-secrets-cee1869e-922a-4c14-bd2f-114b785014af": Phase="Pending", Reason="", readiness=false. Elapsed: 10.125095ms
Sep 20 12:13:15.780: INFO: The phase of Pod pod-projected-secrets-cee1869e-922a-4c14-bd2f-114b785014af is Pending, waiting for it to be Running (with Ready = true)
Sep 20 12:13:17.785: INFO: Pod "pod-projected-secrets-cee1869e-922a-4c14-bd2f-114b785014af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014553188s
Sep 20 12:13:17.785: INFO: The phase of Pod pod-projected-secrets-cee1869e-922a-4c14-bd2f-114b785014af is Pending, waiting for it to be Running (with Ready = true)
Sep 20 12:13:19.987: INFO: Pod "pod-projected-secrets-cee1869e-922a-4c14-bd2f-114b785014af": Phase="Pending", Reason="", readiness=false. Elapsed: 4.217036995s
Sep 20 12:13:19.987: INFO: The phase of Pod pod-projected-secrets-cee1869e-922a-4c14-bd2f-114b785014af is Pending, waiting for it to be Running (with Ready = true)
Sep 20 12:13:21.787: INFO: Pod "pod-projected-secrets-cee1869e-922a-4c14-bd2f-114b785014af": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016647516s
Sep 20 12:13:21.787: INFO: The phase of Pod pod-projected-secrets-cee1869e-922a-4c14-bd2f-114b785014af is Pending, waiting for it to be Running (with Ready = true)
Sep 20 12:13:23.786: INFO: Pod "pod-projected-secrets-cee1869e-922a-4c14-bd2f-114b785014af": Phase="Running", Reason="", readiness=true. Elapsed: 8.015610394s
Sep 20 12:13:23.786: INFO: The phase of Pod pod-projected-secrets-cee1869e-922a-4c14-bd2f-114b785014af is Running (Ready = true)
Sep 20 12:13:23.786: INFO: Pod "pod-projected-secrets-cee1869e-922a-4c14-bd2f-114b785014af" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-51e3c975-e1bd-4e01-a20e-c4479d1e4a9d 09/20/23 12:13:23.804
STEP: Updating secret s-test-opt-upd-97ae9905-cd8a-498c-bdc3-39d444553375 09/20/23 12:13:23.811
STEP: Creating secret with name s-test-opt-create-4332043d-a1ce-4a23-9566-6f5a7a783206 09/20/23 12:13:23.816
STEP: waiting to observe update in volume 09/20/23 12:13:23.821
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep 20 12:14:35.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2339" for this suite. 09/20/23 12:14:35.666
------------------------------
â€¢ [SLOW TEST] [80.747 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:13:15.532
    Sep 20 12:13:15.532: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename projected 09/20/23 12:13:15.532
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:13:15.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:13:15.668
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-51e3c975-e1bd-4e01-a20e-c4479d1e4a9d 09/20/23 12:13:15.711
    STEP: Creating secret with name s-test-opt-upd-97ae9905-cd8a-498c-bdc3-39d444553375 09/20/23 12:13:15.718
    STEP: Creating the pod 09/20/23 12:13:15.754
    Sep 20 12:13:15.770: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cee1869e-922a-4c14-bd2f-114b785014af" in namespace "projected-2339" to be "running and ready"
    Sep 20 12:13:15.780: INFO: Pod "pod-projected-secrets-cee1869e-922a-4c14-bd2f-114b785014af": Phase="Pending", Reason="", readiness=false. Elapsed: 10.125095ms
    Sep 20 12:13:15.780: INFO: The phase of Pod pod-projected-secrets-cee1869e-922a-4c14-bd2f-114b785014af is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 12:13:17.785: INFO: Pod "pod-projected-secrets-cee1869e-922a-4c14-bd2f-114b785014af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014553188s
    Sep 20 12:13:17.785: INFO: The phase of Pod pod-projected-secrets-cee1869e-922a-4c14-bd2f-114b785014af is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 12:13:19.987: INFO: Pod "pod-projected-secrets-cee1869e-922a-4c14-bd2f-114b785014af": Phase="Pending", Reason="", readiness=false. Elapsed: 4.217036995s
    Sep 20 12:13:19.987: INFO: The phase of Pod pod-projected-secrets-cee1869e-922a-4c14-bd2f-114b785014af is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 12:13:21.787: INFO: Pod "pod-projected-secrets-cee1869e-922a-4c14-bd2f-114b785014af": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016647516s
    Sep 20 12:13:21.787: INFO: The phase of Pod pod-projected-secrets-cee1869e-922a-4c14-bd2f-114b785014af is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 12:13:23.786: INFO: Pod "pod-projected-secrets-cee1869e-922a-4c14-bd2f-114b785014af": Phase="Running", Reason="", readiness=true. Elapsed: 8.015610394s
    Sep 20 12:13:23.786: INFO: The phase of Pod pod-projected-secrets-cee1869e-922a-4c14-bd2f-114b785014af is Running (Ready = true)
    Sep 20 12:13:23.786: INFO: Pod "pod-projected-secrets-cee1869e-922a-4c14-bd2f-114b785014af" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-51e3c975-e1bd-4e01-a20e-c4479d1e4a9d 09/20/23 12:13:23.804
    STEP: Updating secret s-test-opt-upd-97ae9905-cd8a-498c-bdc3-39d444553375 09/20/23 12:13:23.811
    STEP: Creating secret with name s-test-opt-create-4332043d-a1ce-4a23-9566-6f5a7a783206 09/20/23 12:13:23.816
    STEP: waiting to observe update in volume 09/20/23 12:13:23.821
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:14:35.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2339" for this suite. 09/20/23 12:14:35.666
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:14:36.28
Sep 20 12:14:36.280: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename projected 09/20/23 12:14:36.281
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:14:36.935
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:14:36.938
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-6598be84-5320-4a52-8462-05a6a6db8da3 09/20/23 12:14:36.944
STEP: Creating a pod to test consume secrets 09/20/23 12:14:37.716
Sep 20 12:14:38.653: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b0cc1710-9a4e-41d7-beb4-908ada82e121" in namespace "projected-708" to be "Succeeded or Failed"
Sep 20 12:14:39.405: INFO: Pod "pod-projected-secrets-b0cc1710-9a4e-41d7-beb4-908ada82e121": Phase="Pending", Reason="", readiness=false. Elapsed: 752.113204ms
Sep 20 12:14:41.521: INFO: Pod "pod-projected-secrets-b0cc1710-9a4e-41d7-beb4-908ada82e121": Phase="Pending", Reason="", readiness=false. Elapsed: 2.868221451s
Sep 20 12:14:43.412: INFO: Pod "pod-projected-secrets-b0cc1710-9a4e-41d7-beb4-908ada82e121": Phase="Pending", Reason="", readiness=false. Elapsed: 4.758550908s
Sep 20 12:14:45.411: INFO: Pod "pod-projected-secrets-b0cc1710-9a4e-41d7-beb4-908ada82e121": Phase="Running", Reason="", readiness=false. Elapsed: 6.757734749s
Sep 20 12:14:47.586: INFO: Pod "pod-projected-secrets-b0cc1710-9a4e-41d7-beb4-908ada82e121": Phase="Running", Reason="", readiness=false. Elapsed: 8.933173109s
Sep 20 12:14:49.411: INFO: Pod "pod-projected-secrets-b0cc1710-9a4e-41d7-beb4-908ada82e121": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.757945633s
STEP: Saw pod success 09/20/23 12:14:49.411
Sep 20 12:14:49.411: INFO: Pod "pod-projected-secrets-b0cc1710-9a4e-41d7-beb4-908ada82e121" satisfied condition "Succeeded or Failed"
Sep 20 12:14:49.414: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-projected-secrets-b0cc1710-9a4e-41d7-beb4-908ada82e121 container secret-volume-test: <nil>
STEP: delete the pod 09/20/23 12:14:49.468
Sep 20 12:14:49.643: INFO: Waiting for pod pod-projected-secrets-b0cc1710-9a4e-41d7-beb4-908ada82e121 to disappear
Sep 20 12:14:49.647: INFO: Pod pod-projected-secrets-b0cc1710-9a4e-41d7-beb4-908ada82e121 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep 20 12:14:49.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-708" for this suite. 09/20/23 12:14:49.651
------------------------------
â€¢ [SLOW TEST] [13.380 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:14:36.28
    Sep 20 12:14:36.280: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename projected 09/20/23 12:14:36.281
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:14:36.935
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:14:36.938
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-6598be84-5320-4a52-8462-05a6a6db8da3 09/20/23 12:14:36.944
    STEP: Creating a pod to test consume secrets 09/20/23 12:14:37.716
    Sep 20 12:14:38.653: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b0cc1710-9a4e-41d7-beb4-908ada82e121" in namespace "projected-708" to be "Succeeded or Failed"
    Sep 20 12:14:39.405: INFO: Pod "pod-projected-secrets-b0cc1710-9a4e-41d7-beb4-908ada82e121": Phase="Pending", Reason="", readiness=false. Elapsed: 752.113204ms
    Sep 20 12:14:41.521: INFO: Pod "pod-projected-secrets-b0cc1710-9a4e-41d7-beb4-908ada82e121": Phase="Pending", Reason="", readiness=false. Elapsed: 2.868221451s
    Sep 20 12:14:43.412: INFO: Pod "pod-projected-secrets-b0cc1710-9a4e-41d7-beb4-908ada82e121": Phase="Pending", Reason="", readiness=false. Elapsed: 4.758550908s
    Sep 20 12:14:45.411: INFO: Pod "pod-projected-secrets-b0cc1710-9a4e-41d7-beb4-908ada82e121": Phase="Running", Reason="", readiness=false. Elapsed: 6.757734749s
    Sep 20 12:14:47.586: INFO: Pod "pod-projected-secrets-b0cc1710-9a4e-41d7-beb4-908ada82e121": Phase="Running", Reason="", readiness=false. Elapsed: 8.933173109s
    Sep 20 12:14:49.411: INFO: Pod "pod-projected-secrets-b0cc1710-9a4e-41d7-beb4-908ada82e121": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.757945633s
    STEP: Saw pod success 09/20/23 12:14:49.411
    Sep 20 12:14:49.411: INFO: Pod "pod-projected-secrets-b0cc1710-9a4e-41d7-beb4-908ada82e121" satisfied condition "Succeeded or Failed"
    Sep 20 12:14:49.414: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-projected-secrets-b0cc1710-9a4e-41d7-beb4-908ada82e121 container secret-volume-test: <nil>
    STEP: delete the pod 09/20/23 12:14:49.468
    Sep 20 12:14:49.643: INFO: Waiting for pod pod-projected-secrets-b0cc1710-9a4e-41d7-beb4-908ada82e121 to disappear
    Sep 20 12:14:49.647: INFO: Pod pod-projected-secrets-b0cc1710-9a4e-41d7-beb4-908ada82e121 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:14:49.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-708" for this suite. 09/20/23 12:14:49.651
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:14:49.661
Sep 20 12:14:49.661: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename emptydir-wrapper 09/20/23 12:14:49.662
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:14:50.188
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:14:50.192
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Sep 20 12:14:50.318: INFO: Waiting up to 5m0s for pod "pod-secrets-4d2ac121-4b39-443b-9b36-d73b4c5ee960" in namespace "emptydir-wrapper-8433" to be "running and ready"
Sep 20 12:14:50.653: INFO: Pod "pod-secrets-4d2ac121-4b39-443b-9b36-d73b4c5ee960": Phase="Pending", Reason="", readiness=false. Elapsed: 334.463354ms
Sep 20 12:14:50.653: INFO: The phase of Pod pod-secrets-4d2ac121-4b39-443b-9b36-d73b4c5ee960 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 12:14:52.658: INFO: Pod "pod-secrets-4d2ac121-4b39-443b-9b36-d73b4c5ee960": Phase="Pending", Reason="", readiness=false. Elapsed: 2.340063581s
Sep 20 12:14:52.658: INFO: The phase of Pod pod-secrets-4d2ac121-4b39-443b-9b36-d73b4c5ee960 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 12:14:54.658: INFO: Pod "pod-secrets-4d2ac121-4b39-443b-9b36-d73b4c5ee960": Phase="Pending", Reason="", readiness=false. Elapsed: 4.340318361s
Sep 20 12:14:54.658: INFO: The phase of Pod pod-secrets-4d2ac121-4b39-443b-9b36-d73b4c5ee960 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 12:14:57.276: INFO: Pod "pod-secrets-4d2ac121-4b39-443b-9b36-d73b4c5ee960": Phase="Pending", Reason="", readiness=false. Elapsed: 6.957899275s
Sep 20 12:14:57.276: INFO: The phase of Pod pod-secrets-4d2ac121-4b39-443b-9b36-d73b4c5ee960 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 12:14:58.658: INFO: Pod "pod-secrets-4d2ac121-4b39-443b-9b36-d73b4c5ee960": Phase="Running", Reason="", readiness=true. Elapsed: 8.34000167s
Sep 20 12:14:58.658: INFO: The phase of Pod pod-secrets-4d2ac121-4b39-443b-9b36-d73b4c5ee960 is Running (Ready = true)
Sep 20 12:14:58.658: INFO: Pod "pod-secrets-4d2ac121-4b39-443b-9b36-d73b4c5ee960" satisfied condition "running and ready"
STEP: Cleaning up the secret 09/20/23 12:14:58.661
STEP: Cleaning up the configmap 09/20/23 12:14:58.668
STEP: Cleaning up the pod 09/20/23 12:14:58.73
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Sep 20 12:14:58.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-8433" for this suite. 09/20/23 12:14:58.754
------------------------------
â€¢ [SLOW TEST] [9.104 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:14:49.661
    Sep 20 12:14:49.661: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename emptydir-wrapper 09/20/23 12:14:49.662
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:14:50.188
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:14:50.192
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Sep 20 12:14:50.318: INFO: Waiting up to 5m0s for pod "pod-secrets-4d2ac121-4b39-443b-9b36-d73b4c5ee960" in namespace "emptydir-wrapper-8433" to be "running and ready"
    Sep 20 12:14:50.653: INFO: Pod "pod-secrets-4d2ac121-4b39-443b-9b36-d73b4c5ee960": Phase="Pending", Reason="", readiness=false. Elapsed: 334.463354ms
    Sep 20 12:14:50.653: INFO: The phase of Pod pod-secrets-4d2ac121-4b39-443b-9b36-d73b4c5ee960 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 12:14:52.658: INFO: Pod "pod-secrets-4d2ac121-4b39-443b-9b36-d73b4c5ee960": Phase="Pending", Reason="", readiness=false. Elapsed: 2.340063581s
    Sep 20 12:14:52.658: INFO: The phase of Pod pod-secrets-4d2ac121-4b39-443b-9b36-d73b4c5ee960 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 12:14:54.658: INFO: Pod "pod-secrets-4d2ac121-4b39-443b-9b36-d73b4c5ee960": Phase="Pending", Reason="", readiness=false. Elapsed: 4.340318361s
    Sep 20 12:14:54.658: INFO: The phase of Pod pod-secrets-4d2ac121-4b39-443b-9b36-d73b4c5ee960 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 12:14:57.276: INFO: Pod "pod-secrets-4d2ac121-4b39-443b-9b36-d73b4c5ee960": Phase="Pending", Reason="", readiness=false. Elapsed: 6.957899275s
    Sep 20 12:14:57.276: INFO: The phase of Pod pod-secrets-4d2ac121-4b39-443b-9b36-d73b4c5ee960 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 12:14:58.658: INFO: Pod "pod-secrets-4d2ac121-4b39-443b-9b36-d73b4c5ee960": Phase="Running", Reason="", readiness=true. Elapsed: 8.34000167s
    Sep 20 12:14:58.658: INFO: The phase of Pod pod-secrets-4d2ac121-4b39-443b-9b36-d73b4c5ee960 is Running (Ready = true)
    Sep 20 12:14:58.658: INFO: Pod "pod-secrets-4d2ac121-4b39-443b-9b36-d73b4c5ee960" satisfied condition "running and ready"
    STEP: Cleaning up the secret 09/20/23 12:14:58.661
    STEP: Cleaning up the configmap 09/20/23 12:14:58.668
    STEP: Cleaning up the pod 09/20/23 12:14:58.73
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:14:58.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-8433" for this suite. 09/20/23 12:14:58.754
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:14:58.766
Sep 20 12:14:58.766: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename services 09/20/23 12:14:58.767
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:14:58.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:14:58.811
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 09/20/23 12:14:58.816
Sep 20 12:14:58.816: INFO: Creating e2e-svc-a-xnhv7
Sep 20 12:14:58.960: INFO: Creating e2e-svc-b-cp6rv
Sep 20 12:14:59.046: INFO: Creating e2e-svc-c-kd2kz
STEP: deleting service collection 09/20/23 12:14:59.253
Sep 20 12:14:59.632: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep 20 12:14:59.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6038" for this suite. 09/20/23 12:14:59.638
------------------------------
â€¢ [0.883 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:14:58.766
    Sep 20 12:14:58.766: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename services 09/20/23 12:14:58.767
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:14:58.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:14:58.811
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 09/20/23 12:14:58.816
    Sep 20 12:14:58.816: INFO: Creating e2e-svc-a-xnhv7
    Sep 20 12:14:58.960: INFO: Creating e2e-svc-b-cp6rv
    Sep 20 12:14:59.046: INFO: Creating e2e-svc-c-kd2kz
    STEP: deleting service collection 09/20/23 12:14:59.253
    Sep 20 12:14:59.632: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:14:59.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6038" for this suite. 09/20/23 12:14:59.638
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:14:59.651
Sep 20 12:14:59.651: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename emptydir 09/20/23 12:14:59.652
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:14:59.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:14:59.729
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 09/20/23 12:14:59.736
Sep 20 12:15:00.183: INFO: Waiting up to 5m0s for pod "pod-f57c280e-d53e-4089-a2a1-7a8ebd99df05" in namespace "emptydir-7039" to be "Succeeded or Failed"
Sep 20 12:15:00.213: INFO: Pod "pod-f57c280e-d53e-4089-a2a1-7a8ebd99df05": Phase="Pending", Reason="", readiness=false. Elapsed: 30.17159ms
Sep 20 12:15:03.615: INFO: Pod "pod-f57c280e-d53e-4089-a2a1-7a8ebd99df05": Phase="Pending", Reason="", readiness=false. Elapsed: 3.431766867s
Sep 20 12:15:04.592: INFO: Pod "pod-f57c280e-d53e-4089-a2a1-7a8ebd99df05": Phase="Pending", Reason="", readiness=false. Elapsed: 4.408757021s
Sep 20 12:15:06.273: INFO: Pod "pod-f57c280e-d53e-4089-a2a1-7a8ebd99df05": Phase="Running", Reason="", readiness=true. Elapsed: 6.090079315s
Sep 20 12:15:09.298: INFO: Pod "pod-f57c280e-d53e-4089-a2a1-7a8ebd99df05": Phase="Running", Reason="", readiness=false. Elapsed: 9.115266227s
Sep 20 12:15:10.220: INFO: Pod "pod-f57c280e-d53e-4089-a2a1-7a8ebd99df05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.036817571s
STEP: Saw pod success 09/20/23 12:15:10.22
Sep 20 12:15:10.220: INFO: Pod "pod-f57c280e-d53e-4089-a2a1-7a8ebd99df05" satisfied condition "Succeeded or Failed"
Sep 20 12:15:10.223: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-f57c280e-d53e-4089-a2a1-7a8ebd99df05 container test-container: <nil>
STEP: delete the pod 09/20/23 12:15:10.229
Sep 20 12:15:10.360: INFO: Waiting for pod pod-f57c280e-d53e-4089-a2a1-7a8ebd99df05 to disappear
Sep 20 12:15:10.367: INFO: Pod pod-f57c280e-d53e-4089-a2a1-7a8ebd99df05 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep 20 12:15:10.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7039" for this suite. 09/20/23 12:15:10.375
------------------------------
â€¢ [SLOW TEST] [10.802 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:14:59.651
    Sep 20 12:14:59.651: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename emptydir 09/20/23 12:14:59.652
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:14:59.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:14:59.729
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 09/20/23 12:14:59.736
    Sep 20 12:15:00.183: INFO: Waiting up to 5m0s for pod "pod-f57c280e-d53e-4089-a2a1-7a8ebd99df05" in namespace "emptydir-7039" to be "Succeeded or Failed"
    Sep 20 12:15:00.213: INFO: Pod "pod-f57c280e-d53e-4089-a2a1-7a8ebd99df05": Phase="Pending", Reason="", readiness=false. Elapsed: 30.17159ms
    Sep 20 12:15:03.615: INFO: Pod "pod-f57c280e-d53e-4089-a2a1-7a8ebd99df05": Phase="Pending", Reason="", readiness=false. Elapsed: 3.431766867s
    Sep 20 12:15:04.592: INFO: Pod "pod-f57c280e-d53e-4089-a2a1-7a8ebd99df05": Phase="Pending", Reason="", readiness=false. Elapsed: 4.408757021s
    Sep 20 12:15:06.273: INFO: Pod "pod-f57c280e-d53e-4089-a2a1-7a8ebd99df05": Phase="Running", Reason="", readiness=true. Elapsed: 6.090079315s
    Sep 20 12:15:09.298: INFO: Pod "pod-f57c280e-d53e-4089-a2a1-7a8ebd99df05": Phase="Running", Reason="", readiness=false. Elapsed: 9.115266227s
    Sep 20 12:15:10.220: INFO: Pod "pod-f57c280e-d53e-4089-a2a1-7a8ebd99df05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.036817571s
    STEP: Saw pod success 09/20/23 12:15:10.22
    Sep 20 12:15:10.220: INFO: Pod "pod-f57c280e-d53e-4089-a2a1-7a8ebd99df05" satisfied condition "Succeeded or Failed"
    Sep 20 12:15:10.223: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-f57c280e-d53e-4089-a2a1-7a8ebd99df05 container test-container: <nil>
    STEP: delete the pod 09/20/23 12:15:10.229
    Sep 20 12:15:10.360: INFO: Waiting for pod pod-f57c280e-d53e-4089-a2a1-7a8ebd99df05 to disappear
    Sep 20 12:15:10.367: INFO: Pod pod-f57c280e-d53e-4089-a2a1-7a8ebd99df05 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:15:10.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7039" for this suite. 09/20/23 12:15:10.375
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:15:10.455
Sep 20 12:15:10.455: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename projected 09/20/23 12:15:10.456
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:15:10.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:15:10.52
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-49fac83c-a2b9-46cf-a0ee-998f4bc34979 09/20/23 12:15:10.524
STEP: Creating a pod to test consume configMaps 09/20/23 12:15:10.529
Sep 20 12:15:10.540: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f2e399ce-ef90-4b85-b81f-75d92db00a15" in namespace "projected-1341" to be "Succeeded or Failed"
Sep 20 12:15:10.545: INFO: Pod "pod-projected-configmaps-f2e399ce-ef90-4b85-b81f-75d92db00a15": Phase="Pending", Reason="", readiness=false. Elapsed: 5.185638ms
Sep 20 12:15:12.574: INFO: Pod "pod-projected-configmaps-f2e399ce-ef90-4b85-b81f-75d92db00a15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034305837s
Sep 20 12:15:14.763: INFO: Pod "pod-projected-configmaps-f2e399ce-ef90-4b85-b81f-75d92db00a15": Phase="Pending", Reason="", readiness=false. Elapsed: 4.222604454s
Sep 20 12:15:16.550: INFO: Pod "pod-projected-configmaps-f2e399ce-ef90-4b85-b81f-75d92db00a15": Phase="Running", Reason="", readiness=false. Elapsed: 6.00982873s
Sep 20 12:15:19.234: INFO: Pod "pod-projected-configmaps-f2e399ce-ef90-4b85-b81f-75d92db00a15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.694163102s
STEP: Saw pod success 09/20/23 12:15:19.234
Sep 20 12:15:19.235: INFO: Pod "pod-projected-configmaps-f2e399ce-ef90-4b85-b81f-75d92db00a15" satisfied condition "Succeeded or Failed"
Sep 20 12:15:19.291: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-projected-configmaps-f2e399ce-ef90-4b85-b81f-75d92db00a15 container agnhost-container: <nil>
STEP: delete the pod 09/20/23 12:15:19.311
Sep 20 12:15:19.603: INFO: Waiting for pod pod-projected-configmaps-f2e399ce-ef90-4b85-b81f-75d92db00a15 to disappear
Sep 20 12:15:19.608: INFO: Pod pod-projected-configmaps-f2e399ce-ef90-4b85-b81f-75d92db00a15 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep 20 12:15:19.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1341" for this suite. 09/20/23 12:15:19.615
------------------------------
â€¢ [SLOW TEST] [9.391 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:15:10.455
    Sep 20 12:15:10.455: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename projected 09/20/23 12:15:10.456
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:15:10.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:15:10.52
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-49fac83c-a2b9-46cf-a0ee-998f4bc34979 09/20/23 12:15:10.524
    STEP: Creating a pod to test consume configMaps 09/20/23 12:15:10.529
    Sep 20 12:15:10.540: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f2e399ce-ef90-4b85-b81f-75d92db00a15" in namespace "projected-1341" to be "Succeeded or Failed"
    Sep 20 12:15:10.545: INFO: Pod "pod-projected-configmaps-f2e399ce-ef90-4b85-b81f-75d92db00a15": Phase="Pending", Reason="", readiness=false. Elapsed: 5.185638ms
    Sep 20 12:15:12.574: INFO: Pod "pod-projected-configmaps-f2e399ce-ef90-4b85-b81f-75d92db00a15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034305837s
    Sep 20 12:15:14.763: INFO: Pod "pod-projected-configmaps-f2e399ce-ef90-4b85-b81f-75d92db00a15": Phase="Pending", Reason="", readiness=false. Elapsed: 4.222604454s
    Sep 20 12:15:16.550: INFO: Pod "pod-projected-configmaps-f2e399ce-ef90-4b85-b81f-75d92db00a15": Phase="Running", Reason="", readiness=false. Elapsed: 6.00982873s
    Sep 20 12:15:19.234: INFO: Pod "pod-projected-configmaps-f2e399ce-ef90-4b85-b81f-75d92db00a15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.694163102s
    STEP: Saw pod success 09/20/23 12:15:19.234
    Sep 20 12:15:19.235: INFO: Pod "pod-projected-configmaps-f2e399ce-ef90-4b85-b81f-75d92db00a15" satisfied condition "Succeeded or Failed"
    Sep 20 12:15:19.291: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-projected-configmaps-f2e399ce-ef90-4b85-b81f-75d92db00a15 container agnhost-container: <nil>
    STEP: delete the pod 09/20/23 12:15:19.311
    Sep 20 12:15:19.603: INFO: Waiting for pod pod-projected-configmaps-f2e399ce-ef90-4b85-b81f-75d92db00a15 to disappear
    Sep 20 12:15:19.608: INFO: Pod pod-projected-configmaps-f2e399ce-ef90-4b85-b81f-75d92db00a15 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:15:19.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1341" for this suite. 09/20/23 12:15:19.615
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:15:19.849
Sep 20 12:15:19.849: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename projected 09/20/23 12:15:19.85
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:15:20.522
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:15:20.528
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-0a2ae1c8-51ab-4aa1-9ba2-a3e1ae20610a 09/20/23 12:15:20.533
STEP: Creating a pod to test consume secrets 09/20/23 12:15:20.541
Sep 20 12:15:20.558: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ebb3e31e-fec5-4bcb-b172-bf3d12d7f422" in namespace "projected-1185" to be "Succeeded or Failed"
Sep 20 12:15:20.571: INFO: Pod "pod-projected-secrets-ebb3e31e-fec5-4bcb-b172-bf3d12d7f422": Phase="Pending", Reason="", readiness=false. Elapsed: 12.372459ms
Sep 20 12:15:22.576: INFO: Pod "pod-projected-secrets-ebb3e31e-fec5-4bcb-b172-bf3d12d7f422": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017234657s
Sep 20 12:15:25.471: INFO: Pod "pod-projected-secrets-ebb3e31e-fec5-4bcb-b172-bf3d12d7f422": Phase="Pending", Reason="", readiness=false. Elapsed: 4.912496891s
Sep 20 12:15:26.776: INFO: Pod "pod-projected-secrets-ebb3e31e-fec5-4bcb-b172-bf3d12d7f422": Phase="Running", Reason="", readiness=false. Elapsed: 6.217570741s
Sep 20 12:15:28.582: INFO: Pod "pod-projected-secrets-ebb3e31e-fec5-4bcb-b172-bf3d12d7f422": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.023497451s
STEP: Saw pod success 09/20/23 12:15:28.582
Sep 20 12:15:28.582: INFO: Pod "pod-projected-secrets-ebb3e31e-fec5-4bcb-b172-bf3d12d7f422" satisfied condition "Succeeded or Failed"
Sep 20 12:15:28.585: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-projected-secrets-ebb3e31e-fec5-4bcb-b172-bf3d12d7f422 container projected-secret-volume-test: <nil>
STEP: delete the pod 09/20/23 12:15:28.594
Sep 20 12:15:28.670: INFO: Waiting for pod pod-projected-secrets-ebb3e31e-fec5-4bcb-b172-bf3d12d7f422 to disappear
Sep 20 12:15:28.673: INFO: Pod pod-projected-secrets-ebb3e31e-fec5-4bcb-b172-bf3d12d7f422 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep 20 12:15:28.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1185" for this suite. 09/20/23 12:15:28.677
------------------------------
â€¢ [SLOW TEST] [8.879 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:15:19.849
    Sep 20 12:15:19.849: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename projected 09/20/23 12:15:19.85
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:15:20.522
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:15:20.528
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-0a2ae1c8-51ab-4aa1-9ba2-a3e1ae20610a 09/20/23 12:15:20.533
    STEP: Creating a pod to test consume secrets 09/20/23 12:15:20.541
    Sep 20 12:15:20.558: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ebb3e31e-fec5-4bcb-b172-bf3d12d7f422" in namespace "projected-1185" to be "Succeeded or Failed"
    Sep 20 12:15:20.571: INFO: Pod "pod-projected-secrets-ebb3e31e-fec5-4bcb-b172-bf3d12d7f422": Phase="Pending", Reason="", readiness=false. Elapsed: 12.372459ms
    Sep 20 12:15:22.576: INFO: Pod "pod-projected-secrets-ebb3e31e-fec5-4bcb-b172-bf3d12d7f422": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017234657s
    Sep 20 12:15:25.471: INFO: Pod "pod-projected-secrets-ebb3e31e-fec5-4bcb-b172-bf3d12d7f422": Phase="Pending", Reason="", readiness=false. Elapsed: 4.912496891s
    Sep 20 12:15:26.776: INFO: Pod "pod-projected-secrets-ebb3e31e-fec5-4bcb-b172-bf3d12d7f422": Phase="Running", Reason="", readiness=false. Elapsed: 6.217570741s
    Sep 20 12:15:28.582: INFO: Pod "pod-projected-secrets-ebb3e31e-fec5-4bcb-b172-bf3d12d7f422": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.023497451s
    STEP: Saw pod success 09/20/23 12:15:28.582
    Sep 20 12:15:28.582: INFO: Pod "pod-projected-secrets-ebb3e31e-fec5-4bcb-b172-bf3d12d7f422" satisfied condition "Succeeded or Failed"
    Sep 20 12:15:28.585: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-projected-secrets-ebb3e31e-fec5-4bcb-b172-bf3d12d7f422 container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/20/23 12:15:28.594
    Sep 20 12:15:28.670: INFO: Waiting for pod pod-projected-secrets-ebb3e31e-fec5-4bcb-b172-bf3d12d7f422 to disappear
    Sep 20 12:15:28.673: INFO: Pod pod-projected-secrets-ebb3e31e-fec5-4bcb-b172-bf3d12d7f422 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:15:28.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1185" for this suite. 09/20/23 12:15:28.677
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:15:28.728
Sep 20 12:15:28.728: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename svcaccounts 09/20/23 12:15:28.729
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:15:28.754
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:15:28.76
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-2c8cf"  09/20/23 12:15:28.765
Sep 20 12:15:28.772: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-2c8cf"  09/20/23 12:15:28.772
Sep 20 12:15:28.784: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep 20 12:15:28.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-3923" for this suite. 09/20/23 12:15:28.788
------------------------------
â€¢ [0.077 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:15:28.728
    Sep 20 12:15:28.728: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename svcaccounts 09/20/23 12:15:28.729
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:15:28.754
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:15:28.76
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-2c8cf"  09/20/23 12:15:28.765
    Sep 20 12:15:28.772: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-2c8cf"  09/20/23 12:15:28.772
    Sep 20 12:15:28.784: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:15:28.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-3923" for this suite. 09/20/23 12:15:28.788
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:15:28.806
Sep 20 12:15:28.806: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename subpath 09/20/23 12:15:28.807
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:15:28.828
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:15:28.831
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/20/23 12:15:28.836
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-f24w 09/20/23 12:15:28.91
STEP: Creating a pod to test atomic-volume-subpath 09/20/23 12:15:28.91
Sep 20 12:15:28.920: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-f24w" in namespace "subpath-6477" to be "Succeeded or Failed"
Sep 20 12:15:28.924: INFO: Pod "pod-subpath-test-configmap-f24w": Phase="Pending", Reason="", readiness=false. Elapsed: 4.919759ms
Sep 20 12:15:30.930: INFO: Pod "pod-subpath-test-configmap-f24w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010384943s
Sep 20 12:15:33.134: INFO: Pod "pod-subpath-test-configmap-f24w": Phase="Pending", Reason="", readiness=false. Elapsed: 4.21421402s
Sep 20 12:15:41.351: INFO: Pod "pod-subpath-test-configmap-f24w": Phase="Pending", Reason="", readiness=false. Elapsed: 12.431132543s
Sep 20 12:15:44.237: INFO: Pod "pod-subpath-test-configmap-f24w": Phase="Running", Reason="", readiness=true. Elapsed: 15.317912554s
Sep 20 12:15:45.607: INFO: Pod "pod-subpath-test-configmap-f24w": Phase="Running", Reason="", readiness=true. Elapsed: 16.687465284s
Sep 20 12:15:47.398: INFO: Pod "pod-subpath-test-configmap-f24w": Phase="Running", Reason="", readiness=true. Elapsed: 18.478824355s
Sep 20 12:15:49.499: INFO: Pod "pod-subpath-test-configmap-f24w": Phase="Running", Reason="", readiness=true. Elapsed: 20.579264929s
Sep 20 12:15:51.231: INFO: Pod "pod-subpath-test-configmap-f24w": Phase="Running", Reason="", readiness=true. Elapsed: 22.311881445s
Sep 20 12:15:52.936: INFO: Pod "pod-subpath-test-configmap-f24w": Phase="Running", Reason="", readiness=true. Elapsed: 24.016760568s
Sep 20 12:15:55.018: INFO: Pod "pod-subpath-test-configmap-f24w": Phase="Running", Reason="", readiness=true. Elapsed: 26.098527962s
Sep 20 12:15:56.931: INFO: Pod "pod-subpath-test-configmap-f24w": Phase="Running", Reason="", readiness=false. Elapsed: 28.011230013s
Sep 20 12:15:58.932: INFO: Pod "pod-subpath-test-configmap-f24w": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.012192338s
STEP: Saw pod success 09/20/23 12:15:58.932
Sep 20 12:15:58.932: INFO: Pod "pod-subpath-test-configmap-f24w" satisfied condition "Succeeded or Failed"
Sep 20 12:15:58.935: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-subpath-test-configmap-f24w container test-container-subpath-configmap-f24w: <nil>
STEP: delete the pod 09/20/23 12:15:58.942
Sep 20 12:15:58.984: INFO: Waiting for pod pod-subpath-test-configmap-f24w to disappear
Sep 20 12:15:58.990: INFO: Pod pod-subpath-test-configmap-f24w no longer exists
STEP: Deleting pod pod-subpath-test-configmap-f24w 09/20/23 12:15:58.99
Sep 20 12:15:58.991: INFO: Deleting pod "pod-subpath-test-configmap-f24w" in namespace "subpath-6477"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Sep 20 12:15:59.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-6477" for this suite. 09/20/23 12:15:59.014
------------------------------
â€¢ [SLOW TEST] [30.233 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:15:28.806
    Sep 20 12:15:28.806: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename subpath 09/20/23 12:15:28.807
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:15:28.828
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:15:28.831
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/20/23 12:15:28.836
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-f24w 09/20/23 12:15:28.91
    STEP: Creating a pod to test atomic-volume-subpath 09/20/23 12:15:28.91
    Sep 20 12:15:28.920: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-f24w" in namespace "subpath-6477" to be "Succeeded or Failed"
    Sep 20 12:15:28.924: INFO: Pod "pod-subpath-test-configmap-f24w": Phase="Pending", Reason="", readiness=false. Elapsed: 4.919759ms
    Sep 20 12:15:30.930: INFO: Pod "pod-subpath-test-configmap-f24w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010384943s
    Sep 20 12:15:33.134: INFO: Pod "pod-subpath-test-configmap-f24w": Phase="Pending", Reason="", readiness=false. Elapsed: 4.21421402s
    Sep 20 12:15:41.351: INFO: Pod "pod-subpath-test-configmap-f24w": Phase="Pending", Reason="", readiness=false. Elapsed: 12.431132543s
    Sep 20 12:15:44.237: INFO: Pod "pod-subpath-test-configmap-f24w": Phase="Running", Reason="", readiness=true. Elapsed: 15.317912554s
    Sep 20 12:15:45.607: INFO: Pod "pod-subpath-test-configmap-f24w": Phase="Running", Reason="", readiness=true. Elapsed: 16.687465284s
    Sep 20 12:15:47.398: INFO: Pod "pod-subpath-test-configmap-f24w": Phase="Running", Reason="", readiness=true. Elapsed: 18.478824355s
    Sep 20 12:15:49.499: INFO: Pod "pod-subpath-test-configmap-f24w": Phase="Running", Reason="", readiness=true. Elapsed: 20.579264929s
    Sep 20 12:15:51.231: INFO: Pod "pod-subpath-test-configmap-f24w": Phase="Running", Reason="", readiness=true. Elapsed: 22.311881445s
    Sep 20 12:15:52.936: INFO: Pod "pod-subpath-test-configmap-f24w": Phase="Running", Reason="", readiness=true. Elapsed: 24.016760568s
    Sep 20 12:15:55.018: INFO: Pod "pod-subpath-test-configmap-f24w": Phase="Running", Reason="", readiness=true. Elapsed: 26.098527962s
    Sep 20 12:15:56.931: INFO: Pod "pod-subpath-test-configmap-f24w": Phase="Running", Reason="", readiness=false. Elapsed: 28.011230013s
    Sep 20 12:15:58.932: INFO: Pod "pod-subpath-test-configmap-f24w": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.012192338s
    STEP: Saw pod success 09/20/23 12:15:58.932
    Sep 20 12:15:58.932: INFO: Pod "pod-subpath-test-configmap-f24w" satisfied condition "Succeeded or Failed"
    Sep 20 12:15:58.935: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-subpath-test-configmap-f24w container test-container-subpath-configmap-f24w: <nil>
    STEP: delete the pod 09/20/23 12:15:58.942
    Sep 20 12:15:58.984: INFO: Waiting for pod pod-subpath-test-configmap-f24w to disappear
    Sep 20 12:15:58.990: INFO: Pod pod-subpath-test-configmap-f24w no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-f24w 09/20/23 12:15:58.99
    Sep 20 12:15:58.991: INFO: Deleting pod "pod-subpath-test-configmap-f24w" in namespace "subpath-6477"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:15:59.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-6477" for this suite. 09/20/23 12:15:59.014
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:15:59.04
Sep 20 12:15:59.040: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename secrets 09/20/23 12:15:59.04
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:15:59.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:15:59.216
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-257/secret-test-7ec3afc9-09af-4292-bf68-c710eb6f6988 09/20/23 12:15:59.221
STEP: Creating a pod to test consume secrets 09/20/23 12:15:59.356
Sep 20 12:15:59.831: INFO: Waiting up to 5m0s for pod "pod-configmaps-268efe53-7de0-489e-bd91-5803c6b62a5a" in namespace "secrets-257" to be "Succeeded or Failed"
Sep 20 12:16:00.150: INFO: Pod "pod-configmaps-268efe53-7de0-489e-bd91-5803c6b62a5a": Phase="Pending", Reason="", readiness=false. Elapsed: 319.216924ms
Sep 20 12:16:02.157: INFO: Pod "pod-configmaps-268efe53-7de0-489e-bd91-5803c6b62a5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.32539748s
Sep 20 12:16:04.154: INFO: Pod "pod-configmaps-268efe53-7de0-489e-bd91-5803c6b62a5a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.323361948s
Sep 20 12:16:06.156: INFO: Pod "pod-configmaps-268efe53-7de0-489e-bd91-5803c6b62a5a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.32457234s
Sep 20 12:16:08.155: INFO: Pod "pod-configmaps-268efe53-7de0-489e-bd91-5803c6b62a5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.324066038s
STEP: Saw pod success 09/20/23 12:16:08.155
Sep 20 12:16:08.156: INFO: Pod "pod-configmaps-268efe53-7de0-489e-bd91-5803c6b62a5a" satisfied condition "Succeeded or Failed"
Sep 20 12:16:08.159: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-2 pod pod-configmaps-268efe53-7de0-489e-bd91-5803c6b62a5a container env-test: <nil>
STEP: delete the pod 09/20/23 12:16:08.229
Sep 20 12:16:09.017: INFO: Waiting for pod pod-configmaps-268efe53-7de0-489e-bd91-5803c6b62a5a to disappear
Sep 20 12:16:09.022: INFO: Pod pod-configmaps-268efe53-7de0-489e-bd91-5803c6b62a5a no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Sep 20 12:16:09.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-257" for this suite. 09/20/23 12:16:09.027
------------------------------
â€¢ [SLOW TEST] [9.998 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:15:59.04
    Sep 20 12:15:59.040: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename secrets 09/20/23 12:15:59.04
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:15:59.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:15:59.216
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-257/secret-test-7ec3afc9-09af-4292-bf68-c710eb6f6988 09/20/23 12:15:59.221
    STEP: Creating a pod to test consume secrets 09/20/23 12:15:59.356
    Sep 20 12:15:59.831: INFO: Waiting up to 5m0s for pod "pod-configmaps-268efe53-7de0-489e-bd91-5803c6b62a5a" in namespace "secrets-257" to be "Succeeded or Failed"
    Sep 20 12:16:00.150: INFO: Pod "pod-configmaps-268efe53-7de0-489e-bd91-5803c6b62a5a": Phase="Pending", Reason="", readiness=false. Elapsed: 319.216924ms
    Sep 20 12:16:02.157: INFO: Pod "pod-configmaps-268efe53-7de0-489e-bd91-5803c6b62a5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.32539748s
    Sep 20 12:16:04.154: INFO: Pod "pod-configmaps-268efe53-7de0-489e-bd91-5803c6b62a5a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.323361948s
    Sep 20 12:16:06.156: INFO: Pod "pod-configmaps-268efe53-7de0-489e-bd91-5803c6b62a5a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.32457234s
    Sep 20 12:16:08.155: INFO: Pod "pod-configmaps-268efe53-7de0-489e-bd91-5803c6b62a5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.324066038s
    STEP: Saw pod success 09/20/23 12:16:08.155
    Sep 20 12:16:08.156: INFO: Pod "pod-configmaps-268efe53-7de0-489e-bd91-5803c6b62a5a" satisfied condition "Succeeded or Failed"
    Sep 20 12:16:08.159: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-2 pod pod-configmaps-268efe53-7de0-489e-bd91-5803c6b62a5a container env-test: <nil>
    STEP: delete the pod 09/20/23 12:16:08.229
    Sep 20 12:16:09.017: INFO: Waiting for pod pod-configmaps-268efe53-7de0-489e-bd91-5803c6b62a5a to disappear
    Sep 20 12:16:09.022: INFO: Pod pod-configmaps-268efe53-7de0-489e-bd91-5803c6b62a5a no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:16:09.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-257" for this suite. 09/20/23 12:16:09.027
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:16:09.038
Sep 20 12:16:09.038: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename certificates 09/20/23 12:16:09.039
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:16:09.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:16:09.166
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 09/20/23 12:16:09.658
STEP: getting /apis/certificates.k8s.io 09/20/23 12:16:09.663
STEP: getting /apis/certificates.k8s.io/v1 09/20/23 12:16:09.665
STEP: creating 09/20/23 12:16:09.667
STEP: getting 09/20/23 12:16:10.02
STEP: listing 09/20/23 12:16:10.024
STEP: watching 09/20/23 12:16:10.027
Sep 20 12:16:10.027: INFO: starting watch
STEP: patching 09/20/23 12:16:10.029
STEP: updating 09/20/23 12:16:10.926
Sep 20 12:16:10.938: INFO: waiting for watch events with expected annotations
Sep 20 12:16:10.938: INFO: saw patched and updated annotations
STEP: getting /approval 09/20/23 12:16:10.938
STEP: patching /approval 09/20/23 12:16:10.976
STEP: updating /approval 09/20/23 12:16:11.175
STEP: getting /status 09/20/23 12:16:11.718
STEP: patching /status 09/20/23 12:16:11.731
STEP: updating /status 09/20/23 12:16:11.741
STEP: deleting 09/20/23 12:16:11.759
STEP: deleting a collection 09/20/23 12:16:11.772
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep 20 12:16:11.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-7587" for this suite. 09/20/23 12:16:11.816
------------------------------
â€¢ [2.960 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:16:09.038
    Sep 20 12:16:09.038: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename certificates 09/20/23 12:16:09.039
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:16:09.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:16:09.166
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 09/20/23 12:16:09.658
    STEP: getting /apis/certificates.k8s.io 09/20/23 12:16:09.663
    STEP: getting /apis/certificates.k8s.io/v1 09/20/23 12:16:09.665
    STEP: creating 09/20/23 12:16:09.667
    STEP: getting 09/20/23 12:16:10.02
    STEP: listing 09/20/23 12:16:10.024
    STEP: watching 09/20/23 12:16:10.027
    Sep 20 12:16:10.027: INFO: starting watch
    STEP: patching 09/20/23 12:16:10.029
    STEP: updating 09/20/23 12:16:10.926
    Sep 20 12:16:10.938: INFO: waiting for watch events with expected annotations
    Sep 20 12:16:10.938: INFO: saw patched and updated annotations
    STEP: getting /approval 09/20/23 12:16:10.938
    STEP: patching /approval 09/20/23 12:16:10.976
    STEP: updating /approval 09/20/23 12:16:11.175
    STEP: getting /status 09/20/23 12:16:11.718
    STEP: patching /status 09/20/23 12:16:11.731
    STEP: updating /status 09/20/23 12:16:11.741
    STEP: deleting 09/20/23 12:16:11.759
    STEP: deleting a collection 09/20/23 12:16:11.772
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:16:11.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-7587" for this suite. 09/20/23 12:16:11.816
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:16:12.002
Sep 20 12:16:12.002: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename job 09/20/23 12:16:12.003
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:16:12.079
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:16:12.083
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 09/20/23 12:16:12.088
STEP: Ensure pods equal to parallelism count is attached to the job 09/20/23 12:16:12.094
STEP: patching /status 09/20/23 12:16:18.099
STEP: updating /status 09/20/23 12:16:19.341
STEP: get /status 09/20/23 12:16:21.004
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep 20 12:16:21.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-947" for this suite. 09/20/23 12:16:21.019
------------------------------
â€¢ [SLOW TEST] [9.459 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:16:12.002
    Sep 20 12:16:12.002: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename job 09/20/23 12:16:12.003
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:16:12.079
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:16:12.083
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 09/20/23 12:16:12.088
    STEP: Ensure pods equal to parallelism count is attached to the job 09/20/23 12:16:12.094
    STEP: patching /status 09/20/23 12:16:18.099
    STEP: updating /status 09/20/23 12:16:19.341
    STEP: get /status 09/20/23 12:16:21.004
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:16:21.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-947" for this suite. 09/20/23 12:16:21.019
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:16:21.462
Sep 20 12:16:21.462: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename init-container 09/20/23 12:16:21.462
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:16:22.33
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:16:22.334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 09/20/23 12:16:22.338
Sep 20 12:16:22.338: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep 20 12:16:40.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-5126" for this suite. 09/20/23 12:16:40.852
------------------------------
â€¢ [SLOW TEST] [19.402 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:16:21.462
    Sep 20 12:16:21.462: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename init-container 09/20/23 12:16:21.462
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:16:22.33
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:16:22.334
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 09/20/23 12:16:22.338
    Sep 20 12:16:22.338: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:16:40.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-5126" for this suite. 09/20/23 12:16:40.852
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:16:40.865
Sep 20 12:16:40.865: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename security-context-test 09/20/23 12:16:40.866
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:16:40.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:16:40.912
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Sep 20 12:16:40.931: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-b634a604-ad38-4b07-8a9e-bd2461a7fd70" in namespace "security-context-test-4088" to be "Succeeded or Failed"
Sep 20 12:16:40.948: INFO: Pod "busybox-privileged-false-b634a604-ad38-4b07-8a9e-bd2461a7fd70": Phase="Pending", Reason="", readiness=false. Elapsed: 16.743923ms
Sep 20 12:16:42.957: INFO: Pod "busybox-privileged-false-b634a604-ad38-4b07-8a9e-bd2461a7fd70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025035608s
Sep 20 12:16:44.954: INFO: Pod "busybox-privileged-false-b634a604-ad38-4b07-8a9e-bd2461a7fd70": Phase="Running", Reason="", readiness=false. Elapsed: 4.022091224s
Sep 20 12:16:46.957: INFO: Pod "busybox-privileged-false-b634a604-ad38-4b07-8a9e-bd2461a7fd70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025282648s
Sep 20 12:16:46.957: INFO: Pod "busybox-privileged-false-b634a604-ad38-4b07-8a9e-bd2461a7fd70" satisfied condition "Succeeded or Failed"
Sep 20 12:16:46.965: INFO: Got logs for pod "busybox-privileged-false-b634a604-ad38-4b07-8a9e-bd2461a7fd70": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep 20 12:16:46.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-4088" for this suite. 09/20/23 12:16:46.971
------------------------------
â€¢ [SLOW TEST] [6.117 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:16:40.865
    Sep 20 12:16:40.865: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename security-context-test 09/20/23 12:16:40.866
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:16:40.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:16:40.912
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Sep 20 12:16:40.931: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-b634a604-ad38-4b07-8a9e-bd2461a7fd70" in namespace "security-context-test-4088" to be "Succeeded or Failed"
    Sep 20 12:16:40.948: INFO: Pod "busybox-privileged-false-b634a604-ad38-4b07-8a9e-bd2461a7fd70": Phase="Pending", Reason="", readiness=false. Elapsed: 16.743923ms
    Sep 20 12:16:42.957: INFO: Pod "busybox-privileged-false-b634a604-ad38-4b07-8a9e-bd2461a7fd70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025035608s
    Sep 20 12:16:44.954: INFO: Pod "busybox-privileged-false-b634a604-ad38-4b07-8a9e-bd2461a7fd70": Phase="Running", Reason="", readiness=false. Elapsed: 4.022091224s
    Sep 20 12:16:46.957: INFO: Pod "busybox-privileged-false-b634a604-ad38-4b07-8a9e-bd2461a7fd70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025282648s
    Sep 20 12:16:46.957: INFO: Pod "busybox-privileged-false-b634a604-ad38-4b07-8a9e-bd2461a7fd70" satisfied condition "Succeeded or Failed"
    Sep 20 12:16:46.965: INFO: Got logs for pod "busybox-privileged-false-b634a604-ad38-4b07-8a9e-bd2461a7fd70": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:16:46.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-4088" for this suite. 09/20/23 12:16:46.971
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:16:46.985
Sep 20 12:16:46.985: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename emptydir 09/20/23 12:16:46.986
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:16:47.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:16:47.633
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 09/20/23 12:16:47.637
Sep 20 12:16:47.648: INFO: Waiting up to 5m0s for pod "pod-8d5a5089-0bd0-44b9-b470-e362ce7b61ab" in namespace "emptydir-6326" to be "Succeeded or Failed"
Sep 20 12:16:47.653: INFO: Pod "pod-8d5a5089-0bd0-44b9-b470-e362ce7b61ab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.131203ms
Sep 20 12:16:49.659: INFO: Pod "pod-8d5a5089-0bd0-44b9-b470-e362ce7b61ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010264789s
Sep 20 12:16:51.660: INFO: Pod "pod-8d5a5089-0bd0-44b9-b470-e362ce7b61ab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011041525s
Sep 20 12:16:53.774: INFO: Pod "pod-8d5a5089-0bd0-44b9-b470-e362ce7b61ab": Phase="Running", Reason="", readiness=false. Elapsed: 6.12504992s
Sep 20 12:16:55.658: INFO: Pod "pod-8d5a5089-0bd0-44b9-b470-e362ce7b61ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009545352s
STEP: Saw pod success 09/20/23 12:16:55.658
Sep 20 12:16:55.658: INFO: Pod "pod-8d5a5089-0bd0-44b9-b470-e362ce7b61ab" satisfied condition "Succeeded or Failed"
Sep 20 12:16:55.663: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-0 pod pod-8d5a5089-0bd0-44b9-b470-e362ce7b61ab container test-container: <nil>
STEP: delete the pod 09/20/23 12:16:55.751
Sep 20 12:16:55.836: INFO: Waiting for pod pod-8d5a5089-0bd0-44b9-b470-e362ce7b61ab to disappear
Sep 20 12:16:55.842: INFO: Pod pod-8d5a5089-0bd0-44b9-b470-e362ce7b61ab no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep 20 12:16:55.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6326" for this suite. 09/20/23 12:16:55.848
------------------------------
â€¢ [SLOW TEST] [8.872 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:16:46.985
    Sep 20 12:16:46.985: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename emptydir 09/20/23 12:16:46.986
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:16:47.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:16:47.633
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 09/20/23 12:16:47.637
    Sep 20 12:16:47.648: INFO: Waiting up to 5m0s for pod "pod-8d5a5089-0bd0-44b9-b470-e362ce7b61ab" in namespace "emptydir-6326" to be "Succeeded or Failed"
    Sep 20 12:16:47.653: INFO: Pod "pod-8d5a5089-0bd0-44b9-b470-e362ce7b61ab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.131203ms
    Sep 20 12:16:49.659: INFO: Pod "pod-8d5a5089-0bd0-44b9-b470-e362ce7b61ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010264789s
    Sep 20 12:16:51.660: INFO: Pod "pod-8d5a5089-0bd0-44b9-b470-e362ce7b61ab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011041525s
    Sep 20 12:16:53.774: INFO: Pod "pod-8d5a5089-0bd0-44b9-b470-e362ce7b61ab": Phase="Running", Reason="", readiness=false. Elapsed: 6.12504992s
    Sep 20 12:16:55.658: INFO: Pod "pod-8d5a5089-0bd0-44b9-b470-e362ce7b61ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009545352s
    STEP: Saw pod success 09/20/23 12:16:55.658
    Sep 20 12:16:55.658: INFO: Pod "pod-8d5a5089-0bd0-44b9-b470-e362ce7b61ab" satisfied condition "Succeeded or Failed"
    Sep 20 12:16:55.663: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-0 pod pod-8d5a5089-0bd0-44b9-b470-e362ce7b61ab container test-container: <nil>
    STEP: delete the pod 09/20/23 12:16:55.751
    Sep 20 12:16:55.836: INFO: Waiting for pod pod-8d5a5089-0bd0-44b9-b470-e362ce7b61ab to disappear
    Sep 20 12:16:55.842: INFO: Pod pod-8d5a5089-0bd0-44b9-b470-e362ce7b61ab no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:16:55.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6326" for this suite. 09/20/23 12:16:55.848
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:16:55.859
Sep 20 12:16:55.859: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename webhook 09/20/23 12:16:55.859
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:16:56.268
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:16:56.273
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/20/23 12:16:56.472
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/20/23 12:16:56.609
STEP: Deploying the webhook pod 09/20/23 12:16:56.629
STEP: Wait for the deployment to be ready 09/20/23 12:16:56.654
Sep 20 12:16:56.735: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 12:16:58.746: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 16, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 16, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 16, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 16, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 12:17:00.998: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 16, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 16, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 16, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 16, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 12:17:03.548: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 16, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 16, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 16, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 16, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/20/23 12:17:04.753
STEP: Verifying the service has paired with the endpoint 09/20/23 12:17:04.851
Sep 20 12:17:05.851: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 09/20/23 12:17:05.859
STEP: create a namespace for the webhook 09/20/23 12:17:05.993
STEP: create a configmap should be unconditionally rejected by the webhook 09/20/23 12:17:06.001
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep 20 12:17:06.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6481" for this suite. 09/20/23 12:17:07.448
STEP: Destroying namespace "webhook-6481-markers" for this suite. 09/20/23 12:17:07.591
------------------------------
â€¢ [SLOW TEST] [12.222 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:16:55.859
    Sep 20 12:16:55.859: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename webhook 09/20/23 12:16:55.859
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:16:56.268
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:16:56.273
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/20/23 12:16:56.472
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/20/23 12:16:56.609
    STEP: Deploying the webhook pod 09/20/23 12:16:56.629
    STEP: Wait for the deployment to be ready 09/20/23 12:16:56.654
    Sep 20 12:16:56.735: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Sep 20 12:16:58.746: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 16, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 16, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 16, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 16, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 12:17:00.998: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 16, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 16, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 16, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 16, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 12:17:03.548: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 16, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 16, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 16, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 16, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/20/23 12:17:04.753
    STEP: Verifying the service has paired with the endpoint 09/20/23 12:17:04.851
    Sep 20 12:17:05.851: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 09/20/23 12:17:05.859
    STEP: create a namespace for the webhook 09/20/23 12:17:05.993
    STEP: create a configmap should be unconditionally rejected by the webhook 09/20/23 12:17:06.001
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:17:06.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6481" for this suite. 09/20/23 12:17:07.448
    STEP: Destroying namespace "webhook-6481-markers" for this suite. 09/20/23 12:17:07.591
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:17:08.081
Sep 20 12:17:08.081: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename endpointslice 09/20/23 12:17:08.081
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:17:11.849
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:17:11.86
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Sep 20 12:17:12.717: INFO: Endpoints addresses: [192.168.10.108 192.168.10.121 192.168.10.155] , ports: [6443]
Sep 20 12:17:12.717: INFO: EndpointSlices addresses: [192.168.10.108 192.168.10.121 192.168.10.155] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Sep 20 12:17:12.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-711" for this suite. 09/20/23 12:17:12.74
------------------------------
â€¢ [4.676 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:17:08.081
    Sep 20 12:17:08.081: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename endpointslice 09/20/23 12:17:08.081
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:17:11.849
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:17:11.86
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Sep 20 12:17:12.717: INFO: Endpoints addresses: [192.168.10.108 192.168.10.121 192.168.10.155] , ports: [6443]
    Sep 20 12:17:12.717: INFO: EndpointSlices addresses: [192.168.10.108 192.168.10.121 192.168.10.155] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:17:12.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-711" for this suite. 09/20/23 12:17:12.74
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:17:12.759
Sep 20 12:17:12.759: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename kubectl 09/20/23 12:17:12.76
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:17:13.207
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:17:13.213
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/20/23 12:17:13.24
Sep 20 12:17:13.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-910 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Sep 20 12:17:13.680: INFO: stderr: ""
Sep 20 12:17:13.680: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 09/20/23 12:17:13.68
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Sep 20 12:17:14.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-910 delete pods e2e-test-httpd-pod'
Sep 20 12:17:21.040: INFO: stderr: ""
Sep 20 12:17:21.040: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep 20 12:17:21.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-910" for this suite. 09/20/23 12:17:21.046
------------------------------
â€¢ [SLOW TEST] [8.297 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:17:12.759
    Sep 20 12:17:12.759: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename kubectl 09/20/23 12:17:12.76
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:17:13.207
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:17:13.213
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/20/23 12:17:13.24
    Sep 20 12:17:13.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-910 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Sep 20 12:17:13.680: INFO: stderr: ""
    Sep 20 12:17:13.680: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 09/20/23 12:17:13.68
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Sep 20 12:17:14.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-910 delete pods e2e-test-httpd-pod'
    Sep 20 12:17:21.040: INFO: stderr: ""
    Sep 20 12:17:21.040: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:17:21.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-910" for this suite. 09/20/23 12:17:21.046
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:17:21.057
Sep 20 12:17:21.057: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename container-runtime 09/20/23 12:17:21.058
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:17:21.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:17:21.258
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 09/20/23 12:17:21.264
STEP: wait for the container to reach Succeeded 09/20/23 12:17:21.275
STEP: get the container status 09/20/23 12:17:27.53
STEP: the container should be terminated 09/20/23 12:17:27.535
STEP: the termination message should be set 09/20/23 12:17:27.535
Sep 20 12:17:27.535: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 09/20/23 12:17:27.535
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Sep 20 12:17:27.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-8453" for this suite. 09/20/23 12:17:27.666
------------------------------
â€¢ [SLOW TEST] [6.629 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:17:21.057
    Sep 20 12:17:21.057: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename container-runtime 09/20/23 12:17:21.058
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:17:21.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:17:21.258
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 09/20/23 12:17:21.264
    STEP: wait for the container to reach Succeeded 09/20/23 12:17:21.275
    STEP: get the container status 09/20/23 12:17:27.53
    STEP: the container should be terminated 09/20/23 12:17:27.535
    STEP: the termination message should be set 09/20/23 12:17:27.535
    Sep 20 12:17:27.535: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 09/20/23 12:17:27.535
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:17:27.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-8453" for this suite. 09/20/23 12:17:27.666
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:17:27.691
Sep 20 12:17:27.691: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename resourcequota 09/20/23 12:17:27.691
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:17:27.716
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:17:27.723
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 09/20/23 12:17:27.73
STEP: Getting a ResourceQuota 09/20/23 12:17:27.738
STEP: Updating a ResourceQuota 09/20/23 12:17:27.743
STEP: Verifying a ResourceQuota was modified 09/20/23 12:17:27.76
STEP: Deleting a ResourceQuota 09/20/23 12:17:27.767
STEP: Verifying the deleted ResourceQuota 09/20/23 12:17:28.018
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep 20 12:17:28.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3980" for this suite. 09/20/23 12:17:28.027
------------------------------
â€¢ [0.345 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:17:27.691
    Sep 20 12:17:27.691: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename resourcequota 09/20/23 12:17:27.691
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:17:27.716
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:17:27.723
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 09/20/23 12:17:27.73
    STEP: Getting a ResourceQuota 09/20/23 12:17:27.738
    STEP: Updating a ResourceQuota 09/20/23 12:17:27.743
    STEP: Verifying a ResourceQuota was modified 09/20/23 12:17:27.76
    STEP: Deleting a ResourceQuota 09/20/23 12:17:27.767
    STEP: Verifying the deleted ResourceQuota 09/20/23 12:17:28.018
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:17:28.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3980" for this suite. 09/20/23 12:17:28.027
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:17:28.037
Sep 20 12:17:28.038: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename deployment 09/20/23 12:17:28.038
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:17:28.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:17:28.136
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Sep 20 12:17:28.390: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Sep 20 12:17:33.795: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/20/23 12:17:33.795
Sep 20 12:17:33.795: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 09/20/23 12:17:34.505
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep 20 12:17:40.739: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-5251  47cf7db9-1e13-42a2-8a79-090076c2f3f3 8316 1 2023-09-20 12:17:33 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-09-20 12:17:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 12:17:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002c44088 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-20 12:17:35 +0000 UTC,LastTransitionTime:2023-09-20 12:17:35 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-7698ff6f6b" has successfully progressed.,LastUpdateTime:2023-09-20 12:17:40 +0000 UTC,LastTransitionTime:2023-09-20 12:17:34 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep 20 12:17:40.742: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-5251  773bbaf5-57a6-4a9b-a527-444636e7a064 8306 1 2023-09-20 12:17:34 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 47cf7db9-1e13-42a2-8a79-090076c2f3f3 0xc002bd1dd7 0xc002bd1dd8}] [] [{kube-controller-manager Update apps/v1 2023-09-20 12:17:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47cf7db9-1e13-42a2-8a79-090076c2f3f3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 12:17:39 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002bd1e88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 20 12:17:40.746: INFO: Pod "test-cleanup-deployment-7698ff6f6b-89dw4" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-89dw4 test-cleanup-deployment-7698ff6f6b- deployment-5251  2680a16c-6271-4c43-91a3-7e92f1a6f670 8304 0 2023-09-20 12:17:34 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b 773bbaf5-57a6-4a9b-a527-444636e7a064 0xc003722207 0xc003722208}] [] [{kube-controller-manager Update v1 2023-09-20 12:17:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"773bbaf5-57a6-4a9b-a527-444636e7a064\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 12:17:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.3.21\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2l8bh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2l8bh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:17:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:17:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:17:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:17:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.172,PodIP:10.100.3.21,StartTime:2023-09-20 12:17:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-20 12:17:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://89194f070bfd9a36d8f9ade38c17b4605c7a4a81571b77aa9e6389b092f0b609,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.3.21,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep 20 12:17:40.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5251" for this suite. 09/20/23 12:17:40.75
------------------------------
â€¢ [SLOW TEST] [12.731 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:17:28.037
    Sep 20 12:17:28.038: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename deployment 09/20/23 12:17:28.038
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:17:28.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:17:28.136
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Sep 20 12:17:28.390: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Sep 20 12:17:33.795: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/20/23 12:17:33.795
    Sep 20 12:17:33.795: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 09/20/23 12:17:34.505
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep 20 12:17:40.739: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-5251  47cf7db9-1e13-42a2-8a79-090076c2f3f3 8316 1 2023-09-20 12:17:33 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-09-20 12:17:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 12:17:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002c44088 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-20 12:17:35 +0000 UTC,LastTransitionTime:2023-09-20 12:17:35 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-7698ff6f6b" has successfully progressed.,LastUpdateTime:2023-09-20 12:17:40 +0000 UTC,LastTransitionTime:2023-09-20 12:17:34 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Sep 20 12:17:40.742: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-5251  773bbaf5-57a6-4a9b-a527-444636e7a064 8306 1 2023-09-20 12:17:34 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 47cf7db9-1e13-42a2-8a79-090076c2f3f3 0xc002bd1dd7 0xc002bd1dd8}] [] [{kube-controller-manager Update apps/v1 2023-09-20 12:17:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47cf7db9-1e13-42a2-8a79-090076c2f3f3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 12:17:39 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002bd1e88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep 20 12:17:40.746: INFO: Pod "test-cleanup-deployment-7698ff6f6b-89dw4" is available:
    &Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-89dw4 test-cleanup-deployment-7698ff6f6b- deployment-5251  2680a16c-6271-4c43-91a3-7e92f1a6f670 8304 0 2023-09-20 12:17:34 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b 773bbaf5-57a6-4a9b-a527-444636e7a064 0xc003722207 0xc003722208}] [] [{kube-controller-manager Update v1 2023-09-20 12:17:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"773bbaf5-57a6-4a9b-a527-444636e7a064\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 12:17:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.3.21\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2l8bh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2l8bh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:17:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:17:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:17:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:17:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.172,PodIP:10.100.3.21,StartTime:2023-09-20 12:17:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-20 12:17:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://89194f070bfd9a36d8f9ade38c17b4605c7a4a81571b77aa9e6389b092f0b609,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.3.21,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:17:40.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5251" for this suite. 09/20/23 12:17:40.75
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:17:40.77
Sep 20 12:17:40.771: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename prestop 09/20/23 12:17:40.772
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:17:41.318
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:17:41.324
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-6939 09/20/23 12:17:41.328
STEP: Waiting for pods to come up. 09/20/23 12:17:41.338
Sep 20 12:17:41.338: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-6939" to be "running"
Sep 20 12:17:41.350: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 11.845596ms
Sep 20 12:17:43.743: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.404963605s
Sep 20 12:17:45.605: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 4.266667911s
Sep 20 12:17:45.605: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-6939 09/20/23 12:17:45.609
Sep 20 12:17:45.668: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-6939" to be "running"
Sep 20 12:17:45.673: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 4.273832ms
Sep 20 12:17:47.796: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.127884294s
Sep 20 12:17:49.676: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00792309s
Sep 20 12:17:51.678: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 6.009689113s
Sep 20 12:17:51.678: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 09/20/23 12:17:51.678
Sep 20 12:17:56.699: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 09/20/23 12:17:56.7
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Sep 20 12:17:56.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-6939" for this suite. 09/20/23 12:17:56.896
------------------------------
â€¢ [SLOW TEST] [16.136 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:17:40.77
    Sep 20 12:17:40.771: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename prestop 09/20/23 12:17:40.772
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:17:41.318
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:17:41.324
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-6939 09/20/23 12:17:41.328
    STEP: Waiting for pods to come up. 09/20/23 12:17:41.338
    Sep 20 12:17:41.338: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-6939" to be "running"
    Sep 20 12:17:41.350: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 11.845596ms
    Sep 20 12:17:43.743: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.404963605s
    Sep 20 12:17:45.605: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 4.266667911s
    Sep 20 12:17:45.605: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-6939 09/20/23 12:17:45.609
    Sep 20 12:17:45.668: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-6939" to be "running"
    Sep 20 12:17:45.673: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 4.273832ms
    Sep 20 12:17:47.796: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.127884294s
    Sep 20 12:17:49.676: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00792309s
    Sep 20 12:17:51.678: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 6.009689113s
    Sep 20 12:17:51.678: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 09/20/23 12:17:51.678
    Sep 20 12:17:56.699: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 09/20/23 12:17:56.7
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:17:56.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-6939" for this suite. 09/20/23 12:17:56.896
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:17:56.907
Sep 20 12:17:56.907: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename resourcequota 09/20/23 12:17:56.907
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:17:56.942
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:17:56.947
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 09/20/23 12:17:56.954
STEP: Creating a ResourceQuota 09/20/23 12:18:01.965
STEP: Ensuring resource quota status is calculated 09/20/23 12:18:02.053
STEP: Creating a ReplicationController 09/20/23 12:18:04.07
STEP: Ensuring resource quota status captures replication controller creation 09/20/23 12:18:04.805
STEP: Deleting a ReplicationController 09/20/23 12:18:06.812
STEP: Ensuring resource quota status released usage 09/20/23 12:18:06.823
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep 20 12:18:08.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4657" for this suite. 09/20/23 12:18:08.835
------------------------------
â€¢ [SLOW TEST] [11.961 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:17:56.907
    Sep 20 12:17:56.907: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename resourcequota 09/20/23 12:17:56.907
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:17:56.942
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:17:56.947
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 09/20/23 12:17:56.954
    STEP: Creating a ResourceQuota 09/20/23 12:18:01.965
    STEP: Ensuring resource quota status is calculated 09/20/23 12:18:02.053
    STEP: Creating a ReplicationController 09/20/23 12:18:04.07
    STEP: Ensuring resource quota status captures replication controller creation 09/20/23 12:18:04.805
    STEP: Deleting a ReplicationController 09/20/23 12:18:06.812
    STEP: Ensuring resource quota status released usage 09/20/23 12:18:06.823
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:18:08.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4657" for this suite. 09/20/23 12:18:08.835
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:18:08.871
Sep 20 12:18:08.871: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename webhook 09/20/23 12:18:08.872
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:18:09.26
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:18:09.264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/20/23 12:18:09.55
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/20/23 12:18:09.855
STEP: Deploying the webhook pod 09/20/23 12:18:09.867
STEP: Wait for the deployment to be ready 09/20/23 12:18:10.204
Sep 20 12:18:10.388: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 12:18:12.876: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 18, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 18, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 18, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 18, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/20/23 12:18:15.54
STEP: Verifying the service has paired with the endpoint 09/20/23 12:18:15.583
Sep 20 12:18:16.583: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 09/20/23 12:18:16.588
STEP: create a configmap that should be updated by the webhook 09/20/23 12:18:16.701
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep 20 12:18:16.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5874" for this suite. 09/20/23 12:18:16.929
STEP: Destroying namespace "webhook-5874-markers" for this suite. 09/20/23 12:18:16.944
------------------------------
â€¢ [SLOW TEST] [8.087 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:18:08.871
    Sep 20 12:18:08.871: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename webhook 09/20/23 12:18:08.872
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:18:09.26
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:18:09.264
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/20/23 12:18:09.55
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/20/23 12:18:09.855
    STEP: Deploying the webhook pod 09/20/23 12:18:09.867
    STEP: Wait for the deployment to be ready 09/20/23 12:18:10.204
    Sep 20 12:18:10.388: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Sep 20 12:18:12.876: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 18, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 18, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 18, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 18, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/20/23 12:18:15.54
    STEP: Verifying the service has paired with the endpoint 09/20/23 12:18:15.583
    Sep 20 12:18:16.583: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 09/20/23 12:18:16.588
    STEP: create a configmap that should be updated by the webhook 09/20/23 12:18:16.701
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:18:16.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5874" for this suite. 09/20/23 12:18:16.929
    STEP: Destroying namespace "webhook-5874-markers" for this suite. 09/20/23 12:18:16.944
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:18:16.959
Sep 20 12:18:16.959: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename webhook 09/20/23 12:18:16.96
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:18:17.003
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:18:17.008
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/20/23 12:18:17.867
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/20/23 12:18:18.148
STEP: Deploying the webhook pod 09/20/23 12:18:18.157
STEP: Wait for the deployment to be ready 09/20/23 12:18:18.616
Sep 20 12:18:18.630: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 12:18:20.646: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 18, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 18, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 18, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 18, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 12:18:22.723: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 18, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 18, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 18, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 18, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/20/23 12:18:24.746
STEP: Verifying the service has paired with the endpoint 09/20/23 12:18:24.792
Sep 20 12:18:25.792: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 09/20/23 12:18:25.799
STEP: Registering slow webhook via the AdmissionRegistration API 09/20/23 12:18:25.799
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 09/20/23 12:18:26
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 09/20/23 12:18:27.145
STEP: Registering slow webhook via the AdmissionRegistration API 09/20/23 12:18:27.145
STEP: Having no error when timeout is longer than webhook latency 09/20/23 12:18:28.374
STEP: Registering slow webhook via the AdmissionRegistration API 09/20/23 12:18:28.374
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 09/20/23 12:18:33.895
STEP: Registering slow webhook via the AdmissionRegistration API 09/20/23 12:18:33.895
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep 20 12:18:39.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4930" for this suite. 09/20/23 12:18:39.537
STEP: Destroying namespace "webhook-4930-markers" for this suite. 09/20/23 12:18:39.559
------------------------------
â€¢ [SLOW TEST] [22.618 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:18:16.959
    Sep 20 12:18:16.959: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename webhook 09/20/23 12:18:16.96
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:18:17.003
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:18:17.008
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/20/23 12:18:17.867
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/20/23 12:18:18.148
    STEP: Deploying the webhook pod 09/20/23 12:18:18.157
    STEP: Wait for the deployment to be ready 09/20/23 12:18:18.616
    Sep 20 12:18:18.630: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Sep 20 12:18:20.646: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 18, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 18, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 18, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 18, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 12:18:22.723: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 18, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 18, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 18, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 18, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/20/23 12:18:24.746
    STEP: Verifying the service has paired with the endpoint 09/20/23 12:18:24.792
    Sep 20 12:18:25.792: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 09/20/23 12:18:25.799
    STEP: Registering slow webhook via the AdmissionRegistration API 09/20/23 12:18:25.799
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 09/20/23 12:18:26
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 09/20/23 12:18:27.145
    STEP: Registering slow webhook via the AdmissionRegistration API 09/20/23 12:18:27.145
    STEP: Having no error when timeout is longer than webhook latency 09/20/23 12:18:28.374
    STEP: Registering slow webhook via the AdmissionRegistration API 09/20/23 12:18:28.374
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 09/20/23 12:18:33.895
    STEP: Registering slow webhook via the AdmissionRegistration API 09/20/23 12:18:33.895
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:18:39.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4930" for this suite. 09/20/23 12:18:39.537
    STEP: Destroying namespace "webhook-4930-markers" for this suite. 09/20/23 12:18:39.559
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:18:39.58
Sep 20 12:18:39.580: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename csiinlinevolumes 09/20/23 12:18:39.581
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:18:39.622
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:18:39.628
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 09/20/23 12:18:39.635
STEP: getting 09/20/23 12:18:39.678
STEP: listing in namespace 09/20/23 12:18:39.689
STEP: patching 09/20/23 12:18:39.693
STEP: deleting 09/20/23 12:18:39.703
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Sep 20 12:18:39.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-6849" for this suite. 09/20/23 12:18:39.723
------------------------------
â€¢ [0.151 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:18:39.58
    Sep 20 12:18:39.580: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename csiinlinevolumes 09/20/23 12:18:39.581
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:18:39.622
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:18:39.628
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 09/20/23 12:18:39.635
    STEP: getting 09/20/23 12:18:39.678
    STEP: listing in namespace 09/20/23 12:18:39.689
    STEP: patching 09/20/23 12:18:39.693
    STEP: deleting 09/20/23 12:18:39.703
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:18:39.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-6849" for this suite. 09/20/23 12:18:39.723
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:18:39.733
Sep 20 12:18:39.733: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename cronjob 09/20/23 12:18:39.733
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:18:39.802
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:18:39.813
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 09/20/23 12:18:39.819
STEP: Ensuring no jobs are scheduled 09/20/23 12:18:40.199
STEP: Ensuring no job exists by listing jobs explicitly 09/20/23 12:23:40.234
STEP: Removing cronjob 09/20/23 12:23:40.238
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Sep 20 12:23:40.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-2565" for this suite. 09/20/23 12:23:40.252
------------------------------
â€¢ [SLOW TEST] [300.528 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:18:39.733
    Sep 20 12:18:39.733: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename cronjob 09/20/23 12:18:39.733
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:18:39.802
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:18:39.813
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 09/20/23 12:18:39.819
    STEP: Ensuring no jobs are scheduled 09/20/23 12:18:40.199
    STEP: Ensuring no job exists by listing jobs explicitly 09/20/23 12:23:40.234
    STEP: Removing cronjob 09/20/23 12:23:40.238
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:23:40.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-2565" for this suite. 09/20/23 12:23:40.252
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:23:40.261
Sep 20 12:23:40.261: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename kubectl 09/20/23 12:23:40.262
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:23:40.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:23:40.503
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 09/20/23 12:23:40.507
Sep 20 12:23:40.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-542 create -f -'
Sep 20 12:23:41.396: INFO: stderr: ""
Sep 20 12:23:41.396: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 09/20/23 12:23:41.396
Sep 20 12:23:41.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-542 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 20 12:23:41.587: INFO: stderr: ""
Sep 20 12:23:41.587: INFO: stdout: "update-demo-nautilus-v4j6l "
STEP: Replicas for name=update-demo: expected=2 actual=1 09/20/23 12:23:41.587
Sep 20 12:23:46.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-542 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 20 12:23:46.664: INFO: stderr: ""
Sep 20 12:23:46.664: INFO: stdout: "update-demo-nautilus-v4j6l update-demo-nautilus-x4nfb "
Sep 20 12:23:46.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-542 get pods update-demo-nautilus-v4j6l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 20 12:23:46.733: INFO: stderr: ""
Sep 20 12:23:46.733: INFO: stdout: ""
Sep 20 12:23:46.733: INFO: update-demo-nautilus-v4j6l is created but not running
Sep 20 12:23:51.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-542 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 20 12:23:51.818: INFO: stderr: ""
Sep 20 12:23:51.818: INFO: stdout: "update-demo-nautilus-v4j6l update-demo-nautilus-x4nfb "
Sep 20 12:23:51.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-542 get pods update-demo-nautilus-v4j6l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 20 12:23:51.905: INFO: stderr: ""
Sep 20 12:23:51.905: INFO: stdout: ""
Sep 20 12:23:51.905: INFO: update-demo-nautilus-v4j6l is created but not running
Sep 20 12:23:56.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-542 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 20 12:23:57.177: INFO: stderr: ""
Sep 20 12:23:57.177: INFO: stdout: "update-demo-nautilus-v4j6l update-demo-nautilus-x4nfb "
Sep 20 12:23:57.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-542 get pods update-demo-nautilus-v4j6l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 20 12:23:57.570: INFO: stderr: ""
Sep 20 12:23:57.571: INFO: stdout: ""
Sep 20 12:23:57.571: INFO: update-demo-nautilus-v4j6l is created but not running
Sep 20 12:24:02.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-542 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 20 12:24:03.244: INFO: stderr: ""
Sep 20 12:24:03.244: INFO: stdout: "update-demo-nautilus-v4j6l update-demo-nautilus-x4nfb "
Sep 20 12:24:03.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-542 get pods update-demo-nautilus-v4j6l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 20 12:24:03.429: INFO: stderr: ""
Sep 20 12:24:03.429: INFO: stdout: ""
Sep 20 12:24:03.429: INFO: update-demo-nautilus-v4j6l is created but not running
Sep 20 12:24:08.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-542 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 20 12:24:08.690: INFO: stderr: ""
Sep 20 12:24:08.690: INFO: stdout: "update-demo-nautilus-v4j6l update-demo-nautilus-x4nfb "
Sep 20 12:24:08.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-542 get pods update-demo-nautilus-v4j6l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 20 12:24:08.765: INFO: stderr: ""
Sep 20 12:24:08.765: INFO: stdout: "true"
Sep 20 12:24:08.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-542 get pods update-demo-nautilus-v4j6l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 20 12:24:08.832: INFO: stderr: ""
Sep 20 12:24:08.833: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep 20 12:24:08.833: INFO: validating pod update-demo-nautilus-v4j6l
Sep 20 12:24:08.839: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 20 12:24:08.839: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 20 12:24:08.839: INFO: update-demo-nautilus-v4j6l is verified up and running
Sep 20 12:24:08.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-542 get pods update-demo-nautilus-x4nfb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 20 12:24:08.928: INFO: stderr: ""
Sep 20 12:24:08.928: INFO: stdout: "true"
Sep 20 12:24:08.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-542 get pods update-demo-nautilus-x4nfb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 20 12:24:08.994: INFO: stderr: ""
Sep 20 12:24:08.994: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep 20 12:24:08.994: INFO: validating pod update-demo-nautilus-x4nfb
Sep 20 12:24:09.002: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 20 12:24:09.002: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 20 12:24:09.002: INFO: update-demo-nautilus-x4nfb is verified up and running
STEP: using delete to clean up resources 09/20/23 12:24:09.002
Sep 20 12:24:09.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-542 delete --grace-period=0 --force -f -'
Sep 20 12:24:09.586: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 20 12:24:09.586: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep 20 12:24:09.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-542 get rc,svc -l name=update-demo --no-headers'
Sep 20 12:24:09.721: INFO: stderr: "No resources found in kubectl-542 namespace.\n"
Sep 20 12:24:09.721: INFO: stdout: ""
Sep 20 12:24:09.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-542 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 20 12:24:09.809: INFO: stderr: ""
Sep 20 12:24:09.809: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep 20 12:24:09.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-542" for this suite. 09/20/23 12:24:09.813
------------------------------
â€¢ [SLOW TEST] [29.563 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:23:40.261
    Sep 20 12:23:40.261: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename kubectl 09/20/23 12:23:40.262
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:23:40.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:23:40.503
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 09/20/23 12:23:40.507
    Sep 20 12:23:40.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-542 create -f -'
    Sep 20 12:23:41.396: INFO: stderr: ""
    Sep 20 12:23:41.396: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 09/20/23 12:23:41.396
    Sep 20 12:23:41.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-542 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep 20 12:23:41.587: INFO: stderr: ""
    Sep 20 12:23:41.587: INFO: stdout: "update-demo-nautilus-v4j6l "
    STEP: Replicas for name=update-demo: expected=2 actual=1 09/20/23 12:23:41.587
    Sep 20 12:23:46.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-542 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep 20 12:23:46.664: INFO: stderr: ""
    Sep 20 12:23:46.664: INFO: stdout: "update-demo-nautilus-v4j6l update-demo-nautilus-x4nfb "
    Sep 20 12:23:46.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-542 get pods update-demo-nautilus-v4j6l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep 20 12:23:46.733: INFO: stderr: ""
    Sep 20 12:23:46.733: INFO: stdout: ""
    Sep 20 12:23:46.733: INFO: update-demo-nautilus-v4j6l is created but not running
    Sep 20 12:23:51.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-542 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep 20 12:23:51.818: INFO: stderr: ""
    Sep 20 12:23:51.818: INFO: stdout: "update-demo-nautilus-v4j6l update-demo-nautilus-x4nfb "
    Sep 20 12:23:51.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-542 get pods update-demo-nautilus-v4j6l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep 20 12:23:51.905: INFO: stderr: ""
    Sep 20 12:23:51.905: INFO: stdout: ""
    Sep 20 12:23:51.905: INFO: update-demo-nautilus-v4j6l is created but not running
    Sep 20 12:23:56.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-542 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep 20 12:23:57.177: INFO: stderr: ""
    Sep 20 12:23:57.177: INFO: stdout: "update-demo-nautilus-v4j6l update-demo-nautilus-x4nfb "
    Sep 20 12:23:57.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-542 get pods update-demo-nautilus-v4j6l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep 20 12:23:57.570: INFO: stderr: ""
    Sep 20 12:23:57.571: INFO: stdout: ""
    Sep 20 12:23:57.571: INFO: update-demo-nautilus-v4j6l is created but not running
    Sep 20 12:24:02.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-542 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep 20 12:24:03.244: INFO: stderr: ""
    Sep 20 12:24:03.244: INFO: stdout: "update-demo-nautilus-v4j6l update-demo-nautilus-x4nfb "
    Sep 20 12:24:03.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-542 get pods update-demo-nautilus-v4j6l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep 20 12:24:03.429: INFO: stderr: ""
    Sep 20 12:24:03.429: INFO: stdout: ""
    Sep 20 12:24:03.429: INFO: update-demo-nautilus-v4j6l is created but not running
    Sep 20 12:24:08.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-542 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep 20 12:24:08.690: INFO: stderr: ""
    Sep 20 12:24:08.690: INFO: stdout: "update-demo-nautilus-v4j6l update-demo-nautilus-x4nfb "
    Sep 20 12:24:08.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-542 get pods update-demo-nautilus-v4j6l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep 20 12:24:08.765: INFO: stderr: ""
    Sep 20 12:24:08.765: INFO: stdout: "true"
    Sep 20 12:24:08.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-542 get pods update-demo-nautilus-v4j6l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep 20 12:24:08.832: INFO: stderr: ""
    Sep 20 12:24:08.833: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep 20 12:24:08.833: INFO: validating pod update-demo-nautilus-v4j6l
    Sep 20 12:24:08.839: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep 20 12:24:08.839: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep 20 12:24:08.839: INFO: update-demo-nautilus-v4j6l is verified up and running
    Sep 20 12:24:08.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-542 get pods update-demo-nautilus-x4nfb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep 20 12:24:08.928: INFO: stderr: ""
    Sep 20 12:24:08.928: INFO: stdout: "true"
    Sep 20 12:24:08.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-542 get pods update-demo-nautilus-x4nfb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep 20 12:24:08.994: INFO: stderr: ""
    Sep 20 12:24:08.994: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep 20 12:24:08.994: INFO: validating pod update-demo-nautilus-x4nfb
    Sep 20 12:24:09.002: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep 20 12:24:09.002: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep 20 12:24:09.002: INFO: update-demo-nautilus-x4nfb is verified up and running
    STEP: using delete to clean up resources 09/20/23 12:24:09.002
    Sep 20 12:24:09.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-542 delete --grace-period=0 --force -f -'
    Sep 20 12:24:09.586: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep 20 12:24:09.586: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Sep 20 12:24:09.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-542 get rc,svc -l name=update-demo --no-headers'
    Sep 20 12:24:09.721: INFO: stderr: "No resources found in kubectl-542 namespace.\n"
    Sep 20 12:24:09.721: INFO: stdout: ""
    Sep 20 12:24:09.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-542 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Sep 20 12:24:09.809: INFO: stderr: ""
    Sep 20 12:24:09.809: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:24:09.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-542" for this suite. 09/20/23 12:24:09.813
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:24:09.825
Sep 20 12:24:09.825: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename gc 09/20/23 12:24:09.826
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:24:09.848
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:24:09.85
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 09/20/23 12:24:10.029
STEP: delete the rc 09/20/23 12:24:15.419
STEP: wait for the rc to be deleted 09/20/23 12:24:15.587
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 09/20/23 12:24:20.592
STEP: Gathering metrics 09/20/23 12:24:50.881
W0920 12:24:50.894360      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Sep 20 12:24:50.894: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Sep 20 12:24:50.894: INFO: Deleting pod "simpletest.rc-2nd9h" in namespace "gc-1282"
Sep 20 12:24:50.935: INFO: Deleting pod "simpletest.rc-2sv27" in namespace "gc-1282"
Sep 20 12:24:50.951: INFO: Deleting pod "simpletest.rc-2whbn" in namespace "gc-1282"
Sep 20 12:24:51.215: INFO: Deleting pod "simpletest.rc-2ztgk" in namespace "gc-1282"
Sep 20 12:24:51.263: INFO: Deleting pod "simpletest.rc-494wp" in namespace "gc-1282"
Sep 20 12:24:51.287: INFO: Deleting pod "simpletest.rc-4grgm" in namespace "gc-1282"
Sep 20 12:24:51.309: INFO: Deleting pod "simpletest.rc-4vmvz" in namespace "gc-1282"
Sep 20 12:24:51.680: INFO: Deleting pod "simpletest.rc-5mcxr" in namespace "gc-1282"
Sep 20 12:24:51.694: INFO: Deleting pod "simpletest.rc-6w6mq" in namespace "gc-1282"
Sep 20 12:24:51.782: INFO: Deleting pod "simpletest.rc-6xhq2" in namespace "gc-1282"
Sep 20 12:24:51.832: INFO: Deleting pod "simpletest.rc-749b9" in namespace "gc-1282"
Sep 20 12:24:51.847: INFO: Deleting pod "simpletest.rc-7m9q6" in namespace "gc-1282"
Sep 20 12:24:51.885: INFO: Deleting pod "simpletest.rc-7rnm5" in namespace "gc-1282"
Sep 20 12:24:52.098: INFO: Deleting pod "simpletest.rc-7szkj" in namespace "gc-1282"
Sep 20 12:24:52.132: INFO: Deleting pod "simpletest.rc-7v672" in namespace "gc-1282"
Sep 20 12:24:52.154: INFO: Deleting pod "simpletest.rc-8k2gc" in namespace "gc-1282"
Sep 20 12:24:52.720: INFO: Deleting pod "simpletest.rc-8pzdz" in namespace "gc-1282"
Sep 20 12:24:52.775: INFO: Deleting pod "simpletest.rc-8r6jf" in namespace "gc-1282"
Sep 20 12:24:52.799: INFO: Deleting pod "simpletest.rc-8wwnp" in namespace "gc-1282"
Sep 20 12:24:53.201: INFO: Deleting pod "simpletest.rc-8zhp6" in namespace "gc-1282"
Sep 20 12:24:53.230: INFO: Deleting pod "simpletest.rc-9gw28" in namespace "gc-1282"
Sep 20 12:24:53.248: INFO: Deleting pod "simpletest.rc-9vd5d" in namespace "gc-1282"
Sep 20 12:24:53.270: INFO: Deleting pod "simpletest.rc-b77h8" in namespace "gc-1282"
Sep 20 12:24:53.376: INFO: Deleting pod "simpletest.rc-b7cg9" in namespace "gc-1282"
Sep 20 12:24:53.394: INFO: Deleting pod "simpletest.rc-bc86w" in namespace "gc-1282"
Sep 20 12:24:53.409: INFO: Deleting pod "simpletest.rc-bh2nx" in namespace "gc-1282"
Sep 20 12:24:53.427: INFO: Deleting pod "simpletest.rc-bkn58" in namespace "gc-1282"
Sep 20 12:24:53.443: INFO: Deleting pod "simpletest.rc-bp7h7" in namespace "gc-1282"
Sep 20 12:24:53.460: INFO: Deleting pod "simpletest.rc-bzf96" in namespace "gc-1282"
Sep 20 12:24:53.840: INFO: Deleting pod "simpletest.rc-c4kt9" in namespace "gc-1282"
Sep 20 12:24:54.292: INFO: Deleting pod "simpletest.rc-cd7fg" in namespace "gc-1282"
Sep 20 12:24:54.316: INFO: Deleting pod "simpletest.rc-cxkzn" in namespace "gc-1282"
Sep 20 12:24:54.337: INFO: Deleting pod "simpletest.rc-dbmps" in namespace "gc-1282"
Sep 20 12:24:54.356: INFO: Deleting pod "simpletest.rc-dlz5q" in namespace "gc-1282"
Sep 20 12:24:54.375: INFO: Deleting pod "simpletest.rc-dwwhg" in namespace "gc-1282"
Sep 20 12:24:54.397: INFO: Deleting pod "simpletest.rc-fs2tn" in namespace "gc-1282"
Sep 20 12:24:54.445: INFO: Deleting pod "simpletest.rc-g5tdn" in namespace "gc-1282"
Sep 20 12:24:54.465: INFO: Deleting pod "simpletest.rc-gj4ds" in namespace "gc-1282"
Sep 20 12:24:54.512: INFO: Deleting pod "simpletest.rc-glrhx" in namespace "gc-1282"
Sep 20 12:24:54.531: INFO: Deleting pod "simpletest.rc-gt6lv" in namespace "gc-1282"
Sep 20 12:24:54.558: INFO: Deleting pod "simpletest.rc-gvkfj" in namespace "gc-1282"
Sep 20 12:24:54.575: INFO: Deleting pod "simpletest.rc-hggz5" in namespace "gc-1282"
Sep 20 12:24:54.885: INFO: Deleting pod "simpletest.rc-hgv98" in namespace "gc-1282"
Sep 20 12:24:55.176: INFO: Deleting pod "simpletest.rc-j4vsc" in namespace "gc-1282"
Sep 20 12:24:55.216: INFO: Deleting pod "simpletest.rc-jhkxb" in namespace "gc-1282"
Sep 20 12:24:55.244: INFO: Deleting pod "simpletest.rc-k5j28" in namespace "gc-1282"
Sep 20 12:24:55.291: INFO: Deleting pod "simpletest.rc-k86j5" in namespace "gc-1282"
Sep 20 12:24:55.324: INFO: Deleting pod "simpletest.rc-kdfsj" in namespace "gc-1282"
Sep 20 12:24:55.345: INFO: Deleting pod "simpletest.rc-kglm5" in namespace "gc-1282"
Sep 20 12:24:55.368: INFO: Deleting pod "simpletest.rc-kqj9v" in namespace "gc-1282"
Sep 20 12:24:55.410: INFO: Deleting pod "simpletest.rc-kt66g" in namespace "gc-1282"
Sep 20 12:24:55.441: INFO: Deleting pod "simpletest.rc-l47p5" in namespace "gc-1282"
Sep 20 12:24:55.464: INFO: Deleting pod "simpletest.rc-l6c2v" in namespace "gc-1282"
Sep 20 12:24:55.484: INFO: Deleting pod "simpletest.rc-l72t9" in namespace "gc-1282"
Sep 20 12:24:55.509: INFO: Deleting pod "simpletest.rc-lwh7t" in namespace "gc-1282"
Sep 20 12:24:55.528: INFO: Deleting pod "simpletest.rc-m7wll" in namespace "gc-1282"
Sep 20 12:24:55.561: INFO: Deleting pod "simpletest.rc-m9dsw" in namespace "gc-1282"
Sep 20 12:24:55.609: INFO: Deleting pod "simpletest.rc-ms6nn" in namespace "gc-1282"
Sep 20 12:24:55.650: INFO: Deleting pod "simpletest.rc-nk5hq" in namespace "gc-1282"
Sep 20 12:24:55.671: INFO: Deleting pod "simpletest.rc-npz7t" in namespace "gc-1282"
Sep 20 12:24:55.743: INFO: Deleting pod "simpletest.rc-nqbkc" in namespace "gc-1282"
Sep 20 12:24:56.358: INFO: Deleting pod "simpletest.rc-nqvcj" in namespace "gc-1282"
Sep 20 12:24:56.386: INFO: Deleting pod "simpletest.rc-nr9fd" in namespace "gc-1282"
Sep 20 12:24:56.408: INFO: Deleting pod "simpletest.rc-ntkwz" in namespace "gc-1282"
Sep 20 12:24:56.427: INFO: Deleting pod "simpletest.rc-pdccm" in namespace "gc-1282"
Sep 20 12:24:57.886: INFO: Deleting pod "simpletest.rc-pqzp4" in namespace "gc-1282"
Sep 20 12:24:59.070: INFO: Deleting pod "simpletest.rc-ps684" in namespace "gc-1282"
Sep 20 12:24:59.876: INFO: Deleting pod "simpletest.rc-pzx94" in namespace "gc-1282"
Sep 20 12:24:59.913: INFO: Deleting pod "simpletest.rc-qc8lt" in namespace "gc-1282"
Sep 20 12:24:59.931: INFO: Deleting pod "simpletest.rc-qdb24" in namespace "gc-1282"
Sep 20 12:25:00.984: INFO: Deleting pod "simpletest.rc-qhqxg" in namespace "gc-1282"
Sep 20 12:25:01.544: INFO: Deleting pod "simpletest.rc-qkst5" in namespace "gc-1282"
Sep 20 12:25:01.772: INFO: Deleting pod "simpletest.rc-qlhqw" in namespace "gc-1282"
Sep 20 12:25:02.721: INFO: Deleting pod "simpletest.rc-qrsr4" in namespace "gc-1282"
Sep 20 12:25:03.204: INFO: Deleting pod "simpletest.rc-r22v2" in namespace "gc-1282"
Sep 20 12:25:03.260: INFO: Deleting pod "simpletest.rc-r4cnj" in namespace "gc-1282"
Sep 20 12:25:03.415: INFO: Deleting pod "simpletest.rc-r775n" in namespace "gc-1282"
Sep 20 12:25:03.455: INFO: Deleting pod "simpletest.rc-r8494" in namespace "gc-1282"
Sep 20 12:25:03.479: INFO: Deleting pod "simpletest.rc-r88mm" in namespace "gc-1282"
Sep 20 12:25:03.526: INFO: Deleting pod "simpletest.rc-rldhg" in namespace "gc-1282"
Sep 20 12:25:03.571: INFO: Deleting pod "simpletest.rc-rtbjp" in namespace "gc-1282"
Sep 20 12:25:03.590: INFO: Deleting pod "simpletest.rc-sdhpj" in namespace "gc-1282"
Sep 20 12:25:03.984: INFO: Deleting pod "simpletest.rc-sjqc4" in namespace "gc-1282"
Sep 20 12:25:04.118: INFO: Deleting pod "simpletest.rc-snntv" in namespace "gc-1282"
Sep 20 12:25:04.140: INFO: Deleting pod "simpletest.rc-sxnfp" in namespace "gc-1282"
Sep 20 12:25:04.723: INFO: Deleting pod "simpletest.rc-t2j7s" in namespace "gc-1282"
Sep 20 12:25:05.068: INFO: Deleting pod "simpletest.rc-v7dwg" in namespace "gc-1282"
Sep 20 12:25:05.741: INFO: Deleting pod "simpletest.rc-vn6qx" in namespace "gc-1282"
Sep 20 12:25:05.758: INFO: Deleting pod "simpletest.rc-vnr2t" in namespace "gc-1282"
Sep 20 12:25:07.110: INFO: Deleting pod "simpletest.rc-vz7mh" in namespace "gc-1282"
Sep 20 12:25:07.240: INFO: Deleting pod "simpletest.rc-w78qj" in namespace "gc-1282"
Sep 20 12:25:07.255: INFO: Deleting pod "simpletest.rc-wfcs7" in namespace "gc-1282"
Sep 20 12:25:07.790: INFO: Deleting pod "simpletest.rc-wjzrl" in namespace "gc-1282"
Sep 20 12:25:07.989: INFO: Deleting pod "simpletest.rc-ww594" in namespace "gc-1282"
Sep 20 12:25:08.003: INFO: Deleting pod "simpletest.rc-xt6pc" in namespace "gc-1282"
Sep 20 12:25:08.022: INFO: Deleting pod "simpletest.rc-xtqgl" in namespace "gc-1282"
Sep 20 12:25:08.042: INFO: Deleting pod "simpletest.rc-zhrpw" in namespace "gc-1282"
Sep 20 12:25:08.063: INFO: Deleting pod "simpletest.rc-zlddq" in namespace "gc-1282"
Sep 20 12:25:08.861: INFO: Deleting pod "simpletest.rc-ztmdl" in namespace "gc-1282"
Sep 20 12:25:09.214: INFO: Deleting pod "simpletest.rc-zzvmn" in namespace "gc-1282"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep 20 12:25:09.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1282" for this suite. 09/20/23 12:25:09.244
------------------------------
â€¢ [SLOW TEST] [59.426 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:24:09.825
    Sep 20 12:24:09.825: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename gc 09/20/23 12:24:09.826
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:24:09.848
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:24:09.85
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 09/20/23 12:24:10.029
    STEP: delete the rc 09/20/23 12:24:15.419
    STEP: wait for the rc to be deleted 09/20/23 12:24:15.587
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 09/20/23 12:24:20.592
    STEP: Gathering metrics 09/20/23 12:24:50.881
    W0920 12:24:50.894360      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Sep 20 12:24:50.894: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Sep 20 12:24:50.894: INFO: Deleting pod "simpletest.rc-2nd9h" in namespace "gc-1282"
    Sep 20 12:24:50.935: INFO: Deleting pod "simpletest.rc-2sv27" in namespace "gc-1282"
    Sep 20 12:24:50.951: INFO: Deleting pod "simpletest.rc-2whbn" in namespace "gc-1282"
    Sep 20 12:24:51.215: INFO: Deleting pod "simpletest.rc-2ztgk" in namespace "gc-1282"
    Sep 20 12:24:51.263: INFO: Deleting pod "simpletest.rc-494wp" in namespace "gc-1282"
    Sep 20 12:24:51.287: INFO: Deleting pod "simpletest.rc-4grgm" in namespace "gc-1282"
    Sep 20 12:24:51.309: INFO: Deleting pod "simpletest.rc-4vmvz" in namespace "gc-1282"
    Sep 20 12:24:51.680: INFO: Deleting pod "simpletest.rc-5mcxr" in namespace "gc-1282"
    Sep 20 12:24:51.694: INFO: Deleting pod "simpletest.rc-6w6mq" in namespace "gc-1282"
    Sep 20 12:24:51.782: INFO: Deleting pod "simpletest.rc-6xhq2" in namespace "gc-1282"
    Sep 20 12:24:51.832: INFO: Deleting pod "simpletest.rc-749b9" in namespace "gc-1282"
    Sep 20 12:24:51.847: INFO: Deleting pod "simpletest.rc-7m9q6" in namespace "gc-1282"
    Sep 20 12:24:51.885: INFO: Deleting pod "simpletest.rc-7rnm5" in namespace "gc-1282"
    Sep 20 12:24:52.098: INFO: Deleting pod "simpletest.rc-7szkj" in namespace "gc-1282"
    Sep 20 12:24:52.132: INFO: Deleting pod "simpletest.rc-7v672" in namespace "gc-1282"
    Sep 20 12:24:52.154: INFO: Deleting pod "simpletest.rc-8k2gc" in namespace "gc-1282"
    Sep 20 12:24:52.720: INFO: Deleting pod "simpletest.rc-8pzdz" in namespace "gc-1282"
    Sep 20 12:24:52.775: INFO: Deleting pod "simpletest.rc-8r6jf" in namespace "gc-1282"
    Sep 20 12:24:52.799: INFO: Deleting pod "simpletest.rc-8wwnp" in namespace "gc-1282"
    Sep 20 12:24:53.201: INFO: Deleting pod "simpletest.rc-8zhp6" in namespace "gc-1282"
    Sep 20 12:24:53.230: INFO: Deleting pod "simpletest.rc-9gw28" in namespace "gc-1282"
    Sep 20 12:24:53.248: INFO: Deleting pod "simpletest.rc-9vd5d" in namespace "gc-1282"
    Sep 20 12:24:53.270: INFO: Deleting pod "simpletest.rc-b77h8" in namespace "gc-1282"
    Sep 20 12:24:53.376: INFO: Deleting pod "simpletest.rc-b7cg9" in namespace "gc-1282"
    Sep 20 12:24:53.394: INFO: Deleting pod "simpletest.rc-bc86w" in namespace "gc-1282"
    Sep 20 12:24:53.409: INFO: Deleting pod "simpletest.rc-bh2nx" in namespace "gc-1282"
    Sep 20 12:24:53.427: INFO: Deleting pod "simpletest.rc-bkn58" in namespace "gc-1282"
    Sep 20 12:24:53.443: INFO: Deleting pod "simpletest.rc-bp7h7" in namespace "gc-1282"
    Sep 20 12:24:53.460: INFO: Deleting pod "simpletest.rc-bzf96" in namespace "gc-1282"
    Sep 20 12:24:53.840: INFO: Deleting pod "simpletest.rc-c4kt9" in namespace "gc-1282"
    Sep 20 12:24:54.292: INFO: Deleting pod "simpletest.rc-cd7fg" in namespace "gc-1282"
    Sep 20 12:24:54.316: INFO: Deleting pod "simpletest.rc-cxkzn" in namespace "gc-1282"
    Sep 20 12:24:54.337: INFO: Deleting pod "simpletest.rc-dbmps" in namespace "gc-1282"
    Sep 20 12:24:54.356: INFO: Deleting pod "simpletest.rc-dlz5q" in namespace "gc-1282"
    Sep 20 12:24:54.375: INFO: Deleting pod "simpletest.rc-dwwhg" in namespace "gc-1282"
    Sep 20 12:24:54.397: INFO: Deleting pod "simpletest.rc-fs2tn" in namespace "gc-1282"
    Sep 20 12:24:54.445: INFO: Deleting pod "simpletest.rc-g5tdn" in namespace "gc-1282"
    Sep 20 12:24:54.465: INFO: Deleting pod "simpletest.rc-gj4ds" in namespace "gc-1282"
    Sep 20 12:24:54.512: INFO: Deleting pod "simpletest.rc-glrhx" in namespace "gc-1282"
    Sep 20 12:24:54.531: INFO: Deleting pod "simpletest.rc-gt6lv" in namespace "gc-1282"
    Sep 20 12:24:54.558: INFO: Deleting pod "simpletest.rc-gvkfj" in namespace "gc-1282"
    Sep 20 12:24:54.575: INFO: Deleting pod "simpletest.rc-hggz5" in namespace "gc-1282"
    Sep 20 12:24:54.885: INFO: Deleting pod "simpletest.rc-hgv98" in namespace "gc-1282"
    Sep 20 12:24:55.176: INFO: Deleting pod "simpletest.rc-j4vsc" in namespace "gc-1282"
    Sep 20 12:24:55.216: INFO: Deleting pod "simpletest.rc-jhkxb" in namespace "gc-1282"
    Sep 20 12:24:55.244: INFO: Deleting pod "simpletest.rc-k5j28" in namespace "gc-1282"
    Sep 20 12:24:55.291: INFO: Deleting pod "simpletest.rc-k86j5" in namespace "gc-1282"
    Sep 20 12:24:55.324: INFO: Deleting pod "simpletest.rc-kdfsj" in namespace "gc-1282"
    Sep 20 12:24:55.345: INFO: Deleting pod "simpletest.rc-kglm5" in namespace "gc-1282"
    Sep 20 12:24:55.368: INFO: Deleting pod "simpletest.rc-kqj9v" in namespace "gc-1282"
    Sep 20 12:24:55.410: INFO: Deleting pod "simpletest.rc-kt66g" in namespace "gc-1282"
    Sep 20 12:24:55.441: INFO: Deleting pod "simpletest.rc-l47p5" in namespace "gc-1282"
    Sep 20 12:24:55.464: INFO: Deleting pod "simpletest.rc-l6c2v" in namespace "gc-1282"
    Sep 20 12:24:55.484: INFO: Deleting pod "simpletest.rc-l72t9" in namespace "gc-1282"
    Sep 20 12:24:55.509: INFO: Deleting pod "simpletest.rc-lwh7t" in namespace "gc-1282"
    Sep 20 12:24:55.528: INFO: Deleting pod "simpletest.rc-m7wll" in namespace "gc-1282"
    Sep 20 12:24:55.561: INFO: Deleting pod "simpletest.rc-m9dsw" in namespace "gc-1282"
    Sep 20 12:24:55.609: INFO: Deleting pod "simpletest.rc-ms6nn" in namespace "gc-1282"
    Sep 20 12:24:55.650: INFO: Deleting pod "simpletest.rc-nk5hq" in namespace "gc-1282"
    Sep 20 12:24:55.671: INFO: Deleting pod "simpletest.rc-npz7t" in namespace "gc-1282"
    Sep 20 12:24:55.743: INFO: Deleting pod "simpletest.rc-nqbkc" in namespace "gc-1282"
    Sep 20 12:24:56.358: INFO: Deleting pod "simpletest.rc-nqvcj" in namespace "gc-1282"
    Sep 20 12:24:56.386: INFO: Deleting pod "simpletest.rc-nr9fd" in namespace "gc-1282"
    Sep 20 12:24:56.408: INFO: Deleting pod "simpletest.rc-ntkwz" in namespace "gc-1282"
    Sep 20 12:24:56.427: INFO: Deleting pod "simpletest.rc-pdccm" in namespace "gc-1282"
    Sep 20 12:24:57.886: INFO: Deleting pod "simpletest.rc-pqzp4" in namespace "gc-1282"
    Sep 20 12:24:59.070: INFO: Deleting pod "simpletest.rc-ps684" in namespace "gc-1282"
    Sep 20 12:24:59.876: INFO: Deleting pod "simpletest.rc-pzx94" in namespace "gc-1282"
    Sep 20 12:24:59.913: INFO: Deleting pod "simpletest.rc-qc8lt" in namespace "gc-1282"
    Sep 20 12:24:59.931: INFO: Deleting pod "simpletest.rc-qdb24" in namespace "gc-1282"
    Sep 20 12:25:00.984: INFO: Deleting pod "simpletest.rc-qhqxg" in namespace "gc-1282"
    Sep 20 12:25:01.544: INFO: Deleting pod "simpletest.rc-qkst5" in namespace "gc-1282"
    Sep 20 12:25:01.772: INFO: Deleting pod "simpletest.rc-qlhqw" in namespace "gc-1282"
    Sep 20 12:25:02.721: INFO: Deleting pod "simpletest.rc-qrsr4" in namespace "gc-1282"
    Sep 20 12:25:03.204: INFO: Deleting pod "simpletest.rc-r22v2" in namespace "gc-1282"
    Sep 20 12:25:03.260: INFO: Deleting pod "simpletest.rc-r4cnj" in namespace "gc-1282"
    Sep 20 12:25:03.415: INFO: Deleting pod "simpletest.rc-r775n" in namespace "gc-1282"
    Sep 20 12:25:03.455: INFO: Deleting pod "simpletest.rc-r8494" in namespace "gc-1282"
    Sep 20 12:25:03.479: INFO: Deleting pod "simpletest.rc-r88mm" in namespace "gc-1282"
    Sep 20 12:25:03.526: INFO: Deleting pod "simpletest.rc-rldhg" in namespace "gc-1282"
    Sep 20 12:25:03.571: INFO: Deleting pod "simpletest.rc-rtbjp" in namespace "gc-1282"
    Sep 20 12:25:03.590: INFO: Deleting pod "simpletest.rc-sdhpj" in namespace "gc-1282"
    Sep 20 12:25:03.984: INFO: Deleting pod "simpletest.rc-sjqc4" in namespace "gc-1282"
    Sep 20 12:25:04.118: INFO: Deleting pod "simpletest.rc-snntv" in namespace "gc-1282"
    Sep 20 12:25:04.140: INFO: Deleting pod "simpletest.rc-sxnfp" in namespace "gc-1282"
    Sep 20 12:25:04.723: INFO: Deleting pod "simpletest.rc-t2j7s" in namespace "gc-1282"
    Sep 20 12:25:05.068: INFO: Deleting pod "simpletest.rc-v7dwg" in namespace "gc-1282"
    Sep 20 12:25:05.741: INFO: Deleting pod "simpletest.rc-vn6qx" in namespace "gc-1282"
    Sep 20 12:25:05.758: INFO: Deleting pod "simpletest.rc-vnr2t" in namespace "gc-1282"
    Sep 20 12:25:07.110: INFO: Deleting pod "simpletest.rc-vz7mh" in namespace "gc-1282"
    Sep 20 12:25:07.240: INFO: Deleting pod "simpletest.rc-w78qj" in namespace "gc-1282"
    Sep 20 12:25:07.255: INFO: Deleting pod "simpletest.rc-wfcs7" in namespace "gc-1282"
    Sep 20 12:25:07.790: INFO: Deleting pod "simpletest.rc-wjzrl" in namespace "gc-1282"
    Sep 20 12:25:07.989: INFO: Deleting pod "simpletest.rc-ww594" in namespace "gc-1282"
    Sep 20 12:25:08.003: INFO: Deleting pod "simpletest.rc-xt6pc" in namespace "gc-1282"
    Sep 20 12:25:08.022: INFO: Deleting pod "simpletest.rc-xtqgl" in namespace "gc-1282"
    Sep 20 12:25:08.042: INFO: Deleting pod "simpletest.rc-zhrpw" in namespace "gc-1282"
    Sep 20 12:25:08.063: INFO: Deleting pod "simpletest.rc-zlddq" in namespace "gc-1282"
    Sep 20 12:25:08.861: INFO: Deleting pod "simpletest.rc-ztmdl" in namespace "gc-1282"
    Sep 20 12:25:09.214: INFO: Deleting pod "simpletest.rc-zzvmn" in namespace "gc-1282"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:25:09.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1282" for this suite. 09/20/23 12:25:09.244
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:25:09.255
Sep 20 12:25:09.255: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename projected 09/20/23 12:25:09.266
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:25:09.63
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:25:09.635
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-f9f47091-6685-4edb-bfa5-da593b60327e 09/20/23 12:25:09.639
STEP: Creating a pod to test consume secrets 09/20/23 12:25:09.662
Sep 20 12:25:09.676: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-aa80b488-d287-457d-9c66-07985995171a" in namespace "projected-88" to be "Succeeded or Failed"
Sep 20 12:25:09.681: INFO: Pod "pod-projected-secrets-aa80b488-d287-457d-9c66-07985995171a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.16983ms
Sep 20 12:25:11.689: INFO: Pod "pod-projected-secrets-aa80b488-d287-457d-9c66-07985995171a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012357671s
Sep 20 12:25:13.777: INFO: Pod "pod-projected-secrets-aa80b488-d287-457d-9c66-07985995171a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.100616022s
Sep 20 12:25:16.177: INFO: Pod "pod-projected-secrets-aa80b488-d287-457d-9c66-07985995171a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.50053704s
Sep 20 12:25:17.839: INFO: Pod "pod-projected-secrets-aa80b488-d287-457d-9c66-07985995171a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.162959152s
Sep 20 12:25:20.004: INFO: Pod "pod-projected-secrets-aa80b488-d287-457d-9c66-07985995171a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.327414931s
Sep 20 12:25:21.687: INFO: Pod "pod-projected-secrets-aa80b488-d287-457d-9c66-07985995171a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.010530876s
Sep 20 12:25:23.690: INFO: Pod "pod-projected-secrets-aa80b488-d287-457d-9c66-07985995171a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.014132422s
STEP: Saw pod success 09/20/23 12:25:23.69
Sep 20 12:25:23.691: INFO: Pod "pod-projected-secrets-aa80b488-d287-457d-9c66-07985995171a" satisfied condition "Succeeded or Failed"
Sep 20 12:25:23.696: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-2 pod pod-projected-secrets-aa80b488-d287-457d-9c66-07985995171a container projected-secret-volume-test: <nil>
STEP: delete the pod 09/20/23 12:25:23.762
Sep 20 12:25:23.785: INFO: Waiting for pod pod-projected-secrets-aa80b488-d287-457d-9c66-07985995171a to disappear
Sep 20 12:25:23.792: INFO: Pod pod-projected-secrets-aa80b488-d287-457d-9c66-07985995171a no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep 20 12:25:23.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-88" for this suite. 09/20/23 12:25:23.797
------------------------------
â€¢ [SLOW TEST] [14.551 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:25:09.255
    Sep 20 12:25:09.255: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename projected 09/20/23 12:25:09.266
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:25:09.63
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:25:09.635
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-f9f47091-6685-4edb-bfa5-da593b60327e 09/20/23 12:25:09.639
    STEP: Creating a pod to test consume secrets 09/20/23 12:25:09.662
    Sep 20 12:25:09.676: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-aa80b488-d287-457d-9c66-07985995171a" in namespace "projected-88" to be "Succeeded or Failed"
    Sep 20 12:25:09.681: INFO: Pod "pod-projected-secrets-aa80b488-d287-457d-9c66-07985995171a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.16983ms
    Sep 20 12:25:11.689: INFO: Pod "pod-projected-secrets-aa80b488-d287-457d-9c66-07985995171a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012357671s
    Sep 20 12:25:13.777: INFO: Pod "pod-projected-secrets-aa80b488-d287-457d-9c66-07985995171a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.100616022s
    Sep 20 12:25:16.177: INFO: Pod "pod-projected-secrets-aa80b488-d287-457d-9c66-07985995171a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.50053704s
    Sep 20 12:25:17.839: INFO: Pod "pod-projected-secrets-aa80b488-d287-457d-9c66-07985995171a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.162959152s
    Sep 20 12:25:20.004: INFO: Pod "pod-projected-secrets-aa80b488-d287-457d-9c66-07985995171a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.327414931s
    Sep 20 12:25:21.687: INFO: Pod "pod-projected-secrets-aa80b488-d287-457d-9c66-07985995171a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.010530876s
    Sep 20 12:25:23.690: INFO: Pod "pod-projected-secrets-aa80b488-d287-457d-9c66-07985995171a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.014132422s
    STEP: Saw pod success 09/20/23 12:25:23.69
    Sep 20 12:25:23.691: INFO: Pod "pod-projected-secrets-aa80b488-d287-457d-9c66-07985995171a" satisfied condition "Succeeded or Failed"
    Sep 20 12:25:23.696: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-2 pod pod-projected-secrets-aa80b488-d287-457d-9c66-07985995171a container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/20/23 12:25:23.762
    Sep 20 12:25:23.785: INFO: Waiting for pod pod-projected-secrets-aa80b488-d287-457d-9c66-07985995171a to disappear
    Sep 20 12:25:23.792: INFO: Pod pod-projected-secrets-aa80b488-d287-457d-9c66-07985995171a no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:25:23.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-88" for this suite. 09/20/23 12:25:23.797
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:25:23.809
Sep 20 12:25:23.809: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename proxy 09/20/23 12:25:23.809
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:25:24.107
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:25:24.112
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Sep 20 12:25:24.119: INFO: Creating pod...
Sep 20 12:25:24.156: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8164" to be "running"
Sep 20 12:25:24.163: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 7.281248ms
Sep 20 12:25:26.243: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.087470266s
Sep 20 12:25:28.994: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.838201978s
Sep 20 12:25:30.201: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 6.044816398s
Sep 20 12:25:30.201: INFO: Pod "agnhost" satisfied condition "running"
Sep 20 12:25:30.201: INFO: Creating service...
Sep 20 12:25:30.318: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8164/pods/agnhost/proxy/some/path/with/DELETE
Sep 20 12:25:30.348: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep 20 12:25:30.348: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8164/pods/agnhost/proxy/some/path/with/GET
Sep 20 12:25:30.371: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Sep 20 12:25:30.371: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8164/pods/agnhost/proxy/some/path/with/HEAD
Sep 20 12:25:30.381: INFO: http.Client request:HEAD | StatusCode:200
Sep 20 12:25:30.381: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8164/pods/agnhost/proxy/some/path/with/OPTIONS
Sep 20 12:25:30.397: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep 20 12:25:30.397: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8164/pods/agnhost/proxy/some/path/with/PATCH
Sep 20 12:25:30.532: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep 20 12:25:30.532: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8164/pods/agnhost/proxy/some/path/with/POST
Sep 20 12:25:30.537: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep 20 12:25:30.537: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8164/pods/agnhost/proxy/some/path/with/PUT
Sep 20 12:25:30.733: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Sep 20 12:25:30.733: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8164/services/test-service/proxy/some/path/with/DELETE
Sep 20 12:25:30.747: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep 20 12:25:30.747: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8164/services/test-service/proxy/some/path/with/GET
Sep 20 12:25:30.758: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Sep 20 12:25:30.758: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8164/services/test-service/proxy/some/path/with/HEAD
Sep 20 12:25:30.966: INFO: http.Client request:HEAD | StatusCode:200
Sep 20 12:25:30.966: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8164/services/test-service/proxy/some/path/with/OPTIONS
Sep 20 12:25:30.976: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep 20 12:25:30.976: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8164/services/test-service/proxy/some/path/with/PATCH
Sep 20 12:25:30.984: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep 20 12:25:30.984: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8164/services/test-service/proxy/some/path/with/POST
Sep 20 12:25:31.002: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep 20 12:25:31.002: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8164/services/test-service/proxy/some/path/with/PUT
Sep 20 12:25:31.176: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Sep 20 12:25:31.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-8164" for this suite. 09/20/23 12:25:31.188
------------------------------
â€¢ [SLOW TEST] [7.395 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:25:23.809
    Sep 20 12:25:23.809: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename proxy 09/20/23 12:25:23.809
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:25:24.107
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:25:24.112
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Sep 20 12:25:24.119: INFO: Creating pod...
    Sep 20 12:25:24.156: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8164" to be "running"
    Sep 20 12:25:24.163: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 7.281248ms
    Sep 20 12:25:26.243: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.087470266s
    Sep 20 12:25:28.994: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.838201978s
    Sep 20 12:25:30.201: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 6.044816398s
    Sep 20 12:25:30.201: INFO: Pod "agnhost" satisfied condition "running"
    Sep 20 12:25:30.201: INFO: Creating service...
    Sep 20 12:25:30.318: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8164/pods/agnhost/proxy/some/path/with/DELETE
    Sep 20 12:25:30.348: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Sep 20 12:25:30.348: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8164/pods/agnhost/proxy/some/path/with/GET
    Sep 20 12:25:30.371: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Sep 20 12:25:30.371: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8164/pods/agnhost/proxy/some/path/with/HEAD
    Sep 20 12:25:30.381: INFO: http.Client request:HEAD | StatusCode:200
    Sep 20 12:25:30.381: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8164/pods/agnhost/proxy/some/path/with/OPTIONS
    Sep 20 12:25:30.397: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Sep 20 12:25:30.397: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8164/pods/agnhost/proxy/some/path/with/PATCH
    Sep 20 12:25:30.532: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Sep 20 12:25:30.532: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8164/pods/agnhost/proxy/some/path/with/POST
    Sep 20 12:25:30.537: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Sep 20 12:25:30.537: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8164/pods/agnhost/proxy/some/path/with/PUT
    Sep 20 12:25:30.733: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Sep 20 12:25:30.733: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8164/services/test-service/proxy/some/path/with/DELETE
    Sep 20 12:25:30.747: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Sep 20 12:25:30.747: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8164/services/test-service/proxy/some/path/with/GET
    Sep 20 12:25:30.758: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Sep 20 12:25:30.758: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8164/services/test-service/proxy/some/path/with/HEAD
    Sep 20 12:25:30.966: INFO: http.Client request:HEAD | StatusCode:200
    Sep 20 12:25:30.966: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8164/services/test-service/proxy/some/path/with/OPTIONS
    Sep 20 12:25:30.976: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Sep 20 12:25:30.976: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8164/services/test-service/proxy/some/path/with/PATCH
    Sep 20 12:25:30.984: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Sep 20 12:25:30.984: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8164/services/test-service/proxy/some/path/with/POST
    Sep 20 12:25:31.002: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Sep 20 12:25:31.002: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8164/services/test-service/proxy/some/path/with/PUT
    Sep 20 12:25:31.176: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:25:31.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-8164" for this suite. 09/20/23 12:25:31.188
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:25:31.204
Sep 20 12:25:31.204: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename statefulset 09/20/23 12:25:31.205
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:25:31.235
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:25:31.242
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9700 09/20/23 12:25:31.413
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 09/20/23 12:25:31.526
Sep 20 12:25:31.579: INFO: Found 0 stateful pods, waiting for 3
Sep 20 12:25:41.586: INFO: Found 2 stateful pods, waiting for 3
Sep 20 12:25:51.585: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 12:25:51.585: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 12:25:51.585: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 09/20/23 12:25:51.604
Sep 20 12:25:51.890: INFO: Updating stateful set ss2
STEP: Creating a new revision 09/20/23 12:25:51.891
STEP: Not applying an update when the partition is greater than the number of replicas 09/20/23 12:26:02.528
STEP: Performing a canary update 09/20/23 12:26:02.528
Sep 20 12:26:02.683: INFO: Updating stateful set ss2
Sep 20 12:26:02.690: INFO: Waiting for Pod statefulset-9700/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 09/20/23 12:26:12.698
Sep 20 12:26:13.657: INFO: Found 2 stateful pods, waiting for 3
Sep 20 12:26:23.667: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 12:26:23.667: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 12:26:23.667: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 09/20/23 12:26:23.675
Sep 20 12:26:23.695: INFO: Updating stateful set ss2
Sep 20 12:26:23.700: INFO: Waiting for Pod statefulset-9700/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Sep 20 12:26:33.743: INFO: Updating stateful set ss2
Sep 20 12:26:33.802: INFO: Waiting for StatefulSet statefulset-9700/ss2 to complete update
Sep 20 12:26:33.802: INFO: Waiting for Pod statefulset-9700/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep 20 12:26:43.816: INFO: Deleting all statefulset in ns statefulset-9700
Sep 20 12:26:43.823: INFO: Scaling statefulset ss2 to 0
Sep 20 12:27:04.170: INFO: Waiting for statefulset status.replicas updated to 0
Sep 20 12:27:04.176: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep 20 12:27:04.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9700" for this suite. 09/20/23 12:27:04.348
------------------------------
â€¢ [SLOW TEST] [93.159 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:25:31.204
    Sep 20 12:25:31.204: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename statefulset 09/20/23 12:25:31.205
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:25:31.235
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:25:31.242
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9700 09/20/23 12:25:31.413
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 09/20/23 12:25:31.526
    Sep 20 12:25:31.579: INFO: Found 0 stateful pods, waiting for 3
    Sep 20 12:25:41.586: INFO: Found 2 stateful pods, waiting for 3
    Sep 20 12:25:51.585: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep 20 12:25:51.585: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep 20 12:25:51.585: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 09/20/23 12:25:51.604
    Sep 20 12:25:51.890: INFO: Updating stateful set ss2
    STEP: Creating a new revision 09/20/23 12:25:51.891
    STEP: Not applying an update when the partition is greater than the number of replicas 09/20/23 12:26:02.528
    STEP: Performing a canary update 09/20/23 12:26:02.528
    Sep 20 12:26:02.683: INFO: Updating stateful set ss2
    Sep 20 12:26:02.690: INFO: Waiting for Pod statefulset-9700/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 09/20/23 12:26:12.698
    Sep 20 12:26:13.657: INFO: Found 2 stateful pods, waiting for 3
    Sep 20 12:26:23.667: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep 20 12:26:23.667: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep 20 12:26:23.667: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 09/20/23 12:26:23.675
    Sep 20 12:26:23.695: INFO: Updating stateful set ss2
    Sep 20 12:26:23.700: INFO: Waiting for Pod statefulset-9700/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Sep 20 12:26:33.743: INFO: Updating stateful set ss2
    Sep 20 12:26:33.802: INFO: Waiting for StatefulSet statefulset-9700/ss2 to complete update
    Sep 20 12:26:33.802: INFO: Waiting for Pod statefulset-9700/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep 20 12:26:43.816: INFO: Deleting all statefulset in ns statefulset-9700
    Sep 20 12:26:43.823: INFO: Scaling statefulset ss2 to 0
    Sep 20 12:27:04.170: INFO: Waiting for statefulset status.replicas updated to 0
    Sep 20 12:27:04.176: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:27:04.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9700" for this suite. 09/20/23 12:27:04.348
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:27:04.363
Sep 20 12:27:04.363: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename configmap 09/20/23 12:27:04.364
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:27:04.384
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:27:04.389
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-d98aa3ed-1be8-4228-becd-307e6a12d352 09/20/23 12:27:04.402
STEP: Creating the pod 09/20/23 12:27:04.521
Sep 20 12:27:04.536: INFO: Waiting up to 5m0s for pod "pod-configmaps-786f08b6-0322-408b-9a4b-89d44dc52fa5" in namespace "configmap-5762" to be "running"
Sep 20 12:27:04.548: INFO: Pod "pod-configmaps-786f08b6-0322-408b-9a4b-89d44dc52fa5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.108848ms
Sep 20 12:27:06.554: INFO: Pod "pod-configmaps-786f08b6-0322-408b-9a4b-89d44dc52fa5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01759942s
Sep 20 12:27:08.597: INFO: Pod "pod-configmaps-786f08b6-0322-408b-9a4b-89d44dc52fa5": Phase="Running", Reason="", readiness=false. Elapsed: 4.06035725s
Sep 20 12:27:08.597: INFO: Pod "pod-configmaps-786f08b6-0322-408b-9a4b-89d44dc52fa5" satisfied condition "running"
STEP: Waiting for pod with text data 09/20/23 12:27:08.597
STEP: Waiting for pod with binary data 09/20/23 12:27:08.654
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep 20 12:27:08.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5762" for this suite. 09/20/23 12:27:08.775
------------------------------
â€¢ [4.508 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:27:04.363
    Sep 20 12:27:04.363: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename configmap 09/20/23 12:27:04.364
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:27:04.384
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:27:04.389
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-d98aa3ed-1be8-4228-becd-307e6a12d352 09/20/23 12:27:04.402
    STEP: Creating the pod 09/20/23 12:27:04.521
    Sep 20 12:27:04.536: INFO: Waiting up to 5m0s for pod "pod-configmaps-786f08b6-0322-408b-9a4b-89d44dc52fa5" in namespace "configmap-5762" to be "running"
    Sep 20 12:27:04.548: INFO: Pod "pod-configmaps-786f08b6-0322-408b-9a4b-89d44dc52fa5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.108848ms
    Sep 20 12:27:06.554: INFO: Pod "pod-configmaps-786f08b6-0322-408b-9a4b-89d44dc52fa5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01759942s
    Sep 20 12:27:08.597: INFO: Pod "pod-configmaps-786f08b6-0322-408b-9a4b-89d44dc52fa5": Phase="Running", Reason="", readiness=false. Elapsed: 4.06035725s
    Sep 20 12:27:08.597: INFO: Pod "pod-configmaps-786f08b6-0322-408b-9a4b-89d44dc52fa5" satisfied condition "running"
    STEP: Waiting for pod with text data 09/20/23 12:27:08.597
    STEP: Waiting for pod with binary data 09/20/23 12:27:08.654
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:27:08.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5762" for this suite. 09/20/23 12:27:08.775
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:27:08.872
Sep 20 12:27:08.872: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename disruption 09/20/23 12:27:08.873
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:27:08.935
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:27:08.939
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 09/20/23 12:27:08.981
STEP: Updating PodDisruptionBudget status 09/20/23 12:27:11.582
STEP: Waiting for all pods to be running 09/20/23 12:27:11.898
Sep 20 12:27:12.048: INFO: running pods: 0 < 1
Sep 20 12:27:14.150: INFO: running pods: 0 < 1
Sep 20 12:27:16.671: INFO: running pods: 0 < 1
Sep 20 12:27:18.055: INFO: running pods: 0 < 1
Sep 20 12:27:20.054: INFO: running pods: 0 < 1
Sep 20 12:27:22.053: INFO: running pods: 0 < 1
STEP: locating a running pod 09/20/23 12:27:25.103
STEP: Waiting for the pdb to be processed 09/20/23 12:27:25.126
STEP: Patching PodDisruptionBudget status 09/20/23 12:27:25.427
STEP: Waiting for the pdb to be processed 09/20/23 12:27:25.442
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Sep 20 12:27:25.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-4061" for this suite. 09/20/23 12:27:25.457
------------------------------
â€¢ [SLOW TEST] [16.688 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:27:08.872
    Sep 20 12:27:08.872: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename disruption 09/20/23 12:27:08.873
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:27:08.935
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:27:08.939
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 09/20/23 12:27:08.981
    STEP: Updating PodDisruptionBudget status 09/20/23 12:27:11.582
    STEP: Waiting for all pods to be running 09/20/23 12:27:11.898
    Sep 20 12:27:12.048: INFO: running pods: 0 < 1
    Sep 20 12:27:14.150: INFO: running pods: 0 < 1
    Sep 20 12:27:16.671: INFO: running pods: 0 < 1
    Sep 20 12:27:18.055: INFO: running pods: 0 < 1
    Sep 20 12:27:20.054: INFO: running pods: 0 < 1
    Sep 20 12:27:22.053: INFO: running pods: 0 < 1
    STEP: locating a running pod 09/20/23 12:27:25.103
    STEP: Waiting for the pdb to be processed 09/20/23 12:27:25.126
    STEP: Patching PodDisruptionBudget status 09/20/23 12:27:25.427
    STEP: Waiting for the pdb to be processed 09/20/23 12:27:25.442
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:27:25.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-4061" for this suite. 09/20/23 12:27:25.457
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:27:25.562
Sep 20 12:27:25.562: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename custom-resource-definition 09/20/23 12:27:25.563
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:27:26.268
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:27:26.271
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Sep 20 12:27:26.276: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep 20 12:27:36.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-7276" for this suite. 09/20/23 12:27:36.484
------------------------------
â€¢ [SLOW TEST] [10.975 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:27:25.562
    Sep 20 12:27:25.562: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename custom-resource-definition 09/20/23 12:27:25.563
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:27:26.268
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:27:26.271
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Sep 20 12:27:26.276: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:27:36.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-7276" for this suite. 09/20/23 12:27:36.484
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:27:36.538
Sep 20 12:27:36.538: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename projected 09/20/23 12:27:36.538
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:27:36.6
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:27:36.604
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-0f22ef0b-7cb2-414e-a213-a82a12faae31 09/20/23 12:27:36.617
STEP: Creating a pod to test consume configMaps 09/20/23 12:27:36.627
Sep 20 12:27:36.647: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c22f9acd-880a-4ed9-8d72-8d2f4e89b0b7" in namespace "projected-9730" to be "Succeeded or Failed"
Sep 20 12:27:36.917: INFO: Pod "pod-projected-configmaps-c22f9acd-880a-4ed9-8d72-8d2f4e89b0b7": Phase="Pending", Reason="", readiness=false. Elapsed: 270.355941ms
Sep 20 12:27:38.922: INFO: Pod "pod-projected-configmaps-c22f9acd-880a-4ed9-8d72-8d2f4e89b0b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.275407974s
Sep 20 12:27:40.922: INFO: Pod "pod-projected-configmaps-c22f9acd-880a-4ed9-8d72-8d2f4e89b0b7": Phase="Running", Reason="", readiness=true. Elapsed: 4.275820215s
Sep 20 12:27:42.925: INFO: Pod "pod-projected-configmaps-c22f9acd-880a-4ed9-8d72-8d2f4e89b0b7": Phase="Running", Reason="", readiness=false. Elapsed: 6.277840025s
Sep 20 12:27:44.923: INFO: Pod "pod-projected-configmaps-c22f9acd-880a-4ed9-8d72-8d2f4e89b0b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.27604543s
STEP: Saw pod success 09/20/23 12:27:44.923
Sep 20 12:27:44.923: INFO: Pod "pod-projected-configmaps-c22f9acd-880a-4ed9-8d72-8d2f4e89b0b7" satisfied condition "Succeeded or Failed"
Sep 20 12:27:44.929: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-2 pod pod-projected-configmaps-c22f9acd-880a-4ed9-8d72-8d2f4e89b0b7 container agnhost-container: <nil>
STEP: delete the pod 09/20/23 12:27:45
Sep 20 12:27:45.228: INFO: Waiting for pod pod-projected-configmaps-c22f9acd-880a-4ed9-8d72-8d2f4e89b0b7 to disappear
Sep 20 12:27:45.232: INFO: Pod pod-projected-configmaps-c22f9acd-880a-4ed9-8d72-8d2f4e89b0b7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep 20 12:27:45.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9730" for this suite. 09/20/23 12:27:45.236
------------------------------
â€¢ [SLOW TEST] [8.705 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:27:36.538
    Sep 20 12:27:36.538: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename projected 09/20/23 12:27:36.538
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:27:36.6
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:27:36.604
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-0f22ef0b-7cb2-414e-a213-a82a12faae31 09/20/23 12:27:36.617
    STEP: Creating a pod to test consume configMaps 09/20/23 12:27:36.627
    Sep 20 12:27:36.647: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c22f9acd-880a-4ed9-8d72-8d2f4e89b0b7" in namespace "projected-9730" to be "Succeeded or Failed"
    Sep 20 12:27:36.917: INFO: Pod "pod-projected-configmaps-c22f9acd-880a-4ed9-8d72-8d2f4e89b0b7": Phase="Pending", Reason="", readiness=false. Elapsed: 270.355941ms
    Sep 20 12:27:38.922: INFO: Pod "pod-projected-configmaps-c22f9acd-880a-4ed9-8d72-8d2f4e89b0b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.275407974s
    Sep 20 12:27:40.922: INFO: Pod "pod-projected-configmaps-c22f9acd-880a-4ed9-8d72-8d2f4e89b0b7": Phase="Running", Reason="", readiness=true. Elapsed: 4.275820215s
    Sep 20 12:27:42.925: INFO: Pod "pod-projected-configmaps-c22f9acd-880a-4ed9-8d72-8d2f4e89b0b7": Phase="Running", Reason="", readiness=false. Elapsed: 6.277840025s
    Sep 20 12:27:44.923: INFO: Pod "pod-projected-configmaps-c22f9acd-880a-4ed9-8d72-8d2f4e89b0b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.27604543s
    STEP: Saw pod success 09/20/23 12:27:44.923
    Sep 20 12:27:44.923: INFO: Pod "pod-projected-configmaps-c22f9acd-880a-4ed9-8d72-8d2f4e89b0b7" satisfied condition "Succeeded or Failed"
    Sep 20 12:27:44.929: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-2 pod pod-projected-configmaps-c22f9acd-880a-4ed9-8d72-8d2f4e89b0b7 container agnhost-container: <nil>
    STEP: delete the pod 09/20/23 12:27:45
    Sep 20 12:27:45.228: INFO: Waiting for pod pod-projected-configmaps-c22f9acd-880a-4ed9-8d72-8d2f4e89b0b7 to disappear
    Sep 20 12:27:45.232: INFO: Pod pod-projected-configmaps-c22f9acd-880a-4ed9-8d72-8d2f4e89b0b7 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:27:45.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9730" for this suite. 09/20/23 12:27:45.236
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:27:45.244
Sep 20 12:27:45.244: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename emptydir 09/20/23 12:27:45.244
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:27:45.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:27:45.27
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 09/20/23 12:27:45.275
Sep 20 12:27:45.285: INFO: Waiting up to 5m0s for pod "pod-0f1380b9-c1dd-44c4-a7a5-8d5e21ed5cc4" in namespace "emptydir-6276" to be "Succeeded or Failed"
Sep 20 12:27:45.297: INFO: Pod "pod-0f1380b9-c1dd-44c4-a7a5-8d5e21ed5cc4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.063223ms
Sep 20 12:27:47.302: INFO: Pod "pod-0f1380b9-c1dd-44c4-a7a5-8d5e21ed5cc4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016385866s
Sep 20 12:27:49.302: INFO: Pod "pod-0f1380b9-c1dd-44c4-a7a5-8d5e21ed5cc4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016026765s
Sep 20 12:27:51.302: INFO: Pod "pod-0f1380b9-c1dd-44c4-a7a5-8d5e21ed5cc4": Phase="Running", Reason="", readiness=false. Elapsed: 6.016337623s
Sep 20 12:27:53.334: INFO: Pod "pod-0f1380b9-c1dd-44c4-a7a5-8d5e21ed5cc4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.048902579s
STEP: Saw pod success 09/20/23 12:27:53.334
Sep 20 12:27:53.335: INFO: Pod "pod-0f1380b9-c1dd-44c4-a7a5-8d5e21ed5cc4" satisfied condition "Succeeded or Failed"
Sep 20 12:27:53.360: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-0f1380b9-c1dd-44c4-a7a5-8d5e21ed5cc4 container test-container: <nil>
STEP: delete the pod 09/20/23 12:27:53.373
Sep 20 12:27:53.407: INFO: Waiting for pod pod-0f1380b9-c1dd-44c4-a7a5-8d5e21ed5cc4 to disappear
Sep 20 12:27:53.412: INFO: Pod pod-0f1380b9-c1dd-44c4-a7a5-8d5e21ed5cc4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep 20 12:27:53.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6276" for this suite. 09/20/23 12:27:53.419
------------------------------
â€¢ [SLOW TEST] [8.187 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:27:45.244
    Sep 20 12:27:45.244: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename emptydir 09/20/23 12:27:45.244
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:27:45.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:27:45.27
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 09/20/23 12:27:45.275
    Sep 20 12:27:45.285: INFO: Waiting up to 5m0s for pod "pod-0f1380b9-c1dd-44c4-a7a5-8d5e21ed5cc4" in namespace "emptydir-6276" to be "Succeeded or Failed"
    Sep 20 12:27:45.297: INFO: Pod "pod-0f1380b9-c1dd-44c4-a7a5-8d5e21ed5cc4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.063223ms
    Sep 20 12:27:47.302: INFO: Pod "pod-0f1380b9-c1dd-44c4-a7a5-8d5e21ed5cc4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016385866s
    Sep 20 12:27:49.302: INFO: Pod "pod-0f1380b9-c1dd-44c4-a7a5-8d5e21ed5cc4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016026765s
    Sep 20 12:27:51.302: INFO: Pod "pod-0f1380b9-c1dd-44c4-a7a5-8d5e21ed5cc4": Phase="Running", Reason="", readiness=false. Elapsed: 6.016337623s
    Sep 20 12:27:53.334: INFO: Pod "pod-0f1380b9-c1dd-44c4-a7a5-8d5e21ed5cc4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.048902579s
    STEP: Saw pod success 09/20/23 12:27:53.334
    Sep 20 12:27:53.335: INFO: Pod "pod-0f1380b9-c1dd-44c4-a7a5-8d5e21ed5cc4" satisfied condition "Succeeded or Failed"
    Sep 20 12:27:53.360: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-0f1380b9-c1dd-44c4-a7a5-8d5e21ed5cc4 container test-container: <nil>
    STEP: delete the pod 09/20/23 12:27:53.373
    Sep 20 12:27:53.407: INFO: Waiting for pod pod-0f1380b9-c1dd-44c4-a7a5-8d5e21ed5cc4 to disappear
    Sep 20 12:27:53.412: INFO: Pod pod-0f1380b9-c1dd-44c4-a7a5-8d5e21ed5cc4 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:27:53.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6276" for this suite. 09/20/23 12:27:53.419
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:27:53.431
Sep 20 12:27:53.431: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename secrets 09/20/23 12:27:53.432
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:27:53.641
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:27:53.646
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-e8d65796-94e3-4115-a6e0-f766e5225cb3 09/20/23 12:27:53.651
STEP: Creating a pod to test consume secrets 09/20/23 12:27:53.657
Sep 20 12:27:53.691: INFO: Waiting up to 5m0s for pod "pod-secrets-09f07428-5766-4dba-92c3-84817f43d474" in namespace "secrets-8379" to be "Succeeded or Failed"
Sep 20 12:27:53.697: INFO: Pod "pod-secrets-09f07428-5766-4dba-92c3-84817f43d474": Phase="Pending", Reason="", readiness=false. Elapsed: 6.887014ms
Sep 20 12:27:55.702: INFO: Pod "pod-secrets-09f07428-5766-4dba-92c3-84817f43d474": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011490851s
Sep 20 12:27:57.704: INFO: Pod "pod-secrets-09f07428-5766-4dba-92c3-84817f43d474": Phase="Running", Reason="", readiness=false. Elapsed: 4.012997966s
Sep 20 12:27:59.780: INFO: Pod "pod-secrets-09f07428-5766-4dba-92c3-84817f43d474": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.089371474s
STEP: Saw pod success 09/20/23 12:27:59.78
Sep 20 12:27:59.780: INFO: Pod "pod-secrets-09f07428-5766-4dba-92c3-84817f43d474" satisfied condition "Succeeded or Failed"
Sep 20 12:27:59.792: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-2 pod pod-secrets-09f07428-5766-4dba-92c3-84817f43d474 container secret-volume-test: <nil>
STEP: delete the pod 09/20/23 12:27:59.806
Sep 20 12:27:59.821: INFO: Waiting for pod pod-secrets-09f07428-5766-4dba-92c3-84817f43d474 to disappear
Sep 20 12:27:59.827: INFO: Pod pod-secrets-09f07428-5766-4dba-92c3-84817f43d474 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep 20 12:27:59.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8379" for this suite. 09/20/23 12:27:59.833
------------------------------
â€¢ [SLOW TEST] [6.413 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:27:53.431
    Sep 20 12:27:53.431: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename secrets 09/20/23 12:27:53.432
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:27:53.641
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:27:53.646
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-e8d65796-94e3-4115-a6e0-f766e5225cb3 09/20/23 12:27:53.651
    STEP: Creating a pod to test consume secrets 09/20/23 12:27:53.657
    Sep 20 12:27:53.691: INFO: Waiting up to 5m0s for pod "pod-secrets-09f07428-5766-4dba-92c3-84817f43d474" in namespace "secrets-8379" to be "Succeeded or Failed"
    Sep 20 12:27:53.697: INFO: Pod "pod-secrets-09f07428-5766-4dba-92c3-84817f43d474": Phase="Pending", Reason="", readiness=false. Elapsed: 6.887014ms
    Sep 20 12:27:55.702: INFO: Pod "pod-secrets-09f07428-5766-4dba-92c3-84817f43d474": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011490851s
    Sep 20 12:27:57.704: INFO: Pod "pod-secrets-09f07428-5766-4dba-92c3-84817f43d474": Phase="Running", Reason="", readiness=false. Elapsed: 4.012997966s
    Sep 20 12:27:59.780: INFO: Pod "pod-secrets-09f07428-5766-4dba-92c3-84817f43d474": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.089371474s
    STEP: Saw pod success 09/20/23 12:27:59.78
    Sep 20 12:27:59.780: INFO: Pod "pod-secrets-09f07428-5766-4dba-92c3-84817f43d474" satisfied condition "Succeeded or Failed"
    Sep 20 12:27:59.792: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-2 pod pod-secrets-09f07428-5766-4dba-92c3-84817f43d474 container secret-volume-test: <nil>
    STEP: delete the pod 09/20/23 12:27:59.806
    Sep 20 12:27:59.821: INFO: Waiting for pod pod-secrets-09f07428-5766-4dba-92c3-84817f43d474 to disappear
    Sep 20 12:27:59.827: INFO: Pod pod-secrets-09f07428-5766-4dba-92c3-84817f43d474 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:27:59.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8379" for this suite. 09/20/23 12:27:59.833
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:27:59.844
Sep 20 12:27:59.844: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename csistoragecapacity 09/20/23 12:27:59.845
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:27:59.969
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:27:59.973
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 09/20/23 12:27:59.981
STEP: getting /apis/storage.k8s.io 09/20/23 12:27:59.989
STEP: getting /apis/storage.k8s.io/v1 09/20/23 12:27:59.992
STEP: creating 09/20/23 12:27:59.997
STEP: watching 09/20/23 12:28:00.96
Sep 20 12:28:00.960: INFO: starting watch
STEP: getting 09/20/23 12:28:01.007
STEP: listing in namespace 09/20/23 12:28:01.017
STEP: listing across namespaces 09/20/23 12:28:01.022
STEP: patching 09/20/23 12:28:01.028
STEP: updating 09/20/23 12:28:01.329
Sep 20 12:28:01.338: INFO: waiting for watch events with expected annotations in namespace
Sep 20 12:28:01.338: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 09/20/23 12:28:01.338
STEP: deleting a collection 09/20/23 12:28:01.352
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Sep 20 12:28:01.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-3533" for this suite. 09/20/23 12:28:01.374
------------------------------
â€¢ [1.538 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:27:59.844
    Sep 20 12:27:59.844: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename csistoragecapacity 09/20/23 12:27:59.845
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:27:59.969
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:27:59.973
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 09/20/23 12:27:59.981
    STEP: getting /apis/storage.k8s.io 09/20/23 12:27:59.989
    STEP: getting /apis/storage.k8s.io/v1 09/20/23 12:27:59.992
    STEP: creating 09/20/23 12:27:59.997
    STEP: watching 09/20/23 12:28:00.96
    Sep 20 12:28:00.960: INFO: starting watch
    STEP: getting 09/20/23 12:28:01.007
    STEP: listing in namespace 09/20/23 12:28:01.017
    STEP: listing across namespaces 09/20/23 12:28:01.022
    STEP: patching 09/20/23 12:28:01.028
    STEP: updating 09/20/23 12:28:01.329
    Sep 20 12:28:01.338: INFO: waiting for watch events with expected annotations in namespace
    Sep 20 12:28:01.338: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 09/20/23 12:28:01.338
    STEP: deleting a collection 09/20/23 12:28:01.352
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:28:01.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-3533" for this suite. 09/20/23 12:28:01.374
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:28:01.382
Sep 20 12:28:01.383: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename resourcequota 09/20/23 12:28:01.383
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:28:01.409
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:28:01.416
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 09/20/23 12:28:01.422
STEP: Creating a ResourceQuota 09/20/23 12:28:06.537
STEP: Ensuring resource quota status is calculated 09/20/23 12:28:06.592
STEP: Creating a ReplicaSet 09/20/23 12:28:08.598
STEP: Ensuring resource quota status captures replicaset creation 09/20/23 12:28:08.612
STEP: Deleting a ReplicaSet 09/20/23 12:28:10.617
STEP: Ensuring resource quota status released usage 09/20/23 12:28:10.625
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep 20 12:28:12.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1485" for this suite. 09/20/23 12:28:12.641
------------------------------
â€¢ [SLOW TEST] [11.271 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:28:01.382
    Sep 20 12:28:01.383: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename resourcequota 09/20/23 12:28:01.383
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:28:01.409
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:28:01.416
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 09/20/23 12:28:01.422
    STEP: Creating a ResourceQuota 09/20/23 12:28:06.537
    STEP: Ensuring resource quota status is calculated 09/20/23 12:28:06.592
    STEP: Creating a ReplicaSet 09/20/23 12:28:08.598
    STEP: Ensuring resource quota status captures replicaset creation 09/20/23 12:28:08.612
    STEP: Deleting a ReplicaSet 09/20/23 12:28:10.617
    STEP: Ensuring resource quota status released usage 09/20/23 12:28:10.625
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:28:12.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1485" for this suite. 09/20/23 12:28:12.641
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:28:12.655
Sep 20 12:28:12.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename podtemplate 09/20/23 12:28:12.656
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:28:12.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:28:12.709
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 09/20/23 12:28:12.855
STEP: Replace a pod template 09/20/23 12:28:13.082
Sep 20 12:28:13.241: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Sep 20 12:28:13.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-4760" for this suite. 09/20/23 12:28:13.247
------------------------------
â€¢ [0.602 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:28:12.655
    Sep 20 12:28:12.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename podtemplate 09/20/23 12:28:12.656
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:28:12.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:28:12.709
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 09/20/23 12:28:12.855
    STEP: Replace a pod template 09/20/23 12:28:13.082
    Sep 20 12:28:13.241: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:28:13.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-4760" for this suite. 09/20/23 12:28:13.247
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:28:13.261
Sep 20 12:28:13.261: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename configmap 09/20/23 12:28:13.262
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:28:13.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:28:13.302
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-572b533d-f5f3-4370-bdcb-fc7c6fdf2058 09/20/23 12:28:13.313
STEP: Creating the pod 09/20/23 12:28:13.326
Sep 20 12:28:13.341: INFO: Waiting up to 5m0s for pod "pod-configmaps-13db0cf9-e905-4861-b0b8-a15fe52bea82" in namespace "configmap-6855" to be "running and ready"
Sep 20 12:28:13.349: INFO: Pod "pod-configmaps-13db0cf9-e905-4861-b0b8-a15fe52bea82": Phase="Pending", Reason="", readiness=false. Elapsed: 8.506316ms
Sep 20 12:28:13.349: INFO: The phase of Pod pod-configmaps-13db0cf9-e905-4861-b0b8-a15fe52bea82 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 12:28:15.355: INFO: Pod "pod-configmaps-13db0cf9-e905-4861-b0b8-a15fe52bea82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013888769s
Sep 20 12:28:15.355: INFO: The phase of Pod pod-configmaps-13db0cf9-e905-4861-b0b8-a15fe52bea82 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 12:28:17.520: INFO: Pod "pod-configmaps-13db0cf9-e905-4861-b0b8-a15fe52bea82": Phase="Running", Reason="", readiness=true. Elapsed: 4.179258239s
Sep 20 12:28:17.520: INFO: The phase of Pod pod-configmaps-13db0cf9-e905-4861-b0b8-a15fe52bea82 is Running (Ready = true)
Sep 20 12:28:17.520: INFO: Pod "pod-configmaps-13db0cf9-e905-4861-b0b8-a15fe52bea82" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-572b533d-f5f3-4370-bdcb-fc7c6fdf2058 09/20/23 12:28:17.534
STEP: waiting to observe update in volume 09/20/23 12:28:17.754
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep 20 12:29:24.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6855" for this suite. 09/20/23 12:29:24.675
------------------------------
â€¢ [SLOW TEST] [71.438 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:28:13.261
    Sep 20 12:28:13.261: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename configmap 09/20/23 12:28:13.262
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:28:13.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:28:13.302
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-572b533d-f5f3-4370-bdcb-fc7c6fdf2058 09/20/23 12:28:13.313
    STEP: Creating the pod 09/20/23 12:28:13.326
    Sep 20 12:28:13.341: INFO: Waiting up to 5m0s for pod "pod-configmaps-13db0cf9-e905-4861-b0b8-a15fe52bea82" in namespace "configmap-6855" to be "running and ready"
    Sep 20 12:28:13.349: INFO: Pod "pod-configmaps-13db0cf9-e905-4861-b0b8-a15fe52bea82": Phase="Pending", Reason="", readiness=false. Elapsed: 8.506316ms
    Sep 20 12:28:13.349: INFO: The phase of Pod pod-configmaps-13db0cf9-e905-4861-b0b8-a15fe52bea82 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 12:28:15.355: INFO: Pod "pod-configmaps-13db0cf9-e905-4861-b0b8-a15fe52bea82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013888769s
    Sep 20 12:28:15.355: INFO: The phase of Pod pod-configmaps-13db0cf9-e905-4861-b0b8-a15fe52bea82 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 12:28:17.520: INFO: Pod "pod-configmaps-13db0cf9-e905-4861-b0b8-a15fe52bea82": Phase="Running", Reason="", readiness=true. Elapsed: 4.179258239s
    Sep 20 12:28:17.520: INFO: The phase of Pod pod-configmaps-13db0cf9-e905-4861-b0b8-a15fe52bea82 is Running (Ready = true)
    Sep 20 12:28:17.520: INFO: Pod "pod-configmaps-13db0cf9-e905-4861-b0b8-a15fe52bea82" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-572b533d-f5f3-4370-bdcb-fc7c6fdf2058 09/20/23 12:28:17.534
    STEP: waiting to observe update in volume 09/20/23 12:28:17.754
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:29:24.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6855" for this suite. 09/20/23 12:29:24.675
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:29:24.701
Sep 20 12:29:24.701: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename projected 09/20/23 12:29:24.702
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:29:24.731
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:29:24.735
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-e24b2990-bb7c-4cb6-9871-e8f834e667c8 09/20/23 12:29:24.743
STEP: Creating a pod to test consume configMaps 09/20/23 12:29:24.751
Sep 20 12:29:24.771: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2dd303c2-282a-479d-8044-ea121f0559ac" in namespace "projected-4996" to be "Succeeded or Failed"
Sep 20 12:29:24.785: INFO: Pod "pod-projected-configmaps-2dd303c2-282a-479d-8044-ea121f0559ac": Phase="Pending", Reason="", readiness=false. Elapsed: 14.076688ms
Sep 20 12:29:26.791: INFO: Pod "pod-projected-configmaps-2dd303c2-282a-479d-8044-ea121f0559ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020271794s
Sep 20 12:29:29.018: INFO: Pod "pod-projected-configmaps-2dd303c2-282a-479d-8044-ea121f0559ac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.246892396s
Sep 20 12:29:30.789: INFO: Pod "pod-projected-configmaps-2dd303c2-282a-479d-8044-ea121f0559ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017970839s
STEP: Saw pod success 09/20/23 12:29:30.789
Sep 20 12:29:30.789: INFO: Pod "pod-projected-configmaps-2dd303c2-282a-479d-8044-ea121f0559ac" satisfied condition "Succeeded or Failed"
Sep 20 12:29:30.794: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-2 pod pod-projected-configmaps-2dd303c2-282a-479d-8044-ea121f0559ac container projected-configmap-volume-test: <nil>
STEP: delete the pod 09/20/23 12:29:30.847
Sep 20 12:29:31.267: INFO: Waiting for pod pod-projected-configmaps-2dd303c2-282a-479d-8044-ea121f0559ac to disappear
Sep 20 12:29:31.272: INFO: Pod pod-projected-configmaps-2dd303c2-282a-479d-8044-ea121f0559ac no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep 20 12:29:31.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4996" for this suite. 09/20/23 12:29:31.277
------------------------------
â€¢ [SLOW TEST] [6.587 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:29:24.701
    Sep 20 12:29:24.701: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename projected 09/20/23 12:29:24.702
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:29:24.731
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:29:24.735
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-e24b2990-bb7c-4cb6-9871-e8f834e667c8 09/20/23 12:29:24.743
    STEP: Creating a pod to test consume configMaps 09/20/23 12:29:24.751
    Sep 20 12:29:24.771: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2dd303c2-282a-479d-8044-ea121f0559ac" in namespace "projected-4996" to be "Succeeded or Failed"
    Sep 20 12:29:24.785: INFO: Pod "pod-projected-configmaps-2dd303c2-282a-479d-8044-ea121f0559ac": Phase="Pending", Reason="", readiness=false. Elapsed: 14.076688ms
    Sep 20 12:29:26.791: INFO: Pod "pod-projected-configmaps-2dd303c2-282a-479d-8044-ea121f0559ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020271794s
    Sep 20 12:29:29.018: INFO: Pod "pod-projected-configmaps-2dd303c2-282a-479d-8044-ea121f0559ac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.246892396s
    Sep 20 12:29:30.789: INFO: Pod "pod-projected-configmaps-2dd303c2-282a-479d-8044-ea121f0559ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017970839s
    STEP: Saw pod success 09/20/23 12:29:30.789
    Sep 20 12:29:30.789: INFO: Pod "pod-projected-configmaps-2dd303c2-282a-479d-8044-ea121f0559ac" satisfied condition "Succeeded or Failed"
    Sep 20 12:29:30.794: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-2 pod pod-projected-configmaps-2dd303c2-282a-479d-8044-ea121f0559ac container projected-configmap-volume-test: <nil>
    STEP: delete the pod 09/20/23 12:29:30.847
    Sep 20 12:29:31.267: INFO: Waiting for pod pod-projected-configmaps-2dd303c2-282a-479d-8044-ea121f0559ac to disappear
    Sep 20 12:29:31.272: INFO: Pod pod-projected-configmaps-2dd303c2-282a-479d-8044-ea121f0559ac no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:29:31.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4996" for this suite. 09/20/23 12:29:31.277
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:29:31.289
Sep 20 12:29:31.289: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename podtemplate 09/20/23 12:29:31.29
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:29:31.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:29:31.339
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 09/20/23 12:29:31.344
Sep 20 12:29:31.438: INFO: created test-podtemplate-1
Sep 20 12:29:31.446: INFO: created test-podtemplate-2
Sep 20 12:29:31.451: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 09/20/23 12:29:31.451
STEP: delete collection of pod templates 09/20/23 12:29:31.455
Sep 20 12:29:31.455: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 09/20/23 12:29:31.478
Sep 20 12:29:31.479: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Sep 20 12:29:31.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-810" for this suite. 09/20/23 12:29:31.492
------------------------------
â€¢ [0.216 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:29:31.289
    Sep 20 12:29:31.289: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename podtemplate 09/20/23 12:29:31.29
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:29:31.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:29:31.339
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 09/20/23 12:29:31.344
    Sep 20 12:29:31.438: INFO: created test-podtemplate-1
    Sep 20 12:29:31.446: INFO: created test-podtemplate-2
    Sep 20 12:29:31.451: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 09/20/23 12:29:31.451
    STEP: delete collection of pod templates 09/20/23 12:29:31.455
    Sep 20 12:29:31.455: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 09/20/23 12:29:31.478
    Sep 20 12:29:31.479: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:29:31.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-810" for this suite. 09/20/23 12:29:31.492
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:29:31.509
Sep 20 12:29:31.509: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename kubelet-test 09/20/23 12:29:31.51
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:29:31.56
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:29:31.564
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 09/20/23 12:29:31.587
Sep 20 12:29:31.588: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesff6eb404-4b43-4d02-bc34-6bfc24a73928" in namespace "kubelet-test-8861" to be "completed"
Sep 20 12:29:32.101: INFO: Pod "agnhost-host-aliasesff6eb404-4b43-4d02-bc34-6bfc24a73928": Phase="Pending", Reason="", readiness=false. Elapsed: 513.520405ms
Sep 20 12:29:34.107: INFO: Pod "agnhost-host-aliasesff6eb404-4b43-4d02-bc34-6bfc24a73928": Phase="Pending", Reason="", readiness=false. Elapsed: 2.519000183s
Sep 20 12:29:36.233: INFO: Pod "agnhost-host-aliasesff6eb404-4b43-4d02-bc34-6bfc24a73928": Phase="Running", Reason="", readiness=false. Elapsed: 4.644968423s
Sep 20 12:29:38.106: INFO: Pod "agnhost-host-aliasesff6eb404-4b43-4d02-bc34-6bfc24a73928": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.518426452s
Sep 20 12:29:38.106: INFO: Pod "agnhost-host-aliasesff6eb404-4b43-4d02-bc34-6bfc24a73928" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Sep 20 12:29:38.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-8861" for this suite. 09/20/23 12:29:38.127
------------------------------
â€¢ [SLOW TEST] [6.626 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:29:31.509
    Sep 20 12:29:31.509: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename kubelet-test 09/20/23 12:29:31.51
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:29:31.56
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:29:31.564
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 09/20/23 12:29:31.587
    Sep 20 12:29:31.588: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesff6eb404-4b43-4d02-bc34-6bfc24a73928" in namespace "kubelet-test-8861" to be "completed"
    Sep 20 12:29:32.101: INFO: Pod "agnhost-host-aliasesff6eb404-4b43-4d02-bc34-6bfc24a73928": Phase="Pending", Reason="", readiness=false. Elapsed: 513.520405ms
    Sep 20 12:29:34.107: INFO: Pod "agnhost-host-aliasesff6eb404-4b43-4d02-bc34-6bfc24a73928": Phase="Pending", Reason="", readiness=false. Elapsed: 2.519000183s
    Sep 20 12:29:36.233: INFO: Pod "agnhost-host-aliasesff6eb404-4b43-4d02-bc34-6bfc24a73928": Phase="Running", Reason="", readiness=false. Elapsed: 4.644968423s
    Sep 20 12:29:38.106: INFO: Pod "agnhost-host-aliasesff6eb404-4b43-4d02-bc34-6bfc24a73928": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.518426452s
    Sep 20 12:29:38.106: INFO: Pod "agnhost-host-aliasesff6eb404-4b43-4d02-bc34-6bfc24a73928" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:29:38.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-8861" for this suite. 09/20/23 12:29:38.127
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:29:38.137
Sep 20 12:29:38.137: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename secrets 09/20/23 12:29:38.138
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:29:38.204
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:29:38.206
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 09/20/23 12:29:38.21
STEP: listing secrets in all namespaces to ensure that there are more than zero 09/20/23 12:29:38.27
STEP: patching the secret 09/20/23 12:29:38.594
STEP: deleting the secret using a LabelSelector 09/20/23 12:29:38.833
STEP: listing secrets in all namespaces, searching for label name and value in patch 09/20/23 12:29:38.95
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Sep 20 12:29:38.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3097" for this suite. 09/20/23 12:29:38.965
------------------------------
â€¢ [0.841 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:29:38.137
    Sep 20 12:29:38.137: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename secrets 09/20/23 12:29:38.138
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:29:38.204
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:29:38.206
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 09/20/23 12:29:38.21
    STEP: listing secrets in all namespaces to ensure that there are more than zero 09/20/23 12:29:38.27
    STEP: patching the secret 09/20/23 12:29:38.594
    STEP: deleting the secret using a LabelSelector 09/20/23 12:29:38.833
    STEP: listing secrets in all namespaces, searching for label name and value in patch 09/20/23 12:29:38.95
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:29:38.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3097" for this suite. 09/20/23 12:29:38.965
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:29:38.98
Sep 20 12:29:38.980: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename namespaces 09/20/23 12:29:38.98
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:29:39.296
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:29:39.301
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 09/20/23 12:29:39.31
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:29:39.336
STEP: Creating a pod in the namespace 09/20/23 12:29:39.341
STEP: Waiting for the pod to have running status 09/20/23 12:29:39.359
Sep 20 12:29:39.360: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-8315" to be "running"
Sep 20 12:29:39.369: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.027376ms
Sep 20 12:29:41.374: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014359265s
Sep 20 12:29:43.376: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016036641s
Sep 20 12:29:45.376: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016771982s
Sep 20 12:29:47.376: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.016437015s
Sep 20 12:29:47.376: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 09/20/23 12:29:47.376
STEP: Waiting for the namespace to be removed. 09/20/23 12:29:47.386
STEP: Recreating the namespace 09/20/23 12:29:59.391
STEP: Verifying there are no pods in the namespace 09/20/23 12:29:59.779
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep 20 12:30:00.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-356" for this suite. 09/20/23 12:30:00.06
STEP: Destroying namespace "nsdeletetest-8315" for this suite. 09/20/23 12:30:00.069
Sep 20 12:30:00.074: INFO: Namespace nsdeletetest-8315 was already deleted
STEP: Destroying namespace "nsdeletetest-4767" for this suite. 09/20/23 12:30:00.074
------------------------------
â€¢ [SLOW TEST] [21.489 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:29:38.98
    Sep 20 12:29:38.980: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename namespaces 09/20/23 12:29:38.98
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:29:39.296
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:29:39.301
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 09/20/23 12:29:39.31
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:29:39.336
    STEP: Creating a pod in the namespace 09/20/23 12:29:39.341
    STEP: Waiting for the pod to have running status 09/20/23 12:29:39.359
    Sep 20 12:29:39.360: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-8315" to be "running"
    Sep 20 12:29:39.369: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.027376ms
    Sep 20 12:29:41.374: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014359265s
    Sep 20 12:29:43.376: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016036641s
    Sep 20 12:29:45.376: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016771982s
    Sep 20 12:29:47.376: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.016437015s
    Sep 20 12:29:47.376: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 09/20/23 12:29:47.376
    STEP: Waiting for the namespace to be removed. 09/20/23 12:29:47.386
    STEP: Recreating the namespace 09/20/23 12:29:59.391
    STEP: Verifying there are no pods in the namespace 09/20/23 12:29:59.779
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:30:00.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-356" for this suite. 09/20/23 12:30:00.06
    STEP: Destroying namespace "nsdeletetest-8315" for this suite. 09/20/23 12:30:00.069
    Sep 20 12:30:00.074: INFO: Namespace nsdeletetest-8315 was already deleted
    STEP: Destroying namespace "nsdeletetest-4767" for this suite. 09/20/23 12:30:00.074
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:30:00.471
Sep 20 12:30:00.471: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename webhook 09/20/23 12:30:00.472
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:30:01.696
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:30:01.701
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/20/23 12:30:01.724
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/20/23 12:30:02.149
STEP: Deploying the webhook pod 09/20/23 12:30:02.225
STEP: Wait for the deployment to be ready 09/20/23 12:30:02.594
Sep 20 12:30:02.602: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep 20 12:30:05.372: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 30, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 30, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 30, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 30, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 12:30:07.508: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 30, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 30, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 30, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 30, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/20/23 12:30:09.559
STEP: Verifying the service has paired with the endpoint 09/20/23 12:30:09.665
Sep 20 12:30:10.665: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 09/20/23 12:30:10.671
STEP: create a pod 09/20/23 12:30:10.692
Sep 20 12:30:10.700: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-7814" to be "running"
Sep 20 12:30:10.706: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.37698ms
Sep 20 12:30:12.709: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009117923s
Sep 20 12:30:14.711: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01058982s
Sep 20 12:30:16.709: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.009237067s
Sep 20 12:30:16.709: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 09/20/23 12:30:16.709
Sep 20 12:30:16.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=webhook-7814 attach --namespace=webhook-7814 to-be-attached-pod -i -c=container1'
Sep 20 12:30:17.758: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep 20 12:30:17.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7814" for this suite. 09/20/23 12:30:18.186
STEP: Destroying namespace "webhook-7814-markers" for this suite. 09/20/23 12:30:18.271
------------------------------
â€¢ [SLOW TEST] [18.277 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:30:00.471
    Sep 20 12:30:00.471: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename webhook 09/20/23 12:30:00.472
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:30:01.696
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:30:01.701
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/20/23 12:30:01.724
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/20/23 12:30:02.149
    STEP: Deploying the webhook pod 09/20/23 12:30:02.225
    STEP: Wait for the deployment to be ready 09/20/23 12:30:02.594
    Sep 20 12:30:02.602: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Sep 20 12:30:05.372: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 30, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 30, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 30, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 30, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 12:30:07.508: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 30, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 30, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 30, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 30, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/20/23 12:30:09.559
    STEP: Verifying the service has paired with the endpoint 09/20/23 12:30:09.665
    Sep 20 12:30:10.665: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 09/20/23 12:30:10.671
    STEP: create a pod 09/20/23 12:30:10.692
    Sep 20 12:30:10.700: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-7814" to be "running"
    Sep 20 12:30:10.706: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.37698ms
    Sep 20 12:30:12.709: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009117923s
    Sep 20 12:30:14.711: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01058982s
    Sep 20 12:30:16.709: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.009237067s
    Sep 20 12:30:16.709: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 09/20/23 12:30:16.709
    Sep 20 12:30:16.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=webhook-7814 attach --namespace=webhook-7814 to-be-attached-pod -i -c=container1'
    Sep 20 12:30:17.758: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:30:17.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7814" for this suite. 09/20/23 12:30:18.186
    STEP: Destroying namespace "webhook-7814-markers" for this suite. 09/20/23 12:30:18.271
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:30:18.748
Sep 20 12:30:18.748: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename controllerrevisions 09/20/23 12:30:18.748
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:30:18.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:30:18.792
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-9dwzn-daemon-set" 09/20/23 12:30:18.83
STEP: Check that daemon pods launch on every node of the cluster. 09/20/23 12:30:19.105
Sep 20 12:30:19.116: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:30:19.117: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:30:19.117: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:30:19.177: INFO: Number of nodes with available pods controlled by daemonset e2e-9dwzn-daemon-set: 0
Sep 20 12:30:19.177: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 12:30:20.797: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:30:20.797: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:30:20.797: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:30:21.411: INFO: Number of nodes with available pods controlled by daemonset e2e-9dwzn-daemon-set: 0
Sep 20 12:30:21.411: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 12:30:22.194: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:30:22.194: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:30:22.194: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:30:22.200: INFO: Number of nodes with available pods controlled by daemonset e2e-9dwzn-daemon-set: 1
Sep 20 12:30:22.200: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 12:30:23.466: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:30:23.467: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:30:23.467: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:30:23.488: INFO: Number of nodes with available pods controlled by daemonset e2e-9dwzn-daemon-set: 2
Sep 20 12:30:23.488: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 12:30:24.278: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:30:24.279: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:30:24.279: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:30:24.287: INFO: Number of nodes with available pods controlled by daemonset e2e-9dwzn-daemon-set: 3
Sep 20 12:30:24.287: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-9dwzn-daemon-set
STEP: Confirm DaemonSet "e2e-9dwzn-daemon-set" successfully created with "daemonset-name=e2e-9dwzn-daemon-set" label 09/20/23 12:30:24.297
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-9dwzn-daemon-set" 09/20/23 12:30:24.477
Sep 20 12:30:24.490: INFO: Located ControllerRevision: "e2e-9dwzn-daemon-set-65c5bd8d5f"
STEP: Patching ControllerRevision "e2e-9dwzn-daemon-set-65c5bd8d5f" 09/20/23 12:30:24.5
Sep 20 12:30:24.515: INFO: e2e-9dwzn-daemon-set-65c5bd8d5f has been patched
STEP: Create a new ControllerRevision 09/20/23 12:30:24.515
Sep 20 12:30:24.581: INFO: Created ControllerRevision: e2e-9dwzn-daemon-set-74d4fbb9b7
STEP: Confirm that there are two ControllerRevisions 09/20/23 12:30:24.582
Sep 20 12:30:24.582: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep 20 12:30:24.590: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-9dwzn-daemon-set-65c5bd8d5f" 09/20/23 12:30:24.591
STEP: Confirm that there is only one ControllerRevision 09/20/23 12:30:24.607
Sep 20 12:30:24.607: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep 20 12:30:24.613: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-9dwzn-daemon-set-74d4fbb9b7" 09/20/23 12:30:24.619
Sep 20 12:30:24.644: INFO: e2e-9dwzn-daemon-set-74d4fbb9b7 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 09/20/23 12:30:24.644
W0920 12:30:26.416360      20 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 09/20/23 12:30:26.416
Sep 20 12:30:26.416: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep 20 12:30:27.469: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep 20 12:30:27.753: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-9dwzn-daemon-set-74d4fbb9b7=updated" 09/20/23 12:30:27.753
STEP: Confirm that there is only one ControllerRevision 09/20/23 12:30:27.764
Sep 20 12:30:27.764: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep 20 12:30:27.769: INFO: Found 1 ControllerRevisions
Sep 20 12:30:27.775: INFO: ControllerRevision "e2e-9dwzn-daemon-set-764f7c6b86" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-9dwzn-daemon-set" 09/20/23 12:30:27.782
STEP: deleting DaemonSet.extensions e2e-9dwzn-daemon-set in namespace controllerrevisions-7489, will wait for the garbage collector to delete the pods 09/20/23 12:30:27.782
Sep 20 12:30:27.847: INFO: Deleting DaemonSet.extensions e2e-9dwzn-daemon-set took: 9.906733ms
Sep 20 12:30:29.248: INFO: Terminating DaemonSet.extensions e2e-9dwzn-daemon-set pods took: 1.401160535s
Sep 20 12:30:31.572: INFO: Number of nodes with available pods controlled by daemonset e2e-9dwzn-daemon-set: 0
Sep 20 12:30:31.572: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-9dwzn-daemon-set
Sep 20 12:30:31.576: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"13718"},"items":null}

Sep 20 12:30:31.578: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"13718"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Sep 20 12:30:31.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-7489" for this suite. 09/20/23 12:30:31.601
------------------------------
â€¢ [SLOW TEST] [14.231 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:30:18.748
    Sep 20 12:30:18.748: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename controllerrevisions 09/20/23 12:30:18.748
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:30:18.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:30:18.792
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-9dwzn-daemon-set" 09/20/23 12:30:18.83
    STEP: Check that daemon pods launch on every node of the cluster. 09/20/23 12:30:19.105
    Sep 20 12:30:19.116: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:30:19.117: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:30:19.117: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:30:19.177: INFO: Number of nodes with available pods controlled by daemonset e2e-9dwzn-daemon-set: 0
    Sep 20 12:30:19.177: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 12:30:20.797: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:30:20.797: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:30:20.797: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:30:21.411: INFO: Number of nodes with available pods controlled by daemonset e2e-9dwzn-daemon-set: 0
    Sep 20 12:30:21.411: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 12:30:22.194: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:30:22.194: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:30:22.194: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:30:22.200: INFO: Number of nodes with available pods controlled by daemonset e2e-9dwzn-daemon-set: 1
    Sep 20 12:30:22.200: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 12:30:23.466: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:30:23.467: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:30:23.467: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:30:23.488: INFO: Number of nodes with available pods controlled by daemonset e2e-9dwzn-daemon-set: 2
    Sep 20 12:30:23.488: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 12:30:24.278: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:30:24.279: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:30:24.279: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:30:24.287: INFO: Number of nodes with available pods controlled by daemonset e2e-9dwzn-daemon-set: 3
    Sep 20 12:30:24.287: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-9dwzn-daemon-set
    STEP: Confirm DaemonSet "e2e-9dwzn-daemon-set" successfully created with "daemonset-name=e2e-9dwzn-daemon-set" label 09/20/23 12:30:24.297
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-9dwzn-daemon-set" 09/20/23 12:30:24.477
    Sep 20 12:30:24.490: INFO: Located ControllerRevision: "e2e-9dwzn-daemon-set-65c5bd8d5f"
    STEP: Patching ControllerRevision "e2e-9dwzn-daemon-set-65c5bd8d5f" 09/20/23 12:30:24.5
    Sep 20 12:30:24.515: INFO: e2e-9dwzn-daemon-set-65c5bd8d5f has been patched
    STEP: Create a new ControllerRevision 09/20/23 12:30:24.515
    Sep 20 12:30:24.581: INFO: Created ControllerRevision: e2e-9dwzn-daemon-set-74d4fbb9b7
    STEP: Confirm that there are two ControllerRevisions 09/20/23 12:30:24.582
    Sep 20 12:30:24.582: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep 20 12:30:24.590: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-9dwzn-daemon-set-65c5bd8d5f" 09/20/23 12:30:24.591
    STEP: Confirm that there is only one ControllerRevision 09/20/23 12:30:24.607
    Sep 20 12:30:24.607: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep 20 12:30:24.613: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-9dwzn-daemon-set-74d4fbb9b7" 09/20/23 12:30:24.619
    Sep 20 12:30:24.644: INFO: e2e-9dwzn-daemon-set-74d4fbb9b7 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 09/20/23 12:30:24.644
    W0920 12:30:26.416360      20 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 09/20/23 12:30:26.416
    Sep 20 12:30:26.416: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep 20 12:30:27.469: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep 20 12:30:27.753: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-9dwzn-daemon-set-74d4fbb9b7=updated" 09/20/23 12:30:27.753
    STEP: Confirm that there is only one ControllerRevision 09/20/23 12:30:27.764
    Sep 20 12:30:27.764: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep 20 12:30:27.769: INFO: Found 1 ControllerRevisions
    Sep 20 12:30:27.775: INFO: ControllerRevision "e2e-9dwzn-daemon-set-764f7c6b86" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-9dwzn-daemon-set" 09/20/23 12:30:27.782
    STEP: deleting DaemonSet.extensions e2e-9dwzn-daemon-set in namespace controllerrevisions-7489, will wait for the garbage collector to delete the pods 09/20/23 12:30:27.782
    Sep 20 12:30:27.847: INFO: Deleting DaemonSet.extensions e2e-9dwzn-daemon-set took: 9.906733ms
    Sep 20 12:30:29.248: INFO: Terminating DaemonSet.extensions e2e-9dwzn-daemon-set pods took: 1.401160535s
    Sep 20 12:30:31.572: INFO: Number of nodes with available pods controlled by daemonset e2e-9dwzn-daemon-set: 0
    Sep 20 12:30:31.572: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-9dwzn-daemon-set
    Sep 20 12:30:31.576: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"13718"},"items":null}

    Sep 20 12:30:31.578: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"13718"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:30:31.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-7489" for this suite. 09/20/23 12:30:31.601
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:30:32.981
Sep 20 12:30:32.981: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename configmap 09/20/23 12:30:32.982
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:30:33.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:30:33.414
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-76c0f5c6-1d67-4dc9-a8a3-2332d1b3ce1c 09/20/23 12:30:33.466
STEP: Creating a pod to test consume configMaps 09/20/23 12:30:34.411
Sep 20 12:30:35.254: INFO: Waiting up to 5m0s for pod "pod-configmaps-2018f61d-3339-446d-8c3a-3a3e238b42b1" in namespace "configmap-7203" to be "Succeeded or Failed"
Sep 20 12:30:35.860: INFO: Pod "pod-configmaps-2018f61d-3339-446d-8c3a-3a3e238b42b1": Phase="Pending", Reason="", readiness=false. Elapsed: 606.136017ms
Sep 20 12:30:38.012: INFO: Pod "pod-configmaps-2018f61d-3339-446d-8c3a-3a3e238b42b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.758759092s
Sep 20 12:30:40.119: INFO: Pod "pod-configmaps-2018f61d-3339-446d-8c3a-3a3e238b42b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.865306314s
Sep 20 12:30:41.932: INFO: Pod "pod-configmaps-2018f61d-3339-446d-8c3a-3a3e238b42b1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.678207799s
Sep 20 12:30:43.866: INFO: Pod "pod-configmaps-2018f61d-3339-446d-8c3a-3a3e238b42b1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.611993956s
Sep 20 12:30:46.352: INFO: Pod "pod-configmaps-2018f61d-3339-446d-8c3a-3a3e238b42b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.098715817s
STEP: Saw pod success 09/20/23 12:30:46.353
Sep 20 12:30:46.353: INFO: Pod "pod-configmaps-2018f61d-3339-446d-8c3a-3a3e238b42b1" satisfied condition "Succeeded or Failed"
Sep 20 12:30:46.358: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-2 pod pod-configmaps-2018f61d-3339-446d-8c3a-3a3e238b42b1 container agnhost-container: <nil>
STEP: delete the pod 09/20/23 12:30:46.37
Sep 20 12:30:46.629: INFO: Waiting for pod pod-configmaps-2018f61d-3339-446d-8c3a-3a3e238b42b1 to disappear
Sep 20 12:30:46.635: INFO: Pod pod-configmaps-2018f61d-3339-446d-8c3a-3a3e238b42b1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep 20 12:30:46.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7203" for this suite. 09/20/23 12:30:46.643
------------------------------
â€¢ [SLOW TEST] [13.672 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:30:32.981
    Sep 20 12:30:32.981: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename configmap 09/20/23 12:30:32.982
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:30:33.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:30:33.414
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-76c0f5c6-1d67-4dc9-a8a3-2332d1b3ce1c 09/20/23 12:30:33.466
    STEP: Creating a pod to test consume configMaps 09/20/23 12:30:34.411
    Sep 20 12:30:35.254: INFO: Waiting up to 5m0s for pod "pod-configmaps-2018f61d-3339-446d-8c3a-3a3e238b42b1" in namespace "configmap-7203" to be "Succeeded or Failed"
    Sep 20 12:30:35.860: INFO: Pod "pod-configmaps-2018f61d-3339-446d-8c3a-3a3e238b42b1": Phase="Pending", Reason="", readiness=false. Elapsed: 606.136017ms
    Sep 20 12:30:38.012: INFO: Pod "pod-configmaps-2018f61d-3339-446d-8c3a-3a3e238b42b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.758759092s
    Sep 20 12:30:40.119: INFO: Pod "pod-configmaps-2018f61d-3339-446d-8c3a-3a3e238b42b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.865306314s
    Sep 20 12:30:41.932: INFO: Pod "pod-configmaps-2018f61d-3339-446d-8c3a-3a3e238b42b1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.678207799s
    Sep 20 12:30:43.866: INFO: Pod "pod-configmaps-2018f61d-3339-446d-8c3a-3a3e238b42b1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.611993956s
    Sep 20 12:30:46.352: INFO: Pod "pod-configmaps-2018f61d-3339-446d-8c3a-3a3e238b42b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.098715817s
    STEP: Saw pod success 09/20/23 12:30:46.353
    Sep 20 12:30:46.353: INFO: Pod "pod-configmaps-2018f61d-3339-446d-8c3a-3a3e238b42b1" satisfied condition "Succeeded or Failed"
    Sep 20 12:30:46.358: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-2 pod pod-configmaps-2018f61d-3339-446d-8c3a-3a3e238b42b1 container agnhost-container: <nil>
    STEP: delete the pod 09/20/23 12:30:46.37
    Sep 20 12:30:46.629: INFO: Waiting for pod pod-configmaps-2018f61d-3339-446d-8c3a-3a3e238b42b1 to disappear
    Sep 20 12:30:46.635: INFO: Pod pod-configmaps-2018f61d-3339-446d-8c3a-3a3e238b42b1 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:30:46.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7203" for this suite. 09/20/23 12:30:46.643
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:30:46.653
Sep 20 12:30:46.653: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename replicaset 09/20/23 12:30:46.654
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:30:46.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:30:46.691
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 09/20/23 12:30:46.697
Sep 20 12:30:46.710: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-9388" to be "running and ready"
Sep 20 12:30:46.718: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 7.779667ms
Sep 20 12:30:46.718: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Sep 20 12:30:48.725: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014265763s
Sep 20 12:30:48.725: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Sep 20 12:30:50.859: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 4.148683899s
Sep 20 12:30:50.859: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Sep 20 12:30:50.859: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 09/20/23 12:30:50.867
STEP: Then the orphan pod is adopted 09/20/23 12:30:50.879
STEP: When the matched label of one of its pods change 09/20/23 12:30:51.981
Sep 20 12:30:51.986: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 09/20/23 12:30:52.079
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep 20 12:30:53.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-9388" for this suite. 09/20/23 12:30:53.296
------------------------------
â€¢ [SLOW TEST] [6.652 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:30:46.653
    Sep 20 12:30:46.653: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename replicaset 09/20/23 12:30:46.654
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:30:46.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:30:46.691
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 09/20/23 12:30:46.697
    Sep 20 12:30:46.710: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-9388" to be "running and ready"
    Sep 20 12:30:46.718: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 7.779667ms
    Sep 20 12:30:46.718: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 12:30:48.725: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014265763s
    Sep 20 12:30:48.725: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 12:30:50.859: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 4.148683899s
    Sep 20 12:30:50.859: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Sep 20 12:30:50.859: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 09/20/23 12:30:50.867
    STEP: Then the orphan pod is adopted 09/20/23 12:30:50.879
    STEP: When the matched label of one of its pods change 09/20/23 12:30:51.981
    Sep 20 12:30:51.986: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 09/20/23 12:30:52.079
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:30:53.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-9388" for this suite. 09/20/23 12:30:53.296
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:30:53.306
Sep 20 12:30:53.306: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename var-expansion 09/20/23 12:30:53.307
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:30:53.339
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:30:53.343
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Sep 20 12:30:53.382: INFO: Waiting up to 2m0s for pod "var-expansion-1d707504-adaa-44df-a22c-1a5d4be30dda" in namespace "var-expansion-1378" to be "container 0 failed with reason CreateContainerConfigError"
Sep 20 12:30:53.386: INFO: Pod "var-expansion-1d707504-adaa-44df-a22c-1a5d4be30dda": Phase="Pending", Reason="", readiness=false. Elapsed: 3.88495ms
Sep 20 12:30:55.392: INFO: Pod "var-expansion-1d707504-adaa-44df-a22c-1a5d4be30dda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009375338s
Sep 20 12:30:57.665: INFO: Pod "var-expansion-1d707504-adaa-44df-a22c-1a5d4be30dda": Phase="Pending", Reason="", readiness=false. Elapsed: 4.283018592s
Sep 20 12:30:57.665: INFO: Pod "var-expansion-1d707504-adaa-44df-a22c-1a5d4be30dda" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Sep 20 12:30:57.665: INFO: Deleting pod "var-expansion-1d707504-adaa-44df-a22c-1a5d4be30dda" in namespace "var-expansion-1378"
Sep 20 12:30:57.854: INFO: Wait up to 5m0s for pod "var-expansion-1d707504-adaa-44df-a22c-1a5d4be30dda" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep 20 12:31:00.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-1378" for this suite. 09/20/23 12:31:00.658
------------------------------
â€¢ [SLOW TEST] [7.365 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:30:53.306
    Sep 20 12:30:53.306: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename var-expansion 09/20/23 12:30:53.307
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:30:53.339
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:30:53.343
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Sep 20 12:30:53.382: INFO: Waiting up to 2m0s for pod "var-expansion-1d707504-adaa-44df-a22c-1a5d4be30dda" in namespace "var-expansion-1378" to be "container 0 failed with reason CreateContainerConfigError"
    Sep 20 12:30:53.386: INFO: Pod "var-expansion-1d707504-adaa-44df-a22c-1a5d4be30dda": Phase="Pending", Reason="", readiness=false. Elapsed: 3.88495ms
    Sep 20 12:30:55.392: INFO: Pod "var-expansion-1d707504-adaa-44df-a22c-1a5d4be30dda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009375338s
    Sep 20 12:30:57.665: INFO: Pod "var-expansion-1d707504-adaa-44df-a22c-1a5d4be30dda": Phase="Pending", Reason="", readiness=false. Elapsed: 4.283018592s
    Sep 20 12:30:57.665: INFO: Pod "var-expansion-1d707504-adaa-44df-a22c-1a5d4be30dda" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Sep 20 12:30:57.665: INFO: Deleting pod "var-expansion-1d707504-adaa-44df-a22c-1a5d4be30dda" in namespace "var-expansion-1378"
    Sep 20 12:30:57.854: INFO: Wait up to 5m0s for pod "var-expansion-1d707504-adaa-44df-a22c-1a5d4be30dda" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:31:00.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-1378" for this suite. 09/20/23 12:31:00.658
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:31:00.672
Sep 20 12:31:00.672: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename daemonsets 09/20/23 12:31:00.673
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:31:02.053
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:31:02.06
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443
Sep 20 12:31:02.311: INFO: Create a RollingUpdate DaemonSet
Sep 20 12:31:02.604: INFO: Check that daemon pods launch on every node of the cluster
Sep 20 12:31:02.670: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:02.671: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:02.671: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:02.692: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 12:31:02.692: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 12:31:03.919: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:03.919: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:03.919: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:03.933: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 12:31:03.933: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 12:31:04.708: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:04.708: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:04.708: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:04.715: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 12:31:04.715: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 12:31:05.712: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:05.713: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:05.713: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:05.716: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 12:31:05.716: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 12:31:06.873: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:06.873: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:06.873: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:06.884: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 12:31:06.884: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 12:31:07.921: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:07.921: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:07.921: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:07.925: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Sep 20 12:31:07.925: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Sep 20 12:31:07.925: INFO: Update the DaemonSet to trigger a rollout
Sep 20 12:31:08.003: INFO: Updating DaemonSet daemon-set
Sep 20 12:31:11.027: INFO: Roll back the DaemonSet before rollout is complete
Sep 20 12:31:11.049: INFO: Updating DaemonSet daemon-set
Sep 20 12:31:11.050: INFO: Make sure DaemonSet rollback is complete
Sep 20 12:31:11.053: INFO: Wrong image for pod: daemon-set-tgxdb. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Sep 20 12:31:11.053: INFO: Pod daemon-set-tgxdb is not available
Sep 20 12:31:11.079: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:11.079: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:11.079: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:12.088: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:12.088: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:12.088: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:13.087: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:13.087: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:13.087: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:14.087: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:14.087: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:14.087: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:15.088: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:15.088: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:15.088: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:16.089: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:16.089: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:16.089: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:17.261: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:17.261: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:17.261: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:18.322: INFO: Pod daemon-set-v8v2h is not available
Sep 20 12:31:18.328: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:18.328: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 12:31:18.328: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 09/20/23 12:31:18.336
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9618, will wait for the garbage collector to delete the pods 09/20/23 12:31:18.336
Sep 20 12:31:18.398: INFO: Deleting DaemonSet.extensions daemon-set took: 8.627704ms
Sep 20 12:31:18.499: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.43062ms
Sep 20 12:31:24.105: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 12:31:24.105: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep 20 12:31:24.108: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"14105"},"items":null}

Sep 20 12:31:24.110: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"14105"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep 20 12:31:24.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-9618" for this suite. 09/20/23 12:31:24.131
------------------------------
â€¢ [SLOW TEST] [23.612 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:31:00.672
    Sep 20 12:31:00.672: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename daemonsets 09/20/23 12:31:00.673
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:31:02.053
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:31:02.06
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:443
    Sep 20 12:31:02.311: INFO: Create a RollingUpdate DaemonSet
    Sep 20 12:31:02.604: INFO: Check that daemon pods launch on every node of the cluster
    Sep 20 12:31:02.670: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:02.671: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:02.671: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:02.692: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 12:31:02.692: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 12:31:03.919: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:03.919: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:03.919: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:03.933: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 12:31:03.933: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 12:31:04.708: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:04.708: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:04.708: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:04.715: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 12:31:04.715: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 12:31:05.712: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:05.713: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:05.713: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:05.716: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 12:31:05.716: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 12:31:06.873: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:06.873: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:06.873: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:06.884: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 12:31:06.884: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 12:31:07.921: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:07.921: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:07.921: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:07.925: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Sep 20 12:31:07.925: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Sep 20 12:31:07.925: INFO: Update the DaemonSet to trigger a rollout
    Sep 20 12:31:08.003: INFO: Updating DaemonSet daemon-set
    Sep 20 12:31:11.027: INFO: Roll back the DaemonSet before rollout is complete
    Sep 20 12:31:11.049: INFO: Updating DaemonSet daemon-set
    Sep 20 12:31:11.050: INFO: Make sure DaemonSet rollback is complete
    Sep 20 12:31:11.053: INFO: Wrong image for pod: daemon-set-tgxdb. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Sep 20 12:31:11.053: INFO: Pod daemon-set-tgxdb is not available
    Sep 20 12:31:11.079: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:11.079: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:11.079: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:12.088: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:12.088: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:12.088: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:13.087: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:13.087: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:13.087: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:14.087: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:14.087: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:14.087: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:15.088: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:15.088: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:15.088: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:16.089: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:16.089: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:16.089: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:17.261: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:17.261: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:17.261: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:18.322: INFO: Pod daemon-set-v8v2h is not available
    Sep 20 12:31:18.328: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:18.328: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 12:31:18.328: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 09/20/23 12:31:18.336
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9618, will wait for the garbage collector to delete the pods 09/20/23 12:31:18.336
    Sep 20 12:31:18.398: INFO: Deleting DaemonSet.extensions daemon-set took: 8.627704ms
    Sep 20 12:31:18.499: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.43062ms
    Sep 20 12:31:24.105: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 12:31:24.105: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep 20 12:31:24.108: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"14105"},"items":null}

    Sep 20 12:31:24.110: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"14105"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:31:24.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-9618" for this suite. 09/20/23 12:31:24.131
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:31:24.288
Sep 20 12:31:24.288: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename downward-api 09/20/23 12:31:24.289
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:31:24.353
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:31:24.357
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 09/20/23 12:31:24.41
Sep 20 12:31:24.434: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ebde28ca-810c-4ec1-9ece-69bed7df696e" in namespace "downward-api-5763" to be "Succeeded or Failed"
Sep 20 12:31:24.442: INFO: Pod "downwardapi-volume-ebde28ca-810c-4ec1-9ece-69bed7df696e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.061327ms
Sep 20 12:31:26.447: INFO: Pod "downwardapi-volume-ebde28ca-810c-4ec1-9ece-69bed7df696e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012721173s
Sep 20 12:31:28.448: INFO: Pod "downwardapi-volume-ebde28ca-810c-4ec1-9ece-69bed7df696e": Phase="Running", Reason="", readiness=true. Elapsed: 4.013992462s
Sep 20 12:31:30.712: INFO: Pod "downwardapi-volume-ebde28ca-810c-4ec1-9ece-69bed7df696e": Phase="Running", Reason="", readiness=false. Elapsed: 6.277605721s
Sep 20 12:31:32.448: INFO: Pod "downwardapi-volume-ebde28ca-810c-4ec1-9ece-69bed7df696e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.014161041s
STEP: Saw pod success 09/20/23 12:31:32.448
Sep 20 12:31:32.448: INFO: Pod "downwardapi-volume-ebde28ca-810c-4ec1-9ece-69bed7df696e" satisfied condition "Succeeded or Failed"
Sep 20 12:31:32.451: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-2 pod downwardapi-volume-ebde28ca-810c-4ec1-9ece-69bed7df696e container client-container: <nil>
STEP: delete the pod 09/20/23 12:31:32.456
Sep 20 12:31:32.512: INFO: Waiting for pod downwardapi-volume-ebde28ca-810c-4ec1-9ece-69bed7df696e to disappear
Sep 20 12:31:32.516: INFO: Pod downwardapi-volume-ebde28ca-810c-4ec1-9ece-69bed7df696e no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep 20 12:31:32.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5763" for this suite. 09/20/23 12:31:32.521
------------------------------
â€¢ [SLOW TEST] [8.250 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:31:24.288
    Sep 20 12:31:24.288: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename downward-api 09/20/23 12:31:24.289
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:31:24.353
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:31:24.357
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 09/20/23 12:31:24.41
    Sep 20 12:31:24.434: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ebde28ca-810c-4ec1-9ece-69bed7df696e" in namespace "downward-api-5763" to be "Succeeded or Failed"
    Sep 20 12:31:24.442: INFO: Pod "downwardapi-volume-ebde28ca-810c-4ec1-9ece-69bed7df696e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.061327ms
    Sep 20 12:31:26.447: INFO: Pod "downwardapi-volume-ebde28ca-810c-4ec1-9ece-69bed7df696e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012721173s
    Sep 20 12:31:28.448: INFO: Pod "downwardapi-volume-ebde28ca-810c-4ec1-9ece-69bed7df696e": Phase="Running", Reason="", readiness=true. Elapsed: 4.013992462s
    Sep 20 12:31:30.712: INFO: Pod "downwardapi-volume-ebde28ca-810c-4ec1-9ece-69bed7df696e": Phase="Running", Reason="", readiness=false. Elapsed: 6.277605721s
    Sep 20 12:31:32.448: INFO: Pod "downwardapi-volume-ebde28ca-810c-4ec1-9ece-69bed7df696e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.014161041s
    STEP: Saw pod success 09/20/23 12:31:32.448
    Sep 20 12:31:32.448: INFO: Pod "downwardapi-volume-ebde28ca-810c-4ec1-9ece-69bed7df696e" satisfied condition "Succeeded or Failed"
    Sep 20 12:31:32.451: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-2 pod downwardapi-volume-ebde28ca-810c-4ec1-9ece-69bed7df696e container client-container: <nil>
    STEP: delete the pod 09/20/23 12:31:32.456
    Sep 20 12:31:32.512: INFO: Waiting for pod downwardapi-volume-ebde28ca-810c-4ec1-9ece-69bed7df696e to disappear
    Sep 20 12:31:32.516: INFO: Pod downwardapi-volume-ebde28ca-810c-4ec1-9ece-69bed7df696e no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:31:32.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5763" for this suite. 09/20/23 12:31:32.521
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:31:32.538
Sep 20 12:31:32.538: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename secrets 09/20/23 12:31:32.539
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:31:32.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:31:32.566
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep 20 12:31:33.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2151" for this suite. 09/20/23 12:31:33.448
------------------------------
â€¢ [0.961 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:31:32.538
    Sep 20 12:31:32.538: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename secrets 09/20/23 12:31:32.539
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:31:32.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:31:32.566
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:31:33.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2151" for this suite. 09/20/23 12:31:33.448
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:31:33.502
Sep 20 12:31:33.502: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename replicaset 09/20/23 12:31:33.503
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:31:33.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:31:33.521
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 09/20/23 12:31:33.541
STEP: Verify that the required pods have come up. 09/20/23 12:31:33.55
Sep 20 12:31:33.558: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep 20 12:31:38.666: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/20/23 12:31:38.666
STEP: Getting /status 09/20/23 12:31:38.666
Sep 20 12:31:38.677: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 09/20/23 12:31:38.677
Sep 20 12:31:38.971: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 09/20/23 12:31:38.971
Sep 20 12:31:38.976: INFO: Observed &ReplicaSet event: ADDED
Sep 20 12:31:38.976: INFO: Observed &ReplicaSet event: MODIFIED
Sep 20 12:31:38.976: INFO: Observed &ReplicaSet event: MODIFIED
Sep 20 12:31:38.976: INFO: Observed &ReplicaSet event: MODIFIED
Sep 20 12:31:38.976: INFO: Found replicaset test-rs in namespace replicaset-8993 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep 20 12:31:38.976: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 09/20/23 12:31:38.976
Sep 20 12:31:38.976: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Sep 20 12:31:39.597: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 09/20/23 12:31:39.597
Sep 20 12:31:39.600: INFO: Observed &ReplicaSet event: ADDED
Sep 20 12:31:39.601: INFO: Observed &ReplicaSet event: MODIFIED
Sep 20 12:31:39.601: INFO: Observed &ReplicaSet event: MODIFIED
Sep 20 12:31:39.601: INFO: Observed &ReplicaSet event: MODIFIED
Sep 20 12:31:39.601: INFO: Observed replicaset test-rs in namespace replicaset-8993 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep 20 12:31:39.601: INFO: Observed &ReplicaSet event: MODIFIED
Sep 20 12:31:39.601: INFO: Found replicaset test-rs in namespace replicaset-8993 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Sep 20 12:31:39.601: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep 20 12:31:39.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-8993" for this suite. 09/20/23 12:31:39.61
------------------------------
â€¢ [SLOW TEST] [6.124 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:31:33.502
    Sep 20 12:31:33.502: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename replicaset 09/20/23 12:31:33.503
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:31:33.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:31:33.521
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 09/20/23 12:31:33.541
    STEP: Verify that the required pods have come up. 09/20/23 12:31:33.55
    Sep 20 12:31:33.558: INFO: Pod name sample-pod: Found 0 pods out of 1
    Sep 20 12:31:38.666: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/20/23 12:31:38.666
    STEP: Getting /status 09/20/23 12:31:38.666
    Sep 20 12:31:38.677: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 09/20/23 12:31:38.677
    Sep 20 12:31:38.971: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 09/20/23 12:31:38.971
    Sep 20 12:31:38.976: INFO: Observed &ReplicaSet event: ADDED
    Sep 20 12:31:38.976: INFO: Observed &ReplicaSet event: MODIFIED
    Sep 20 12:31:38.976: INFO: Observed &ReplicaSet event: MODIFIED
    Sep 20 12:31:38.976: INFO: Observed &ReplicaSet event: MODIFIED
    Sep 20 12:31:38.976: INFO: Found replicaset test-rs in namespace replicaset-8993 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Sep 20 12:31:38.976: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 09/20/23 12:31:38.976
    Sep 20 12:31:38.976: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Sep 20 12:31:39.597: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 09/20/23 12:31:39.597
    Sep 20 12:31:39.600: INFO: Observed &ReplicaSet event: ADDED
    Sep 20 12:31:39.601: INFO: Observed &ReplicaSet event: MODIFIED
    Sep 20 12:31:39.601: INFO: Observed &ReplicaSet event: MODIFIED
    Sep 20 12:31:39.601: INFO: Observed &ReplicaSet event: MODIFIED
    Sep 20 12:31:39.601: INFO: Observed replicaset test-rs in namespace replicaset-8993 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Sep 20 12:31:39.601: INFO: Observed &ReplicaSet event: MODIFIED
    Sep 20 12:31:39.601: INFO: Found replicaset test-rs in namespace replicaset-8993 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Sep 20 12:31:39.601: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:31:39.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-8993" for this suite. 09/20/23 12:31:39.61
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:31:39.631
Sep 20 12:31:39.631: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename configmap 09/20/23 12:31:39.631
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:31:39.661
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:31:39.666
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep 20 12:31:40.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2247" for this suite. 09/20/23 12:31:40.56
------------------------------
â€¢ [0.948 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:31:39.631
    Sep 20 12:31:39.631: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename configmap 09/20/23 12:31:39.631
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:31:39.661
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:31:39.666
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:31:40.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2247" for this suite. 09/20/23 12:31:40.56
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:31:40.581
Sep 20 12:31:40.581: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename watch 09/20/23 12:31:40.582
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:31:40.614
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:31:40.618
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 09/20/23 12:31:40.638
STEP: modifying the configmap once 09/20/23 12:31:40.71
STEP: modifying the configmap a second time 09/20/23 12:31:40.736
STEP: deleting the configmap 09/20/23 12:31:40.761
STEP: creating a watch on configmaps from the resource version returned by the first update 09/20/23 12:31:40.884
STEP: Expecting to observe notifications for all changes to the configmap after the first update 09/20/23 12:31:40.893
Sep 20 12:31:40.893: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7615  54c75e18-5b81-4c35-8261-bdcd1e8dc174 14267 0 2023-09-20 12:31:40 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-09-20 12:31:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 20 12:31:40.893: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7615  54c75e18-5b81-4c35-8261-bdcd1e8dc174 14270 0 2023-09-20 12:31:40 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-09-20 12:31:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Sep 20 12:31:40.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-7615" for this suite. 09/20/23 12:31:40.899
------------------------------
â€¢ [0.330 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:31:40.581
    Sep 20 12:31:40.581: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename watch 09/20/23 12:31:40.582
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:31:40.614
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:31:40.618
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 09/20/23 12:31:40.638
    STEP: modifying the configmap once 09/20/23 12:31:40.71
    STEP: modifying the configmap a second time 09/20/23 12:31:40.736
    STEP: deleting the configmap 09/20/23 12:31:40.761
    STEP: creating a watch on configmaps from the resource version returned by the first update 09/20/23 12:31:40.884
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 09/20/23 12:31:40.893
    Sep 20 12:31:40.893: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7615  54c75e18-5b81-4c35-8261-bdcd1e8dc174 14267 0 2023-09-20 12:31:40 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-09-20 12:31:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep 20 12:31:40.893: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7615  54c75e18-5b81-4c35-8261-bdcd1e8dc174 14270 0 2023-09-20 12:31:40 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-09-20 12:31:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:31:40.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-7615" for this suite. 09/20/23 12:31:40.899
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:31:40.914
Sep 20 12:31:40.914: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename disruption 09/20/23 12:31:40.914
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:31:40.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:31:40.949
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 09/20/23 12:31:41.052
STEP: Waiting for all pods to be running 09/20/23 12:31:41.12
Sep 20 12:31:41.193: INFO: running pods: 0 < 3
Sep 20 12:31:43.198: INFO: running pods: 0 < 3
Sep 20 12:31:45.207: INFO: running pods: 1 < 3
Sep 20 12:31:47.371: INFO: running pods: 2 < 3
Sep 20 12:31:49.341: INFO: running pods: 2 < 3
Sep 20 12:31:51.308: INFO: running pods: 2 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Sep 20 12:31:53.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-9704" for this suite. 09/20/23 12:31:53.211
------------------------------
â€¢ [SLOW TEST] [12.308 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:31:40.914
    Sep 20 12:31:40.914: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename disruption 09/20/23 12:31:40.914
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:31:40.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:31:40.949
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 09/20/23 12:31:41.052
    STEP: Waiting for all pods to be running 09/20/23 12:31:41.12
    Sep 20 12:31:41.193: INFO: running pods: 0 < 3
    Sep 20 12:31:43.198: INFO: running pods: 0 < 3
    Sep 20 12:31:45.207: INFO: running pods: 1 < 3
    Sep 20 12:31:47.371: INFO: running pods: 2 < 3
    Sep 20 12:31:49.341: INFO: running pods: 2 < 3
    Sep 20 12:31:51.308: INFO: running pods: 2 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:31:53.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-9704" for this suite. 09/20/23 12:31:53.211
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:31:53.222
Sep 20 12:31:53.222: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename services 09/20/23 12:31:53.223
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:31:53.34
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:31:53.343
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7983 09/20/23 12:31:53.796
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 09/20/23 12:31:54.013
STEP: creating service externalsvc in namespace services-7983 09/20/23 12:31:54.013
STEP: creating replication controller externalsvc in namespace services-7983 09/20/23 12:31:54.088
I0920 12:31:54.120569      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-7983, replica count: 2
I0920 12:31:57.170900      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0920 12:32:00.171350      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 09/20/23 12:32:00.183
Sep 20 12:32:01.992: INFO: Creating new exec pod
Sep 20 12:32:02.042: INFO: Waiting up to 5m0s for pod "execpod7462h" in namespace "services-7983" to be "running"
Sep 20 12:32:02.076: INFO: Pod "execpod7462h": Phase="Pending", Reason="", readiness=false. Elapsed: 34.0664ms
Sep 20 12:32:04.578: INFO: Pod "execpod7462h": Phase="Pending", Reason="", readiness=false. Elapsed: 2.535857571s
Sep 20 12:32:06.081: INFO: Pod "execpod7462h": Phase="Running", Reason="", readiness=true. Elapsed: 4.038737871s
Sep 20 12:32:06.081: INFO: Pod "execpod7462h" satisfied condition "running"
Sep 20 12:32:06.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-7983 exec execpod7462h -- /bin/sh -x -c nslookup clusterip-service.services-7983.svc.cluster.local'
Sep 20 12:32:06.355: INFO: stderr: "+ nslookup clusterip-service.services-7983.svc.cluster.local\n"
Sep 20 12:32:06.355: INFO: stdout: "Server:\t\t10.254.0.10\nAddress:\t10.254.0.10#53\n\nclusterip-service.services-7983.svc.cluster.local\tcanonical name = externalsvc.services-7983.svc.cluster.local.\nName:\texternalsvc.services-7983.svc.cluster.local\nAddress: 10.254.117.63\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7983, will wait for the garbage collector to delete the pods 09/20/23 12:32:06.355
Sep 20 12:32:06.699: INFO: Deleting ReplicationController externalsvc took: 10.286811ms
Sep 20 12:32:06.999: INFO: Terminating ReplicationController externalsvc pods took: 300.280152ms
Sep 20 12:32:10.325: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep 20 12:32:10.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7983" for this suite. 09/20/23 12:32:10.354
------------------------------
â€¢ [SLOW TEST] [17.151 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:31:53.222
    Sep 20 12:31:53.222: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename services 09/20/23 12:31:53.223
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:31:53.34
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:31:53.343
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7983 09/20/23 12:31:53.796
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 09/20/23 12:31:54.013
    STEP: creating service externalsvc in namespace services-7983 09/20/23 12:31:54.013
    STEP: creating replication controller externalsvc in namespace services-7983 09/20/23 12:31:54.088
    I0920 12:31:54.120569      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-7983, replica count: 2
    I0920 12:31:57.170900      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0920 12:32:00.171350      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 09/20/23 12:32:00.183
    Sep 20 12:32:01.992: INFO: Creating new exec pod
    Sep 20 12:32:02.042: INFO: Waiting up to 5m0s for pod "execpod7462h" in namespace "services-7983" to be "running"
    Sep 20 12:32:02.076: INFO: Pod "execpod7462h": Phase="Pending", Reason="", readiness=false. Elapsed: 34.0664ms
    Sep 20 12:32:04.578: INFO: Pod "execpod7462h": Phase="Pending", Reason="", readiness=false. Elapsed: 2.535857571s
    Sep 20 12:32:06.081: INFO: Pod "execpod7462h": Phase="Running", Reason="", readiness=true. Elapsed: 4.038737871s
    Sep 20 12:32:06.081: INFO: Pod "execpod7462h" satisfied condition "running"
    Sep 20 12:32:06.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-7983 exec execpod7462h -- /bin/sh -x -c nslookup clusterip-service.services-7983.svc.cluster.local'
    Sep 20 12:32:06.355: INFO: stderr: "+ nslookup clusterip-service.services-7983.svc.cluster.local\n"
    Sep 20 12:32:06.355: INFO: stdout: "Server:\t\t10.254.0.10\nAddress:\t10.254.0.10#53\n\nclusterip-service.services-7983.svc.cluster.local\tcanonical name = externalsvc.services-7983.svc.cluster.local.\nName:\texternalsvc.services-7983.svc.cluster.local\nAddress: 10.254.117.63\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-7983, will wait for the garbage collector to delete the pods 09/20/23 12:32:06.355
    Sep 20 12:32:06.699: INFO: Deleting ReplicationController externalsvc took: 10.286811ms
    Sep 20 12:32:06.999: INFO: Terminating ReplicationController externalsvc pods took: 300.280152ms
    Sep 20 12:32:10.325: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:32:10.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7983" for this suite. 09/20/23 12:32:10.354
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:32:10.374
Sep 20 12:32:10.374: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename downward-api 09/20/23 12:32:10.375
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:32:11.322
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:32:11.328
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 09/20/23 12:32:11.335
Sep 20 12:32:11.603: INFO: Waiting up to 5m0s for pod "labelsupdate905fd582-c0b9-4887-be3a-a7726a256881" in namespace "downward-api-8783" to be "running and ready"
Sep 20 12:32:12.539: INFO: Pod "labelsupdate905fd582-c0b9-4887-be3a-a7726a256881": Phase="Pending", Reason="", readiness=false. Elapsed: 935.273758ms
Sep 20 12:32:12.539: INFO: The phase of Pod labelsupdate905fd582-c0b9-4887-be3a-a7726a256881 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 12:32:14.621: INFO: Pod "labelsupdate905fd582-c0b9-4887-be3a-a7726a256881": Phase="Pending", Reason="", readiness=false. Elapsed: 3.017136541s
Sep 20 12:32:14.621: INFO: The phase of Pod labelsupdate905fd582-c0b9-4887-be3a-a7726a256881 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 12:32:16.604: INFO: Pod "labelsupdate905fd582-c0b9-4887-be3a-a7726a256881": Phase="Pending", Reason="", readiness=false. Elapsed: 5.000292164s
Sep 20 12:32:16.604: INFO: The phase of Pod labelsupdate905fd582-c0b9-4887-be3a-a7726a256881 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 12:32:18.543: INFO: Pod "labelsupdate905fd582-c0b9-4887-be3a-a7726a256881": Phase="Running", Reason="", readiness=true. Elapsed: 6.94003232s
Sep 20 12:32:18.544: INFO: The phase of Pod labelsupdate905fd582-c0b9-4887-be3a-a7726a256881 is Running (Ready = true)
Sep 20 12:32:18.544: INFO: Pod "labelsupdate905fd582-c0b9-4887-be3a-a7726a256881" satisfied condition "running and ready"
Sep 20 12:32:19.763: INFO: Successfully updated pod "labelsupdate905fd582-c0b9-4887-be3a-a7726a256881"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep 20 12:32:21.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8783" for this suite. 09/20/23 12:32:21.805
------------------------------
â€¢ [SLOW TEST] [11.748 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:32:10.374
    Sep 20 12:32:10.374: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename downward-api 09/20/23 12:32:10.375
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:32:11.322
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:32:11.328
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 09/20/23 12:32:11.335
    Sep 20 12:32:11.603: INFO: Waiting up to 5m0s for pod "labelsupdate905fd582-c0b9-4887-be3a-a7726a256881" in namespace "downward-api-8783" to be "running and ready"
    Sep 20 12:32:12.539: INFO: Pod "labelsupdate905fd582-c0b9-4887-be3a-a7726a256881": Phase="Pending", Reason="", readiness=false. Elapsed: 935.273758ms
    Sep 20 12:32:12.539: INFO: The phase of Pod labelsupdate905fd582-c0b9-4887-be3a-a7726a256881 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 12:32:14.621: INFO: Pod "labelsupdate905fd582-c0b9-4887-be3a-a7726a256881": Phase="Pending", Reason="", readiness=false. Elapsed: 3.017136541s
    Sep 20 12:32:14.621: INFO: The phase of Pod labelsupdate905fd582-c0b9-4887-be3a-a7726a256881 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 12:32:16.604: INFO: Pod "labelsupdate905fd582-c0b9-4887-be3a-a7726a256881": Phase="Pending", Reason="", readiness=false. Elapsed: 5.000292164s
    Sep 20 12:32:16.604: INFO: The phase of Pod labelsupdate905fd582-c0b9-4887-be3a-a7726a256881 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 12:32:18.543: INFO: Pod "labelsupdate905fd582-c0b9-4887-be3a-a7726a256881": Phase="Running", Reason="", readiness=true. Elapsed: 6.94003232s
    Sep 20 12:32:18.544: INFO: The phase of Pod labelsupdate905fd582-c0b9-4887-be3a-a7726a256881 is Running (Ready = true)
    Sep 20 12:32:18.544: INFO: Pod "labelsupdate905fd582-c0b9-4887-be3a-a7726a256881" satisfied condition "running and ready"
    Sep 20 12:32:19.763: INFO: Successfully updated pod "labelsupdate905fd582-c0b9-4887-be3a-a7726a256881"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:32:21.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8783" for this suite. 09/20/23 12:32:21.805
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:32:22.123
Sep 20 12:32:22.123: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename projected 09/20/23 12:32:22.124
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:32:22.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:32:22.148
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 09/20/23 12:32:22.153
Sep 20 12:32:22.281: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cb498556-9705-4749-b4d4-ece7bbc1e024" in namespace "projected-4801" to be "Succeeded or Failed"
Sep 20 12:32:22.296: INFO: Pod "downwardapi-volume-cb498556-9705-4749-b4d4-ece7bbc1e024": Phase="Pending", Reason="", readiness=false. Elapsed: 14.813157ms
Sep 20 12:32:24.310: INFO: Pod "downwardapi-volume-cb498556-9705-4749-b4d4-ece7bbc1e024": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029209916s
Sep 20 12:32:26.751: INFO: Pod "downwardapi-volume-cb498556-9705-4749-b4d4-ece7bbc1e024": Phase="Running", Reason="", readiness=true. Elapsed: 4.470113556s
Sep 20 12:32:28.300: INFO: Pod "downwardapi-volume-cb498556-9705-4749-b4d4-ece7bbc1e024": Phase="Running", Reason="", readiness=false. Elapsed: 6.019728374s
Sep 20 12:32:30.733: INFO: Pod "downwardapi-volume-cb498556-9705-4749-b4d4-ece7bbc1e024": Phase="Running", Reason="", readiness=false. Elapsed: 8.452495115s
Sep 20 12:32:32.887: INFO: Pod "downwardapi-volume-cb498556-9705-4749-b4d4-ece7bbc1e024": Phase="Running", Reason="", readiness=false. Elapsed: 10.606538856s
Sep 20 12:32:34.301: INFO: Pod "downwardapi-volume-cb498556-9705-4749-b4d4-ece7bbc1e024": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.02024262s
STEP: Saw pod success 09/20/23 12:32:34.301
Sep 20 12:32:34.301: INFO: Pod "downwardapi-volume-cb498556-9705-4749-b4d4-ece7bbc1e024" satisfied condition "Succeeded or Failed"
Sep 20 12:32:34.306: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-2 pod downwardapi-volume-cb498556-9705-4749-b4d4-ece7bbc1e024 container client-container: <nil>
STEP: delete the pod 09/20/23 12:32:34.447
Sep 20 12:32:34.757: INFO: Waiting for pod downwardapi-volume-cb498556-9705-4749-b4d4-ece7bbc1e024 to disappear
Sep 20 12:32:34.761: INFO: Pod downwardapi-volume-cb498556-9705-4749-b4d4-ece7bbc1e024 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep 20 12:32:34.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4801" for this suite. 09/20/23 12:32:34.765
------------------------------
â€¢ [SLOW TEST] [12.650 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:32:22.123
    Sep 20 12:32:22.123: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename projected 09/20/23 12:32:22.124
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:32:22.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:32:22.148
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 09/20/23 12:32:22.153
    Sep 20 12:32:22.281: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cb498556-9705-4749-b4d4-ece7bbc1e024" in namespace "projected-4801" to be "Succeeded or Failed"
    Sep 20 12:32:22.296: INFO: Pod "downwardapi-volume-cb498556-9705-4749-b4d4-ece7bbc1e024": Phase="Pending", Reason="", readiness=false. Elapsed: 14.813157ms
    Sep 20 12:32:24.310: INFO: Pod "downwardapi-volume-cb498556-9705-4749-b4d4-ece7bbc1e024": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029209916s
    Sep 20 12:32:26.751: INFO: Pod "downwardapi-volume-cb498556-9705-4749-b4d4-ece7bbc1e024": Phase="Running", Reason="", readiness=true. Elapsed: 4.470113556s
    Sep 20 12:32:28.300: INFO: Pod "downwardapi-volume-cb498556-9705-4749-b4d4-ece7bbc1e024": Phase="Running", Reason="", readiness=false. Elapsed: 6.019728374s
    Sep 20 12:32:30.733: INFO: Pod "downwardapi-volume-cb498556-9705-4749-b4d4-ece7bbc1e024": Phase="Running", Reason="", readiness=false. Elapsed: 8.452495115s
    Sep 20 12:32:32.887: INFO: Pod "downwardapi-volume-cb498556-9705-4749-b4d4-ece7bbc1e024": Phase="Running", Reason="", readiness=false. Elapsed: 10.606538856s
    Sep 20 12:32:34.301: INFO: Pod "downwardapi-volume-cb498556-9705-4749-b4d4-ece7bbc1e024": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.02024262s
    STEP: Saw pod success 09/20/23 12:32:34.301
    Sep 20 12:32:34.301: INFO: Pod "downwardapi-volume-cb498556-9705-4749-b4d4-ece7bbc1e024" satisfied condition "Succeeded or Failed"
    Sep 20 12:32:34.306: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-2 pod downwardapi-volume-cb498556-9705-4749-b4d4-ece7bbc1e024 container client-container: <nil>
    STEP: delete the pod 09/20/23 12:32:34.447
    Sep 20 12:32:34.757: INFO: Waiting for pod downwardapi-volume-cb498556-9705-4749-b4d4-ece7bbc1e024 to disappear
    Sep 20 12:32:34.761: INFO: Pod downwardapi-volume-cb498556-9705-4749-b4d4-ece7bbc1e024 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:32:34.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4801" for this suite. 09/20/23 12:32:34.765
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:32:34.774
Sep 20 12:32:34.774: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename downward-api 09/20/23 12:32:34.775
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:32:34.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:32:34.98
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 09/20/23 12:32:35
Sep 20 12:32:35.011: INFO: Waiting up to 5m0s for pod "annotationupdatef943c8b1-4e42-4691-bc4f-176909e04d40" in namespace "downward-api-9890" to be "running and ready"
Sep 20 12:32:35.031: INFO: Pod "annotationupdatef943c8b1-4e42-4691-bc4f-176909e04d40": Phase="Pending", Reason="", readiness=false. Elapsed: 20.544215ms
Sep 20 12:32:35.031: INFO: The phase of Pod annotationupdatef943c8b1-4e42-4691-bc4f-176909e04d40 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 12:32:37.038: INFO: Pod "annotationupdatef943c8b1-4e42-4691-bc4f-176909e04d40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02680741s
Sep 20 12:32:37.038: INFO: The phase of Pod annotationupdatef943c8b1-4e42-4691-bc4f-176909e04d40 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 12:32:39.160: INFO: Pod "annotationupdatef943c8b1-4e42-4691-bc4f-176909e04d40": Phase="Pending", Reason="", readiness=false. Elapsed: 4.149128606s
Sep 20 12:32:39.160: INFO: The phase of Pod annotationupdatef943c8b1-4e42-4691-bc4f-176909e04d40 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 12:32:41.155: INFO: Pod "annotationupdatef943c8b1-4e42-4691-bc4f-176909e04d40": Phase="Running", Reason="", readiness=true. Elapsed: 6.143726797s
Sep 20 12:32:41.155: INFO: The phase of Pod annotationupdatef943c8b1-4e42-4691-bc4f-176909e04d40 is Running (Ready = true)
Sep 20 12:32:41.155: INFO: Pod "annotationupdatef943c8b1-4e42-4691-bc4f-176909e04d40" satisfied condition "running and ready"
Sep 20 12:32:41.922: INFO: Successfully updated pod "annotationupdatef943c8b1-4e42-4691-bc4f-176909e04d40"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep 20 12:32:43.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9890" for this suite. 09/20/23 12:32:43.943
------------------------------
â€¢ [SLOW TEST] [9.177 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:32:34.774
    Sep 20 12:32:34.774: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename downward-api 09/20/23 12:32:34.775
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:32:34.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:32:34.98
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 09/20/23 12:32:35
    Sep 20 12:32:35.011: INFO: Waiting up to 5m0s for pod "annotationupdatef943c8b1-4e42-4691-bc4f-176909e04d40" in namespace "downward-api-9890" to be "running and ready"
    Sep 20 12:32:35.031: INFO: Pod "annotationupdatef943c8b1-4e42-4691-bc4f-176909e04d40": Phase="Pending", Reason="", readiness=false. Elapsed: 20.544215ms
    Sep 20 12:32:35.031: INFO: The phase of Pod annotationupdatef943c8b1-4e42-4691-bc4f-176909e04d40 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 12:32:37.038: INFO: Pod "annotationupdatef943c8b1-4e42-4691-bc4f-176909e04d40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02680741s
    Sep 20 12:32:37.038: INFO: The phase of Pod annotationupdatef943c8b1-4e42-4691-bc4f-176909e04d40 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 12:32:39.160: INFO: Pod "annotationupdatef943c8b1-4e42-4691-bc4f-176909e04d40": Phase="Pending", Reason="", readiness=false. Elapsed: 4.149128606s
    Sep 20 12:32:39.160: INFO: The phase of Pod annotationupdatef943c8b1-4e42-4691-bc4f-176909e04d40 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 12:32:41.155: INFO: Pod "annotationupdatef943c8b1-4e42-4691-bc4f-176909e04d40": Phase="Running", Reason="", readiness=true. Elapsed: 6.143726797s
    Sep 20 12:32:41.155: INFO: The phase of Pod annotationupdatef943c8b1-4e42-4691-bc4f-176909e04d40 is Running (Ready = true)
    Sep 20 12:32:41.155: INFO: Pod "annotationupdatef943c8b1-4e42-4691-bc4f-176909e04d40" satisfied condition "running and ready"
    Sep 20 12:32:41.922: INFO: Successfully updated pod "annotationupdatef943c8b1-4e42-4691-bc4f-176909e04d40"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:32:43.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9890" for this suite. 09/20/23 12:32:43.943
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:32:43.952
Sep 20 12:32:43.952: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename sched-preemption 09/20/23 12:32:43.953
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:32:44.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:32:44.055
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Sep 20 12:32:44.266: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 20 12:33:44.313: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 09/20/23 12:33:44.318
Sep 20 12:33:45.054: INFO: Created pod: pod0-0-sched-preemption-low-priority
Sep 20 12:33:45.381: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Sep 20 12:33:45.548: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Sep 20 12:33:45.623: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Sep 20 12:33:46.902: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Sep 20 12:33:47.397: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 09/20/23 12:33:47.397
Sep 20 12:33:47.397: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-4874" to be "running"
Sep 20 12:33:47.429: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 31.702127ms
Sep 20 12:33:49.504: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.106658569s
Sep 20 12:33:49.504: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Sep 20 12:33:49.504: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-4874" to be "running"
Sep 20 12:33:50.241: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 737.269883ms
Sep 20 12:33:50.241: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Sep 20 12:33:50.241: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-4874" to be "running"
Sep 20 12:33:50.247: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.493413ms
Sep 20 12:33:52.854: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.612816302s
Sep 20 12:33:54.530: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.28914197s
Sep 20 12:33:54.530: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Sep 20 12:33:54.530: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-4874" to be "running"
Sep 20 12:33:54.537: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.310503ms
Sep 20 12:33:54.537: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Sep 20 12:33:54.537: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-4874" to be "running"
Sep 20 12:33:54.542: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.863303ms
Sep 20 12:33:54.542: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Sep 20 12:33:54.542: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-4874" to be "running"
Sep 20 12:33:54.548: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.228329ms
Sep 20 12:33:54.548: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 09/20/23 12:33:54.548
Sep 20 12:33:54.605: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Sep 20 12:33:54.612: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.613289ms
Sep 20 12:33:56.619: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014093477s
Sep 20 12:33:58.737: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.131550037s
Sep 20 12:34:00.617: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012474828s
Sep 20 12:34:02.625: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.020334192s
Sep 20 12:34:02.625: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Sep 20 12:34:02.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-4874" for this suite. 09/20/23 12:34:03.223
------------------------------
â€¢ [SLOW TEST] [79.673 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:32:43.952
    Sep 20 12:32:43.952: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename sched-preemption 09/20/23 12:32:43.953
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:32:44.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:32:44.055
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Sep 20 12:32:44.266: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep 20 12:33:44.313: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 09/20/23 12:33:44.318
    Sep 20 12:33:45.054: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Sep 20 12:33:45.381: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Sep 20 12:33:45.548: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Sep 20 12:33:45.623: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Sep 20 12:33:46.902: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Sep 20 12:33:47.397: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 09/20/23 12:33:47.397
    Sep 20 12:33:47.397: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-4874" to be "running"
    Sep 20 12:33:47.429: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 31.702127ms
    Sep 20 12:33:49.504: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.106658569s
    Sep 20 12:33:49.504: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Sep 20 12:33:49.504: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-4874" to be "running"
    Sep 20 12:33:50.241: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 737.269883ms
    Sep 20 12:33:50.241: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Sep 20 12:33:50.241: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-4874" to be "running"
    Sep 20 12:33:50.247: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.493413ms
    Sep 20 12:33:52.854: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.612816302s
    Sep 20 12:33:54.530: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.28914197s
    Sep 20 12:33:54.530: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Sep 20 12:33:54.530: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-4874" to be "running"
    Sep 20 12:33:54.537: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.310503ms
    Sep 20 12:33:54.537: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Sep 20 12:33:54.537: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-4874" to be "running"
    Sep 20 12:33:54.542: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.863303ms
    Sep 20 12:33:54.542: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Sep 20 12:33:54.542: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-4874" to be "running"
    Sep 20 12:33:54.548: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.228329ms
    Sep 20 12:33:54.548: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 09/20/23 12:33:54.548
    Sep 20 12:33:54.605: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Sep 20 12:33:54.612: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.613289ms
    Sep 20 12:33:56.619: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014093477s
    Sep 20 12:33:58.737: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.131550037s
    Sep 20 12:34:00.617: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012474828s
    Sep 20 12:34:02.625: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.020334192s
    Sep 20 12:34:02.625: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:34:02.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-4874" for this suite. 09/20/23 12:34:03.223
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:34:03.626
Sep 20 12:34:03.626: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename statefulset 09/20/23 12:34:03.627
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:34:03.676
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:34:03.681
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-3373 09/20/23 12:34:03.686
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 09/20/23 12:34:03.84
STEP: Creating pod with conflicting port in namespace statefulset-3373 09/20/23 12:34:03.993
STEP: Waiting until pod test-pod will start running in namespace statefulset-3373 09/20/23 12:34:04.215
Sep 20 12:34:04.215: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-3373" to be "running"
Sep 20 12:34:04.227: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.304091ms
Sep 20 12:34:06.688: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.472665089s
Sep 20 12:34:08.318: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.103371933s
Sep 20 12:34:08.318: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-3373 09/20/23 12:34:08.318
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3373 09/20/23 12:34:08.492
Sep 20 12:34:08.574: INFO: Observed stateful pod in namespace: statefulset-3373, name: ss-0, uid: 542335e1-a635-434e-92ec-8eb129586329, status phase: Pending. Waiting for statefulset controller to delete.
Sep 20 12:34:08.817: INFO: Observed stateful pod in namespace: statefulset-3373, name: ss-0, uid: 542335e1-a635-434e-92ec-8eb129586329, status phase: Failed. Waiting for statefulset controller to delete.
Sep 20 12:34:09.165: INFO: Observed stateful pod in namespace: statefulset-3373, name: ss-0, uid: 542335e1-a635-434e-92ec-8eb129586329, status phase: Failed. Waiting for statefulset controller to delete.
Sep 20 12:34:09.176: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3373
STEP: Removing pod with conflicting port in namespace statefulset-3373 09/20/23 12:34:09.176
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3373 and will be in running state 09/20/23 12:34:09.298
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep 20 12:34:15.737: INFO: Deleting all statefulset in ns statefulset-3373
Sep 20 12:34:15.742: INFO: Scaling statefulset ss to 0
Sep 20 12:34:26.237: INFO: Waiting for statefulset status.replicas updated to 0
Sep 20 12:34:26.240: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep 20 12:34:26.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-3373" for this suite. 09/20/23 12:34:26.569
------------------------------
â€¢ [SLOW TEST] [22.955 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:34:03.626
    Sep 20 12:34:03.626: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename statefulset 09/20/23 12:34:03.627
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:34:03.676
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:34:03.681
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-3373 09/20/23 12:34:03.686
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 09/20/23 12:34:03.84
    STEP: Creating pod with conflicting port in namespace statefulset-3373 09/20/23 12:34:03.993
    STEP: Waiting until pod test-pod will start running in namespace statefulset-3373 09/20/23 12:34:04.215
    Sep 20 12:34:04.215: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-3373" to be "running"
    Sep 20 12:34:04.227: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.304091ms
    Sep 20 12:34:06.688: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.472665089s
    Sep 20 12:34:08.318: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.103371933s
    Sep 20 12:34:08.318: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-3373 09/20/23 12:34:08.318
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3373 09/20/23 12:34:08.492
    Sep 20 12:34:08.574: INFO: Observed stateful pod in namespace: statefulset-3373, name: ss-0, uid: 542335e1-a635-434e-92ec-8eb129586329, status phase: Pending. Waiting for statefulset controller to delete.
    Sep 20 12:34:08.817: INFO: Observed stateful pod in namespace: statefulset-3373, name: ss-0, uid: 542335e1-a635-434e-92ec-8eb129586329, status phase: Failed. Waiting for statefulset controller to delete.
    Sep 20 12:34:09.165: INFO: Observed stateful pod in namespace: statefulset-3373, name: ss-0, uid: 542335e1-a635-434e-92ec-8eb129586329, status phase: Failed. Waiting for statefulset controller to delete.
    Sep 20 12:34:09.176: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3373
    STEP: Removing pod with conflicting port in namespace statefulset-3373 09/20/23 12:34:09.176
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3373 and will be in running state 09/20/23 12:34:09.298
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep 20 12:34:15.737: INFO: Deleting all statefulset in ns statefulset-3373
    Sep 20 12:34:15.742: INFO: Scaling statefulset ss to 0
    Sep 20 12:34:26.237: INFO: Waiting for statefulset status.replicas updated to 0
    Sep 20 12:34:26.240: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:34:26.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-3373" for this suite. 09/20/23 12:34:26.569
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:34:26.584
Sep 20 12:34:26.584: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename replication-controller 09/20/23 12:34:26.585
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:34:26.602
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:34:26.606
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Sep 20 12:34:26.612: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 09/20/23 12:34:27.628
STEP: Checking rc "condition-test" has the desired failure condition set 09/20/23 12:34:27.837
STEP: Scaling down rc "condition-test" to satisfy pod quota 09/20/23 12:34:28.847
Sep 20 12:34:28.900: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 09/20/23 12:34:28.9
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep 20 12:34:29.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-8393" for this suite. 09/20/23 12:34:29.915
------------------------------
â€¢ [3.546 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:34:26.584
    Sep 20 12:34:26.584: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename replication-controller 09/20/23 12:34:26.585
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:34:26.602
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:34:26.606
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Sep 20 12:34:26.612: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 09/20/23 12:34:27.628
    STEP: Checking rc "condition-test" has the desired failure condition set 09/20/23 12:34:27.837
    STEP: Scaling down rc "condition-test" to satisfy pod quota 09/20/23 12:34:28.847
    Sep 20 12:34:28.900: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 09/20/23 12:34:28.9
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:34:29.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-8393" for this suite. 09/20/23 12:34:29.915
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:34:30.13
Sep 20 12:34:30.130: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename services 09/20/23 12:34:30.131
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:34:30.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:34:30.238
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 09/20/23 12:34:30.248
STEP: waiting for available Endpoint 09/20/23 12:34:30.256
STEP: listing all Endpoints 09/20/23 12:34:30.258
STEP: updating the Endpoint 09/20/23 12:34:30.261
STEP: fetching the Endpoint 09/20/23 12:34:30.453
STEP: patching the Endpoint 09/20/23 12:34:30.46
STEP: fetching the Endpoint 09/20/23 12:34:30.664
STEP: deleting the Endpoint by Collection 09/20/23 12:34:30.667
STEP: waiting for Endpoint deletion 09/20/23 12:34:30.695
STEP: fetching the Endpoint 09/20/23 12:34:30.697
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep 20 12:34:30.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5144" for this suite. 09/20/23 12:34:30.705
------------------------------
â€¢ [0.799 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:34:30.13
    Sep 20 12:34:30.130: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename services 09/20/23 12:34:30.131
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:34:30.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:34:30.238
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 09/20/23 12:34:30.248
    STEP: waiting for available Endpoint 09/20/23 12:34:30.256
    STEP: listing all Endpoints 09/20/23 12:34:30.258
    STEP: updating the Endpoint 09/20/23 12:34:30.261
    STEP: fetching the Endpoint 09/20/23 12:34:30.453
    STEP: patching the Endpoint 09/20/23 12:34:30.46
    STEP: fetching the Endpoint 09/20/23 12:34:30.664
    STEP: deleting the Endpoint by Collection 09/20/23 12:34:30.667
    STEP: waiting for Endpoint deletion 09/20/23 12:34:30.695
    STEP: fetching the Endpoint 09/20/23 12:34:30.697
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:34:30.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5144" for this suite. 09/20/23 12:34:30.705
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:34:30.931
Sep 20 12:34:30.931: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename ingress 09/20/23 12:34:30.932
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:34:31.123
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:34:31.127
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 09/20/23 12:34:31.132
STEP: getting /apis/networking.k8s.io 09/20/23 12:34:31.137
STEP: getting /apis/networking.k8s.iov1 09/20/23 12:34:31.139
STEP: creating 09/20/23 12:34:31.146
STEP: getting 09/20/23 12:34:31.341
STEP: listing 09/20/23 12:34:31.348
STEP: watching 09/20/23 12:34:31.353
Sep 20 12:34:31.354: INFO: starting watch
STEP: cluster-wide listing 09/20/23 12:34:31.356
STEP: cluster-wide watching 09/20/23 12:34:31.361
Sep 20 12:34:31.361: INFO: starting watch
STEP: patching 09/20/23 12:34:31.363
STEP: updating 09/20/23 12:34:31.373
Sep 20 12:34:31.386: INFO: waiting for watch events with expected annotations
Sep 20 12:34:31.386: INFO: saw patched and updated annotations
STEP: patching /status 09/20/23 12:34:31.386
STEP: updating /status 09/20/23 12:34:31.396
STEP: get /status 09/20/23 12:34:31.41
STEP: deleting 09/20/23 12:34:31.415
STEP: deleting a collection 09/20/23 12:34:31.638
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Sep 20 12:34:31.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-6400" for this suite. 09/20/23 12:34:31.73
------------------------------
â€¢ [0.816 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:34:30.931
    Sep 20 12:34:30.931: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename ingress 09/20/23 12:34:30.932
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:34:31.123
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:34:31.127
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 09/20/23 12:34:31.132
    STEP: getting /apis/networking.k8s.io 09/20/23 12:34:31.137
    STEP: getting /apis/networking.k8s.iov1 09/20/23 12:34:31.139
    STEP: creating 09/20/23 12:34:31.146
    STEP: getting 09/20/23 12:34:31.341
    STEP: listing 09/20/23 12:34:31.348
    STEP: watching 09/20/23 12:34:31.353
    Sep 20 12:34:31.354: INFO: starting watch
    STEP: cluster-wide listing 09/20/23 12:34:31.356
    STEP: cluster-wide watching 09/20/23 12:34:31.361
    Sep 20 12:34:31.361: INFO: starting watch
    STEP: patching 09/20/23 12:34:31.363
    STEP: updating 09/20/23 12:34:31.373
    Sep 20 12:34:31.386: INFO: waiting for watch events with expected annotations
    Sep 20 12:34:31.386: INFO: saw patched and updated annotations
    STEP: patching /status 09/20/23 12:34:31.386
    STEP: updating /status 09/20/23 12:34:31.396
    STEP: get /status 09/20/23 12:34:31.41
    STEP: deleting 09/20/23 12:34:31.415
    STEP: deleting a collection 09/20/23 12:34:31.638
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:34:31.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-6400" for this suite. 09/20/23 12:34:31.73
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:34:31.749
Sep 20 12:34:31.750: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename svcaccounts 09/20/23 12:34:31.75
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:34:32.461
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:34:32.464
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Sep 20 12:34:32.665: INFO: Got root ca configmap in namespace "svcaccounts-4253"
Sep 20 12:34:32.730: INFO: Deleted root ca configmap in namespace "svcaccounts-4253"
STEP: waiting for a new root ca configmap created 09/20/23 12:34:33.231
Sep 20 12:34:33.381: INFO: Recreated root ca configmap in namespace "svcaccounts-4253"
Sep 20 12:34:33.877: INFO: Updated root ca configmap in namespace "svcaccounts-4253"
STEP: waiting for the root ca configmap reconciled 09/20/23 12:34:34.377
Sep 20 12:34:34.382: INFO: Reconciled root ca configmap in namespace "svcaccounts-4253"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep 20 12:34:34.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4253" for this suite. 09/20/23 12:34:34.387
------------------------------
â€¢ [2.646 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:34:31.749
    Sep 20 12:34:31.750: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename svcaccounts 09/20/23 12:34:31.75
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:34:32.461
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:34:32.464
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Sep 20 12:34:32.665: INFO: Got root ca configmap in namespace "svcaccounts-4253"
    Sep 20 12:34:32.730: INFO: Deleted root ca configmap in namespace "svcaccounts-4253"
    STEP: waiting for a new root ca configmap created 09/20/23 12:34:33.231
    Sep 20 12:34:33.381: INFO: Recreated root ca configmap in namespace "svcaccounts-4253"
    Sep 20 12:34:33.877: INFO: Updated root ca configmap in namespace "svcaccounts-4253"
    STEP: waiting for the root ca configmap reconciled 09/20/23 12:34:34.377
    Sep 20 12:34:34.382: INFO: Reconciled root ca configmap in namespace "svcaccounts-4253"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:34:34.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4253" for this suite. 09/20/23 12:34:34.387
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:34:34.397
Sep 20 12:34:34.397: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename statefulset 09/20/23 12:34:34.397
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:34:34.641
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:34:34.645
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-1898 09/20/23 12:34:34.652
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-1898 09/20/23 12:34:34.779
Sep 20 12:34:35.089: INFO: Found 0 stateful pods, waiting for 1
Sep 20 12:34:45.096: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 09/20/23 12:34:45.103
STEP: Getting /status 09/20/23 12:34:45.118
Sep 20 12:34:45.124: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 09/20/23 12:34:45.124
Sep 20 12:34:45.338: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 09/20/23 12:34:45.338
Sep 20 12:34:45.342: INFO: Observed &StatefulSet event: ADDED
Sep 20 12:34:45.342: INFO: Found Statefulset ss in namespace statefulset-1898 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep 20 12:34:45.343: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 09/20/23 12:34:45.343
Sep 20 12:34:45.343: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Sep 20 12:34:45.357: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 09/20/23 12:34:45.357
Sep 20 12:34:45.360: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep 20 12:34:45.360: INFO: Deleting all statefulset in ns statefulset-1898
Sep 20 12:34:45.368: INFO: Scaling statefulset ss to 0
Sep 20 12:34:55.681: INFO: Waiting for statefulset status.replicas updated to 0
Sep 20 12:34:55.683: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep 20 12:34:55.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-1898" for this suite. 09/20/23 12:34:55.704
------------------------------
â€¢ [SLOW TEST] [21.342 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:34:34.397
    Sep 20 12:34:34.397: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename statefulset 09/20/23 12:34:34.397
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:34:34.641
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:34:34.645
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-1898 09/20/23 12:34:34.652
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-1898 09/20/23 12:34:34.779
    Sep 20 12:34:35.089: INFO: Found 0 stateful pods, waiting for 1
    Sep 20 12:34:45.096: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 09/20/23 12:34:45.103
    STEP: Getting /status 09/20/23 12:34:45.118
    Sep 20 12:34:45.124: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 09/20/23 12:34:45.124
    Sep 20 12:34:45.338: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 09/20/23 12:34:45.338
    Sep 20 12:34:45.342: INFO: Observed &StatefulSet event: ADDED
    Sep 20 12:34:45.342: INFO: Found Statefulset ss in namespace statefulset-1898 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Sep 20 12:34:45.343: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 09/20/23 12:34:45.343
    Sep 20 12:34:45.343: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Sep 20 12:34:45.357: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 09/20/23 12:34:45.357
    Sep 20 12:34:45.360: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep 20 12:34:45.360: INFO: Deleting all statefulset in ns statefulset-1898
    Sep 20 12:34:45.368: INFO: Scaling statefulset ss to 0
    Sep 20 12:34:55.681: INFO: Waiting for statefulset status.replicas updated to 0
    Sep 20 12:34:55.683: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:34:55.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-1898" for this suite. 09/20/23 12:34:55.704
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:34:55.741
Sep 20 12:34:55.741: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename statefulset 09/20/23 12:34:55.742
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:34:55.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:34:55.884
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7401 09/20/23 12:34:55.893
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Sep 20 12:34:56.880: INFO: Found 0 stateful pods, waiting for 1
Sep 20 12:35:06.887: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 09/20/23 12:35:06.899
W0920 12:35:07.264366      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Sep 20 12:35:07.277: INFO: Found 1 stateful pods, waiting for 2
Sep 20 12:35:17.564: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Sep 20 12:35:27.283: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 12:35:27.283: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 09/20/23 12:35:27.29
STEP: Delete all of the StatefulSets 09/20/23 12:35:27.294
STEP: Verify that StatefulSets have been deleted 09/20/23 12:35:27.305
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep 20 12:35:27.314: INFO: Deleting all statefulset in ns statefulset-7401
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep 20 12:35:28.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7401" for this suite. 09/20/23 12:35:28.055
------------------------------
â€¢ [SLOW TEST] [32.919 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:34:55.741
    Sep 20 12:34:55.741: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename statefulset 09/20/23 12:34:55.742
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:34:55.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:34:55.884
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7401 09/20/23 12:34:55.893
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Sep 20 12:34:56.880: INFO: Found 0 stateful pods, waiting for 1
    Sep 20 12:35:06.887: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 09/20/23 12:35:06.899
    W0920 12:35:07.264366      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Sep 20 12:35:07.277: INFO: Found 1 stateful pods, waiting for 2
    Sep 20 12:35:17.564: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Pending - Ready=false
    Sep 20 12:35:27.283: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep 20 12:35:27.283: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 09/20/23 12:35:27.29
    STEP: Delete all of the StatefulSets 09/20/23 12:35:27.294
    STEP: Verify that StatefulSets have been deleted 09/20/23 12:35:27.305
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep 20 12:35:27.314: INFO: Deleting all statefulset in ns statefulset-7401
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:35:28.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7401" for this suite. 09/20/23 12:35:28.055
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:35:28.665
Sep 20 12:35:28.665: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename ingressclass 09/20/23 12:35:28.666
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:35:28.93
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:35:28.934
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 09/20/23 12:35:28.94
STEP: getting /apis/networking.k8s.io 09/20/23 12:35:28.944
STEP: getting /apis/networking.k8s.iov1 09/20/23 12:35:28.945
STEP: creating 09/20/23 12:35:28.946
STEP: getting 09/20/23 12:35:29.413
STEP: listing 09/20/23 12:35:29.418
STEP: watching 09/20/23 12:35:29.423
Sep 20 12:35:29.423: INFO: starting watch
STEP: patching 09/20/23 12:35:29.425
STEP: updating 09/20/23 12:35:29.59
Sep 20 12:35:29.609: INFO: waiting for watch events with expected annotations
Sep 20 12:35:29.609: INFO: saw patched and updated annotations
STEP: deleting 09/20/23 12:35:29.609
STEP: deleting a collection 09/20/23 12:35:29.622
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Sep 20 12:35:29.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-5929" for this suite. 09/20/23 12:35:29.641
------------------------------
â€¢ [0.983 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:35:28.665
    Sep 20 12:35:28.665: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename ingressclass 09/20/23 12:35:28.666
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:35:28.93
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:35:28.934
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 09/20/23 12:35:28.94
    STEP: getting /apis/networking.k8s.io 09/20/23 12:35:28.944
    STEP: getting /apis/networking.k8s.iov1 09/20/23 12:35:28.945
    STEP: creating 09/20/23 12:35:28.946
    STEP: getting 09/20/23 12:35:29.413
    STEP: listing 09/20/23 12:35:29.418
    STEP: watching 09/20/23 12:35:29.423
    Sep 20 12:35:29.423: INFO: starting watch
    STEP: patching 09/20/23 12:35:29.425
    STEP: updating 09/20/23 12:35:29.59
    Sep 20 12:35:29.609: INFO: waiting for watch events with expected annotations
    Sep 20 12:35:29.609: INFO: saw patched and updated annotations
    STEP: deleting 09/20/23 12:35:29.609
    STEP: deleting a collection 09/20/23 12:35:29.622
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:35:29.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-5929" for this suite. 09/20/23 12:35:29.641
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:35:29.648
Sep 20 12:35:29.648: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename services 09/20/23 12:35:29.649
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:35:29.694
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:35:29.698
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 09/20/23 12:35:29.707
STEP: watching for the Service to be added 09/20/23 12:35:29.801
Sep 20 12:35:29.807: INFO: Found Service test-service-gdq95 in namespace services-4811 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Sep 20 12:35:29.807: INFO: Service test-service-gdq95 created
STEP: Getting /status 09/20/23 12:35:29.807
Sep 20 12:35:29.813: INFO: Service test-service-gdq95 has LoadBalancer: {[]}
STEP: patching the ServiceStatus 09/20/23 12:35:29.813
STEP: watching for the Service to be patched 09/20/23 12:35:29.82
Sep 20 12:35:29.825: INFO: observed Service test-service-gdq95 in namespace services-4811 with annotations: map[] & LoadBalancer: {[]}
Sep 20 12:35:29.825: INFO: Found Service test-service-gdq95 in namespace services-4811 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Sep 20 12:35:29.825: INFO: Service test-service-gdq95 has service status patched
STEP: updating the ServiceStatus 09/20/23 12:35:29.825
Sep 20 12:35:29.845: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 09/20/23 12:35:29.845
Sep 20 12:35:29.847: INFO: Observed Service test-service-gdq95 in namespace services-4811 with annotations: map[] & Conditions: {[]}
Sep 20 12:35:29.847: INFO: Observed event: &Service{ObjectMeta:{test-service-gdq95  services-4811  d4f3bb11-3fd7-41f1-8631-768b215f137c 15791 0 2023-09-20 12:35:29 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-09-20 12:35:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-09-20 12:35:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.254.251.98,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.254.251.98],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Sep 20 12:35:29.847: INFO: Found Service test-service-gdq95 in namespace services-4811 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep 20 12:35:29.847: INFO: Service test-service-gdq95 has service status updated
STEP: patching the service 09/20/23 12:35:29.847
STEP: watching for the Service to be patched 09/20/23 12:35:30.109
Sep 20 12:35:30.115: INFO: observed Service test-service-gdq95 in namespace services-4811 with labels: map[test-service-static:true]
Sep 20 12:35:30.115: INFO: observed Service test-service-gdq95 in namespace services-4811 with labels: map[test-service-static:true]
Sep 20 12:35:30.115: INFO: observed Service test-service-gdq95 in namespace services-4811 with labels: map[test-service-static:true]
Sep 20 12:35:30.115: INFO: Found Service test-service-gdq95 in namespace services-4811 with labels: map[test-service:patched test-service-static:true]
Sep 20 12:35:30.115: INFO: Service test-service-gdq95 patched
STEP: deleting the service 09/20/23 12:35:30.115
STEP: watching for the Service to be deleted 09/20/23 12:35:30.362
Sep 20 12:35:30.366: INFO: Observed event: ADDED
Sep 20 12:35:30.366: INFO: Observed event: MODIFIED
Sep 20 12:35:30.367: INFO: Observed event: MODIFIED
Sep 20 12:35:30.367: INFO: Observed event: MODIFIED
Sep 20 12:35:30.367: INFO: Found Service test-service-gdq95 in namespace services-4811 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Sep 20 12:35:30.367: INFO: Service test-service-gdq95 deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep 20 12:35:30.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4811" for this suite. 09/20/23 12:35:30.375
------------------------------
â€¢ [0.738 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:35:29.648
    Sep 20 12:35:29.648: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename services 09/20/23 12:35:29.649
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:35:29.694
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:35:29.698
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 09/20/23 12:35:29.707
    STEP: watching for the Service to be added 09/20/23 12:35:29.801
    Sep 20 12:35:29.807: INFO: Found Service test-service-gdq95 in namespace services-4811 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Sep 20 12:35:29.807: INFO: Service test-service-gdq95 created
    STEP: Getting /status 09/20/23 12:35:29.807
    Sep 20 12:35:29.813: INFO: Service test-service-gdq95 has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 09/20/23 12:35:29.813
    STEP: watching for the Service to be patched 09/20/23 12:35:29.82
    Sep 20 12:35:29.825: INFO: observed Service test-service-gdq95 in namespace services-4811 with annotations: map[] & LoadBalancer: {[]}
    Sep 20 12:35:29.825: INFO: Found Service test-service-gdq95 in namespace services-4811 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Sep 20 12:35:29.825: INFO: Service test-service-gdq95 has service status patched
    STEP: updating the ServiceStatus 09/20/23 12:35:29.825
    Sep 20 12:35:29.845: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 09/20/23 12:35:29.845
    Sep 20 12:35:29.847: INFO: Observed Service test-service-gdq95 in namespace services-4811 with annotations: map[] & Conditions: {[]}
    Sep 20 12:35:29.847: INFO: Observed event: &Service{ObjectMeta:{test-service-gdq95  services-4811  d4f3bb11-3fd7-41f1-8631-768b215f137c 15791 0 2023-09-20 12:35:29 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-09-20 12:35:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-09-20 12:35:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.254.251.98,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.254.251.98],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Sep 20 12:35:29.847: INFO: Found Service test-service-gdq95 in namespace services-4811 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Sep 20 12:35:29.847: INFO: Service test-service-gdq95 has service status updated
    STEP: patching the service 09/20/23 12:35:29.847
    STEP: watching for the Service to be patched 09/20/23 12:35:30.109
    Sep 20 12:35:30.115: INFO: observed Service test-service-gdq95 in namespace services-4811 with labels: map[test-service-static:true]
    Sep 20 12:35:30.115: INFO: observed Service test-service-gdq95 in namespace services-4811 with labels: map[test-service-static:true]
    Sep 20 12:35:30.115: INFO: observed Service test-service-gdq95 in namespace services-4811 with labels: map[test-service-static:true]
    Sep 20 12:35:30.115: INFO: Found Service test-service-gdq95 in namespace services-4811 with labels: map[test-service:patched test-service-static:true]
    Sep 20 12:35:30.115: INFO: Service test-service-gdq95 patched
    STEP: deleting the service 09/20/23 12:35:30.115
    STEP: watching for the Service to be deleted 09/20/23 12:35:30.362
    Sep 20 12:35:30.366: INFO: Observed event: ADDED
    Sep 20 12:35:30.366: INFO: Observed event: MODIFIED
    Sep 20 12:35:30.367: INFO: Observed event: MODIFIED
    Sep 20 12:35:30.367: INFO: Observed event: MODIFIED
    Sep 20 12:35:30.367: INFO: Found Service test-service-gdq95 in namespace services-4811 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Sep 20 12:35:30.367: INFO: Service test-service-gdq95 deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:35:30.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4811" for this suite. 09/20/23 12:35:30.375
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:35:30.389
Sep 20 12:35:30.390: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename replicaset 09/20/23 12:35:30.39
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:35:30.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:35:30.42
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Sep 20 12:35:30.427: INFO: Creating ReplicaSet my-hostname-basic-616aaf1f-240d-43eb-a655-304815342b42
Sep 20 12:35:30.456: INFO: Pod name my-hostname-basic-616aaf1f-240d-43eb-a655-304815342b42: Found 0 pods out of 1
Sep 20 12:35:35.497: INFO: Pod name my-hostname-basic-616aaf1f-240d-43eb-a655-304815342b42: Found 1 pods out of 1
Sep 20 12:35:35.497: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-616aaf1f-240d-43eb-a655-304815342b42" is running
Sep 20 12:35:35.497: INFO: Waiting up to 5m0s for pod "my-hostname-basic-616aaf1f-240d-43eb-a655-304815342b42-g29v8" in namespace "replicaset-8676" to be "running"
Sep 20 12:35:35.900: INFO: Pod "my-hostname-basic-616aaf1f-240d-43eb-a655-304815342b42-g29v8": Phase="Running", Reason="", readiness=true. Elapsed: 402.616763ms
Sep 20 12:35:35.900: INFO: Pod "my-hostname-basic-616aaf1f-240d-43eb-a655-304815342b42-g29v8" satisfied condition "running"
Sep 20 12:35:35.900: INFO: Pod "my-hostname-basic-616aaf1f-240d-43eb-a655-304815342b42-g29v8" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-20 12:35:30 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-20 12:35:34 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-20 12:35:34 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-20 12:35:30 +0000 UTC Reason: Message:}])
Sep 20 12:35:35.900: INFO: Trying to dial the pod
Sep 20 12:35:40.919: INFO: Controller my-hostname-basic-616aaf1f-240d-43eb-a655-304815342b42: Got expected result from replica 1 [my-hostname-basic-616aaf1f-240d-43eb-a655-304815342b42-g29v8]: "my-hostname-basic-616aaf1f-240d-43eb-a655-304815342b42-g29v8", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep 20 12:35:40.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-8676" for this suite. 09/20/23 12:35:40.929
------------------------------
â€¢ [SLOW TEST] [10.559 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:35:30.389
    Sep 20 12:35:30.390: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename replicaset 09/20/23 12:35:30.39
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:35:30.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:35:30.42
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Sep 20 12:35:30.427: INFO: Creating ReplicaSet my-hostname-basic-616aaf1f-240d-43eb-a655-304815342b42
    Sep 20 12:35:30.456: INFO: Pod name my-hostname-basic-616aaf1f-240d-43eb-a655-304815342b42: Found 0 pods out of 1
    Sep 20 12:35:35.497: INFO: Pod name my-hostname-basic-616aaf1f-240d-43eb-a655-304815342b42: Found 1 pods out of 1
    Sep 20 12:35:35.497: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-616aaf1f-240d-43eb-a655-304815342b42" is running
    Sep 20 12:35:35.497: INFO: Waiting up to 5m0s for pod "my-hostname-basic-616aaf1f-240d-43eb-a655-304815342b42-g29v8" in namespace "replicaset-8676" to be "running"
    Sep 20 12:35:35.900: INFO: Pod "my-hostname-basic-616aaf1f-240d-43eb-a655-304815342b42-g29v8": Phase="Running", Reason="", readiness=true. Elapsed: 402.616763ms
    Sep 20 12:35:35.900: INFO: Pod "my-hostname-basic-616aaf1f-240d-43eb-a655-304815342b42-g29v8" satisfied condition "running"
    Sep 20 12:35:35.900: INFO: Pod "my-hostname-basic-616aaf1f-240d-43eb-a655-304815342b42-g29v8" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-20 12:35:30 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-20 12:35:34 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-20 12:35:34 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-20 12:35:30 +0000 UTC Reason: Message:}])
    Sep 20 12:35:35.900: INFO: Trying to dial the pod
    Sep 20 12:35:40.919: INFO: Controller my-hostname-basic-616aaf1f-240d-43eb-a655-304815342b42: Got expected result from replica 1 [my-hostname-basic-616aaf1f-240d-43eb-a655-304815342b42-g29v8]: "my-hostname-basic-616aaf1f-240d-43eb-a655-304815342b42-g29v8", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:35:40.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-8676" for this suite. 09/20/23 12:35:40.929
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:35:40.95
Sep 20 12:35:40.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename dns 09/20/23 12:35:40.951
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:35:41.538
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:35:41.545
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9020.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9020.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 09/20/23 12:35:41.551
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9020.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9020.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 09/20/23 12:35:41.551
STEP: creating a pod to probe /etc/hosts 09/20/23 12:35:41.551
STEP: submitting the pod to kubernetes 09/20/23 12:35:41.551
Sep 20 12:35:41.603: INFO: Waiting up to 15m0s for pod "dns-test-df59f644-2086-4ee1-b00b-443dfa5b0486" in namespace "dns-9020" to be "running"
Sep 20 12:35:41.613: INFO: Pod "dns-test-df59f644-2086-4ee1-b00b-443dfa5b0486": Phase="Pending", Reason="", readiness=false. Elapsed: 10.010781ms
Sep 20 12:35:43.747: INFO: Pod "dns-test-df59f644-2086-4ee1-b00b-443dfa5b0486": Phase="Pending", Reason="", readiness=false. Elapsed: 2.144438576s
Sep 20 12:35:45.618: INFO: Pod "dns-test-df59f644-2086-4ee1-b00b-443dfa5b0486": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015238631s
Sep 20 12:35:47.618: INFO: Pod "dns-test-df59f644-2086-4ee1-b00b-443dfa5b0486": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014760466s
Sep 20 12:35:49.618: INFO: Pod "dns-test-df59f644-2086-4ee1-b00b-443dfa5b0486": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015360492s
Sep 20 12:35:51.620: INFO: Pod "dns-test-df59f644-2086-4ee1-b00b-443dfa5b0486": Phase="Pending", Reason="", readiness=false. Elapsed: 10.016462695s
Sep 20 12:35:53.622: INFO: Pod "dns-test-df59f644-2086-4ee1-b00b-443dfa5b0486": Phase="Pending", Reason="", readiness=false. Elapsed: 12.019356942s
Sep 20 12:35:55.619: INFO: Pod "dns-test-df59f644-2086-4ee1-b00b-443dfa5b0486": Phase="Pending", Reason="", readiness=false. Elapsed: 14.015808964s
Sep 20 12:35:57.622: INFO: Pod "dns-test-df59f644-2086-4ee1-b00b-443dfa5b0486": Phase="Pending", Reason="", readiness=false. Elapsed: 16.019351796s
Sep 20 12:35:59.619: INFO: Pod "dns-test-df59f644-2086-4ee1-b00b-443dfa5b0486": Phase="Pending", Reason="", readiness=false. Elapsed: 18.015788286s
Sep 20 12:36:01.682: INFO: Pod "dns-test-df59f644-2086-4ee1-b00b-443dfa5b0486": Phase="Pending", Reason="", readiness=false. Elapsed: 20.079223713s
Sep 20 12:36:03.621: INFO: Pod "dns-test-df59f644-2086-4ee1-b00b-443dfa5b0486": Phase="Pending", Reason="", readiness=false. Elapsed: 22.018054846s
Sep 20 12:36:05.751: INFO: Pod "dns-test-df59f644-2086-4ee1-b00b-443dfa5b0486": Phase="Pending", Reason="", readiness=false. Elapsed: 24.148407865s
Sep 20 12:36:07.617: INFO: Pod "dns-test-df59f644-2086-4ee1-b00b-443dfa5b0486": Phase="Running", Reason="", readiness=true. Elapsed: 26.014419007s
Sep 20 12:36:07.618: INFO: Pod "dns-test-df59f644-2086-4ee1-b00b-443dfa5b0486" satisfied condition "running"
STEP: retrieving the pod 09/20/23 12:36:07.618
STEP: looking for the results for each expected name from probers 09/20/23 12:36:07.62
Sep 20 12:36:07.637: INFO: DNS probes using dns-9020/dns-test-df59f644-2086-4ee1-b00b-443dfa5b0486 succeeded

STEP: deleting the pod 09/20/23 12:36:07.637
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep 20 12:36:07.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9020" for this suite. 09/20/23 12:36:07.737
------------------------------
â€¢ [SLOW TEST] [26.798 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:35:40.95
    Sep 20 12:35:40.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename dns 09/20/23 12:35:40.951
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:35:41.538
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:35:41.545
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9020.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9020.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     09/20/23 12:35:41.551
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9020.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9020.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     09/20/23 12:35:41.551
    STEP: creating a pod to probe /etc/hosts 09/20/23 12:35:41.551
    STEP: submitting the pod to kubernetes 09/20/23 12:35:41.551
    Sep 20 12:35:41.603: INFO: Waiting up to 15m0s for pod "dns-test-df59f644-2086-4ee1-b00b-443dfa5b0486" in namespace "dns-9020" to be "running"
    Sep 20 12:35:41.613: INFO: Pod "dns-test-df59f644-2086-4ee1-b00b-443dfa5b0486": Phase="Pending", Reason="", readiness=false. Elapsed: 10.010781ms
    Sep 20 12:35:43.747: INFO: Pod "dns-test-df59f644-2086-4ee1-b00b-443dfa5b0486": Phase="Pending", Reason="", readiness=false. Elapsed: 2.144438576s
    Sep 20 12:35:45.618: INFO: Pod "dns-test-df59f644-2086-4ee1-b00b-443dfa5b0486": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015238631s
    Sep 20 12:35:47.618: INFO: Pod "dns-test-df59f644-2086-4ee1-b00b-443dfa5b0486": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014760466s
    Sep 20 12:35:49.618: INFO: Pod "dns-test-df59f644-2086-4ee1-b00b-443dfa5b0486": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015360492s
    Sep 20 12:35:51.620: INFO: Pod "dns-test-df59f644-2086-4ee1-b00b-443dfa5b0486": Phase="Pending", Reason="", readiness=false. Elapsed: 10.016462695s
    Sep 20 12:35:53.622: INFO: Pod "dns-test-df59f644-2086-4ee1-b00b-443dfa5b0486": Phase="Pending", Reason="", readiness=false. Elapsed: 12.019356942s
    Sep 20 12:35:55.619: INFO: Pod "dns-test-df59f644-2086-4ee1-b00b-443dfa5b0486": Phase="Pending", Reason="", readiness=false. Elapsed: 14.015808964s
    Sep 20 12:35:57.622: INFO: Pod "dns-test-df59f644-2086-4ee1-b00b-443dfa5b0486": Phase="Pending", Reason="", readiness=false. Elapsed: 16.019351796s
    Sep 20 12:35:59.619: INFO: Pod "dns-test-df59f644-2086-4ee1-b00b-443dfa5b0486": Phase="Pending", Reason="", readiness=false. Elapsed: 18.015788286s
    Sep 20 12:36:01.682: INFO: Pod "dns-test-df59f644-2086-4ee1-b00b-443dfa5b0486": Phase="Pending", Reason="", readiness=false. Elapsed: 20.079223713s
    Sep 20 12:36:03.621: INFO: Pod "dns-test-df59f644-2086-4ee1-b00b-443dfa5b0486": Phase="Pending", Reason="", readiness=false. Elapsed: 22.018054846s
    Sep 20 12:36:05.751: INFO: Pod "dns-test-df59f644-2086-4ee1-b00b-443dfa5b0486": Phase="Pending", Reason="", readiness=false. Elapsed: 24.148407865s
    Sep 20 12:36:07.617: INFO: Pod "dns-test-df59f644-2086-4ee1-b00b-443dfa5b0486": Phase="Running", Reason="", readiness=true. Elapsed: 26.014419007s
    Sep 20 12:36:07.618: INFO: Pod "dns-test-df59f644-2086-4ee1-b00b-443dfa5b0486" satisfied condition "running"
    STEP: retrieving the pod 09/20/23 12:36:07.618
    STEP: looking for the results for each expected name from probers 09/20/23 12:36:07.62
    Sep 20 12:36:07.637: INFO: DNS probes using dns-9020/dns-test-df59f644-2086-4ee1-b00b-443dfa5b0486 succeeded

    STEP: deleting the pod 09/20/23 12:36:07.637
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:36:07.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9020" for this suite. 09/20/23 12:36:07.737
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:36:07.751
Sep 20 12:36:07.751: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename server-version 09/20/23 12:36:07.752
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:36:07.918
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:36:07.925
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 09/20/23 12:36:07.933
STEP: Confirm major version 09/20/23 12:36:07.936
Sep 20 12:36:07.936: INFO: Major version: 1
STEP: Confirm minor version 09/20/23 12:36:07.936
Sep 20 12:36:07.936: INFO: cleanMinorVersion: 26
Sep 20 12:36:07.936: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Sep 20 12:36:07.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-7749" for this suite. 09/20/23 12:36:07.941
------------------------------
â€¢ [0.200 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:36:07.751
    Sep 20 12:36:07.751: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename server-version 09/20/23 12:36:07.752
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:36:07.918
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:36:07.925
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 09/20/23 12:36:07.933
    STEP: Confirm major version 09/20/23 12:36:07.936
    Sep 20 12:36:07.936: INFO: Major version: 1
    STEP: Confirm minor version 09/20/23 12:36:07.936
    Sep 20 12:36:07.936: INFO: cleanMinorVersion: 26
    Sep 20 12:36:07.936: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:36:07.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-7749" for this suite. 09/20/23 12:36:07.941
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:36:07.955
Sep 20 12:36:07.955: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename sched-preemption 09/20/23 12:36:07.956
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:36:08.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:36:08.192
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Sep 20 12:36:08.227: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 20 12:37:08.265: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:37:08.269
Sep 20 12:37:08.270: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename sched-preemption-path 09/20/23 12:37:08.27
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:37:08.292
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:37:08.296
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
Sep 20 12:37:08.384: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Sep 20 12:37:08.387: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Sep 20 12:37:08.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Sep 20 12:37:08.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-7553" for this suite. 09/20/23 12:37:09.353
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-7627" for this suite. 09/20/23 12:37:09.365
------------------------------
â€¢ [SLOW TEST] [61.420 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:36:07.955
    Sep 20 12:36:07.955: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename sched-preemption 09/20/23 12:36:07.956
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:36:08.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:36:08.192
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Sep 20 12:36:08.227: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep 20 12:37:08.265: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:37:08.269
    Sep 20 12:37:08.270: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename sched-preemption-path 09/20/23 12:37:08.27
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:37:08.292
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:37:08.296
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    Sep 20 12:37:08.384: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Sep 20 12:37:08.387: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:37:08.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:37:08.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-7553" for this suite. 09/20/23 12:37:09.353
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-7627" for this suite. 09/20/23 12:37:09.365
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:37:09.376
Sep 20 12:37:09.376: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename secrets 09/20/23 12:37:09.377
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:37:09.613
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:37:09.619
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-297f323f-c1ef-4c29-9d45-55b6993f2c99 09/20/23 12:37:09.628
STEP: Creating a pod to test consume secrets 09/20/23 12:37:09.781
Sep 20 12:37:10.012: INFO: Waiting up to 5m0s for pod "pod-secrets-c7826aa5-ccdb-4fa7-91ef-2c50434987ab" in namespace "secrets-7891" to be "Succeeded or Failed"
Sep 20 12:37:10.016: INFO: Pod "pod-secrets-c7826aa5-ccdb-4fa7-91ef-2c50434987ab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.454823ms
Sep 20 12:37:12.022: INFO: Pod "pod-secrets-c7826aa5-ccdb-4fa7-91ef-2c50434987ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009851826s
Sep 20 12:37:14.044: INFO: Pod "pod-secrets-c7826aa5-ccdb-4fa7-91ef-2c50434987ab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032097202s
Sep 20 12:37:16.022: INFO: Pod "pod-secrets-c7826aa5-ccdb-4fa7-91ef-2c50434987ab": Phase="Running", Reason="", readiness=true. Elapsed: 6.010090276s
Sep 20 12:37:18.024: INFO: Pod "pod-secrets-c7826aa5-ccdb-4fa7-91ef-2c50434987ab": Phase="Running", Reason="", readiness=false. Elapsed: 8.011862441s
Sep 20 12:37:20.311: INFO: Pod "pod-secrets-c7826aa5-ccdb-4fa7-91ef-2c50434987ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.299076043s
STEP: Saw pod success 09/20/23 12:37:20.311
Sep 20 12:37:20.311: INFO: Pod "pod-secrets-c7826aa5-ccdb-4fa7-91ef-2c50434987ab" satisfied condition "Succeeded or Failed"
Sep 20 12:37:20.315: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-2 pod pod-secrets-c7826aa5-ccdb-4fa7-91ef-2c50434987ab container secret-volume-test: <nil>
STEP: delete the pod 09/20/23 12:37:20.376
Sep 20 12:37:20.597: INFO: Waiting for pod pod-secrets-c7826aa5-ccdb-4fa7-91ef-2c50434987ab to disappear
Sep 20 12:37:20.600: INFO: Pod pod-secrets-c7826aa5-ccdb-4fa7-91ef-2c50434987ab no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep 20 12:37:20.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7891" for this suite. 09/20/23 12:37:20.604
------------------------------
â€¢ [SLOW TEST] [11.233 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:37:09.376
    Sep 20 12:37:09.376: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename secrets 09/20/23 12:37:09.377
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:37:09.613
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:37:09.619
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-297f323f-c1ef-4c29-9d45-55b6993f2c99 09/20/23 12:37:09.628
    STEP: Creating a pod to test consume secrets 09/20/23 12:37:09.781
    Sep 20 12:37:10.012: INFO: Waiting up to 5m0s for pod "pod-secrets-c7826aa5-ccdb-4fa7-91ef-2c50434987ab" in namespace "secrets-7891" to be "Succeeded or Failed"
    Sep 20 12:37:10.016: INFO: Pod "pod-secrets-c7826aa5-ccdb-4fa7-91ef-2c50434987ab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.454823ms
    Sep 20 12:37:12.022: INFO: Pod "pod-secrets-c7826aa5-ccdb-4fa7-91ef-2c50434987ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009851826s
    Sep 20 12:37:14.044: INFO: Pod "pod-secrets-c7826aa5-ccdb-4fa7-91ef-2c50434987ab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032097202s
    Sep 20 12:37:16.022: INFO: Pod "pod-secrets-c7826aa5-ccdb-4fa7-91ef-2c50434987ab": Phase="Running", Reason="", readiness=true. Elapsed: 6.010090276s
    Sep 20 12:37:18.024: INFO: Pod "pod-secrets-c7826aa5-ccdb-4fa7-91ef-2c50434987ab": Phase="Running", Reason="", readiness=false. Elapsed: 8.011862441s
    Sep 20 12:37:20.311: INFO: Pod "pod-secrets-c7826aa5-ccdb-4fa7-91ef-2c50434987ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.299076043s
    STEP: Saw pod success 09/20/23 12:37:20.311
    Sep 20 12:37:20.311: INFO: Pod "pod-secrets-c7826aa5-ccdb-4fa7-91ef-2c50434987ab" satisfied condition "Succeeded or Failed"
    Sep 20 12:37:20.315: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-2 pod pod-secrets-c7826aa5-ccdb-4fa7-91ef-2c50434987ab container secret-volume-test: <nil>
    STEP: delete the pod 09/20/23 12:37:20.376
    Sep 20 12:37:20.597: INFO: Waiting for pod pod-secrets-c7826aa5-ccdb-4fa7-91ef-2c50434987ab to disappear
    Sep 20 12:37:20.600: INFO: Pod pod-secrets-c7826aa5-ccdb-4fa7-91ef-2c50434987ab no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:37:20.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7891" for this suite. 09/20/23 12:37:20.604
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:37:20.611
Sep 20 12:37:20.611: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename subpath 09/20/23 12:37:20.612
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:37:20.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:37:20.853
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/20/23 12:37:20.864
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-cz2w 09/20/23 12:37:20.968
STEP: Creating a pod to test atomic-volume-subpath 09/20/23 12:37:20.969
Sep 20 12:37:20.988: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-cz2w" in namespace "subpath-4279" to be "Succeeded or Failed"
Sep 20 12:37:20.993: INFO: Pod "pod-subpath-test-projected-cz2w": Phase="Pending", Reason="", readiness=false. Elapsed: 5.605932ms
Sep 20 12:37:23.006: INFO: Pod "pod-subpath-test-projected-cz2w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01837317s
Sep 20 12:37:25.019: INFO: Pod "pod-subpath-test-projected-cz2w": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030807413s
Sep 20 12:37:27.261: INFO: Pod "pod-subpath-test-projected-cz2w": Phase="Running", Reason="", readiness=true. Elapsed: 6.27295667s
Sep 20 12:37:29.000: INFO: Pod "pod-subpath-test-projected-cz2w": Phase="Running", Reason="", readiness=true. Elapsed: 8.01196335s
Sep 20 12:37:30.999: INFO: Pod "pod-subpath-test-projected-cz2w": Phase="Running", Reason="", readiness=true. Elapsed: 10.010763606s
Sep 20 12:37:32.999: INFO: Pod "pod-subpath-test-projected-cz2w": Phase="Running", Reason="", readiness=true. Elapsed: 12.010806963s
Sep 20 12:37:34.998: INFO: Pod "pod-subpath-test-projected-cz2w": Phase="Running", Reason="", readiness=true. Elapsed: 14.010145602s
Sep 20 12:37:36.999: INFO: Pod "pod-subpath-test-projected-cz2w": Phase="Running", Reason="", readiness=true. Elapsed: 16.011734492s
Sep 20 12:37:39.007: INFO: Pod "pod-subpath-test-projected-cz2w": Phase="Running", Reason="", readiness=true. Elapsed: 18.018773928s
Sep 20 12:37:41.004: INFO: Pod "pod-subpath-test-projected-cz2w": Phase="Running", Reason="", readiness=true. Elapsed: 20.015869272s
Sep 20 12:37:42.998: INFO: Pod "pod-subpath-test-projected-cz2w": Phase="Running", Reason="", readiness=true. Elapsed: 22.010038211s
Sep 20 12:37:44.998: INFO: Pod "pod-subpath-test-projected-cz2w": Phase="Running", Reason="", readiness=true. Elapsed: 24.010439783s
Sep 20 12:37:47.000: INFO: Pod "pod-subpath-test-projected-cz2w": Phase="Running", Reason="", readiness=false. Elapsed: 26.011880242s
Sep 20 12:37:49.072: INFO: Pod "pod-subpath-test-projected-cz2w": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.084719426s
STEP: Saw pod success 09/20/23 12:37:49.073
Sep 20 12:37:49.073: INFO: Pod "pod-subpath-test-projected-cz2w" satisfied condition "Succeeded or Failed"
Sep 20 12:37:49.899: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-subpath-test-projected-cz2w container test-container-subpath-projected-cz2w: <nil>
STEP: delete the pod 09/20/23 12:37:49.969
Sep 20 12:37:50.216: INFO: Waiting for pod pod-subpath-test-projected-cz2w to disappear
Sep 20 12:37:50.221: INFO: Pod pod-subpath-test-projected-cz2w no longer exists
STEP: Deleting pod pod-subpath-test-projected-cz2w 09/20/23 12:37:50.221
Sep 20 12:37:50.222: INFO: Deleting pod "pod-subpath-test-projected-cz2w" in namespace "subpath-4279"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Sep 20 12:37:50.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-4279" for this suite. 09/20/23 12:37:50.474
------------------------------
â€¢ [SLOW TEST] [29.891 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:37:20.611
    Sep 20 12:37:20.611: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename subpath 09/20/23 12:37:20.612
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:37:20.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:37:20.853
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/20/23 12:37:20.864
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-cz2w 09/20/23 12:37:20.968
    STEP: Creating a pod to test atomic-volume-subpath 09/20/23 12:37:20.969
    Sep 20 12:37:20.988: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-cz2w" in namespace "subpath-4279" to be "Succeeded or Failed"
    Sep 20 12:37:20.993: INFO: Pod "pod-subpath-test-projected-cz2w": Phase="Pending", Reason="", readiness=false. Elapsed: 5.605932ms
    Sep 20 12:37:23.006: INFO: Pod "pod-subpath-test-projected-cz2w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01837317s
    Sep 20 12:37:25.019: INFO: Pod "pod-subpath-test-projected-cz2w": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030807413s
    Sep 20 12:37:27.261: INFO: Pod "pod-subpath-test-projected-cz2w": Phase="Running", Reason="", readiness=true. Elapsed: 6.27295667s
    Sep 20 12:37:29.000: INFO: Pod "pod-subpath-test-projected-cz2w": Phase="Running", Reason="", readiness=true. Elapsed: 8.01196335s
    Sep 20 12:37:30.999: INFO: Pod "pod-subpath-test-projected-cz2w": Phase="Running", Reason="", readiness=true. Elapsed: 10.010763606s
    Sep 20 12:37:32.999: INFO: Pod "pod-subpath-test-projected-cz2w": Phase="Running", Reason="", readiness=true. Elapsed: 12.010806963s
    Sep 20 12:37:34.998: INFO: Pod "pod-subpath-test-projected-cz2w": Phase="Running", Reason="", readiness=true. Elapsed: 14.010145602s
    Sep 20 12:37:36.999: INFO: Pod "pod-subpath-test-projected-cz2w": Phase="Running", Reason="", readiness=true. Elapsed: 16.011734492s
    Sep 20 12:37:39.007: INFO: Pod "pod-subpath-test-projected-cz2w": Phase="Running", Reason="", readiness=true. Elapsed: 18.018773928s
    Sep 20 12:37:41.004: INFO: Pod "pod-subpath-test-projected-cz2w": Phase="Running", Reason="", readiness=true. Elapsed: 20.015869272s
    Sep 20 12:37:42.998: INFO: Pod "pod-subpath-test-projected-cz2w": Phase="Running", Reason="", readiness=true. Elapsed: 22.010038211s
    Sep 20 12:37:44.998: INFO: Pod "pod-subpath-test-projected-cz2w": Phase="Running", Reason="", readiness=true. Elapsed: 24.010439783s
    Sep 20 12:37:47.000: INFO: Pod "pod-subpath-test-projected-cz2w": Phase="Running", Reason="", readiness=false. Elapsed: 26.011880242s
    Sep 20 12:37:49.072: INFO: Pod "pod-subpath-test-projected-cz2w": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.084719426s
    STEP: Saw pod success 09/20/23 12:37:49.073
    Sep 20 12:37:49.073: INFO: Pod "pod-subpath-test-projected-cz2w" satisfied condition "Succeeded or Failed"
    Sep 20 12:37:49.899: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-subpath-test-projected-cz2w container test-container-subpath-projected-cz2w: <nil>
    STEP: delete the pod 09/20/23 12:37:49.969
    Sep 20 12:37:50.216: INFO: Waiting for pod pod-subpath-test-projected-cz2w to disappear
    Sep 20 12:37:50.221: INFO: Pod pod-subpath-test-projected-cz2w no longer exists
    STEP: Deleting pod pod-subpath-test-projected-cz2w 09/20/23 12:37:50.221
    Sep 20 12:37:50.222: INFO: Deleting pod "pod-subpath-test-projected-cz2w" in namespace "subpath-4279"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:37:50.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-4279" for this suite. 09/20/23 12:37:50.474
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:37:50.504
Sep 20 12:37:50.504: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename deployment 09/20/23 12:37:50.505
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:37:50.537
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:37:50.543
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 09/20/23 12:37:50.553
STEP: waiting for Deployment to be created 09/20/23 12:37:50.561
STEP: waiting for all Replicas to be Ready 09/20/23 12:37:50.566
Sep 20 12:37:50.568: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 20 12:37:50.568: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 20 12:37:50.998: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 20 12:37:50.998: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 20 12:37:51.037: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 20 12:37:51.037: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 20 12:37:51.081: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 20 12:37:51.081: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 20 12:37:54.092: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Sep 20 12:37:54.092: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Sep 20 12:37:55.332: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 09/20/23 12:37:55.332
W0920 12:37:55.357333      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Sep 20 12:37:55.361: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 09/20/23 12:37:55.361
Sep 20 12:37:55.364: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 0
Sep 20 12:37:55.364: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 0
Sep 20 12:37:55.364: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 0
Sep 20 12:37:55.364: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 0
Sep 20 12:37:55.364: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 0
Sep 20 12:37:55.364: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 0
Sep 20 12:37:55.364: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 0
Sep 20 12:37:55.364: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 0
Sep 20 12:37:55.364: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1
Sep 20 12:37:55.364: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1
Sep 20 12:37:55.364: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 2
Sep 20 12:37:55.364: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 2
Sep 20 12:37:55.364: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 2
Sep 20 12:37:55.364: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 2
Sep 20 12:37:56.606: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 2
Sep 20 12:37:56.606: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 2
Sep 20 12:37:57.640: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 2
Sep 20 12:37:57.640: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 2
Sep 20 12:37:58.374: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1
Sep 20 12:37:58.374: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1
Sep 20 12:37:58.668: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1
Sep 20 12:37:58.668: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1
Sep 20 12:38:01.907: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 2
Sep 20 12:38:01.907: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 2
Sep 20 12:38:01.951: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1
STEP: listing Deployments 09/20/23 12:38:01.951
Sep 20 12:38:01.958: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 09/20/23 12:38:01.958
Sep 20 12:38:02.637: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 09/20/23 12:38:02.637
Sep 20 12:38:02.649: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep 20 12:38:02.970: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep 20 12:38:03.296: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep 20 12:38:03.339: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep 20 12:38:03.496: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep 20 12:38:06.012: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep 20 12:38:07.481: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Sep 20 12:38:08.152: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep 20 12:38:08.220: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep 20 12:38:13.906: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 09/20/23 12:38:14.074
STEP: fetching the DeploymentStatus 09/20/23 12:38:14.082
Sep 20 12:38:14.086: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1
Sep 20 12:38:14.086: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1
Sep 20 12:38:14.087: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1
Sep 20 12:38:14.087: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1
Sep 20 12:38:14.087: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1
Sep 20 12:38:14.087: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 2
Sep 20 12:38:14.087: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 3
Sep 20 12:38:14.087: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 2
Sep 20 12:38:14.087: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 2
Sep 20 12:38:14.087: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 3
STEP: deleting the Deployment 09/20/23 12:38:14.087
Sep 20 12:38:14.105: INFO: observed event type MODIFIED
Sep 20 12:38:14.105: INFO: observed event type MODIFIED
Sep 20 12:38:14.106: INFO: observed event type MODIFIED
Sep 20 12:38:14.106: INFO: observed event type MODIFIED
Sep 20 12:38:14.106: INFO: observed event type MODIFIED
Sep 20 12:38:14.106: INFO: observed event type MODIFIED
Sep 20 12:38:14.106: INFO: observed event type MODIFIED
Sep 20 12:38:14.106: INFO: observed event type MODIFIED
Sep 20 12:38:14.106: INFO: observed event type MODIFIED
Sep 20 12:38:14.106: INFO: observed event type MODIFIED
Sep 20 12:38:14.106: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep 20 12:38:14.140: INFO: Log out all the ReplicaSets if there is no deployment created
Sep 20 12:38:14.150: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-6297  026b4579-375a-4b92-b146-ed11957bab48 16693 2 2023-09-20 12:38:02 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment ce2056c1-82c8-4547-8ba8-7014355eff30 0xc004b53697 0xc004b53698}] [] [{kube-controller-manager Update apps/v1 2023-09-20 12:38:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce2056c1-82c8-4547-8ba8-7014355eff30\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 12:38:12 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b53720 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Sep 20 12:38:14.166: INFO: pod: "test-deployment-7b7876f9d6-8ssb6":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-8ssb6 test-deployment-7b7876f9d6- deployment-6297  b59b1216-7a1b-4d8c-9ff9-a2bafa85424b 16647 0 2023-09-20 12:38:03 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 026b4579-375a-4b92-b146-ed11957bab48 0xc004b53ba7 0xc004b53ba8}] [] [{kube-controller-manager Update v1 2023-09-20 12:38:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"026b4579-375a-4b92-b146-ed11957bab48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 12:38:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.4.88\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k8bkx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k8bkx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:38:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:38:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:38:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:38:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.64,PodIP:10.100.4.88,StartTime:2023-09-20 12:38:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-20 12:38:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://aaa2abc2419f05b77e6fb344112a154ba523493570b982aa503e5a0a595f2424,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.4.88,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Sep 20 12:38:14.166: INFO: pod: "test-deployment-7b7876f9d6-h24p5":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-h24p5 test-deployment-7b7876f9d6- deployment-6297  6cf8f26a-4281-443b-9e81-363d7e2d70dc 16692 0 2023-09-20 12:38:07 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 026b4579-375a-4b92-b146-ed11957bab48 0xc004b53d87 0xc004b53d88}] [] [{kube-controller-manager Update v1 2023-09-20 12:38:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"026b4579-375a-4b92-b146-ed11957bab48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 12:38:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.3.85\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4t895,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4t895,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:38:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:38:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:38:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:38:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.172,PodIP:10.100.3.85,StartTime:2023-09-20 12:38:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-20 12:38:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://589aa10ff639fed0fdead8e3c0b3451be24cc1c5603d1cdc285b0943d57cf562,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.3.85,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Sep 20 12:38:14.166: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-6297  940720ab-0a5a-446b-b081-a1a4413820a0 16706 4 2023-09-20 12:37:56 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment ce2056c1-82c8-4547-8ba8-7014355eff30 0xc004b53787 0xc004b53788}] [] [{kube-controller-manager Update apps/v1 2023-09-20 12:38:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce2056c1-82c8-4547-8ba8-7014355eff30\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 12:38:14 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b53810 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Sep 20 12:38:14.175: INFO: pod: "test-deployment-7df74c55ff-rbtd2":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-rbtd2 test-deployment-7df74c55ff- deployment-6297  d30f4208-a113-4ff7-98f3-c9c03f3e0241 16699 0 2023-09-20 12:38:02 +0000 UTC 2023-09-20 12:38:13 +0000 UTC 0xc004c3d138 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 940720ab-0a5a-446b-b081-a1a4413820a0 0xc004c3d167 0xc004c3d168}] [] [{kube-controller-manager Update v1 2023-09-20 12:38:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"940720ab-0a5a-446b-b081-a1a4413820a0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 12:38:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.5.54\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8sqkq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8sqkq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:38:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:38:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:38:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:38:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.173,PodIP:10.100.5.54,StartTime:2023-09-20 12:38:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-20 12:38:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://f3990bdf902998a19d7d7e675baa065f03f6039eaa5e05d9e98d5e6716b7d922,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.5.54,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Sep 20 12:38:14.175: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-6297  dfa0d92a-49ea-4192-9acd-288305a977c8 16595 3 2023-09-20 12:37:50 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment ce2056c1-82c8-4547-8ba8-7014355eff30 0xc004b53877 0xc004b53878}] [] [{kube-controller-manager Update apps/v1 2023-09-20 12:38:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce2056c1-82c8-4547-8ba8-7014355eff30\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 12:38:01 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b53900 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep 20 12:38:14.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-6297" for this suite. 09/20/23 12:38:14.189
------------------------------
â€¢ [SLOW TEST] [23.707 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:37:50.504
    Sep 20 12:37:50.504: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename deployment 09/20/23 12:37:50.505
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:37:50.537
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:37:50.543
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 09/20/23 12:37:50.553
    STEP: waiting for Deployment to be created 09/20/23 12:37:50.561
    STEP: waiting for all Replicas to be Ready 09/20/23 12:37:50.566
    Sep 20 12:37:50.568: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep 20 12:37:50.568: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep 20 12:37:50.998: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep 20 12:37:50.998: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep 20 12:37:51.037: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep 20 12:37:51.037: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep 20 12:37:51.081: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep 20 12:37:51.081: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep 20 12:37:54.092: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Sep 20 12:37:54.092: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Sep 20 12:37:55.332: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 09/20/23 12:37:55.332
    W0920 12:37:55.357333      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Sep 20 12:37:55.361: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 09/20/23 12:37:55.361
    Sep 20 12:37:55.364: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 0
    Sep 20 12:37:55.364: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 0
    Sep 20 12:37:55.364: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 0
    Sep 20 12:37:55.364: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 0
    Sep 20 12:37:55.364: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 0
    Sep 20 12:37:55.364: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 0
    Sep 20 12:37:55.364: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 0
    Sep 20 12:37:55.364: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 0
    Sep 20 12:37:55.364: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1
    Sep 20 12:37:55.364: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1
    Sep 20 12:37:55.364: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 2
    Sep 20 12:37:55.364: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 2
    Sep 20 12:37:55.364: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 2
    Sep 20 12:37:55.364: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 2
    Sep 20 12:37:56.606: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 2
    Sep 20 12:37:56.606: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 2
    Sep 20 12:37:57.640: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 2
    Sep 20 12:37:57.640: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 2
    Sep 20 12:37:58.374: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1
    Sep 20 12:37:58.374: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1
    Sep 20 12:37:58.668: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1
    Sep 20 12:37:58.668: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1
    Sep 20 12:38:01.907: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 2
    Sep 20 12:38:01.907: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 2
    Sep 20 12:38:01.951: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1
    STEP: listing Deployments 09/20/23 12:38:01.951
    Sep 20 12:38:01.958: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 09/20/23 12:38:01.958
    Sep 20 12:38:02.637: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 09/20/23 12:38:02.637
    Sep 20 12:38:02.649: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep 20 12:38:02.970: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep 20 12:38:03.296: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep 20 12:38:03.339: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep 20 12:38:03.496: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep 20 12:38:06.012: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Sep 20 12:38:07.481: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Sep 20 12:38:08.152: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Sep 20 12:38:08.220: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Sep 20 12:38:13.906: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 09/20/23 12:38:14.074
    STEP: fetching the DeploymentStatus 09/20/23 12:38:14.082
    Sep 20 12:38:14.086: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1
    Sep 20 12:38:14.086: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1
    Sep 20 12:38:14.087: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1
    Sep 20 12:38:14.087: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1
    Sep 20 12:38:14.087: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 1
    Sep 20 12:38:14.087: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 2
    Sep 20 12:38:14.087: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 3
    Sep 20 12:38:14.087: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 2
    Sep 20 12:38:14.087: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 2
    Sep 20 12:38:14.087: INFO: observed Deployment test-deployment in namespace deployment-6297 with ReadyReplicas 3
    STEP: deleting the Deployment 09/20/23 12:38:14.087
    Sep 20 12:38:14.105: INFO: observed event type MODIFIED
    Sep 20 12:38:14.105: INFO: observed event type MODIFIED
    Sep 20 12:38:14.106: INFO: observed event type MODIFIED
    Sep 20 12:38:14.106: INFO: observed event type MODIFIED
    Sep 20 12:38:14.106: INFO: observed event type MODIFIED
    Sep 20 12:38:14.106: INFO: observed event type MODIFIED
    Sep 20 12:38:14.106: INFO: observed event type MODIFIED
    Sep 20 12:38:14.106: INFO: observed event type MODIFIED
    Sep 20 12:38:14.106: INFO: observed event type MODIFIED
    Sep 20 12:38:14.106: INFO: observed event type MODIFIED
    Sep 20 12:38:14.106: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep 20 12:38:14.140: INFO: Log out all the ReplicaSets if there is no deployment created
    Sep 20 12:38:14.150: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-6297  026b4579-375a-4b92-b146-ed11957bab48 16693 2 2023-09-20 12:38:02 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment ce2056c1-82c8-4547-8ba8-7014355eff30 0xc004b53697 0xc004b53698}] [] [{kube-controller-manager Update apps/v1 2023-09-20 12:38:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce2056c1-82c8-4547-8ba8-7014355eff30\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 12:38:12 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b53720 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Sep 20 12:38:14.166: INFO: pod: "test-deployment-7b7876f9d6-8ssb6":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-8ssb6 test-deployment-7b7876f9d6- deployment-6297  b59b1216-7a1b-4d8c-9ff9-a2bafa85424b 16647 0 2023-09-20 12:38:03 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 026b4579-375a-4b92-b146-ed11957bab48 0xc004b53ba7 0xc004b53ba8}] [] [{kube-controller-manager Update v1 2023-09-20 12:38:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"026b4579-375a-4b92-b146-ed11957bab48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 12:38:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.4.88\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k8bkx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k8bkx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:38:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:38:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:38:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:38:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.64,PodIP:10.100.4.88,StartTime:2023-09-20 12:38:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-20 12:38:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://aaa2abc2419f05b77e6fb344112a154ba523493570b982aa503e5a0a595f2424,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.4.88,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Sep 20 12:38:14.166: INFO: pod: "test-deployment-7b7876f9d6-h24p5":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-h24p5 test-deployment-7b7876f9d6- deployment-6297  6cf8f26a-4281-443b-9e81-363d7e2d70dc 16692 0 2023-09-20 12:38:07 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 026b4579-375a-4b92-b146-ed11957bab48 0xc004b53d87 0xc004b53d88}] [] [{kube-controller-manager Update v1 2023-09-20 12:38:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"026b4579-375a-4b92-b146-ed11957bab48\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 12:38:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.3.85\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4t895,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4t895,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:38:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:38:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:38:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:38:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.172,PodIP:10.100.3.85,StartTime:2023-09-20 12:38:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-20 12:38:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://589aa10ff639fed0fdead8e3c0b3451be24cc1c5603d1cdc285b0943d57cf562,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.3.85,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Sep 20 12:38:14.166: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-6297  940720ab-0a5a-446b-b081-a1a4413820a0 16706 4 2023-09-20 12:37:56 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment ce2056c1-82c8-4547-8ba8-7014355eff30 0xc004b53787 0xc004b53788}] [] [{kube-controller-manager Update apps/v1 2023-09-20 12:38:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce2056c1-82c8-4547-8ba8-7014355eff30\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 12:38:14 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b53810 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Sep 20 12:38:14.175: INFO: pod: "test-deployment-7df74c55ff-rbtd2":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-rbtd2 test-deployment-7df74c55ff- deployment-6297  d30f4208-a113-4ff7-98f3-c9c03f3e0241 16699 0 2023-09-20 12:38:02 +0000 UTC 2023-09-20 12:38:13 +0000 UTC 0xc004c3d138 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 940720ab-0a5a-446b-b081-a1a4413820a0 0xc004c3d167 0xc004c3d168}] [] [{kube-controller-manager Update v1 2023-09-20 12:38:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"940720ab-0a5a-446b-b081-a1a4413820a0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 12:38:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.5.54\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8sqkq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8sqkq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:38:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:38:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:38:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:38:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.173,PodIP:10.100.5.54,StartTime:2023-09-20 12:38:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-20 12:38:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://f3990bdf902998a19d7d7e675baa065f03f6039eaa5e05d9e98d5e6716b7d922,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.5.54,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Sep 20 12:38:14.175: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-6297  dfa0d92a-49ea-4192-9acd-288305a977c8 16595 3 2023-09-20 12:37:50 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment ce2056c1-82c8-4547-8ba8-7014355eff30 0xc004b53877 0xc004b53878}] [] [{kube-controller-manager Update apps/v1 2023-09-20 12:38:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce2056c1-82c8-4547-8ba8-7014355eff30\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 12:38:01 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b53900 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:38:14.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-6297" for this suite. 09/20/23 12:38:14.189
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:38:14.211
Sep 20 12:38:14.211: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename init-container 09/20/23 12:38:14.212
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:38:14.244
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:38:14.25
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 09/20/23 12:38:14.258
Sep 20 12:38:14.258: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep 20 12:38:20.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-2655" for this suite. 09/20/23 12:38:20.73
------------------------------
â€¢ [SLOW TEST] [6.676 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:38:14.211
    Sep 20 12:38:14.211: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename init-container 09/20/23 12:38:14.212
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:38:14.244
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:38:14.25
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 09/20/23 12:38:14.258
    Sep 20 12:38:14.258: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:38:20.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-2655" for this suite. 09/20/23 12:38:20.73
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:38:20.89
Sep 20 12:38:20.890: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename crd-publish-openapi 09/20/23 12:38:20.891
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:38:21.139
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:38:21.142
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Sep 20 12:38:21.154: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/20/23 12:38:24.072
Sep 20 12:38:24.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-1774 --namespace=crd-publish-openapi-1774 create -f -'
Sep 20 12:38:25.473: INFO: stderr: ""
Sep 20 12:38:25.473: INFO: stdout: "e2e-test-crd-publish-openapi-9460-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep 20 12:38:25.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-1774 --namespace=crd-publish-openapi-1774 delete e2e-test-crd-publish-openapi-9460-crds test-cr'
Sep 20 12:38:25.567: INFO: stderr: ""
Sep 20 12:38:25.567: INFO: stdout: "e2e-test-crd-publish-openapi-9460-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Sep 20 12:38:25.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-1774 --namespace=crd-publish-openapi-1774 apply -f -'
Sep 20 12:38:26.676: INFO: stderr: ""
Sep 20 12:38:26.676: INFO: stdout: "e2e-test-crd-publish-openapi-9460-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep 20 12:38:26.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-1774 --namespace=crd-publish-openapi-1774 delete e2e-test-crd-publish-openapi-9460-crds test-cr'
Sep 20 12:38:26.992: INFO: stderr: ""
Sep 20 12:38:26.992: INFO: stdout: "e2e-test-crd-publish-openapi-9460-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 09/20/23 12:38:26.992
Sep 20 12:38:26.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-1774 explain e2e-test-crd-publish-openapi-9460-crds'
Sep 20 12:38:27.178: INFO: stderr: ""
Sep 20 12:38:27.178: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9460-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep 20 12:38:34.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1774" for this suite. 09/20/23 12:38:34.224
------------------------------
â€¢ [SLOW TEST] [13.340 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:38:20.89
    Sep 20 12:38:20.890: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename crd-publish-openapi 09/20/23 12:38:20.891
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:38:21.139
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:38:21.142
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Sep 20 12:38:21.154: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/20/23 12:38:24.072
    Sep 20 12:38:24.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-1774 --namespace=crd-publish-openapi-1774 create -f -'
    Sep 20 12:38:25.473: INFO: stderr: ""
    Sep 20 12:38:25.473: INFO: stdout: "e2e-test-crd-publish-openapi-9460-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Sep 20 12:38:25.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-1774 --namespace=crd-publish-openapi-1774 delete e2e-test-crd-publish-openapi-9460-crds test-cr'
    Sep 20 12:38:25.567: INFO: stderr: ""
    Sep 20 12:38:25.567: INFO: stdout: "e2e-test-crd-publish-openapi-9460-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Sep 20 12:38:25.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-1774 --namespace=crd-publish-openapi-1774 apply -f -'
    Sep 20 12:38:26.676: INFO: stderr: ""
    Sep 20 12:38:26.676: INFO: stdout: "e2e-test-crd-publish-openapi-9460-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Sep 20 12:38:26.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-1774 --namespace=crd-publish-openapi-1774 delete e2e-test-crd-publish-openapi-9460-crds test-cr'
    Sep 20 12:38:26.992: INFO: stderr: ""
    Sep 20 12:38:26.992: INFO: stdout: "e2e-test-crd-publish-openapi-9460-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 09/20/23 12:38:26.992
    Sep 20 12:38:26.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-1774 explain e2e-test-crd-publish-openapi-9460-crds'
    Sep 20 12:38:27.178: INFO: stderr: ""
    Sep 20 12:38:27.178: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9460-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:38:34.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1774" for this suite. 09/20/23 12:38:34.224
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:38:34.231
Sep 20 12:38:34.231: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename endpointslice 09/20/23 12:38:34.232
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:38:34.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:38:34.413
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Sep 20 12:38:37.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-1710" for this suite. 09/20/23 12:38:37.104
------------------------------
â€¢ [2.880 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:38:34.231
    Sep 20 12:38:34.231: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename endpointslice 09/20/23 12:38:34.232
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:38:34.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:38:34.413
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:38:37.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-1710" for this suite. 09/20/23 12:38:37.104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:38:37.111
Sep 20 12:38:37.111: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename namespaces 09/20/23 12:38:37.112
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:38:37.129
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:38:37.135
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 09/20/23 12:38:37.145
Sep 20 12:38:37.150: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 09/20/23 12:38:37.15
Sep 20 12:38:37.159: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 09/20/23 12:38:37.159
Sep 20 12:38:37.169: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep 20 12:38:37.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-2609" for this suite. 09/20/23 12:38:37.174
------------------------------
â€¢ [0.068 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:38:37.111
    Sep 20 12:38:37.111: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename namespaces 09/20/23 12:38:37.112
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:38:37.129
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:38:37.135
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 09/20/23 12:38:37.145
    Sep 20 12:38:37.150: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 09/20/23 12:38:37.15
    Sep 20 12:38:37.159: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 09/20/23 12:38:37.159
    Sep 20 12:38:37.169: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:38:37.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-2609" for this suite. 09/20/23 12:38:37.174
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:38:37.182
Sep 20 12:38:37.182: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename cronjob 09/20/23 12:38:37.183
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:38:37.199
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:38:37.202
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 09/20/23 12:38:37.206
STEP: Ensuring more than one job is running at a time 09/20/23 12:38:37.215
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 09/20/23 12:40:01.22
STEP: Removing cronjob 09/20/23 12:40:01.223
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Sep 20 12:40:01.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-37" for this suite. 09/20/23 12:40:01.237
------------------------------
â€¢ [SLOW TEST] [84.201 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:38:37.182
    Sep 20 12:38:37.182: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename cronjob 09/20/23 12:38:37.183
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:38:37.199
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:38:37.202
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 09/20/23 12:38:37.206
    STEP: Ensuring more than one job is running at a time 09/20/23 12:38:37.215
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 09/20/23 12:40:01.22
    STEP: Removing cronjob 09/20/23 12:40:01.223
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:40:01.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-37" for this suite. 09/20/23 12:40:01.237
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:40:01.385
Sep 20 12:40:01.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename webhook 09/20/23 12:40:01.386
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:40:02.371
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:40:02.376
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/20/23 12:40:02.418
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/20/23 12:40:02.898
STEP: Deploying the webhook pod 09/20/23 12:40:02.908
STEP: Wait for the deployment to be ready 09/20/23 12:40:03.191
Sep 20 12:40:03.266: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 12:40:05.279: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 40, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 40, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 40, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 40, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 12:40:07.459: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 40, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 40, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 40, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 40, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/20/23 12:40:09.282
STEP: Verifying the service has paired with the endpoint 09/20/23 12:40:09.631
Sep 20 12:40:10.632: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 09/20/23 12:40:11.072
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 09/20/23 12:40:11.075
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 09/20/23 12:40:11.075
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 09/20/23 12:40:11.075
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 09/20/23 12:40:11.078
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 09/20/23 12:40:11.078
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 09/20/23 12:40:11.08
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep 20 12:40:11.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8866" for this suite. 09/20/23 12:40:11.355
STEP: Destroying namespace "webhook-8866-markers" for this suite. 09/20/23 12:40:11.397
------------------------------
â€¢ [SLOW TEST] [10.024 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:40:01.385
    Sep 20 12:40:01.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename webhook 09/20/23 12:40:01.386
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:40:02.371
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:40:02.376
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/20/23 12:40:02.418
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/20/23 12:40:02.898
    STEP: Deploying the webhook pod 09/20/23 12:40:02.908
    STEP: Wait for the deployment to be ready 09/20/23 12:40:03.191
    Sep 20 12:40:03.266: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Sep 20 12:40:05.279: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 40, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 40, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 40, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 40, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 12:40:07.459: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 40, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 40, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 40, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 40, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/20/23 12:40:09.282
    STEP: Verifying the service has paired with the endpoint 09/20/23 12:40:09.631
    Sep 20 12:40:10.632: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 09/20/23 12:40:11.072
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 09/20/23 12:40:11.075
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 09/20/23 12:40:11.075
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 09/20/23 12:40:11.075
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 09/20/23 12:40:11.078
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 09/20/23 12:40:11.078
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 09/20/23 12:40:11.08
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:40:11.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8866" for this suite. 09/20/23 12:40:11.355
    STEP: Destroying namespace "webhook-8866-markers" for this suite. 09/20/23 12:40:11.397
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:40:11.411
Sep 20 12:40:11.411: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename container-probe 09/20/23 12:40:11.411
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:40:11.43
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:40:11.433
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-ad9419a9-eb3b-461d-85d5-14b3d084a7f2 in namespace container-probe-7408 09/20/23 12:40:11.653
Sep 20 12:40:11.753: INFO: Waiting up to 5m0s for pod "liveness-ad9419a9-eb3b-461d-85d5-14b3d084a7f2" in namespace "container-probe-7408" to be "not pending"
Sep 20 12:40:11.762: INFO: Pod "liveness-ad9419a9-eb3b-461d-85d5-14b3d084a7f2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.939784ms
Sep 20 12:40:13.773: INFO: Pod "liveness-ad9419a9-eb3b-461d-85d5-14b3d084a7f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019197312s
Sep 20 12:40:16.184: INFO: Pod "liveness-ad9419a9-eb3b-461d-85d5-14b3d084a7f2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.430200692s
Sep 20 12:40:17.810: INFO: Pod "liveness-ad9419a9-eb3b-461d-85d5-14b3d084a7f2": Phase="Running", Reason="", readiness=true. Elapsed: 6.05638332s
Sep 20 12:40:17.810: INFO: Pod "liveness-ad9419a9-eb3b-461d-85d5-14b3d084a7f2" satisfied condition "not pending"
Sep 20 12:40:17.810: INFO: Started pod liveness-ad9419a9-eb3b-461d-85d5-14b3d084a7f2 in namespace container-probe-7408
STEP: checking the pod's current state and verifying that restartCount is present 09/20/23 12:40:17.81
Sep 20 12:40:17.816: INFO: Initial restart count of pod liveness-ad9419a9-eb3b-461d-85d5-14b3d084a7f2 is 0
Sep 20 12:40:41.510: INFO: Restart count of pod container-probe-7408/liveness-ad9419a9-eb3b-461d-85d5-14b3d084a7f2 is now 1 (23.694695573s elapsed)
Sep 20 12:40:55.900: INFO: Restart count of pod container-probe-7408/liveness-ad9419a9-eb3b-461d-85d5-14b3d084a7f2 is now 2 (38.084001728s elapsed)
Sep 20 12:41:14.333: INFO: Restart count of pod container-probe-7408/liveness-ad9419a9-eb3b-461d-85d5-14b3d084a7f2 is now 3 (56.517320504s elapsed)
Sep 20 12:41:34.684: INFO: Restart count of pod container-probe-7408/liveness-ad9419a9-eb3b-461d-85d5-14b3d084a7f2 is now 4 (1m16.868368098s elapsed)
Sep 20 12:42:44.701: INFO: Restart count of pod container-probe-7408/liveness-ad9419a9-eb3b-461d-85d5-14b3d084a7f2 is now 5 (2m26.885187863s elapsed)
STEP: deleting the pod 09/20/23 12:42:44.701
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep 20 12:42:45.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-7408" for this suite. 09/20/23 12:42:45.093
------------------------------
â€¢ [SLOW TEST] [153.715 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:40:11.411
    Sep 20 12:40:11.411: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename container-probe 09/20/23 12:40:11.411
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:40:11.43
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:40:11.433
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-ad9419a9-eb3b-461d-85d5-14b3d084a7f2 in namespace container-probe-7408 09/20/23 12:40:11.653
    Sep 20 12:40:11.753: INFO: Waiting up to 5m0s for pod "liveness-ad9419a9-eb3b-461d-85d5-14b3d084a7f2" in namespace "container-probe-7408" to be "not pending"
    Sep 20 12:40:11.762: INFO: Pod "liveness-ad9419a9-eb3b-461d-85d5-14b3d084a7f2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.939784ms
    Sep 20 12:40:13.773: INFO: Pod "liveness-ad9419a9-eb3b-461d-85d5-14b3d084a7f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019197312s
    Sep 20 12:40:16.184: INFO: Pod "liveness-ad9419a9-eb3b-461d-85d5-14b3d084a7f2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.430200692s
    Sep 20 12:40:17.810: INFO: Pod "liveness-ad9419a9-eb3b-461d-85d5-14b3d084a7f2": Phase="Running", Reason="", readiness=true. Elapsed: 6.05638332s
    Sep 20 12:40:17.810: INFO: Pod "liveness-ad9419a9-eb3b-461d-85d5-14b3d084a7f2" satisfied condition "not pending"
    Sep 20 12:40:17.810: INFO: Started pod liveness-ad9419a9-eb3b-461d-85d5-14b3d084a7f2 in namespace container-probe-7408
    STEP: checking the pod's current state and verifying that restartCount is present 09/20/23 12:40:17.81
    Sep 20 12:40:17.816: INFO: Initial restart count of pod liveness-ad9419a9-eb3b-461d-85d5-14b3d084a7f2 is 0
    Sep 20 12:40:41.510: INFO: Restart count of pod container-probe-7408/liveness-ad9419a9-eb3b-461d-85d5-14b3d084a7f2 is now 1 (23.694695573s elapsed)
    Sep 20 12:40:55.900: INFO: Restart count of pod container-probe-7408/liveness-ad9419a9-eb3b-461d-85d5-14b3d084a7f2 is now 2 (38.084001728s elapsed)
    Sep 20 12:41:14.333: INFO: Restart count of pod container-probe-7408/liveness-ad9419a9-eb3b-461d-85d5-14b3d084a7f2 is now 3 (56.517320504s elapsed)
    Sep 20 12:41:34.684: INFO: Restart count of pod container-probe-7408/liveness-ad9419a9-eb3b-461d-85d5-14b3d084a7f2 is now 4 (1m16.868368098s elapsed)
    Sep 20 12:42:44.701: INFO: Restart count of pod container-probe-7408/liveness-ad9419a9-eb3b-461d-85d5-14b3d084a7f2 is now 5 (2m26.885187863s elapsed)
    STEP: deleting the pod 09/20/23 12:42:44.701
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:42:45.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-7408" for this suite. 09/20/23 12:42:45.093
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:42:45.126
Sep 20 12:42:45.126: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename taint-multiple-pods 09/20/23 12:42:45.127
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:42:45.197
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:42:45.201
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Sep 20 12:42:45.204: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 20 12:43:45.234: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Sep 20 12:43:45.239: INFO: Starting informer...
STEP: Starting pods... 09/20/23 12:43:45.239
Sep 20 12:43:45.696: INFO: Pod1 is running on mycluster-ww3cg64etuwi-node-1. Tainting Node
Sep 20 12:43:46.361: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-8867" to be "running"
Sep 20 12:43:46.366: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.811656ms
Sep 20 12:43:49.058: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.696611916s
Sep 20 12:43:50.471: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 4.109046573s
Sep 20 12:43:50.471: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Sep 20 12:43:50.471: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-8867" to be "running"
Sep 20 12:43:50.509: INFO: Pod "taint-eviction-b2": Phase="Pending", Reason="", readiness=false. Elapsed: 38.014102ms
Sep 20 12:43:52.515: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.044298646s
Sep 20 12:43:52.515: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Sep 20 12:43:52.515: INFO: Pod2 is running on mycluster-ww3cg64etuwi-node-1. Tainting Node
STEP: Trying to apply a taint on the Node 09/20/23 12:43:52.515
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/20/23 12:43:52.558
STEP: Waiting for Pod1 and Pod2 to be deleted 09/20/23 12:43:52.571
Sep 20 12:43:59.804: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Sep 20 12:44:19.926: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/20/23 12:44:20.364
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Sep 20 12:44:20.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-8867" for this suite. 09/20/23 12:44:20.521
------------------------------
â€¢ [SLOW TEST] [95.405 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:42:45.126
    Sep 20 12:42:45.126: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename taint-multiple-pods 09/20/23 12:42:45.127
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:42:45.197
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:42:45.201
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Sep 20 12:42:45.204: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep 20 12:43:45.234: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Sep 20 12:43:45.239: INFO: Starting informer...
    STEP: Starting pods... 09/20/23 12:43:45.239
    Sep 20 12:43:45.696: INFO: Pod1 is running on mycluster-ww3cg64etuwi-node-1. Tainting Node
    Sep 20 12:43:46.361: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-8867" to be "running"
    Sep 20 12:43:46.366: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.811656ms
    Sep 20 12:43:49.058: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.696611916s
    Sep 20 12:43:50.471: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 4.109046573s
    Sep 20 12:43:50.471: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Sep 20 12:43:50.471: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-8867" to be "running"
    Sep 20 12:43:50.509: INFO: Pod "taint-eviction-b2": Phase="Pending", Reason="", readiness=false. Elapsed: 38.014102ms
    Sep 20 12:43:52.515: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.044298646s
    Sep 20 12:43:52.515: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Sep 20 12:43:52.515: INFO: Pod2 is running on mycluster-ww3cg64etuwi-node-1. Tainting Node
    STEP: Trying to apply a taint on the Node 09/20/23 12:43:52.515
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/20/23 12:43:52.558
    STEP: Waiting for Pod1 and Pod2 to be deleted 09/20/23 12:43:52.571
    Sep 20 12:43:59.804: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Sep 20 12:44:19.926: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/20/23 12:44:20.364
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:44:20.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-8867" for this suite. 09/20/23 12:44:20.521
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:44:20.536
Sep 20 12:44:20.536: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename events 09/20/23 12:44:20.536
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:44:20.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:44:20.561
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 09/20/23 12:44:20.566
Sep 20 12:44:20.572: INFO: created test-event-1
Sep 20 12:44:20.576: INFO: created test-event-2
Sep 20 12:44:20.582: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 09/20/23 12:44:20.582
STEP: delete collection of events 09/20/23 12:44:20.586
Sep 20 12:44:20.586: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 09/20/23 12:44:20.605
Sep 20 12:44:20.606: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Sep 20 12:44:20.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-7073" for this suite. 09/20/23 12:44:20.612
------------------------------
â€¢ [0.082 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:44:20.536
    Sep 20 12:44:20.536: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename events 09/20/23 12:44:20.536
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:44:20.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:44:20.561
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 09/20/23 12:44:20.566
    Sep 20 12:44:20.572: INFO: created test-event-1
    Sep 20 12:44:20.576: INFO: created test-event-2
    Sep 20 12:44:20.582: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 09/20/23 12:44:20.582
    STEP: delete collection of events 09/20/23 12:44:20.586
    Sep 20 12:44:20.586: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 09/20/23 12:44:20.605
    Sep 20 12:44:20.606: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:44:20.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-7073" for this suite. 09/20/23 12:44:20.612
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:44:20.619
Sep 20 12:44:20.619: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename replication-controller 09/20/23 12:44:20.62
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:44:20.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:44:20.986
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 09/20/23 12:44:20.999
STEP: waiting for RC to be added 09/20/23 12:44:21.007
STEP: waiting for available Replicas 09/20/23 12:44:21.007
STEP: patching ReplicationController 09/20/23 12:44:25.723
STEP: waiting for RC to be modified 09/20/23 12:44:25.751
STEP: patching ReplicationController status 09/20/23 12:44:25.752
STEP: waiting for RC to be modified 09/20/23 12:44:26.292
STEP: waiting for available Replicas 09/20/23 12:44:26.292
STEP: fetching ReplicationController status 09/20/23 12:44:26.318
STEP: patching ReplicationController scale 09/20/23 12:44:26.364
STEP: waiting for RC to be modified 09/20/23 12:44:26.381
STEP: waiting for ReplicationController's scale to be the max amount 09/20/23 12:44:26.382
STEP: fetching ReplicationController; ensuring that it's patched 09/20/23 12:44:31.35
STEP: updating ReplicationController status 09/20/23 12:44:31.354
STEP: waiting for RC to be modified 09/20/23 12:44:31.447
STEP: listing all ReplicationControllers 09/20/23 12:44:31.447
STEP: checking that ReplicationController has expected values 09/20/23 12:44:31.452
STEP: deleting ReplicationControllers by collection 09/20/23 12:44:31.453
STEP: waiting for ReplicationController to have a DELETED watchEvent 09/20/23 12:44:31.562
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep 20 12:44:32.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-7660" for this suite. 09/20/23 12:44:32.326
------------------------------
â€¢ [SLOW TEST] [11.727 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:44:20.619
    Sep 20 12:44:20.619: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename replication-controller 09/20/23 12:44:20.62
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:44:20.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:44:20.986
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 09/20/23 12:44:20.999
    STEP: waiting for RC to be added 09/20/23 12:44:21.007
    STEP: waiting for available Replicas 09/20/23 12:44:21.007
    STEP: patching ReplicationController 09/20/23 12:44:25.723
    STEP: waiting for RC to be modified 09/20/23 12:44:25.751
    STEP: patching ReplicationController status 09/20/23 12:44:25.752
    STEP: waiting for RC to be modified 09/20/23 12:44:26.292
    STEP: waiting for available Replicas 09/20/23 12:44:26.292
    STEP: fetching ReplicationController status 09/20/23 12:44:26.318
    STEP: patching ReplicationController scale 09/20/23 12:44:26.364
    STEP: waiting for RC to be modified 09/20/23 12:44:26.381
    STEP: waiting for ReplicationController's scale to be the max amount 09/20/23 12:44:26.382
    STEP: fetching ReplicationController; ensuring that it's patched 09/20/23 12:44:31.35
    STEP: updating ReplicationController status 09/20/23 12:44:31.354
    STEP: waiting for RC to be modified 09/20/23 12:44:31.447
    STEP: listing all ReplicationControllers 09/20/23 12:44:31.447
    STEP: checking that ReplicationController has expected values 09/20/23 12:44:31.452
    STEP: deleting ReplicationControllers by collection 09/20/23 12:44:31.453
    STEP: waiting for ReplicationController to have a DELETED watchEvent 09/20/23 12:44:31.562
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:44:32.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-7660" for this suite. 09/20/23 12:44:32.326
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:44:32.35
Sep 20 12:44:32.350: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename kubectl 09/20/23 12:44:32.351
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:44:32.4
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:44:32.404
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 09/20/23 12:44:32.409
Sep 20 12:44:32.410: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-3073 proxy --unix-socket=/tmp/kubectl-proxy-unix1019511340/test'
STEP: retrieving proxy /api/ output 09/20/23 12:44:32.45
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep 20 12:44:32.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3073" for this suite. 09/20/23 12:44:32.459
------------------------------
â€¢ [0.308 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:44:32.35
    Sep 20 12:44:32.350: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename kubectl 09/20/23 12:44:32.351
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:44:32.4
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:44:32.404
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 09/20/23 12:44:32.409
    Sep 20 12:44:32.410: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-3073 proxy --unix-socket=/tmp/kubectl-proxy-unix1019511340/test'
    STEP: retrieving proxy /api/ output 09/20/23 12:44:32.45
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:44:32.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3073" for this suite. 09/20/23 12:44:32.459
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:44:32.659
Sep 20 12:44:32.659: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename conformance-tests 09/20/23 12:44:32.66
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:44:33.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:44:33.201
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 09/20/23 12:44:33.206
Sep 20 12:44:33.206: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Sep 20 12:44:33.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-3915" for this suite. 09/20/23 12:44:33.465
------------------------------
â€¢ [1.203 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:44:32.659
    Sep 20 12:44:32.659: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename conformance-tests 09/20/23 12:44:32.66
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:44:33.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:44:33.201
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 09/20/23 12:44:33.206
    Sep 20 12:44:33.206: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:44:33.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-3915" for this suite. 09/20/23 12:44:33.465
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:44:33.863
Sep 20 12:44:33.863: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename disruption 09/20/23 12:44:33.863
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:44:34.33
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:44:34.335
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:44:34.341
Sep 20 12:44:34.341: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename disruption-2 09/20/23 12:44:34.341
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:44:34.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:44:34.367
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 09/20/23 12:44:34.378
STEP: Waiting for the pdb to be processed 09/20/23 12:44:36.399
STEP: Waiting for the pdb to be processed 09/20/23 12:44:36.465
STEP: listing a collection of PDBs across all namespaces 09/20/23 12:44:36.488
STEP: listing a collection of PDBs in namespace disruption-2299 09/20/23 12:44:36.492
STEP: deleting a collection of PDBs 09/20/23 12:44:36.495
STEP: Waiting for the PDB collection to be deleted 09/20/23 12:44:36.518
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Sep 20 12:44:36.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Sep 20 12:44:36.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-7859" for this suite. 09/20/23 12:44:36.529
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2299" for this suite. 09/20/23 12:44:36.537
------------------------------
â€¢ [2.685 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:44:33.863
    Sep 20 12:44:33.863: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename disruption 09/20/23 12:44:33.863
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:44:34.33
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:44:34.335
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:44:34.341
    Sep 20 12:44:34.341: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename disruption-2 09/20/23 12:44:34.341
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:44:34.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:44:34.367
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 09/20/23 12:44:34.378
    STEP: Waiting for the pdb to be processed 09/20/23 12:44:36.399
    STEP: Waiting for the pdb to be processed 09/20/23 12:44:36.465
    STEP: listing a collection of PDBs across all namespaces 09/20/23 12:44:36.488
    STEP: listing a collection of PDBs in namespace disruption-2299 09/20/23 12:44:36.492
    STEP: deleting a collection of PDBs 09/20/23 12:44:36.495
    STEP: Waiting for the PDB collection to be deleted 09/20/23 12:44:36.518
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:44:36.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:44:36.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-7859" for this suite. 09/20/23 12:44:36.529
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2299" for this suite. 09/20/23 12:44:36.537
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:44:36.55
Sep 20 12:44:36.550: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename emptydir 09/20/23 12:44:36.55
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:44:36.572
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:44:36.576
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 09/20/23 12:44:36.582
Sep 20 12:44:36.677: INFO: Waiting up to 5m0s for pod "pod-e7d3c882-5ad3-40aa-b530-a7f2ca9ae6f2" in namespace "emptydir-7912" to be "Succeeded or Failed"
Sep 20 12:44:36.804: INFO: Pod "pod-e7d3c882-5ad3-40aa-b530-a7f2ca9ae6f2": Phase="Pending", Reason="", readiness=false. Elapsed: 126.018771ms
Sep 20 12:44:39.342: INFO: Pod "pod-e7d3c882-5ad3-40aa-b530-a7f2ca9ae6f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.664687246s
Sep 20 12:44:40.820: INFO: Pod "pod-e7d3c882-5ad3-40aa-b530-a7f2ca9ae6f2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.142502075s
Sep 20 12:44:42.907: INFO: Pod "pod-e7d3c882-5ad3-40aa-b530-a7f2ca9ae6f2": Phase="Running", Reason="", readiness=true. Elapsed: 6.229547942s
Sep 20 12:44:44.914: INFO: Pod "pod-e7d3c882-5ad3-40aa-b530-a7f2ca9ae6f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.236948569s
STEP: Saw pod success 09/20/23 12:44:44.914
Sep 20 12:44:44.915: INFO: Pod "pod-e7d3c882-5ad3-40aa-b530-a7f2ca9ae6f2" satisfied condition "Succeeded or Failed"
Sep 20 12:44:44.952: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-e7d3c882-5ad3-40aa-b530-a7f2ca9ae6f2 container test-container: <nil>
STEP: delete the pod 09/20/23 12:44:45.033
Sep 20 12:44:45.930: INFO: Waiting for pod pod-e7d3c882-5ad3-40aa-b530-a7f2ca9ae6f2 to disappear
Sep 20 12:44:45.936: INFO: Pod pod-e7d3c882-5ad3-40aa-b530-a7f2ca9ae6f2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep 20 12:44:45.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7912" for this suite. 09/20/23 12:44:46.122
------------------------------
â€¢ [SLOW TEST] [9.581 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:44:36.55
    Sep 20 12:44:36.550: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename emptydir 09/20/23 12:44:36.55
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:44:36.572
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:44:36.576
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 09/20/23 12:44:36.582
    Sep 20 12:44:36.677: INFO: Waiting up to 5m0s for pod "pod-e7d3c882-5ad3-40aa-b530-a7f2ca9ae6f2" in namespace "emptydir-7912" to be "Succeeded or Failed"
    Sep 20 12:44:36.804: INFO: Pod "pod-e7d3c882-5ad3-40aa-b530-a7f2ca9ae6f2": Phase="Pending", Reason="", readiness=false. Elapsed: 126.018771ms
    Sep 20 12:44:39.342: INFO: Pod "pod-e7d3c882-5ad3-40aa-b530-a7f2ca9ae6f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.664687246s
    Sep 20 12:44:40.820: INFO: Pod "pod-e7d3c882-5ad3-40aa-b530-a7f2ca9ae6f2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.142502075s
    Sep 20 12:44:42.907: INFO: Pod "pod-e7d3c882-5ad3-40aa-b530-a7f2ca9ae6f2": Phase="Running", Reason="", readiness=true. Elapsed: 6.229547942s
    Sep 20 12:44:44.914: INFO: Pod "pod-e7d3c882-5ad3-40aa-b530-a7f2ca9ae6f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.236948569s
    STEP: Saw pod success 09/20/23 12:44:44.914
    Sep 20 12:44:44.915: INFO: Pod "pod-e7d3c882-5ad3-40aa-b530-a7f2ca9ae6f2" satisfied condition "Succeeded or Failed"
    Sep 20 12:44:44.952: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-e7d3c882-5ad3-40aa-b530-a7f2ca9ae6f2 container test-container: <nil>
    STEP: delete the pod 09/20/23 12:44:45.033
    Sep 20 12:44:45.930: INFO: Waiting for pod pod-e7d3c882-5ad3-40aa-b530-a7f2ca9ae6f2 to disappear
    Sep 20 12:44:45.936: INFO: Pod pod-e7d3c882-5ad3-40aa-b530-a7f2ca9ae6f2 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:44:45.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7912" for this suite. 09/20/23 12:44:46.122
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:44:46.133
Sep 20 12:44:46.133: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename dns 09/20/23 12:44:46.134
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:44:46.15
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:44:46.154
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 09/20/23 12:44:46.158
Sep 20 12:44:46.168: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-519  0e024c3d-257a-45c3-a313-aab4a724e71b 18659 0 2023-09-20 12:44:46 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-09-20 12:44:46 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-68rfp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-68rfp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 12:44:46.168: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-519" to be "running and ready"
Sep 20 12:44:46.173: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 5.58845ms
Sep 20 12:44:46.173: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Sep 20 12:44:48.213: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045410472s
Sep 20 12:44:48.213: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Sep 20 12:44:50.178: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 4.00995302s
Sep 20 12:44:50.178: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Sep 20 12:44:50.178: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 09/20/23 12:44:50.178
Sep 20 12:44:50.178: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-519 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 20 12:44:50.178: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
Sep 20 12:44:50.179: INFO: ExecWithOptions: Clientset creation
Sep 20 12:44:50.179: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/dns-519/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 09/20/23 12:44:50.319
Sep 20 12:44:50.319: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-519 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 20 12:44:50.319: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
Sep 20 12:44:50.320: INFO: ExecWithOptions: Clientset creation
Sep 20 12:44:50.320: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/dns-519/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep 20 12:44:50.435: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep 20 12:44:50.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-519" for this suite. 09/20/23 12:44:50.909
------------------------------
â€¢ [4.888 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:44:46.133
    Sep 20 12:44:46.133: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename dns 09/20/23 12:44:46.134
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:44:46.15
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:44:46.154
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 09/20/23 12:44:46.158
    Sep 20 12:44:46.168: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-519  0e024c3d-257a-45c3-a313-aab4a724e71b 18659 0 2023-09-20 12:44:46 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-09-20 12:44:46 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-68rfp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-68rfp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep 20 12:44:46.168: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-519" to be "running and ready"
    Sep 20 12:44:46.173: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 5.58845ms
    Sep 20 12:44:46.173: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 12:44:48.213: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045410472s
    Sep 20 12:44:48.213: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 12:44:50.178: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 4.00995302s
    Sep 20 12:44:50.178: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Sep 20 12:44:50.178: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 09/20/23 12:44:50.178
    Sep 20 12:44:50.178: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-519 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep 20 12:44:50.178: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    Sep 20 12:44:50.179: INFO: ExecWithOptions: Clientset creation
    Sep 20 12:44:50.179: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/dns-519/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 09/20/23 12:44:50.319
    Sep 20 12:44:50.319: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-519 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep 20 12:44:50.319: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    Sep 20 12:44:50.320: INFO: ExecWithOptions: Clientset creation
    Sep 20 12:44:50.320: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/dns-519/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep 20 12:44:50.435: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:44:50.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-519" for this suite. 09/20/23 12:44:50.909
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:44:51.028
Sep 20 12:44:51.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename custom-resource-definition 09/20/23 12:44:51.029
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:44:51.543
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:44:51.546
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Sep 20 12:44:51.552: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep 20 12:44:55.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-8886" for this suite. 09/20/23 12:44:55.29
------------------------------
â€¢ [4.469 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:44:51.028
    Sep 20 12:44:51.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename custom-resource-definition 09/20/23 12:44:51.029
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:44:51.543
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:44:51.546
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Sep 20 12:44:51.552: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:44:55.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-8886" for this suite. 09/20/23 12:44:55.29
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:44:55.498
Sep 20 12:44:55.498: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename emptydir 09/20/23 12:44:55.499
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:44:55.525
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:44:55.53
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 09/20/23 12:44:55.536
Sep 20 12:44:55.565: INFO: Waiting up to 5m0s for pod "pod-1bf5362e-3a0b-4c2a-be0a-38092e0f60bd" in namespace "emptydir-9856" to be "Succeeded or Failed"
Sep 20 12:44:55.580: INFO: Pod "pod-1bf5362e-3a0b-4c2a-be0a-38092e0f60bd": Phase="Pending", Reason="", readiness=false. Elapsed: 14.47337ms
Sep 20 12:44:57.586: INFO: Pod "pod-1bf5362e-3a0b-4c2a-be0a-38092e0f60bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020496952s
Sep 20 12:44:59.585: INFO: Pod "pod-1bf5362e-3a0b-4c2a-be0a-38092e0f60bd": Phase="Running", Reason="", readiness=true. Elapsed: 4.020088186s
Sep 20 12:45:02.531: INFO: Pod "pod-1bf5362e-3a0b-4c2a-be0a-38092e0f60bd": Phase="Running", Reason="", readiness=false. Elapsed: 6.965694389s
Sep 20 12:45:03.584: INFO: Pod "pod-1bf5362e-3a0b-4c2a-be0a-38092e0f60bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.018740684s
STEP: Saw pod success 09/20/23 12:45:03.584
Sep 20 12:45:03.584: INFO: Pod "pod-1bf5362e-3a0b-4c2a-be0a-38092e0f60bd" satisfied condition "Succeeded or Failed"
Sep 20 12:45:03.588: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-1bf5362e-3a0b-4c2a-be0a-38092e0f60bd container test-container: <nil>
STEP: delete the pod 09/20/23 12:45:03.595
Sep 20 12:45:04.660: INFO: Waiting for pod pod-1bf5362e-3a0b-4c2a-be0a-38092e0f60bd to disappear
Sep 20 12:45:04.699: INFO: Pod pod-1bf5362e-3a0b-4c2a-be0a-38092e0f60bd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep 20 12:45:04.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9856" for this suite. 09/20/23 12:45:04.705
------------------------------
â€¢ [SLOW TEST] [9.213 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:44:55.498
    Sep 20 12:44:55.498: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename emptydir 09/20/23 12:44:55.499
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:44:55.525
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:44:55.53
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 09/20/23 12:44:55.536
    Sep 20 12:44:55.565: INFO: Waiting up to 5m0s for pod "pod-1bf5362e-3a0b-4c2a-be0a-38092e0f60bd" in namespace "emptydir-9856" to be "Succeeded or Failed"
    Sep 20 12:44:55.580: INFO: Pod "pod-1bf5362e-3a0b-4c2a-be0a-38092e0f60bd": Phase="Pending", Reason="", readiness=false. Elapsed: 14.47337ms
    Sep 20 12:44:57.586: INFO: Pod "pod-1bf5362e-3a0b-4c2a-be0a-38092e0f60bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020496952s
    Sep 20 12:44:59.585: INFO: Pod "pod-1bf5362e-3a0b-4c2a-be0a-38092e0f60bd": Phase="Running", Reason="", readiness=true. Elapsed: 4.020088186s
    Sep 20 12:45:02.531: INFO: Pod "pod-1bf5362e-3a0b-4c2a-be0a-38092e0f60bd": Phase="Running", Reason="", readiness=false. Elapsed: 6.965694389s
    Sep 20 12:45:03.584: INFO: Pod "pod-1bf5362e-3a0b-4c2a-be0a-38092e0f60bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.018740684s
    STEP: Saw pod success 09/20/23 12:45:03.584
    Sep 20 12:45:03.584: INFO: Pod "pod-1bf5362e-3a0b-4c2a-be0a-38092e0f60bd" satisfied condition "Succeeded or Failed"
    Sep 20 12:45:03.588: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-1bf5362e-3a0b-4c2a-be0a-38092e0f60bd container test-container: <nil>
    STEP: delete the pod 09/20/23 12:45:03.595
    Sep 20 12:45:04.660: INFO: Waiting for pod pod-1bf5362e-3a0b-4c2a-be0a-38092e0f60bd to disappear
    Sep 20 12:45:04.699: INFO: Pod pod-1bf5362e-3a0b-4c2a-be0a-38092e0f60bd no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:45:04.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9856" for this suite. 09/20/23 12:45:04.705
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:45:04.714
Sep 20 12:45:04.714: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename crd-publish-openapi 09/20/23 12:45:04.714
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:45:04.731
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:45:04.734
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 09/20/23 12:45:04.741
Sep 20 12:45:04.741: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: mark a version not serverd 09/20/23 12:45:10.205
STEP: check the unserved version gets removed 09/20/23 12:45:10.365
STEP: check the other version is not changed 09/20/23 12:45:12.587
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep 20 12:45:16.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1588" for this suite. 09/20/23 12:45:16.796
------------------------------
â€¢ [SLOW TEST] [12.089 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:45:04.714
    Sep 20 12:45:04.714: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename crd-publish-openapi 09/20/23 12:45:04.714
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:45:04.731
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:45:04.734
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 09/20/23 12:45:04.741
    Sep 20 12:45:04.741: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: mark a version not serverd 09/20/23 12:45:10.205
    STEP: check the unserved version gets removed 09/20/23 12:45:10.365
    STEP: check the other version is not changed 09/20/23 12:45:12.587
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:45:16.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1588" for this suite. 09/20/23 12:45:16.796
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:45:16.804
Sep 20 12:45:16.804: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename job 09/20/23 12:45:16.805
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:45:16.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:45:16.873
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 09/20/23 12:45:16.881
STEP: Patching the Job 09/20/23 12:45:16.887
STEP: Watching for Job to be patched 09/20/23 12:45:16.897
Sep 20 12:45:16.899: INFO: Event ADDED observed for Job e2e-zhkxp in namespace job-6154 with labels: map[e2e-job-label:e2e-zhkxp] and annotations: map[batch.kubernetes.io/job-tracking:]
Sep 20 12:45:16.899: INFO: Event MODIFIED found for Job e2e-zhkxp in namespace job-6154 with labels: map[e2e-job-label:e2e-zhkxp e2e-zhkxp:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 09/20/23 12:45:16.899
STEP: Watching for Job to be updated 09/20/23 12:45:16.909
Sep 20 12:45:16.910: INFO: Event MODIFIED found for Job e2e-zhkxp in namespace job-6154 with labels: map[e2e-job-label:e2e-zhkxp e2e-zhkxp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep 20 12:45:16.910: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 09/20/23 12:45:16.91
Sep 20 12:45:16.913: INFO: Job: e2e-zhkxp as labels: map[e2e-job-label:e2e-zhkxp e2e-zhkxp:patched]
STEP: Waiting for job to complete 09/20/23 12:45:16.913
STEP: Delete a job collection with a labelselector 09/20/23 12:45:48.917
STEP: Watching for Job to be deleted 09/20/23 12:45:48.925
Sep 20 12:45:48.927: INFO: Event MODIFIED observed for Job e2e-zhkxp in namespace job-6154 with labels: map[e2e-job-label:e2e-zhkxp e2e-zhkxp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep 20 12:45:48.927: INFO: Event MODIFIED observed for Job e2e-zhkxp in namespace job-6154 with labels: map[e2e-job-label:e2e-zhkxp e2e-zhkxp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep 20 12:45:48.927: INFO: Event MODIFIED observed for Job e2e-zhkxp in namespace job-6154 with labels: map[e2e-job-label:e2e-zhkxp e2e-zhkxp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep 20 12:45:48.927: INFO: Event MODIFIED observed for Job e2e-zhkxp in namespace job-6154 with labels: map[e2e-job-label:e2e-zhkxp e2e-zhkxp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep 20 12:45:48.927: INFO: Event MODIFIED observed for Job e2e-zhkxp in namespace job-6154 with labels: map[e2e-job-label:e2e-zhkxp e2e-zhkxp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep 20 12:45:48.927: INFO: Event DELETED found for Job e2e-zhkxp in namespace job-6154 with labels: map[e2e-job-label:e2e-zhkxp e2e-zhkxp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 09/20/23 12:45:48.927
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep 20 12:45:48.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-6154" for this suite. 09/20/23 12:45:48.934
------------------------------
â€¢ [SLOW TEST] [32.431 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:45:16.804
    Sep 20 12:45:16.804: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename job 09/20/23 12:45:16.805
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:45:16.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:45:16.873
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 09/20/23 12:45:16.881
    STEP: Patching the Job 09/20/23 12:45:16.887
    STEP: Watching for Job to be patched 09/20/23 12:45:16.897
    Sep 20 12:45:16.899: INFO: Event ADDED observed for Job e2e-zhkxp in namespace job-6154 with labels: map[e2e-job-label:e2e-zhkxp] and annotations: map[batch.kubernetes.io/job-tracking:]
    Sep 20 12:45:16.899: INFO: Event MODIFIED found for Job e2e-zhkxp in namespace job-6154 with labels: map[e2e-job-label:e2e-zhkxp e2e-zhkxp:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 09/20/23 12:45:16.899
    STEP: Watching for Job to be updated 09/20/23 12:45:16.909
    Sep 20 12:45:16.910: INFO: Event MODIFIED found for Job e2e-zhkxp in namespace job-6154 with labels: map[e2e-job-label:e2e-zhkxp e2e-zhkxp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep 20 12:45:16.910: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 09/20/23 12:45:16.91
    Sep 20 12:45:16.913: INFO: Job: e2e-zhkxp as labels: map[e2e-job-label:e2e-zhkxp e2e-zhkxp:patched]
    STEP: Waiting for job to complete 09/20/23 12:45:16.913
    STEP: Delete a job collection with a labelselector 09/20/23 12:45:48.917
    STEP: Watching for Job to be deleted 09/20/23 12:45:48.925
    Sep 20 12:45:48.927: INFO: Event MODIFIED observed for Job e2e-zhkxp in namespace job-6154 with labels: map[e2e-job-label:e2e-zhkxp e2e-zhkxp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep 20 12:45:48.927: INFO: Event MODIFIED observed for Job e2e-zhkxp in namespace job-6154 with labels: map[e2e-job-label:e2e-zhkxp e2e-zhkxp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep 20 12:45:48.927: INFO: Event MODIFIED observed for Job e2e-zhkxp in namespace job-6154 with labels: map[e2e-job-label:e2e-zhkxp e2e-zhkxp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep 20 12:45:48.927: INFO: Event MODIFIED observed for Job e2e-zhkxp in namespace job-6154 with labels: map[e2e-job-label:e2e-zhkxp e2e-zhkxp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep 20 12:45:48.927: INFO: Event MODIFIED observed for Job e2e-zhkxp in namespace job-6154 with labels: map[e2e-job-label:e2e-zhkxp e2e-zhkxp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep 20 12:45:48.927: INFO: Event DELETED found for Job e2e-zhkxp in namespace job-6154 with labels: map[e2e-job-label:e2e-zhkxp e2e-zhkxp:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 09/20/23 12:45:48.927
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:45:48.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-6154" for this suite. 09/20/23 12:45:48.934
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:45:49.239
Sep 20 12:45:49.239: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename gc 09/20/23 12:45:49.24
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:45:50.072
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:45:50.075
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 09/20/23 12:45:50.18
STEP: create the rc2 09/20/23 12:45:50.516
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 09/20/23 12:46:00.796
STEP: delete the rc simpletest-rc-to-be-deleted 09/20/23 12:46:03.892
STEP: wait for the rc to be deleted 09/20/23 12:46:03.901
Sep 20 12:46:08.914: INFO: 72 pods remaining
Sep 20 12:46:08.914: INFO: 71 pods has nil DeletionTimestamp
Sep 20 12:46:08.914: INFO: 
Sep 20 12:46:14.733: INFO: 50 pods remaining
Sep 20 12:46:14.733: INFO: 50 pods has nil DeletionTimestamp
Sep 20 12:46:14.733: INFO: 
STEP: Gathering metrics 09/20/23 12:46:18.957
W0920 12:46:18.967670      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Sep 20 12:46:18.967: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Sep 20 12:46:18.967: INFO: Deleting pod "simpletest-rc-to-be-deleted-29mjb" in namespace "gc-8308"
Sep 20 12:46:18.975: INFO: Deleting pod "simpletest-rc-to-be-deleted-2q9lk" in namespace "gc-8308"
Sep 20 12:46:18.984: INFO: Deleting pod "simpletest-rc-to-be-deleted-2rtlq" in namespace "gc-8308"
Sep 20 12:46:18.992: INFO: Deleting pod "simpletest-rc-to-be-deleted-45qmj" in namespace "gc-8308"
Sep 20 12:46:19.009: INFO: Deleting pod "simpletest-rc-to-be-deleted-49gtx" in namespace "gc-8308"
Sep 20 12:46:19.079: INFO: Deleting pod "simpletest-rc-to-be-deleted-4cjh9" in namespace "gc-8308"
Sep 20 12:46:19.093: INFO: Deleting pod "simpletest-rc-to-be-deleted-4kr6p" in namespace "gc-8308"
Sep 20 12:46:19.105: INFO: Deleting pod "simpletest-rc-to-be-deleted-4mgqc" in namespace "gc-8308"
Sep 20 12:46:19.132: INFO: Deleting pod "simpletest-rc-to-be-deleted-4rffd" in namespace "gc-8308"
Sep 20 12:46:19.396: INFO: Deleting pod "simpletest-rc-to-be-deleted-4szmp" in namespace "gc-8308"
Sep 20 12:46:19.417: INFO: Deleting pod "simpletest-rc-to-be-deleted-4thwk" in namespace "gc-8308"
Sep 20 12:46:19.687: INFO: Deleting pod "simpletest-rc-to-be-deleted-4v8cx" in namespace "gc-8308"
Sep 20 12:46:19.773: INFO: Deleting pod "simpletest-rc-to-be-deleted-4xqt4" in namespace "gc-8308"
Sep 20 12:46:19.791: INFO: Deleting pod "simpletest-rc-to-be-deleted-54h4v" in namespace "gc-8308"
Sep 20 12:46:20.046: INFO: Deleting pod "simpletest-rc-to-be-deleted-55q7t" in namespace "gc-8308"
Sep 20 12:46:20.082: INFO: Deleting pod "simpletest-rc-to-be-deleted-56qg2" in namespace "gc-8308"
Sep 20 12:46:20.101: INFO: Deleting pod "simpletest-rc-to-be-deleted-5jt8p" in namespace "gc-8308"
Sep 20 12:46:20.117: INFO: Deleting pod "simpletest-rc-to-be-deleted-5nwpp" in namespace "gc-8308"
Sep 20 12:46:20.138: INFO: Deleting pod "simpletest-rc-to-be-deleted-5sjgm" in namespace "gc-8308"
Sep 20 12:46:20.176: INFO: Deleting pod "simpletest-rc-to-be-deleted-6294g" in namespace "gc-8308"
Sep 20 12:46:20.202: INFO: Deleting pod "simpletest-rc-to-be-deleted-69vxm" in namespace "gc-8308"
Sep 20 12:46:20.215: INFO: Deleting pod "simpletest-rc-to-be-deleted-6kzcx" in namespace "gc-8308"
Sep 20 12:46:20.258: INFO: Deleting pod "simpletest-rc-to-be-deleted-7d6vt" in namespace "gc-8308"
Sep 20 12:46:20.402: INFO: Deleting pod "simpletest-rc-to-be-deleted-7q97m" in namespace "gc-8308"
Sep 20 12:46:20.584: INFO: Deleting pod "simpletest-rc-to-be-deleted-8g77b" in namespace "gc-8308"
Sep 20 12:46:20.877: INFO: Deleting pod "simpletest-rc-to-be-deleted-8xqgg" in namespace "gc-8308"
Sep 20 12:46:21.202: INFO: Deleting pod "simpletest-rc-to-be-deleted-9789v" in namespace "gc-8308"
Sep 20 12:46:21.218: INFO: Deleting pod "simpletest-rc-to-be-deleted-9f4mv" in namespace "gc-8308"
Sep 20 12:46:21.316: INFO: Deleting pod "simpletest-rc-to-be-deleted-9w4r9" in namespace "gc-8308"
Sep 20 12:46:21.342: INFO: Deleting pod "simpletest-rc-to-be-deleted-b5gc9" in namespace "gc-8308"
Sep 20 12:46:21.362: INFO: Deleting pod "simpletest-rc-to-be-deleted-b7f2q" in namespace "gc-8308"
Sep 20 12:46:21.379: INFO: Deleting pod "simpletest-rc-to-be-deleted-b8p9r" in namespace "gc-8308"
Sep 20 12:46:21.389: INFO: Deleting pod "simpletest-rc-to-be-deleted-b8qps" in namespace "gc-8308"
Sep 20 12:46:21.424: INFO: Deleting pod "simpletest-rc-to-be-deleted-bd69c" in namespace "gc-8308"
Sep 20 12:46:21.435: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvlss" in namespace "gc-8308"
Sep 20 12:46:21.449: INFO: Deleting pod "simpletest-rc-to-be-deleted-c65fg" in namespace "gc-8308"
Sep 20 12:46:21.457: INFO: Deleting pod "simpletest-rc-to-be-deleted-c6dl2" in namespace "gc-8308"
Sep 20 12:46:21.471: INFO: Deleting pod "simpletest-rc-to-be-deleted-cck5g" in namespace "gc-8308"
Sep 20 12:46:21.490: INFO: Deleting pod "simpletest-rc-to-be-deleted-cg4pj" in namespace "gc-8308"
Sep 20 12:46:21.502: INFO: Deleting pod "simpletest-rc-to-be-deleted-cl6qq" in namespace "gc-8308"
Sep 20 12:46:21.923: INFO: Deleting pod "simpletest-rc-to-be-deleted-cltvw" in namespace "gc-8308"
Sep 20 12:46:22.547: INFO: Deleting pod "simpletest-rc-to-be-deleted-cs68j" in namespace "gc-8308"
Sep 20 12:46:22.834: INFO: Deleting pod "simpletest-rc-to-be-deleted-d54bj" in namespace "gc-8308"
Sep 20 12:46:23.042: INFO: Deleting pod "simpletest-rc-to-be-deleted-d8pdd" in namespace "gc-8308"
Sep 20 12:46:23.360: INFO: Deleting pod "simpletest-rc-to-be-deleted-dmm42" in namespace "gc-8308"
Sep 20 12:46:23.373: INFO: Deleting pod "simpletest-rc-to-be-deleted-ds4vc" in namespace "gc-8308"
Sep 20 12:46:23.619: INFO: Deleting pod "simpletest-rc-to-be-deleted-dvm6p" in namespace "gc-8308"
Sep 20 12:46:24.221: INFO: Deleting pod "simpletest-rc-to-be-deleted-fk2z6" in namespace "gc-8308"
Sep 20 12:46:24.286: INFO: Deleting pod "simpletest-rc-to-be-deleted-fs84c" in namespace "gc-8308"
Sep 20 12:46:24.299: INFO: Deleting pod "simpletest-rc-to-be-deleted-gb8kz" in namespace "gc-8308"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep 20 12:46:24.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-8308" for this suite. 09/20/23 12:46:24.318
------------------------------
â€¢ [SLOW TEST] [35.095 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:45:49.239
    Sep 20 12:45:49.239: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename gc 09/20/23 12:45:49.24
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:45:50.072
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:45:50.075
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 09/20/23 12:45:50.18
    STEP: create the rc2 09/20/23 12:45:50.516
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 09/20/23 12:46:00.796
    STEP: delete the rc simpletest-rc-to-be-deleted 09/20/23 12:46:03.892
    STEP: wait for the rc to be deleted 09/20/23 12:46:03.901
    Sep 20 12:46:08.914: INFO: 72 pods remaining
    Sep 20 12:46:08.914: INFO: 71 pods has nil DeletionTimestamp
    Sep 20 12:46:08.914: INFO: 
    Sep 20 12:46:14.733: INFO: 50 pods remaining
    Sep 20 12:46:14.733: INFO: 50 pods has nil DeletionTimestamp
    Sep 20 12:46:14.733: INFO: 
    STEP: Gathering metrics 09/20/23 12:46:18.957
    W0920 12:46:18.967670      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Sep 20 12:46:18.967: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Sep 20 12:46:18.967: INFO: Deleting pod "simpletest-rc-to-be-deleted-29mjb" in namespace "gc-8308"
    Sep 20 12:46:18.975: INFO: Deleting pod "simpletest-rc-to-be-deleted-2q9lk" in namespace "gc-8308"
    Sep 20 12:46:18.984: INFO: Deleting pod "simpletest-rc-to-be-deleted-2rtlq" in namespace "gc-8308"
    Sep 20 12:46:18.992: INFO: Deleting pod "simpletest-rc-to-be-deleted-45qmj" in namespace "gc-8308"
    Sep 20 12:46:19.009: INFO: Deleting pod "simpletest-rc-to-be-deleted-49gtx" in namespace "gc-8308"
    Sep 20 12:46:19.079: INFO: Deleting pod "simpletest-rc-to-be-deleted-4cjh9" in namespace "gc-8308"
    Sep 20 12:46:19.093: INFO: Deleting pod "simpletest-rc-to-be-deleted-4kr6p" in namespace "gc-8308"
    Sep 20 12:46:19.105: INFO: Deleting pod "simpletest-rc-to-be-deleted-4mgqc" in namespace "gc-8308"
    Sep 20 12:46:19.132: INFO: Deleting pod "simpletest-rc-to-be-deleted-4rffd" in namespace "gc-8308"
    Sep 20 12:46:19.396: INFO: Deleting pod "simpletest-rc-to-be-deleted-4szmp" in namespace "gc-8308"
    Sep 20 12:46:19.417: INFO: Deleting pod "simpletest-rc-to-be-deleted-4thwk" in namespace "gc-8308"
    Sep 20 12:46:19.687: INFO: Deleting pod "simpletest-rc-to-be-deleted-4v8cx" in namespace "gc-8308"
    Sep 20 12:46:19.773: INFO: Deleting pod "simpletest-rc-to-be-deleted-4xqt4" in namespace "gc-8308"
    Sep 20 12:46:19.791: INFO: Deleting pod "simpletest-rc-to-be-deleted-54h4v" in namespace "gc-8308"
    Sep 20 12:46:20.046: INFO: Deleting pod "simpletest-rc-to-be-deleted-55q7t" in namespace "gc-8308"
    Sep 20 12:46:20.082: INFO: Deleting pod "simpletest-rc-to-be-deleted-56qg2" in namespace "gc-8308"
    Sep 20 12:46:20.101: INFO: Deleting pod "simpletest-rc-to-be-deleted-5jt8p" in namespace "gc-8308"
    Sep 20 12:46:20.117: INFO: Deleting pod "simpletest-rc-to-be-deleted-5nwpp" in namespace "gc-8308"
    Sep 20 12:46:20.138: INFO: Deleting pod "simpletest-rc-to-be-deleted-5sjgm" in namespace "gc-8308"
    Sep 20 12:46:20.176: INFO: Deleting pod "simpletest-rc-to-be-deleted-6294g" in namespace "gc-8308"
    Sep 20 12:46:20.202: INFO: Deleting pod "simpletest-rc-to-be-deleted-69vxm" in namespace "gc-8308"
    Sep 20 12:46:20.215: INFO: Deleting pod "simpletest-rc-to-be-deleted-6kzcx" in namespace "gc-8308"
    Sep 20 12:46:20.258: INFO: Deleting pod "simpletest-rc-to-be-deleted-7d6vt" in namespace "gc-8308"
    Sep 20 12:46:20.402: INFO: Deleting pod "simpletest-rc-to-be-deleted-7q97m" in namespace "gc-8308"
    Sep 20 12:46:20.584: INFO: Deleting pod "simpletest-rc-to-be-deleted-8g77b" in namespace "gc-8308"
    Sep 20 12:46:20.877: INFO: Deleting pod "simpletest-rc-to-be-deleted-8xqgg" in namespace "gc-8308"
    Sep 20 12:46:21.202: INFO: Deleting pod "simpletest-rc-to-be-deleted-9789v" in namespace "gc-8308"
    Sep 20 12:46:21.218: INFO: Deleting pod "simpletest-rc-to-be-deleted-9f4mv" in namespace "gc-8308"
    Sep 20 12:46:21.316: INFO: Deleting pod "simpletest-rc-to-be-deleted-9w4r9" in namespace "gc-8308"
    Sep 20 12:46:21.342: INFO: Deleting pod "simpletest-rc-to-be-deleted-b5gc9" in namespace "gc-8308"
    Sep 20 12:46:21.362: INFO: Deleting pod "simpletest-rc-to-be-deleted-b7f2q" in namespace "gc-8308"
    Sep 20 12:46:21.379: INFO: Deleting pod "simpletest-rc-to-be-deleted-b8p9r" in namespace "gc-8308"
    Sep 20 12:46:21.389: INFO: Deleting pod "simpletest-rc-to-be-deleted-b8qps" in namespace "gc-8308"
    Sep 20 12:46:21.424: INFO: Deleting pod "simpletest-rc-to-be-deleted-bd69c" in namespace "gc-8308"
    Sep 20 12:46:21.435: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvlss" in namespace "gc-8308"
    Sep 20 12:46:21.449: INFO: Deleting pod "simpletest-rc-to-be-deleted-c65fg" in namespace "gc-8308"
    Sep 20 12:46:21.457: INFO: Deleting pod "simpletest-rc-to-be-deleted-c6dl2" in namespace "gc-8308"
    Sep 20 12:46:21.471: INFO: Deleting pod "simpletest-rc-to-be-deleted-cck5g" in namespace "gc-8308"
    Sep 20 12:46:21.490: INFO: Deleting pod "simpletest-rc-to-be-deleted-cg4pj" in namespace "gc-8308"
    Sep 20 12:46:21.502: INFO: Deleting pod "simpletest-rc-to-be-deleted-cl6qq" in namespace "gc-8308"
    Sep 20 12:46:21.923: INFO: Deleting pod "simpletest-rc-to-be-deleted-cltvw" in namespace "gc-8308"
    Sep 20 12:46:22.547: INFO: Deleting pod "simpletest-rc-to-be-deleted-cs68j" in namespace "gc-8308"
    Sep 20 12:46:22.834: INFO: Deleting pod "simpletest-rc-to-be-deleted-d54bj" in namespace "gc-8308"
    Sep 20 12:46:23.042: INFO: Deleting pod "simpletest-rc-to-be-deleted-d8pdd" in namespace "gc-8308"
    Sep 20 12:46:23.360: INFO: Deleting pod "simpletest-rc-to-be-deleted-dmm42" in namespace "gc-8308"
    Sep 20 12:46:23.373: INFO: Deleting pod "simpletest-rc-to-be-deleted-ds4vc" in namespace "gc-8308"
    Sep 20 12:46:23.619: INFO: Deleting pod "simpletest-rc-to-be-deleted-dvm6p" in namespace "gc-8308"
    Sep 20 12:46:24.221: INFO: Deleting pod "simpletest-rc-to-be-deleted-fk2z6" in namespace "gc-8308"
    Sep 20 12:46:24.286: INFO: Deleting pod "simpletest-rc-to-be-deleted-fs84c" in namespace "gc-8308"
    Sep 20 12:46:24.299: INFO: Deleting pod "simpletest-rc-to-be-deleted-gb8kz" in namespace "gc-8308"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:46:24.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-8308" for this suite. 09/20/23 12:46:24.318
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:46:24.334
Sep 20 12:46:24.334: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename pods 09/20/23 12:46:24.335
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:46:24.643
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:46:24.646
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 09/20/23 12:46:24.653
STEP: watching for Pod to be ready 09/20/23 12:46:24.664
Sep 20 12:46:24.666: INFO: observed Pod pod-test in namespace pods-8669 in phase Pending with labels: map[test-pod-static:true] & conditions []
Sep 20 12:46:24.673: INFO: observed Pod pod-test in namespace pods-8669 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:46:24 +0000 UTC  }]
Sep 20 12:46:25.288: INFO: observed Pod pod-test in namespace pods-8669 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:46:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:46:25 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:46:25 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:46:24 +0000 UTC  }]
Sep 20 12:46:38.285: INFO: Found Pod pod-test in namespace pods-8669 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:46:25 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:46:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:46:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:46:24 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 09/20/23 12:46:38.288
STEP: getting the Pod and ensuring that it's patched 09/20/23 12:46:38.301
STEP: replacing the Pod's status Ready condition to False 09/20/23 12:46:38.303
STEP: check the Pod again to ensure its Ready conditions are False 09/20/23 12:46:38.419
STEP: deleting the Pod via a Collection with a LabelSelector 09/20/23 12:46:38.42
STEP: watching for the Pod to be deleted 09/20/23 12:46:38.427
Sep 20 12:46:38.429: INFO: observed event type MODIFIED
Sep 20 12:46:39.174: INFO: observed event type MODIFIED
Sep 20 12:46:42.307: INFO: observed event type MODIFIED
Sep 20 12:46:42.353: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep 20 12:46:42.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8669" for this suite. 09/20/23 12:46:42.52
------------------------------
â€¢ [SLOW TEST] [18.253 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:46:24.334
    Sep 20 12:46:24.334: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename pods 09/20/23 12:46:24.335
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:46:24.643
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:46:24.646
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 09/20/23 12:46:24.653
    STEP: watching for Pod to be ready 09/20/23 12:46:24.664
    Sep 20 12:46:24.666: INFO: observed Pod pod-test in namespace pods-8669 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Sep 20 12:46:24.673: INFO: observed Pod pod-test in namespace pods-8669 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:46:24 +0000 UTC  }]
    Sep 20 12:46:25.288: INFO: observed Pod pod-test in namespace pods-8669 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:46:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:46:25 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:46:25 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:46:24 +0000 UTC  }]
    Sep 20 12:46:38.285: INFO: Found Pod pod-test in namespace pods-8669 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:46:25 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:46:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:46:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:46:24 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 09/20/23 12:46:38.288
    STEP: getting the Pod and ensuring that it's patched 09/20/23 12:46:38.301
    STEP: replacing the Pod's status Ready condition to False 09/20/23 12:46:38.303
    STEP: check the Pod again to ensure its Ready conditions are False 09/20/23 12:46:38.419
    STEP: deleting the Pod via a Collection with a LabelSelector 09/20/23 12:46:38.42
    STEP: watching for the Pod to be deleted 09/20/23 12:46:38.427
    Sep 20 12:46:38.429: INFO: observed event type MODIFIED
    Sep 20 12:46:39.174: INFO: observed event type MODIFIED
    Sep 20 12:46:42.307: INFO: observed event type MODIFIED
    Sep 20 12:46:42.353: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:46:42.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8669" for this suite. 09/20/23 12:46:42.52
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:46:42.589
Sep 20 12:46:42.589: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename custom-resource-definition 09/20/23 12:46:42.589
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:46:42.734
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:46:42.737
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 09/20/23 12:46:42.741
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 09/20/23 12:46:42.742
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 09/20/23 12:46:42.743
STEP: fetching the /apis/apiextensions.k8s.io discovery document 09/20/23 12:46:42.743
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 09/20/23 12:46:42.744
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 09/20/23 12:46:42.744
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 09/20/23 12:46:42.745
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep 20 12:46:42.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-8320" for this suite. 09/20/23 12:46:42.749
------------------------------
â€¢ [0.165 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:46:42.589
    Sep 20 12:46:42.589: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename custom-resource-definition 09/20/23 12:46:42.589
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:46:42.734
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:46:42.737
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 09/20/23 12:46:42.741
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 09/20/23 12:46:42.742
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 09/20/23 12:46:42.743
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 09/20/23 12:46:42.743
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 09/20/23 12:46:42.744
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 09/20/23 12:46:42.744
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 09/20/23 12:46:42.745
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:46:42.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-8320" for this suite. 09/20/23 12:46:42.749
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:46:42.755
Sep 20 12:46:42.756: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename projected 09/20/23 12:46:42.757
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:46:42.775
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:46:42.777
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-28e7d7b2-b285-4464-b2fa-72de790034ea 09/20/23 12:46:42.782
STEP: Creating a pod to test consume configMaps 09/20/23 12:46:42.786
Sep 20 12:46:43.299: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-890eba7b-32cf-4823-85aa-14ae531bb025" in namespace "projected-151" to be "Succeeded or Failed"
Sep 20 12:46:43.303: INFO: Pod "pod-projected-configmaps-890eba7b-32cf-4823-85aa-14ae531bb025": Phase="Pending", Reason="", readiness=false. Elapsed: 3.320385ms
Sep 20 12:46:45.307: INFO: Pod "pod-projected-configmaps-890eba7b-32cf-4823-85aa-14ae531bb025": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007513469s
Sep 20 12:46:47.593: INFO: Pod "pod-projected-configmaps-890eba7b-32cf-4823-85aa-14ae531bb025": Phase="Pending", Reason="", readiness=false. Elapsed: 4.293358792s
Sep 20 12:46:49.306: INFO: Pod "pod-projected-configmaps-890eba7b-32cf-4823-85aa-14ae531bb025": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006587723s
Sep 20 12:46:51.307: INFO: Pod "pod-projected-configmaps-890eba7b-32cf-4823-85aa-14ae531bb025": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.007272227s
STEP: Saw pod success 09/20/23 12:46:51.307
Sep 20 12:46:51.307: INFO: Pod "pod-projected-configmaps-890eba7b-32cf-4823-85aa-14ae531bb025" satisfied condition "Succeeded or Failed"
Sep 20 12:46:51.309: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-projected-configmaps-890eba7b-32cf-4823-85aa-14ae531bb025 container agnhost-container: <nil>
STEP: delete the pod 09/20/23 12:46:51.357
Sep 20 12:46:51.391: INFO: Waiting for pod pod-projected-configmaps-890eba7b-32cf-4823-85aa-14ae531bb025 to disappear
Sep 20 12:46:51.396: INFO: Pod pod-projected-configmaps-890eba7b-32cf-4823-85aa-14ae531bb025 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep 20 12:46:51.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-151" for this suite. 09/20/23 12:46:51.4
------------------------------
â€¢ [SLOW TEST] [8.654 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:46:42.755
    Sep 20 12:46:42.756: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename projected 09/20/23 12:46:42.757
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:46:42.775
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:46:42.777
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-28e7d7b2-b285-4464-b2fa-72de790034ea 09/20/23 12:46:42.782
    STEP: Creating a pod to test consume configMaps 09/20/23 12:46:42.786
    Sep 20 12:46:43.299: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-890eba7b-32cf-4823-85aa-14ae531bb025" in namespace "projected-151" to be "Succeeded or Failed"
    Sep 20 12:46:43.303: INFO: Pod "pod-projected-configmaps-890eba7b-32cf-4823-85aa-14ae531bb025": Phase="Pending", Reason="", readiness=false. Elapsed: 3.320385ms
    Sep 20 12:46:45.307: INFO: Pod "pod-projected-configmaps-890eba7b-32cf-4823-85aa-14ae531bb025": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007513469s
    Sep 20 12:46:47.593: INFO: Pod "pod-projected-configmaps-890eba7b-32cf-4823-85aa-14ae531bb025": Phase="Pending", Reason="", readiness=false. Elapsed: 4.293358792s
    Sep 20 12:46:49.306: INFO: Pod "pod-projected-configmaps-890eba7b-32cf-4823-85aa-14ae531bb025": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006587723s
    Sep 20 12:46:51.307: INFO: Pod "pod-projected-configmaps-890eba7b-32cf-4823-85aa-14ae531bb025": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.007272227s
    STEP: Saw pod success 09/20/23 12:46:51.307
    Sep 20 12:46:51.307: INFO: Pod "pod-projected-configmaps-890eba7b-32cf-4823-85aa-14ae531bb025" satisfied condition "Succeeded or Failed"
    Sep 20 12:46:51.309: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-projected-configmaps-890eba7b-32cf-4823-85aa-14ae531bb025 container agnhost-container: <nil>
    STEP: delete the pod 09/20/23 12:46:51.357
    Sep 20 12:46:51.391: INFO: Waiting for pod pod-projected-configmaps-890eba7b-32cf-4823-85aa-14ae531bb025 to disappear
    Sep 20 12:46:51.396: INFO: Pod pod-projected-configmaps-890eba7b-32cf-4823-85aa-14ae531bb025 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:46:51.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-151" for this suite. 09/20/23 12:46:51.4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:46:51.41
Sep 20 12:46:51.410: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename svcaccounts 09/20/23 12:46:51.411
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:46:51.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:46:51.441
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  09/20/23 12:46:51.445
Sep 20 12:46:51.454: INFO: Waiting up to 5m0s for pod "test-pod-15913adb-a1df-4548-b317-d89d5ba2a8f8" in namespace "svcaccounts-1307" to be "Succeeded or Failed"
Sep 20 12:46:51.457: INFO: Pod "test-pod-15913adb-a1df-4548-b317-d89d5ba2a8f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.916625ms
Sep 20 12:46:53.567: INFO: Pod "test-pod-15913adb-a1df-4548-b317-d89d5ba2a8f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.112267015s
Sep 20 12:46:55.469: INFO: Pod "test-pod-15913adb-a1df-4548-b317-d89d5ba2a8f8": Phase="Running", Reason="", readiness=false. Elapsed: 4.015113684s
Sep 20 12:46:57.469: INFO: Pod "test-pod-15913adb-a1df-4548-b317-d89d5ba2a8f8": Phase="Running", Reason="", readiness=false. Elapsed: 6.014778016s
Sep 20 12:46:59.461: INFO: Pod "test-pod-15913adb-a1df-4548-b317-d89d5ba2a8f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.006286651s
STEP: Saw pod success 09/20/23 12:46:59.461
Sep 20 12:46:59.461: INFO: Pod "test-pod-15913adb-a1df-4548-b317-d89d5ba2a8f8" satisfied condition "Succeeded or Failed"
Sep 20 12:46:59.463: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod test-pod-15913adb-a1df-4548-b317-d89d5ba2a8f8 container agnhost-container: <nil>
STEP: delete the pod 09/20/23 12:46:59.468
Sep 20 12:46:59.507: INFO: Waiting for pod test-pod-15913adb-a1df-4548-b317-d89d5ba2a8f8 to disappear
Sep 20 12:46:59.514: INFO: Pod test-pod-15913adb-a1df-4548-b317-d89d5ba2a8f8 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep 20 12:46:59.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1307" for this suite. 09/20/23 12:46:59.519
------------------------------
â€¢ [SLOW TEST] [8.117 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:46:51.41
    Sep 20 12:46:51.410: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename svcaccounts 09/20/23 12:46:51.411
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:46:51.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:46:51.441
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  09/20/23 12:46:51.445
    Sep 20 12:46:51.454: INFO: Waiting up to 5m0s for pod "test-pod-15913adb-a1df-4548-b317-d89d5ba2a8f8" in namespace "svcaccounts-1307" to be "Succeeded or Failed"
    Sep 20 12:46:51.457: INFO: Pod "test-pod-15913adb-a1df-4548-b317-d89d5ba2a8f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.916625ms
    Sep 20 12:46:53.567: INFO: Pod "test-pod-15913adb-a1df-4548-b317-d89d5ba2a8f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.112267015s
    Sep 20 12:46:55.469: INFO: Pod "test-pod-15913adb-a1df-4548-b317-d89d5ba2a8f8": Phase="Running", Reason="", readiness=false. Elapsed: 4.015113684s
    Sep 20 12:46:57.469: INFO: Pod "test-pod-15913adb-a1df-4548-b317-d89d5ba2a8f8": Phase="Running", Reason="", readiness=false. Elapsed: 6.014778016s
    Sep 20 12:46:59.461: INFO: Pod "test-pod-15913adb-a1df-4548-b317-d89d5ba2a8f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.006286651s
    STEP: Saw pod success 09/20/23 12:46:59.461
    Sep 20 12:46:59.461: INFO: Pod "test-pod-15913adb-a1df-4548-b317-d89d5ba2a8f8" satisfied condition "Succeeded or Failed"
    Sep 20 12:46:59.463: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod test-pod-15913adb-a1df-4548-b317-d89d5ba2a8f8 container agnhost-container: <nil>
    STEP: delete the pod 09/20/23 12:46:59.468
    Sep 20 12:46:59.507: INFO: Waiting for pod test-pod-15913adb-a1df-4548-b317-d89d5ba2a8f8 to disappear
    Sep 20 12:46:59.514: INFO: Pod test-pod-15913adb-a1df-4548-b317-d89d5ba2a8f8 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:46:59.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1307" for this suite. 09/20/23 12:46:59.519
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:46:59.529
Sep 20 12:46:59.529: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename statefulset 09/20/23 12:46:59.53
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:46:59.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:46:59.775
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-6155 09/20/23 12:46:59.87
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-6155 09/20/23 12:47:00.062
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6155 09/20/23 12:47:00.069
Sep 20 12:47:00.077: INFO: Found 0 stateful pods, waiting for 1
Sep 20 12:47:10.080: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 09/20/23 12:47:10.081
Sep 20 12:47:10.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-6155 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 20 12:47:10.318: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 20 12:47:10.318: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 20 12:47:10.318: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 20 12:47:10.321: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep 20 12:47:20.325: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 20 12:47:20.325: INFO: Waiting for statefulset status.replicas updated to 0
Sep 20 12:47:20.336: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Sep 20 12:47:20.336: INFO: ss-0  mycluster-ww3cg64etuwi-node-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:00 +0000 UTC  }]
Sep 20 12:47:20.336: INFO: 
Sep 20 12:47:20.336: INFO: StatefulSet ss has not reached scale 3, at 1
Sep 20 12:47:21.338: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.998064372s
Sep 20 12:47:22.342: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.995140911s
Sep 20 12:47:23.345: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.991186889s
Sep 20 12:47:24.349: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.988918113s
Sep 20 12:47:25.353: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.984232082s
Sep 20 12:47:26.357: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.980464983s
Sep 20 12:47:27.360: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.976934449s
Sep 20 12:47:28.363: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.973966616s
Sep 20 12:47:29.366: INFO: Verifying statefulset ss doesn't scale past 3 for another 970.502356ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6155 09/20/23 12:47:30.368
Sep 20 12:47:30.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-6155 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 12:47:30.574: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 20 12:47:30.574: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 20 12:47:30.574: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 20 12:47:30.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-6155 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 12:47:30.903: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep 20 12:47:30.903: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 20 12:47:30.903: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 20 12:47:30.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-6155 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 12:47:31.481: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep 20 12:47:31.481: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 20 12:47:31.481: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 20 12:47:31.612: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 12:47:31.612: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 12:47:31.612: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 09/20/23 12:47:31.612
Sep 20 12:47:31.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-6155 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 20 12:47:32.776: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 20 12:47:32.776: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 20 12:47:32.776: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 20 12:47:32.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-6155 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 20 12:47:33.439: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 20 12:47:33.439: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 20 12:47:33.439: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 20 12:47:33.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-6155 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 20 12:47:33.812: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 20 12:47:33.812: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 20 12:47:33.812: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 20 12:47:33.812: INFO: Waiting for statefulset status.replicas updated to 0
Sep 20 12:47:33.815: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Sep 20 12:47:43.973: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 20 12:47:43.973: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep 20 12:47:43.973: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep 20 12:47:44.126: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Sep 20 12:47:44.126: INFO: ss-0  mycluster-ww3cg64etuwi-node-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:00 +0000 UTC  }]
Sep 20 12:47:44.126: INFO: ss-1  mycluster-ww3cg64etuwi-node-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:20 +0000 UTC  }]
Sep 20 12:47:44.126: INFO: ss-2  mycluster-ww3cg64etuwi-node-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:20 +0000 UTC  }]
Sep 20 12:47:44.126: INFO: 
Sep 20 12:47:44.126: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 20 12:47:45.131: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Sep 20 12:47:45.131: INFO: ss-0  mycluster-ww3cg64etuwi-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:00 +0000 UTC  }]
Sep 20 12:47:45.131: INFO: ss-1  mycluster-ww3cg64etuwi-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:20 +0000 UTC  }]
Sep 20 12:47:45.131: INFO: ss-2  mycluster-ww3cg64etuwi-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:20 +0000 UTC  }]
Sep 20 12:47:45.131: INFO: 
Sep 20 12:47:45.131: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 20 12:47:46.135: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Sep 20 12:47:46.135: INFO: ss-0  mycluster-ww3cg64etuwi-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:00 +0000 UTC  }]
Sep 20 12:47:46.135: INFO: ss-1  mycluster-ww3cg64etuwi-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:20 +0000 UTC  }]
Sep 20 12:47:46.135: INFO: ss-2  mycluster-ww3cg64etuwi-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:20 +0000 UTC  }]
Sep 20 12:47:46.135: INFO: 
Sep 20 12:47:46.135: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 20 12:47:47.138: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Sep 20 12:47:47.138: INFO: ss-1  mycluster-ww3cg64etuwi-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:20 +0000 UTC  }]
Sep 20 12:47:47.138: INFO: ss-2  mycluster-ww3cg64etuwi-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:20 +0000 UTC  }]
Sep 20 12:47:47.138: INFO: 
Sep 20 12:47:47.138: INFO: StatefulSet ss has not reached scale 0, at 2
Sep 20 12:47:48.145: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.911042597s
Sep 20 12:47:49.148: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.904286398s
Sep 20 12:47:50.349: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.900671826s
Sep 20 12:47:51.353: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.700637159s
Sep 20 12:47:52.357: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.69649831s
Sep 20 12:47:53.360: INFO: Verifying statefulset ss doesn't scale past 0 for another 692.063432ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6155 09/20/23 12:47:54.361
Sep 20 12:47:54.364: INFO: Scaling statefulset ss to 0
Sep 20 12:47:54.372: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep 20 12:47:54.374: INFO: Deleting all statefulset in ns statefulset-6155
Sep 20 12:47:54.376: INFO: Scaling statefulset ss to 0
Sep 20 12:47:54.382: INFO: Waiting for statefulset status.replicas updated to 0
Sep 20 12:47:54.384: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep 20 12:47:54.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-6155" for this suite. 09/20/23 12:47:54.427
------------------------------
â€¢ [SLOW TEST] [54.952 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:46:59.529
    Sep 20 12:46:59.529: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename statefulset 09/20/23 12:46:59.53
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:46:59.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:46:59.775
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-6155 09/20/23 12:46:59.87
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-6155 09/20/23 12:47:00.062
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6155 09/20/23 12:47:00.069
    Sep 20 12:47:00.077: INFO: Found 0 stateful pods, waiting for 1
    Sep 20 12:47:10.080: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 09/20/23 12:47:10.081
    Sep 20 12:47:10.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-6155 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep 20 12:47:10.318: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep 20 12:47:10.318: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep 20 12:47:10.318: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep 20 12:47:10.321: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Sep 20 12:47:20.325: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Sep 20 12:47:20.325: INFO: Waiting for statefulset status.replicas updated to 0
    Sep 20 12:47:20.336: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
    Sep 20 12:47:20.336: INFO: ss-0  mycluster-ww3cg64etuwi-node-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:00 +0000 UTC  }]
    Sep 20 12:47:20.336: INFO: 
    Sep 20 12:47:20.336: INFO: StatefulSet ss has not reached scale 3, at 1
    Sep 20 12:47:21.338: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.998064372s
    Sep 20 12:47:22.342: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.995140911s
    Sep 20 12:47:23.345: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.991186889s
    Sep 20 12:47:24.349: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.988918113s
    Sep 20 12:47:25.353: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.984232082s
    Sep 20 12:47:26.357: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.980464983s
    Sep 20 12:47:27.360: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.976934449s
    Sep 20 12:47:28.363: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.973966616s
    Sep 20 12:47:29.366: INFO: Verifying statefulset ss doesn't scale past 3 for another 970.502356ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6155 09/20/23 12:47:30.368
    Sep 20 12:47:30.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-6155 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep 20 12:47:30.574: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep 20 12:47:30.574: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep 20 12:47:30.574: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep 20 12:47:30.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-6155 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep 20 12:47:30.903: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Sep 20 12:47:30.903: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep 20 12:47:30.903: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep 20 12:47:30.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-6155 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep 20 12:47:31.481: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Sep 20 12:47:31.481: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep 20 12:47:31.481: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep 20 12:47:31.612: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep 20 12:47:31.612: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep 20 12:47:31.612: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 09/20/23 12:47:31.612
    Sep 20 12:47:31.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-6155 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep 20 12:47:32.776: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep 20 12:47:32.776: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep 20 12:47:32.776: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep 20 12:47:32.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-6155 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep 20 12:47:33.439: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep 20 12:47:33.439: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep 20 12:47:33.439: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep 20 12:47:33.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-6155 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep 20 12:47:33.812: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep 20 12:47:33.812: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep 20 12:47:33.812: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep 20 12:47:33.812: INFO: Waiting for statefulset status.replicas updated to 0
    Sep 20 12:47:33.815: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Sep 20 12:47:43.973: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Sep 20 12:47:43.973: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Sep 20 12:47:43.973: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Sep 20 12:47:44.126: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
    Sep 20 12:47:44.126: INFO: ss-0  mycluster-ww3cg64etuwi-node-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:00 +0000 UTC  }]
    Sep 20 12:47:44.126: INFO: ss-1  mycluster-ww3cg64etuwi-node-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:20 +0000 UTC  }]
    Sep 20 12:47:44.126: INFO: ss-2  mycluster-ww3cg64etuwi-node-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:20 +0000 UTC  }]
    Sep 20 12:47:44.126: INFO: 
    Sep 20 12:47:44.126: INFO: StatefulSet ss has not reached scale 0, at 3
    Sep 20 12:47:45.131: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
    Sep 20 12:47:45.131: INFO: ss-0  mycluster-ww3cg64etuwi-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:00 +0000 UTC  }]
    Sep 20 12:47:45.131: INFO: ss-1  mycluster-ww3cg64etuwi-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:20 +0000 UTC  }]
    Sep 20 12:47:45.131: INFO: ss-2  mycluster-ww3cg64etuwi-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:20 +0000 UTC  }]
    Sep 20 12:47:45.131: INFO: 
    Sep 20 12:47:45.131: INFO: StatefulSet ss has not reached scale 0, at 3
    Sep 20 12:47:46.135: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
    Sep 20 12:47:46.135: INFO: ss-0  mycluster-ww3cg64etuwi-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:00 +0000 UTC  }]
    Sep 20 12:47:46.135: INFO: ss-1  mycluster-ww3cg64etuwi-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:20 +0000 UTC  }]
    Sep 20 12:47:46.135: INFO: ss-2  mycluster-ww3cg64etuwi-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:20 +0000 UTC  }]
    Sep 20 12:47:46.135: INFO: 
    Sep 20 12:47:46.135: INFO: StatefulSet ss has not reached scale 0, at 3
    Sep 20 12:47:47.138: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
    Sep 20 12:47:47.138: INFO: ss-1  mycluster-ww3cg64etuwi-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:20 +0000 UTC  }]
    Sep 20 12:47:47.138: INFO: ss-2  mycluster-ww3cg64etuwi-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 12:47:20 +0000 UTC  }]
    Sep 20 12:47:47.138: INFO: 
    Sep 20 12:47:47.138: INFO: StatefulSet ss has not reached scale 0, at 2
    Sep 20 12:47:48.145: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.911042597s
    Sep 20 12:47:49.148: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.904286398s
    Sep 20 12:47:50.349: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.900671826s
    Sep 20 12:47:51.353: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.700637159s
    Sep 20 12:47:52.357: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.69649831s
    Sep 20 12:47:53.360: INFO: Verifying statefulset ss doesn't scale past 0 for another 692.063432ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6155 09/20/23 12:47:54.361
    Sep 20 12:47:54.364: INFO: Scaling statefulset ss to 0
    Sep 20 12:47:54.372: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep 20 12:47:54.374: INFO: Deleting all statefulset in ns statefulset-6155
    Sep 20 12:47:54.376: INFO: Scaling statefulset ss to 0
    Sep 20 12:47:54.382: INFO: Waiting for statefulset status.replicas updated to 0
    Sep 20 12:47:54.384: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:47:54.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-6155" for this suite. 09/20/23 12:47:54.427
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:47:54.482
Sep 20 12:47:54.482: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename namespaces 09/20/23 12:47:54.483
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:47:54.622
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:47:54.624
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 09/20/23 12:47:54.723
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:47:55.419
STEP: Creating a service in the namespace 09/20/23 12:47:55.421
STEP: Deleting the namespace 09/20/23 12:47:55.514
STEP: Waiting for the namespace to be removed. 09/20/23 12:47:55.599
STEP: Recreating the namespace 09/20/23 12:48:01.602
STEP: Verifying there is no service in the namespace 09/20/23 12:48:01.976
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep 20 12:48:02.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-6899" for this suite. 09/20/23 12:48:02.019
STEP: Destroying namespace "nsdeletetest-742" for this suite. 09/20/23 12:48:02.025
Sep 20 12:48:02.141: INFO: Namespace nsdeletetest-742 was already deleted
STEP: Destroying namespace "nsdeletetest-6650" for this suite. 09/20/23 12:48:02.141
------------------------------
â€¢ [SLOW TEST] [7.675 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:47:54.482
    Sep 20 12:47:54.482: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename namespaces 09/20/23 12:47:54.483
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:47:54.622
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:47:54.624
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 09/20/23 12:47:54.723
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:47:55.419
    STEP: Creating a service in the namespace 09/20/23 12:47:55.421
    STEP: Deleting the namespace 09/20/23 12:47:55.514
    STEP: Waiting for the namespace to be removed. 09/20/23 12:47:55.599
    STEP: Recreating the namespace 09/20/23 12:48:01.602
    STEP: Verifying there is no service in the namespace 09/20/23 12:48:01.976
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:48:02.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-6899" for this suite. 09/20/23 12:48:02.019
    STEP: Destroying namespace "nsdeletetest-742" for this suite. 09/20/23 12:48:02.025
    Sep 20 12:48:02.141: INFO: Namespace nsdeletetest-742 was already deleted
    STEP: Destroying namespace "nsdeletetest-6650" for this suite. 09/20/23 12:48:02.141
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:48:02.158
Sep 20 12:48:02.158: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename init-container 09/20/23 12:48:02.159
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:48:02.174
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:48:02.176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 09/20/23 12:48:02.18
Sep 20 12:48:02.180: INFO: PodSpec: initContainers in spec.initContainers
Sep 20 12:48:53.698: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-53ddbdf7-f8f4-4e22-ac9c-b143c5447582", GenerateName:"", Namespace:"init-container-6819", SelfLink:"", UID:"1a5a2c0f-fbe7-404e-828a-a3152067252a", ResourceVersion:"21571", Generation:0, CreationTimestamp:time.Date(2023, time.September, 20, 12, 48, 2, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"180340056"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.September, 20, 12, 48, 2, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0012560d8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.September, 20, 12, 48, 53, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001256138), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-d5h7p", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc000e5c060), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-d5h7p", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-d5h7p", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-d5h7p", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004fb20d8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"mycluster-ww3cg64etuwi-node-1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000c98070), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004fb2150)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004fb2170)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004fb2178), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004fb217c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc001550050), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 20, 12, 48, 2, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 20, 12, 48, 2, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 20, 12, 48, 2, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 20, 12, 48, 2, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.10.64", PodIP:"10.100.4.141", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.100.4.141"}}, StartTime:time.Date(2023, time.September, 20, 12, 48, 2, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc001256210), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000c98150)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://f8b54777e47f31a5ee98cd624c531adaebb3c8f22596a95ae9909cd92843637e", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000e5c160), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000e5c120), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc004fb21ff)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep 20 12:48:53.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-6819" for this suite. 09/20/23 12:48:53.703
------------------------------
â€¢ [SLOW TEST] [51.551 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:48:02.158
    Sep 20 12:48:02.158: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename init-container 09/20/23 12:48:02.159
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:48:02.174
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:48:02.176
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 09/20/23 12:48:02.18
    Sep 20 12:48:02.180: INFO: PodSpec: initContainers in spec.initContainers
    Sep 20 12:48:53.698: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-53ddbdf7-f8f4-4e22-ac9c-b143c5447582", GenerateName:"", Namespace:"init-container-6819", SelfLink:"", UID:"1a5a2c0f-fbe7-404e-828a-a3152067252a", ResourceVersion:"21571", Generation:0, CreationTimestamp:time.Date(2023, time.September, 20, 12, 48, 2, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"180340056"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.September, 20, 12, 48, 2, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0012560d8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.September, 20, 12, 48, 53, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001256138), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-d5h7p", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc000e5c060), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-d5h7p", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-d5h7p", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-d5h7p", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004fb20d8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"mycluster-ww3cg64etuwi-node-1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000c98070), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004fb2150)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004fb2170)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004fb2178), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004fb217c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc001550050), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 20, 12, 48, 2, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 20, 12, 48, 2, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 20, 12, 48, 2, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 20, 12, 48, 2, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.10.64", PodIP:"10.100.4.141", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.100.4.141"}}, StartTime:time.Date(2023, time.September, 20, 12, 48, 2, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc001256210), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000c98150)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://f8b54777e47f31a5ee98cd624c531adaebb3c8f22596a95ae9909cd92843637e", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000e5c160), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000e5c120), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc004fb21ff)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:48:53.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-6819" for this suite. 09/20/23 12:48:53.703
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:48:53.71
Sep 20 12:48:53.710: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename deployment 09/20/23 12:48:53.711
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:48:53.833
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:48:53.836
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Sep 20 12:48:53.854: INFO: Pod name rollover-pod: Found 0 pods out of 1
Sep 20 12:48:58.943: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/20/23 12:48:58.943
Sep 20 12:48:58.943: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Sep 20 12:49:00.946: INFO: Creating deployment "test-rollover-deployment"
Sep 20 12:49:00.973: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Sep 20 12:49:02.979: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Sep 20 12:49:02.983: INFO: Ensure that both replica sets have 1 created replica
Sep 20 12:49:02.986: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Sep 20 12:49:03.022: INFO: Updating deployment test-rollover-deployment
Sep 20 12:49:03.022: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Sep 20 12:49:05.027: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Sep 20 12:49:05.031: INFO: Make sure deployment "test-rollover-deployment" is complete
Sep 20 12:49:05.034: INFO: all replica sets need to contain the pod-template-hash label
Sep 20 12:49:05.034: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 12:49:07.040: INFO: all replica sets need to contain the pod-template-hash label
Sep 20 12:49:07.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 12:49:09.411: INFO: all replica sets need to contain the pod-template-hash label
Sep 20 12:49:09.411: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 12:49:11.040: INFO: all replica sets need to contain the pod-template-hash label
Sep 20 12:49:11.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 12:49:13.043: INFO: all replica sets need to contain the pod-template-hash label
Sep 20 12:49:13.043: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 12:49:15.041: INFO: all replica sets need to contain the pod-template-hash label
Sep 20 12:49:15.041: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 12:49:17.040: INFO: all replica sets need to contain the pod-template-hash label
Sep 20 12:49:17.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 12:49:19.061: INFO: 
Sep 20 12:49:19.061: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 12:49:21.042: INFO: 
Sep 20 12:49:21.042: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep 20 12:49:21.062: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-3613  af145b6c-316f-4d7b-898a-5d83f8adcc1a 21744 2 2023-09-20 12:49:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-20 12:49:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 12:49:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005d75038 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-20 12:49:01 +0000 UTC,LastTransitionTime:2023-09-20 12:49:01 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-09-20 12:49:19 +0000 UTC,LastTransitionTime:2023-09-20 12:49:01 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep 20 12:49:21.064: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-3613  b8adbf2a-29e6-4ada-b286-81331aab7499 21733 2 2023-09-20 12:49:03 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment af145b6c-316f-4d7b-898a-5d83f8adcc1a 0xc005d754b7 0xc005d754b8}] [] [{kube-controller-manager Update apps/v1 2023-09-20 12:49:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af145b6c-316f-4d7b-898a-5d83f8adcc1a\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 12:49:18 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005d75568 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 20 12:49:21.064: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Sep 20 12:49:21.064: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3613  031bbb4c-8bb5-4f9f-90e0-4f3f08b9cd4e 21743 2 2023-09-20 12:48:53 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment af145b6c-316f-4d7b-898a-5d83f8adcc1a 0xc005d75387 0xc005d75388}] [] [{e2e.test Update apps/v1 2023-09-20 12:48:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 12:49:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af145b6c-316f-4d7b-898a-5d83f8adcc1a\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-09-20 12:49:19 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005d75448 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 20 12:49:21.064: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-3613  755a87a8-857b-4e4c-a9e4-ad6ef96c8e44 21670 2 2023-09-20 12:49:01 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment af145b6c-316f-4d7b-898a-5d83f8adcc1a 0xc005d755d7 0xc005d755d8}] [] [{kube-controller-manager Update apps/v1 2023-09-20 12:49:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af145b6c-316f-4d7b-898a-5d83f8adcc1a\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 12:49:04 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005d75688 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 20 12:49:21.067: INFO: Pod "test-rollover-deployment-6c6df9974f-pnjtb" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-pnjtb test-rollover-deployment-6c6df9974f- deployment-3613  3b06503d-0e7f-4b37-8f29-e8b19ce683d7 21693 0 2023-09-20 12:49:03 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f b8adbf2a-29e6-4ada-b286-81331aab7499 0xc005d75bd7 0xc005d75bd8}] [] [{kube-controller-manager Update v1 2023-09-20 12:49:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8adbf2a-29e6-4ada-b286-81331aab7499\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 12:49:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.4.144\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7876h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7876h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:49:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:49:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:49:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:49:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.64,PodIP:10.100.4.144,StartTime:2023-09-20 12:49:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-20 12:49:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://11dc986c41a3374993634cc94ea95ff770ffbf693996db513c00e7ea5a73e3ac,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.4.144,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep 20 12:49:21.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-3613" for this suite. 09/20/23 12:49:21.07
------------------------------
â€¢ [SLOW TEST] [27.365 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:48:53.71
    Sep 20 12:48:53.710: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename deployment 09/20/23 12:48:53.711
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:48:53.833
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:48:53.836
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Sep 20 12:48:53.854: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Sep 20 12:48:58.943: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/20/23 12:48:58.943
    Sep 20 12:48:58.943: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Sep 20 12:49:00.946: INFO: Creating deployment "test-rollover-deployment"
    Sep 20 12:49:00.973: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Sep 20 12:49:02.979: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Sep 20 12:49:02.983: INFO: Ensure that both replica sets have 1 created replica
    Sep 20 12:49:02.986: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Sep 20 12:49:03.022: INFO: Updating deployment test-rollover-deployment
    Sep 20 12:49:03.022: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Sep 20 12:49:05.027: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Sep 20 12:49:05.031: INFO: Make sure deployment "test-rollover-deployment" is complete
    Sep 20 12:49:05.034: INFO: all replica sets need to contain the pod-template-hash label
    Sep 20 12:49:05.034: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 12:49:07.040: INFO: all replica sets need to contain the pod-template-hash label
    Sep 20 12:49:07.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 12:49:09.411: INFO: all replica sets need to contain the pod-template-hash label
    Sep 20 12:49:09.411: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 12:49:11.040: INFO: all replica sets need to contain the pod-template-hash label
    Sep 20 12:49:11.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 12:49:13.043: INFO: all replica sets need to contain the pod-template-hash label
    Sep 20 12:49:13.043: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 12:49:15.041: INFO: all replica sets need to contain the pod-template-hash label
    Sep 20 12:49:15.041: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 12:49:17.040: INFO: all replica sets need to contain the pod-template-hash label
    Sep 20 12:49:17.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 12:49:19.061: INFO: 
    Sep 20 12:49:19.061: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 12:49:21.042: INFO: 
    Sep 20 12:49:21.042: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep 20 12:49:21.062: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-3613  af145b6c-316f-4d7b-898a-5d83f8adcc1a 21744 2 2023-09-20 12:49:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-20 12:49:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 12:49:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005d75038 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-20 12:49:01 +0000 UTC,LastTransitionTime:2023-09-20 12:49:01 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-09-20 12:49:19 +0000 UTC,LastTransitionTime:2023-09-20 12:49:01 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Sep 20 12:49:21.064: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-3613  b8adbf2a-29e6-4ada-b286-81331aab7499 21733 2 2023-09-20 12:49:03 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment af145b6c-316f-4d7b-898a-5d83f8adcc1a 0xc005d754b7 0xc005d754b8}] [] [{kube-controller-manager Update apps/v1 2023-09-20 12:49:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af145b6c-316f-4d7b-898a-5d83f8adcc1a\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 12:49:18 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005d75568 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep 20 12:49:21.064: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Sep 20 12:49:21.064: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3613  031bbb4c-8bb5-4f9f-90e0-4f3f08b9cd4e 21743 2 2023-09-20 12:48:53 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment af145b6c-316f-4d7b-898a-5d83f8adcc1a 0xc005d75387 0xc005d75388}] [] [{e2e.test Update apps/v1 2023-09-20 12:48:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 12:49:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af145b6c-316f-4d7b-898a-5d83f8adcc1a\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-09-20 12:49:19 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005d75448 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep 20 12:49:21.064: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-3613  755a87a8-857b-4e4c-a9e4-ad6ef96c8e44 21670 2 2023-09-20 12:49:01 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment af145b6c-316f-4d7b-898a-5d83f8adcc1a 0xc005d755d7 0xc005d755d8}] [] [{kube-controller-manager Update apps/v1 2023-09-20 12:49:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af145b6c-316f-4d7b-898a-5d83f8adcc1a\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 12:49:04 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005d75688 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep 20 12:49:21.067: INFO: Pod "test-rollover-deployment-6c6df9974f-pnjtb" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-pnjtb test-rollover-deployment-6c6df9974f- deployment-3613  3b06503d-0e7f-4b37-8f29-e8b19ce683d7 21693 0 2023-09-20 12:49:03 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f b8adbf2a-29e6-4ada-b286-81331aab7499 0xc005d75bd7 0xc005d75bd8}] [] [{kube-controller-manager Update v1 2023-09-20 12:49:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8adbf2a-29e6-4ada-b286-81331aab7499\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 12:49:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.4.144\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7876h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7876h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:49:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:49:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:49:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 12:49:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.64,PodIP:10.100.4.144,StartTime:2023-09-20 12:49:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-20 12:49:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://11dc986c41a3374993634cc94ea95ff770ffbf693996db513c00e7ea5a73e3ac,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.4.144,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:49:21.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-3613" for this suite. 09/20/23 12:49:21.07
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:49:21.076
Sep 20 12:49:21.076: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename lease-test 09/20/23 12:49:21.077
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:49:21.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:49:21.102
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Sep 20 12:49:21.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-7333" for this suite. 09/20/23 12:49:21.402
------------------------------
â€¢ [0.441 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:49:21.076
    Sep 20 12:49:21.076: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename lease-test 09/20/23 12:49:21.077
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:49:21.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:49:21.102
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:49:21.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-7333" for this suite. 09/20/23 12:49:21.402
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:49:21.518
Sep 20 12:49:21.518: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename security-context 09/20/23 12:49:21.519
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:49:21.588
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:49:21.594
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 09/20/23 12:49:21.598
Sep 20 12:49:21.610: INFO: Waiting up to 5m0s for pod "security-context-b3df091c-e1b8-414c-a4c1-11f882e01b9f" in namespace "security-context-2002" to be "Succeeded or Failed"
Sep 20 12:49:21.612: INFO: Pod "security-context-b3df091c-e1b8-414c-a4c1-11f882e01b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.528515ms
Sep 20 12:49:23.706: INFO: Pod "security-context-b3df091c-e1b8-414c-a4c1-11f882e01b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.096367475s
Sep 20 12:49:25.615: INFO: Pod "security-context-b3df091c-e1b8-414c-a4c1-11f882e01b9f": Phase="Running", Reason="", readiness=false. Elapsed: 4.005369439s
Sep 20 12:49:27.812: INFO: Pod "security-context-b3df091c-e1b8-414c-a4c1-11f882e01b9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.202433595s
STEP: Saw pod success 09/20/23 12:49:27.812
Sep 20 12:49:27.812: INFO: Pod "security-context-b3df091c-e1b8-414c-a4c1-11f882e01b9f" satisfied condition "Succeeded or Failed"
Sep 20 12:49:27.817: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod security-context-b3df091c-e1b8-414c-a4c1-11f882e01b9f container test-container: <nil>
STEP: delete the pod 09/20/23 12:49:27.866
Sep 20 12:49:28.130: INFO: Waiting for pod security-context-b3df091c-e1b8-414c-a4c1-11f882e01b9f to disappear
Sep 20 12:49:28.133: INFO: Pod security-context-b3df091c-e1b8-414c-a4c1-11f882e01b9f no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep 20 12:49:28.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-2002" for this suite. 09/20/23 12:49:28.137
------------------------------
â€¢ [SLOW TEST] [6.628 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:49:21.518
    Sep 20 12:49:21.518: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename security-context 09/20/23 12:49:21.519
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:49:21.588
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:49:21.594
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 09/20/23 12:49:21.598
    Sep 20 12:49:21.610: INFO: Waiting up to 5m0s for pod "security-context-b3df091c-e1b8-414c-a4c1-11f882e01b9f" in namespace "security-context-2002" to be "Succeeded or Failed"
    Sep 20 12:49:21.612: INFO: Pod "security-context-b3df091c-e1b8-414c-a4c1-11f882e01b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.528515ms
    Sep 20 12:49:23.706: INFO: Pod "security-context-b3df091c-e1b8-414c-a4c1-11f882e01b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.096367475s
    Sep 20 12:49:25.615: INFO: Pod "security-context-b3df091c-e1b8-414c-a4c1-11f882e01b9f": Phase="Running", Reason="", readiness=false. Elapsed: 4.005369439s
    Sep 20 12:49:27.812: INFO: Pod "security-context-b3df091c-e1b8-414c-a4c1-11f882e01b9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.202433595s
    STEP: Saw pod success 09/20/23 12:49:27.812
    Sep 20 12:49:27.812: INFO: Pod "security-context-b3df091c-e1b8-414c-a4c1-11f882e01b9f" satisfied condition "Succeeded or Failed"
    Sep 20 12:49:27.817: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod security-context-b3df091c-e1b8-414c-a4c1-11f882e01b9f container test-container: <nil>
    STEP: delete the pod 09/20/23 12:49:27.866
    Sep 20 12:49:28.130: INFO: Waiting for pod security-context-b3df091c-e1b8-414c-a4c1-11f882e01b9f to disappear
    Sep 20 12:49:28.133: INFO: Pod security-context-b3df091c-e1b8-414c-a4c1-11f882e01b9f no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:49:28.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-2002" for this suite. 09/20/23 12:49:28.137
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:49:28.146
Sep 20 12:49:28.146: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename services 09/20/23 12:49:28.147
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:49:28.195
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:49:28.197
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6754 09/20/23 12:49:28.2
STEP: changing the ExternalName service to type=NodePort 09/20/23 12:49:28.306
STEP: creating replication controller externalname-service in namespace services-6754 09/20/23 12:49:28.385
I0920 12:49:28.582591      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6754, replica count: 2
I0920 12:49:31.634570      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0920 12:49:34.635478      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 20 12:49:34.635: INFO: Creating new exec pod
Sep 20 12:49:35.041: INFO: Waiting up to 5m0s for pod "execpoddln69" in namespace "services-6754" to be "running"
Sep 20 12:49:35.044: INFO: Pod "execpoddln69": Phase="Pending", Reason="", readiness=false. Elapsed: 3.211501ms
Sep 20 12:49:37.048: INFO: Pod "execpoddln69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007458193s
Sep 20 12:49:39.047: INFO: Pod "execpoddln69": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006328375s
Sep 20 12:49:41.325: INFO: Pod "execpoddln69": Phase="Running", Reason="", readiness=true. Elapsed: 6.2840102s
Sep 20 12:49:41.325: INFO: Pod "execpoddln69" satisfied condition "running"
Sep 20 12:49:42.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-6754 exec execpoddln69 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Sep 20 12:49:42.544: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep 20 12:49:42.544: INFO: stdout: ""
Sep 20 12:49:42.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-6754 exec execpoddln69 -- /bin/sh -x -c nc -v -z -w 2 10.254.239.183 80'
Sep 20 12:49:42.754: INFO: stderr: "+ nc -v -z -w 2 10.254.239.183 80\nConnection to 10.254.239.183 80 port [tcp/http] succeeded!\n"
Sep 20 12:49:42.754: INFO: stdout: ""
Sep 20 12:49:42.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-6754 exec execpoddln69 -- /bin/sh -x -c nc -v -z -w 2 192.168.10.173 30809'
Sep 20 12:49:43.003: INFO: stderr: "+ nc -v -z -w 2 192.168.10.173 30809\nConnection to 192.168.10.173 30809 port [tcp/*] succeeded!\n"
Sep 20 12:49:43.003: INFO: stdout: ""
Sep 20 12:49:43.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-6754 exec execpoddln69 -- /bin/sh -x -c nc -v -z -w 2 192.168.10.172 30809'
Sep 20 12:49:43.235: INFO: stderr: "+ nc -v -z -w 2 192.168.10.172 30809\nConnection to 192.168.10.172 30809 port [tcp/*] succeeded!\n"
Sep 20 12:49:43.235: INFO: stdout: ""
Sep 20 12:49:43.235: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep 20 12:49:43.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6754" for this suite. 09/20/23 12:49:43.339
------------------------------
â€¢ [SLOW TEST] [15.343 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:49:28.146
    Sep 20 12:49:28.146: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename services 09/20/23 12:49:28.147
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:49:28.195
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:49:28.197
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-6754 09/20/23 12:49:28.2
    STEP: changing the ExternalName service to type=NodePort 09/20/23 12:49:28.306
    STEP: creating replication controller externalname-service in namespace services-6754 09/20/23 12:49:28.385
    I0920 12:49:28.582591      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6754, replica count: 2
    I0920 12:49:31.634570      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0920 12:49:34.635478      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep 20 12:49:34.635: INFO: Creating new exec pod
    Sep 20 12:49:35.041: INFO: Waiting up to 5m0s for pod "execpoddln69" in namespace "services-6754" to be "running"
    Sep 20 12:49:35.044: INFO: Pod "execpoddln69": Phase="Pending", Reason="", readiness=false. Elapsed: 3.211501ms
    Sep 20 12:49:37.048: INFO: Pod "execpoddln69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007458193s
    Sep 20 12:49:39.047: INFO: Pod "execpoddln69": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006328375s
    Sep 20 12:49:41.325: INFO: Pod "execpoddln69": Phase="Running", Reason="", readiness=true. Elapsed: 6.2840102s
    Sep 20 12:49:41.325: INFO: Pod "execpoddln69" satisfied condition "running"
    Sep 20 12:49:42.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-6754 exec execpoddln69 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Sep 20 12:49:42.544: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Sep 20 12:49:42.544: INFO: stdout: ""
    Sep 20 12:49:42.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-6754 exec execpoddln69 -- /bin/sh -x -c nc -v -z -w 2 10.254.239.183 80'
    Sep 20 12:49:42.754: INFO: stderr: "+ nc -v -z -w 2 10.254.239.183 80\nConnection to 10.254.239.183 80 port [tcp/http] succeeded!\n"
    Sep 20 12:49:42.754: INFO: stdout: ""
    Sep 20 12:49:42.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-6754 exec execpoddln69 -- /bin/sh -x -c nc -v -z -w 2 192.168.10.173 30809'
    Sep 20 12:49:43.003: INFO: stderr: "+ nc -v -z -w 2 192.168.10.173 30809\nConnection to 192.168.10.173 30809 port [tcp/*] succeeded!\n"
    Sep 20 12:49:43.003: INFO: stdout: ""
    Sep 20 12:49:43.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-6754 exec execpoddln69 -- /bin/sh -x -c nc -v -z -w 2 192.168.10.172 30809'
    Sep 20 12:49:43.235: INFO: stderr: "+ nc -v -z -w 2 192.168.10.172 30809\nConnection to 192.168.10.172 30809 port [tcp/*] succeeded!\n"
    Sep 20 12:49:43.235: INFO: stdout: ""
    Sep 20 12:49:43.235: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:49:43.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6754" for this suite. 09/20/23 12:49:43.339
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:49:43.49
Sep 20 12:49:43.490: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename webhook 09/20/23 12:49:43.49
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:49:43.646
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:49:43.649
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/20/23 12:49:43.759
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/20/23 12:49:44.358
STEP: Deploying the webhook pod 09/20/23 12:49:44.366
STEP: Wait for the deployment to be ready 09/20/23 12:49:44.381
Sep 20 12:49:44.385: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep 20 12:49:46.395: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 12:49:48.485: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/20/23 12:49:50.409
STEP: Verifying the service has paired with the endpoint 09/20/23 12:49:50.45
Sep 20 12:49:51.450: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 09/20/23 12:49:51.455
STEP: create a pod that should be updated by the webhook 09/20/23 12:49:51.527
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep 20 12:49:51.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1388" for this suite. 09/20/23 12:49:52.211
STEP: Destroying namespace "webhook-1388-markers" for this suite. 09/20/23 12:49:52.215
------------------------------
â€¢ [SLOW TEST] [8.731 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:49:43.49
    Sep 20 12:49:43.490: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename webhook 09/20/23 12:49:43.49
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:49:43.646
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:49:43.649
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/20/23 12:49:43.759
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/20/23 12:49:44.358
    STEP: Deploying the webhook pod 09/20/23 12:49:44.366
    STEP: Wait for the deployment to be ready 09/20/23 12:49:44.381
    Sep 20 12:49:44.385: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Sep 20 12:49:46.395: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 12:49:48.485: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 49, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 49, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/20/23 12:49:50.409
    STEP: Verifying the service has paired with the endpoint 09/20/23 12:49:50.45
    Sep 20 12:49:51.450: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 09/20/23 12:49:51.455
    STEP: create a pod that should be updated by the webhook 09/20/23 12:49:51.527
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:49:51.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1388" for this suite. 09/20/23 12:49:52.211
    STEP: Destroying namespace "webhook-1388-markers" for this suite. 09/20/23 12:49:52.215
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:49:52.224
Sep 20 12:49:52.224: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename projected 09/20/23 12:49:52.225
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:49:52.279
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:49:52.281
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 09/20/23 12:49:52.631
Sep 20 12:49:52.641: INFO: Waiting up to 5m0s for pod "annotationupdatea94e516e-107e-4e6a-a97e-771267214ab1" in namespace "projected-510" to be "running and ready"
Sep 20 12:49:52.645: INFO: Pod "annotationupdatea94e516e-107e-4e6a-a97e-771267214ab1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.530642ms
Sep 20 12:49:52.645: INFO: The phase of Pod annotationupdatea94e516e-107e-4e6a-a97e-771267214ab1 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 12:49:55.055: INFO: Pod "annotationupdatea94e516e-107e-4e6a-a97e-771267214ab1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.414018432s
Sep 20 12:49:55.056: INFO: The phase of Pod annotationupdatea94e516e-107e-4e6a-a97e-771267214ab1 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 12:49:56.649: INFO: Pod "annotationupdatea94e516e-107e-4e6a-a97e-771267214ab1": Phase="Running", Reason="", readiness=true. Elapsed: 4.007424489s
Sep 20 12:49:56.649: INFO: The phase of Pod annotationupdatea94e516e-107e-4e6a-a97e-771267214ab1 is Running (Ready = true)
Sep 20 12:49:56.649: INFO: Pod "annotationupdatea94e516e-107e-4e6a-a97e-771267214ab1" satisfied condition "running and ready"
Sep 20 12:49:57.213: INFO: Successfully updated pod "annotationupdatea94e516e-107e-4e6a-a97e-771267214ab1"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep 20 12:50:01.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-510" for this suite. 09/20/23 12:50:01.409
------------------------------
â€¢ [SLOW TEST] [9.191 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:49:52.224
    Sep 20 12:49:52.224: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename projected 09/20/23 12:49:52.225
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:49:52.279
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:49:52.281
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 09/20/23 12:49:52.631
    Sep 20 12:49:52.641: INFO: Waiting up to 5m0s for pod "annotationupdatea94e516e-107e-4e6a-a97e-771267214ab1" in namespace "projected-510" to be "running and ready"
    Sep 20 12:49:52.645: INFO: Pod "annotationupdatea94e516e-107e-4e6a-a97e-771267214ab1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.530642ms
    Sep 20 12:49:52.645: INFO: The phase of Pod annotationupdatea94e516e-107e-4e6a-a97e-771267214ab1 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 12:49:55.055: INFO: Pod "annotationupdatea94e516e-107e-4e6a-a97e-771267214ab1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.414018432s
    Sep 20 12:49:55.056: INFO: The phase of Pod annotationupdatea94e516e-107e-4e6a-a97e-771267214ab1 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 12:49:56.649: INFO: Pod "annotationupdatea94e516e-107e-4e6a-a97e-771267214ab1": Phase="Running", Reason="", readiness=true. Elapsed: 4.007424489s
    Sep 20 12:49:56.649: INFO: The phase of Pod annotationupdatea94e516e-107e-4e6a-a97e-771267214ab1 is Running (Ready = true)
    Sep 20 12:49:56.649: INFO: Pod "annotationupdatea94e516e-107e-4e6a-a97e-771267214ab1" satisfied condition "running and ready"
    Sep 20 12:49:57.213: INFO: Successfully updated pod "annotationupdatea94e516e-107e-4e6a-a97e-771267214ab1"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:50:01.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-510" for this suite. 09/20/23 12:50:01.409
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:50:01.415
Sep 20 12:50:01.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename webhook 09/20/23 12:50:01.416
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:50:01.463
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:50:01.465
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/20/23 12:50:01.965
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/20/23 12:50:02.537
STEP: Deploying the webhook pod 09/20/23 12:50:02.541
STEP: Wait for the deployment to be ready 09/20/23 12:50:02.735
Sep 20 12:50:02.753: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep 20 12:50:04.762: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 50, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 50, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 50, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 50, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 12:50:07.225: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 50, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 50, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 50, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 50, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/20/23 12:50:08.778
STEP: Verifying the service has paired with the endpoint 09/20/23 12:50:09.057
Sep 20 12:50:10.058: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 09/20/23 12:50:10.061
STEP: create a pod that should be denied by the webhook 09/20/23 12:50:10.201
STEP: create a pod that causes the webhook to hang 09/20/23 12:50:10.214
STEP: create a configmap that should be denied by the webhook 09/20/23 12:50:20.227
STEP: create a configmap that should be admitted by the webhook 09/20/23 12:50:20.437
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 09/20/23 12:50:20.445
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 09/20/23 12:50:20.451
STEP: create a namespace that bypass the webhook 09/20/23 12:50:20.455
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 09/20/23 12:50:20.461
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep 20 12:50:20.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1468" for this suite. 09/20/23 12:50:20.54
STEP: Destroying namespace "webhook-1468-markers" for this suite. 09/20/23 12:50:20.551
------------------------------
â€¢ [SLOW TEST] [19.148 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:50:01.415
    Sep 20 12:50:01.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename webhook 09/20/23 12:50:01.416
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:50:01.463
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:50:01.465
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/20/23 12:50:01.965
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/20/23 12:50:02.537
    STEP: Deploying the webhook pod 09/20/23 12:50:02.541
    STEP: Wait for the deployment to be ready 09/20/23 12:50:02.735
    Sep 20 12:50:02.753: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Sep 20 12:50:04.762: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 50, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 50, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 50, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 50, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 12:50:07.225: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 50, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 50, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 50, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 50, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/20/23 12:50:08.778
    STEP: Verifying the service has paired with the endpoint 09/20/23 12:50:09.057
    Sep 20 12:50:10.058: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 09/20/23 12:50:10.061
    STEP: create a pod that should be denied by the webhook 09/20/23 12:50:10.201
    STEP: create a pod that causes the webhook to hang 09/20/23 12:50:10.214
    STEP: create a configmap that should be denied by the webhook 09/20/23 12:50:20.227
    STEP: create a configmap that should be admitted by the webhook 09/20/23 12:50:20.437
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 09/20/23 12:50:20.445
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 09/20/23 12:50:20.451
    STEP: create a namespace that bypass the webhook 09/20/23 12:50:20.455
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 09/20/23 12:50:20.461
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:50:20.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1468" for this suite. 09/20/23 12:50:20.54
    STEP: Destroying namespace "webhook-1468-markers" for this suite. 09/20/23 12:50:20.551
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:50:20.566
Sep 20 12:50:20.566: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename projected 09/20/23 12:50:20.567
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:50:20.637
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:50:20.642
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 09/20/23 12:50:20.652
Sep 20 12:50:20.682: INFO: Waiting up to 5m0s for pod "downwardapi-volume-56226113-c10a-4bcf-b28b-c0d30fc50b52" in namespace "projected-8262" to be "Succeeded or Failed"
Sep 20 12:50:20.685: INFO: Pod "downwardapi-volume-56226113-c10a-4bcf-b28b-c0d30fc50b52": Phase="Pending", Reason="", readiness=false. Elapsed: 3.258529ms
Sep 20 12:50:22.868: INFO: Pod "downwardapi-volume-56226113-c10a-4bcf-b28b-c0d30fc50b52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.185588114s
Sep 20 12:50:24.689: INFO: Pod "downwardapi-volume-56226113-c10a-4bcf-b28b-c0d30fc50b52": Phase="Running", Reason="", readiness=true. Elapsed: 4.006974652s
Sep 20 12:50:26.688: INFO: Pod "downwardapi-volume-56226113-c10a-4bcf-b28b-c0d30fc50b52": Phase="Running", Reason="", readiness=false. Elapsed: 6.006342381s
Sep 20 12:50:28.690: INFO: Pod "downwardapi-volume-56226113-c10a-4bcf-b28b-c0d30fc50b52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.0079383s
STEP: Saw pod success 09/20/23 12:50:28.69
Sep 20 12:50:28.690: INFO: Pod "downwardapi-volume-56226113-c10a-4bcf-b28b-c0d30fc50b52" satisfied condition "Succeeded or Failed"
Sep 20 12:50:28.693: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downwardapi-volume-56226113-c10a-4bcf-b28b-c0d30fc50b52 container client-container: <nil>
STEP: delete the pod 09/20/23 12:50:28.699
Sep 20 12:50:28.970: INFO: Waiting for pod downwardapi-volume-56226113-c10a-4bcf-b28b-c0d30fc50b52 to disappear
Sep 20 12:50:28.973: INFO: Pod downwardapi-volume-56226113-c10a-4bcf-b28b-c0d30fc50b52 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep 20 12:50:28.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8262" for this suite. 09/20/23 12:50:28.977
------------------------------
â€¢ [SLOW TEST] [8.418 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:50:20.566
    Sep 20 12:50:20.566: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename projected 09/20/23 12:50:20.567
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:50:20.637
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:50:20.642
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 09/20/23 12:50:20.652
    Sep 20 12:50:20.682: INFO: Waiting up to 5m0s for pod "downwardapi-volume-56226113-c10a-4bcf-b28b-c0d30fc50b52" in namespace "projected-8262" to be "Succeeded or Failed"
    Sep 20 12:50:20.685: INFO: Pod "downwardapi-volume-56226113-c10a-4bcf-b28b-c0d30fc50b52": Phase="Pending", Reason="", readiness=false. Elapsed: 3.258529ms
    Sep 20 12:50:22.868: INFO: Pod "downwardapi-volume-56226113-c10a-4bcf-b28b-c0d30fc50b52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.185588114s
    Sep 20 12:50:24.689: INFO: Pod "downwardapi-volume-56226113-c10a-4bcf-b28b-c0d30fc50b52": Phase="Running", Reason="", readiness=true. Elapsed: 4.006974652s
    Sep 20 12:50:26.688: INFO: Pod "downwardapi-volume-56226113-c10a-4bcf-b28b-c0d30fc50b52": Phase="Running", Reason="", readiness=false. Elapsed: 6.006342381s
    Sep 20 12:50:28.690: INFO: Pod "downwardapi-volume-56226113-c10a-4bcf-b28b-c0d30fc50b52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.0079383s
    STEP: Saw pod success 09/20/23 12:50:28.69
    Sep 20 12:50:28.690: INFO: Pod "downwardapi-volume-56226113-c10a-4bcf-b28b-c0d30fc50b52" satisfied condition "Succeeded or Failed"
    Sep 20 12:50:28.693: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downwardapi-volume-56226113-c10a-4bcf-b28b-c0d30fc50b52 container client-container: <nil>
    STEP: delete the pod 09/20/23 12:50:28.699
    Sep 20 12:50:28.970: INFO: Waiting for pod downwardapi-volume-56226113-c10a-4bcf-b28b-c0d30fc50b52 to disappear
    Sep 20 12:50:28.973: INFO: Pod downwardapi-volume-56226113-c10a-4bcf-b28b-c0d30fc50b52 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:50:28.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8262" for this suite. 09/20/23 12:50:28.977
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:50:28.988
Sep 20 12:50:28.988: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename kubectl 09/20/23 12:50:28.988
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:50:29.349
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:50:29.352
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 09/20/23 12:50:29.356
Sep 20 12:50:29.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-840 create -f -'
Sep 20 12:50:30.478: INFO: stderr: ""
Sep 20 12:50:30.478: INFO: stdout: "pod/pause created\n"
Sep 20 12:50:30.478: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Sep 20 12:50:30.478: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-840" to be "running and ready"
Sep 20 12:50:30.574: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 96.241988ms
Sep 20 12:50:30.574: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '' to be 'Running' but was 'Pending'
Sep 20 12:50:32.653: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.175707958s
Sep 20 12:50:32.653: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'mycluster-ww3cg64etuwi-node-1' to be 'Running' but was 'Pending'
Sep 20 12:50:34.587: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.109646624s
Sep 20 12:50:34.587: INFO: Pod "pause" satisfied condition "running and ready"
Sep 20 12:50:34.587: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 09/20/23 12:50:34.587
Sep 20 12:50:34.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-840 label pods pause testing-label=testing-label-value'
Sep 20 12:50:35.333: INFO: stderr: ""
Sep 20 12:50:35.333: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 09/20/23 12:50:35.333
Sep 20 12:50:35.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-840 get pod pause -L testing-label'
Sep 20 12:50:35.401: INFO: stderr: ""
Sep 20 12:50:35.401: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod 09/20/23 12:50:35.401
Sep 20 12:50:35.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-840 label pods pause testing-label-'
Sep 20 12:50:35.633: INFO: stderr: ""
Sep 20 12:50:35.633: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 09/20/23 12:50:35.633
Sep 20 12:50:35.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-840 get pod pause -L testing-label'
Sep 20 12:50:35.709: INFO: stderr: ""
Sep 20 12:50:35.709: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 09/20/23 12:50:35.709
Sep 20 12:50:35.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-840 delete --grace-period=0 --force -f -'
Sep 20 12:50:35.908: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 20 12:50:35.908: INFO: stdout: "pod \"pause\" force deleted\n"
Sep 20 12:50:35.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-840 get rc,svc -l name=pause --no-headers'
Sep 20 12:50:36.440: INFO: stderr: "No resources found in kubectl-840 namespace.\n"
Sep 20 12:50:36.440: INFO: stdout: ""
Sep 20 12:50:36.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-840 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 20 12:50:36.603: INFO: stderr: ""
Sep 20 12:50:36.603: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep 20 12:50:36.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-840" for this suite. 09/20/23 12:50:36.608
------------------------------
â€¢ [SLOW TEST] [7.789 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:50:28.988
    Sep 20 12:50:28.988: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename kubectl 09/20/23 12:50:28.988
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:50:29.349
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:50:29.352
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 09/20/23 12:50:29.356
    Sep 20 12:50:29.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-840 create -f -'
    Sep 20 12:50:30.478: INFO: stderr: ""
    Sep 20 12:50:30.478: INFO: stdout: "pod/pause created\n"
    Sep 20 12:50:30.478: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Sep 20 12:50:30.478: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-840" to be "running and ready"
    Sep 20 12:50:30.574: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 96.241988ms
    Sep 20 12:50:30.574: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '' to be 'Running' but was 'Pending'
    Sep 20 12:50:32.653: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.175707958s
    Sep 20 12:50:32.653: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'mycluster-ww3cg64etuwi-node-1' to be 'Running' but was 'Pending'
    Sep 20 12:50:34.587: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.109646624s
    Sep 20 12:50:34.587: INFO: Pod "pause" satisfied condition "running and ready"
    Sep 20 12:50:34.587: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 09/20/23 12:50:34.587
    Sep 20 12:50:34.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-840 label pods pause testing-label=testing-label-value'
    Sep 20 12:50:35.333: INFO: stderr: ""
    Sep 20 12:50:35.333: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 09/20/23 12:50:35.333
    Sep 20 12:50:35.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-840 get pod pause -L testing-label'
    Sep 20 12:50:35.401: INFO: stderr: ""
    Sep 20 12:50:35.401: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 09/20/23 12:50:35.401
    Sep 20 12:50:35.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-840 label pods pause testing-label-'
    Sep 20 12:50:35.633: INFO: stderr: ""
    Sep 20 12:50:35.633: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 09/20/23 12:50:35.633
    Sep 20 12:50:35.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-840 get pod pause -L testing-label'
    Sep 20 12:50:35.709: INFO: stderr: ""
    Sep 20 12:50:35.709: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 09/20/23 12:50:35.709
    Sep 20 12:50:35.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-840 delete --grace-period=0 --force -f -'
    Sep 20 12:50:35.908: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep 20 12:50:35.908: INFO: stdout: "pod \"pause\" force deleted\n"
    Sep 20 12:50:35.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-840 get rc,svc -l name=pause --no-headers'
    Sep 20 12:50:36.440: INFO: stderr: "No resources found in kubectl-840 namespace.\n"
    Sep 20 12:50:36.440: INFO: stdout: ""
    Sep 20 12:50:36.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-840 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Sep 20 12:50:36.603: INFO: stderr: ""
    Sep 20 12:50:36.603: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:50:36.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-840" for this suite. 09/20/23 12:50:36.608
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:50:36.777
Sep 20 12:50:36.777: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename configmap 09/20/23 12:50:36.777
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:50:37.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:50:37.566
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-8a3b0dd2-1da6-43b3-8d2d-02dcea752ad3 09/20/23 12:50:37.57
STEP: Creating a pod to test consume configMaps 09/20/23 12:50:38.043
Sep 20 12:50:38.079: INFO: Waiting up to 5m0s for pod "pod-configmaps-529425f3-de4e-4d1d-a8c3-e8d173e5470e" in namespace "configmap-6780" to be "Succeeded or Failed"
Sep 20 12:50:38.082: INFO: Pod "pod-configmaps-529425f3-de4e-4d1d-a8c3-e8d173e5470e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.969115ms
Sep 20 12:50:40.086: INFO: Pod "pod-configmaps-529425f3-de4e-4d1d-a8c3-e8d173e5470e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007046096s
Sep 20 12:50:42.117: INFO: Pod "pod-configmaps-529425f3-de4e-4d1d-a8c3-e8d173e5470e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037431638s
Sep 20 12:50:44.087: INFO: Pod "pod-configmaps-529425f3-de4e-4d1d-a8c3-e8d173e5470e": Phase="Running", Reason="", readiness=false. Elapsed: 6.007478503s
Sep 20 12:50:46.091: INFO: Pod "pod-configmaps-529425f3-de4e-4d1d-a8c3-e8d173e5470e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.011708776s
STEP: Saw pod success 09/20/23 12:50:46.091
Sep 20 12:50:46.091: INFO: Pod "pod-configmaps-529425f3-de4e-4d1d-a8c3-e8d173e5470e" satisfied condition "Succeeded or Failed"
Sep 20 12:50:46.093: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-configmaps-529425f3-de4e-4d1d-a8c3-e8d173e5470e container agnhost-container: <nil>
STEP: delete the pod 09/20/23 12:50:46.098
Sep 20 12:50:46.116: INFO: Waiting for pod pod-configmaps-529425f3-de4e-4d1d-a8c3-e8d173e5470e to disappear
Sep 20 12:50:46.119: INFO: Pod pod-configmaps-529425f3-de4e-4d1d-a8c3-e8d173e5470e no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep 20 12:50:46.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6780" for this suite. 09/20/23 12:50:46.13
------------------------------
â€¢ [SLOW TEST] [9.361 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:50:36.777
    Sep 20 12:50:36.777: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename configmap 09/20/23 12:50:36.777
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:50:37.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:50:37.566
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-8a3b0dd2-1da6-43b3-8d2d-02dcea752ad3 09/20/23 12:50:37.57
    STEP: Creating a pod to test consume configMaps 09/20/23 12:50:38.043
    Sep 20 12:50:38.079: INFO: Waiting up to 5m0s for pod "pod-configmaps-529425f3-de4e-4d1d-a8c3-e8d173e5470e" in namespace "configmap-6780" to be "Succeeded or Failed"
    Sep 20 12:50:38.082: INFO: Pod "pod-configmaps-529425f3-de4e-4d1d-a8c3-e8d173e5470e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.969115ms
    Sep 20 12:50:40.086: INFO: Pod "pod-configmaps-529425f3-de4e-4d1d-a8c3-e8d173e5470e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007046096s
    Sep 20 12:50:42.117: INFO: Pod "pod-configmaps-529425f3-de4e-4d1d-a8c3-e8d173e5470e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037431638s
    Sep 20 12:50:44.087: INFO: Pod "pod-configmaps-529425f3-de4e-4d1d-a8c3-e8d173e5470e": Phase="Running", Reason="", readiness=false. Elapsed: 6.007478503s
    Sep 20 12:50:46.091: INFO: Pod "pod-configmaps-529425f3-de4e-4d1d-a8c3-e8d173e5470e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.011708776s
    STEP: Saw pod success 09/20/23 12:50:46.091
    Sep 20 12:50:46.091: INFO: Pod "pod-configmaps-529425f3-de4e-4d1d-a8c3-e8d173e5470e" satisfied condition "Succeeded or Failed"
    Sep 20 12:50:46.093: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-configmaps-529425f3-de4e-4d1d-a8c3-e8d173e5470e container agnhost-container: <nil>
    STEP: delete the pod 09/20/23 12:50:46.098
    Sep 20 12:50:46.116: INFO: Waiting for pod pod-configmaps-529425f3-de4e-4d1d-a8c3-e8d173e5470e to disappear
    Sep 20 12:50:46.119: INFO: Pod pod-configmaps-529425f3-de4e-4d1d-a8c3-e8d173e5470e no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:50:46.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6780" for this suite. 09/20/23 12:50:46.13
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:50:46.14
Sep 20 12:50:46.140: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename container-runtime 09/20/23 12:50:46.141
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:50:46.377
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:50:46.379
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 09/20/23 12:50:46.382
STEP: wait for the container to reach Failed 09/20/23 12:50:46.389
STEP: get the container status 09/20/23 12:50:53.45
STEP: the container should be terminated 09/20/23 12:50:53.452
STEP: the termination message should be set 09/20/23 12:50:53.452
Sep 20 12:50:53.452: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 09/20/23 12:50:53.452
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Sep 20 12:50:53.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-2214" for this suite. 09/20/23 12:50:53.473
------------------------------
â€¢ [SLOW TEST] [7.344 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:50:46.14
    Sep 20 12:50:46.140: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename container-runtime 09/20/23 12:50:46.141
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:50:46.377
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:50:46.379
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 09/20/23 12:50:46.382
    STEP: wait for the container to reach Failed 09/20/23 12:50:46.389
    STEP: get the container status 09/20/23 12:50:53.45
    STEP: the container should be terminated 09/20/23 12:50:53.452
    STEP: the termination message should be set 09/20/23 12:50:53.452
    Sep 20 12:50:53.452: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 09/20/23 12:50:53.452
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:50:53.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-2214" for this suite. 09/20/23 12:50:53.473
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:50:53.485
Sep 20 12:50:53.485: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename downward-api 09/20/23 12:50:53.485
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:50:53.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:50:53.534
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 09/20/23 12:50:53.538
Sep 20 12:50:53.550: INFO: Waiting up to 5m0s for pod "downward-api-007aeca0-49a1-4c35-808c-7cd9ccb6d8de" in namespace "downward-api-7796" to be "Succeeded or Failed"
Sep 20 12:50:53.553: INFO: Pod "downward-api-007aeca0-49a1-4c35-808c-7cd9ccb6d8de": Phase="Pending", Reason="", readiness=false. Elapsed: 3.323091ms
Sep 20 12:50:55.557: INFO: Pod "downward-api-007aeca0-49a1-4c35-808c-7cd9ccb6d8de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006659588s
Sep 20 12:50:57.557: INFO: Pod "downward-api-007aeca0-49a1-4c35-808c-7cd9ccb6d8de": Phase="Running", Reason="", readiness=true. Elapsed: 4.006546717s
Sep 20 12:50:59.557: INFO: Pod "downward-api-007aeca0-49a1-4c35-808c-7cd9ccb6d8de": Phase="Running", Reason="", readiness=false. Elapsed: 6.007093108s
Sep 20 12:51:01.557: INFO: Pod "downward-api-007aeca0-49a1-4c35-808c-7cd9ccb6d8de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.00713099s
STEP: Saw pod success 09/20/23 12:51:01.557
Sep 20 12:51:01.557: INFO: Pod "downward-api-007aeca0-49a1-4c35-808c-7cd9ccb6d8de" satisfied condition "Succeeded or Failed"
Sep 20 12:51:01.559: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downward-api-007aeca0-49a1-4c35-808c-7cd9ccb6d8de container dapi-container: <nil>
STEP: delete the pod 09/20/23 12:51:01.565
Sep 20 12:51:01.661: INFO: Waiting for pod downward-api-007aeca0-49a1-4c35-808c-7cd9ccb6d8de to disappear
Sep 20 12:51:01.664: INFO: Pod downward-api-007aeca0-49a1-4c35-808c-7cd9ccb6d8de no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Sep 20 12:51:01.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7796" for this suite. 09/20/23 12:51:01.668
------------------------------
â€¢ [SLOW TEST] [8.237 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:50:53.485
    Sep 20 12:50:53.485: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename downward-api 09/20/23 12:50:53.485
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:50:53.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:50:53.534
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 09/20/23 12:50:53.538
    Sep 20 12:50:53.550: INFO: Waiting up to 5m0s for pod "downward-api-007aeca0-49a1-4c35-808c-7cd9ccb6d8de" in namespace "downward-api-7796" to be "Succeeded or Failed"
    Sep 20 12:50:53.553: INFO: Pod "downward-api-007aeca0-49a1-4c35-808c-7cd9ccb6d8de": Phase="Pending", Reason="", readiness=false. Elapsed: 3.323091ms
    Sep 20 12:50:55.557: INFO: Pod "downward-api-007aeca0-49a1-4c35-808c-7cd9ccb6d8de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006659588s
    Sep 20 12:50:57.557: INFO: Pod "downward-api-007aeca0-49a1-4c35-808c-7cd9ccb6d8de": Phase="Running", Reason="", readiness=true. Elapsed: 4.006546717s
    Sep 20 12:50:59.557: INFO: Pod "downward-api-007aeca0-49a1-4c35-808c-7cd9ccb6d8de": Phase="Running", Reason="", readiness=false. Elapsed: 6.007093108s
    Sep 20 12:51:01.557: INFO: Pod "downward-api-007aeca0-49a1-4c35-808c-7cd9ccb6d8de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.00713099s
    STEP: Saw pod success 09/20/23 12:51:01.557
    Sep 20 12:51:01.557: INFO: Pod "downward-api-007aeca0-49a1-4c35-808c-7cd9ccb6d8de" satisfied condition "Succeeded or Failed"
    Sep 20 12:51:01.559: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downward-api-007aeca0-49a1-4c35-808c-7cd9ccb6d8de container dapi-container: <nil>
    STEP: delete the pod 09/20/23 12:51:01.565
    Sep 20 12:51:01.661: INFO: Waiting for pod downward-api-007aeca0-49a1-4c35-808c-7cd9ccb6d8de to disappear
    Sep 20 12:51:01.664: INFO: Pod downward-api-007aeca0-49a1-4c35-808c-7cd9ccb6d8de no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:51:01.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7796" for this suite. 09/20/23 12:51:01.668
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:51:01.723
Sep 20 12:51:01.723: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename statefulset 09/20/23 12:51:01.724
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:51:02.224
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:51:02.227
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2537 09/20/23 12:51:02.231
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 09/20/23 12:51:02.238
Sep 20 12:51:02.494: INFO: Found 0 stateful pods, waiting for 3
Sep 20 12:51:12.497: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 12:51:12.497: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 12:51:12.497: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep 20 12:51:22.498: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 12:51:22.498: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 12:51:22.498: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 12:51:22.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-2537 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 20 12:51:22.729: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 20 12:51:22.729: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 20 12:51:22.729: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 09/20/23 12:51:32.745
Sep 20 12:51:32.763: INFO: Updating stateful set ss2
STEP: Creating a new revision 09/20/23 12:51:32.763
STEP: Updating Pods in reverse ordinal order 09/20/23 12:51:42.775
Sep 20 12:51:42.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-2537 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 12:51:43.006: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 20 12:51:43.006: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 20 12:51:43.006: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 09/20/23 12:52:03.034
Sep 20 12:52:03.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-2537 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 20 12:52:03.382: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 20 12:52:03.382: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 20 12:52:03.382: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 20 12:52:13.723: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 09/20/23 12:52:23.803
Sep 20 12:52:23.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-2537 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 12:52:24.035: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 20 12:52:24.035: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 20 12:52:24.035: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep 20 12:52:44.124: INFO: Deleting all statefulset in ns statefulset-2537
Sep 20 12:52:44.127: INFO: Scaling statefulset ss2 to 0
Sep 20 12:52:54.333: INFO: Waiting for statefulset status.replicas updated to 0
Sep 20 12:52:54.336: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep 20 12:52:54.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2537" for this suite. 09/20/23 12:52:54.354
------------------------------
â€¢ [SLOW TEST] [112.655 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:51:01.723
    Sep 20 12:51:01.723: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename statefulset 09/20/23 12:51:01.724
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:51:02.224
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:51:02.227
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2537 09/20/23 12:51:02.231
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 09/20/23 12:51:02.238
    Sep 20 12:51:02.494: INFO: Found 0 stateful pods, waiting for 3
    Sep 20 12:51:12.497: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep 20 12:51:12.497: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep 20 12:51:12.497: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
    Sep 20 12:51:22.498: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep 20 12:51:22.498: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep 20 12:51:22.498: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Sep 20 12:51:22.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-2537 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep 20 12:51:22.729: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep 20 12:51:22.729: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep 20 12:51:22.729: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 09/20/23 12:51:32.745
    Sep 20 12:51:32.763: INFO: Updating stateful set ss2
    STEP: Creating a new revision 09/20/23 12:51:32.763
    STEP: Updating Pods in reverse ordinal order 09/20/23 12:51:42.775
    Sep 20 12:51:42.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-2537 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep 20 12:51:43.006: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep 20 12:51:43.006: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep 20 12:51:43.006: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 09/20/23 12:52:03.034
    Sep 20 12:52:03.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-2537 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep 20 12:52:03.382: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep 20 12:52:03.382: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep 20 12:52:03.382: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep 20 12:52:13.723: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 09/20/23 12:52:23.803
    Sep 20 12:52:23.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-2537 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep 20 12:52:24.035: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep 20 12:52:24.035: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep 20 12:52:24.035: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep 20 12:52:44.124: INFO: Deleting all statefulset in ns statefulset-2537
    Sep 20 12:52:44.127: INFO: Scaling statefulset ss2 to 0
    Sep 20 12:52:54.333: INFO: Waiting for statefulset status.replicas updated to 0
    Sep 20 12:52:54.336: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:52:54.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2537" for this suite. 09/20/23 12:52:54.354
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:52:54.381
Sep 20 12:52:54.381: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename csiinlinevolumes 09/20/23 12:52:54.381
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:52:54.588
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:52:54.591
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 09/20/23 12:52:54.594
STEP: getting 09/20/23 12:52:54.816
STEP: listing 09/20/23 12:52:54.821
STEP: deleting 09/20/23 12:52:54.823
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Sep 20 12:52:54.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-1669" for this suite. 09/20/23 12:52:54.916
------------------------------
â€¢ [0.542 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:52:54.381
    Sep 20 12:52:54.381: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename csiinlinevolumes 09/20/23 12:52:54.381
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:52:54.588
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:52:54.591
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 09/20/23 12:52:54.594
    STEP: getting 09/20/23 12:52:54.816
    STEP: listing 09/20/23 12:52:54.821
    STEP: deleting 09/20/23 12:52:54.823
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:52:54.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-1669" for this suite. 09/20/23 12:52:54.916
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:52:54.923
Sep 20 12:52:54.923: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename cronjob 09/20/23 12:52:54.923
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:52:55.019
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:52:55.022
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 09/20/23 12:52:55.025
STEP: creating 09/20/23 12:52:55.025
STEP: getting 09/20/23 12:52:55.033
STEP: listing 09/20/23 12:52:55.036
STEP: watching 09/20/23 12:52:55.039
Sep 20 12:52:55.039: INFO: starting watch
STEP: cluster-wide listing 09/20/23 12:52:55.04
STEP: cluster-wide watching 09/20/23 12:52:55.042
Sep 20 12:52:55.042: INFO: starting watch
STEP: patching 09/20/23 12:52:55.044
STEP: updating 09/20/23 12:52:55.05
Sep 20 12:52:55.059: INFO: waiting for watch events with expected annotations
Sep 20 12:52:55.059: INFO: saw patched and updated annotations
STEP: patching /status 09/20/23 12:52:55.059
STEP: updating /status 09/20/23 12:52:55.065
STEP: get /status 09/20/23 12:52:55.069
STEP: deleting 09/20/23 12:52:55.072
STEP: deleting a collection 09/20/23 12:52:55.082
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Sep 20 12:52:55.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-1031" for this suite. 09/20/23 12:52:55.093
------------------------------
â€¢ [0.265 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:52:54.923
    Sep 20 12:52:54.923: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename cronjob 09/20/23 12:52:54.923
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:52:55.019
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:52:55.022
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 09/20/23 12:52:55.025
    STEP: creating 09/20/23 12:52:55.025
    STEP: getting 09/20/23 12:52:55.033
    STEP: listing 09/20/23 12:52:55.036
    STEP: watching 09/20/23 12:52:55.039
    Sep 20 12:52:55.039: INFO: starting watch
    STEP: cluster-wide listing 09/20/23 12:52:55.04
    STEP: cluster-wide watching 09/20/23 12:52:55.042
    Sep 20 12:52:55.042: INFO: starting watch
    STEP: patching 09/20/23 12:52:55.044
    STEP: updating 09/20/23 12:52:55.05
    Sep 20 12:52:55.059: INFO: waiting for watch events with expected annotations
    Sep 20 12:52:55.059: INFO: saw patched and updated annotations
    STEP: patching /status 09/20/23 12:52:55.059
    STEP: updating /status 09/20/23 12:52:55.065
    STEP: get /status 09/20/23 12:52:55.069
    STEP: deleting 09/20/23 12:52:55.072
    STEP: deleting a collection 09/20/23 12:52:55.082
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:52:55.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-1031" for this suite. 09/20/23 12:52:55.093
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:52:55.192
Sep 20 12:52:55.192: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename crd-webhook 09/20/23 12:52:55.193
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:52:55.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:52:55.211
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 09/20/23 12:52:55.214
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 09/20/23 12:52:55.529
STEP: Deploying the custom resource conversion webhook pod 09/20/23 12:52:55.582
STEP: Wait for the deployment to be ready 09/20/23 12:52:55.697
Sep 20 12:52:55.707: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Sep 20 12:52:57.715: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 52, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 52, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 52, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 52, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/20/23 12:52:59.799
STEP: Verifying the service has paired with the endpoint 09/20/23 12:53:00.48
Sep 20 12:53:01.481: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Sep 20 12:53:01.572: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Creating a v1 custom resource 09/20/23 12:53:04.274
STEP: Create a v2 custom resource 09/20/23 12:53:04.482
STEP: List CRs in v1 09/20/23 12:53:04.526
STEP: List CRs in v2 09/20/23 12:53:04.53
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep 20 12:53:05.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-8949" for this suite. 09/20/23 12:53:05.702
------------------------------
â€¢ [SLOW TEST] [10.690 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:52:55.192
    Sep 20 12:52:55.192: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename crd-webhook 09/20/23 12:52:55.193
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:52:55.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:52:55.211
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 09/20/23 12:52:55.214
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 09/20/23 12:52:55.529
    STEP: Deploying the custom resource conversion webhook pod 09/20/23 12:52:55.582
    STEP: Wait for the deployment to be ready 09/20/23 12:52:55.697
    Sep 20 12:52:55.707: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    Sep 20 12:52:57.715: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 52, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 52, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 52, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 52, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/20/23 12:52:59.799
    STEP: Verifying the service has paired with the endpoint 09/20/23 12:53:00.48
    Sep 20 12:53:01.481: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Sep 20 12:53:01.572: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Creating a v1 custom resource 09/20/23 12:53:04.274
    STEP: Create a v2 custom resource 09/20/23 12:53:04.482
    STEP: List CRs in v1 09/20/23 12:53:04.526
    STEP: List CRs in v2 09/20/23 12:53:04.53
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:53:05.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-8949" for this suite. 09/20/23 12:53:05.702
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:53:05.883
Sep 20 12:53:05.884: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename pods 09/20/23 12:53:05.884
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:53:05.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:53:05.96
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 09/20/23 12:53:05.965
STEP: submitting the pod to kubernetes 09/20/23 12:53:05.965
STEP: verifying QOS class is set on the pod 09/20/23 12:53:05.978
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Sep 20 12:53:05.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9592" for this suite. 09/20/23 12:53:05.99
------------------------------
â€¢ [0.117 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:53:05.883
    Sep 20 12:53:05.884: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename pods 09/20/23 12:53:05.884
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:53:05.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:53:05.96
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 09/20/23 12:53:05.965
    STEP: submitting the pod to kubernetes 09/20/23 12:53:05.965
    STEP: verifying QOS class is set on the pod 09/20/23 12:53:05.978
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:53:05.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9592" for this suite. 09/20/23 12:53:05.99
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:53:06
Sep 20 12:53:06.000: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename webhook 09/20/23 12:53:06.001
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:53:06.045
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:53:06.048
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/20/23 12:53:06.069
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/20/23 12:53:06.508
STEP: Deploying the webhook pod 09/20/23 12:53:06.512
STEP: Wait for the deployment to be ready 09/20/23 12:53:06.529
Sep 20 12:53:06.534: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep 20 12:53:08.549: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 53, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 53, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 53, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 53, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/20/23 12:53:10.579
STEP: Verifying the service has paired with the endpoint 09/20/23 12:53:10.765
Sep 20 12:53:11.766: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Sep 20 12:53:11.952: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8143-crds.webhook.example.com via the AdmissionRegistration API 09/20/23 12:53:12.545
STEP: Creating a custom resource that should be mutated by the webhook 09/20/23 12:53:12.6
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep 20 12:53:15.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6151" for this suite. 09/20/23 12:53:16.229
STEP: Destroying namespace "webhook-6151-markers" for this suite. 09/20/23 12:53:16.271
------------------------------
â€¢ [SLOW TEST] [10.335 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:53:06
    Sep 20 12:53:06.000: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename webhook 09/20/23 12:53:06.001
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:53:06.045
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:53:06.048
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/20/23 12:53:06.069
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/20/23 12:53:06.508
    STEP: Deploying the webhook pod 09/20/23 12:53:06.512
    STEP: Wait for the deployment to be ready 09/20/23 12:53:06.529
    Sep 20 12:53:06.534: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Sep 20 12:53:08.549: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 53, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 53, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 53, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 53, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/20/23 12:53:10.579
    STEP: Verifying the service has paired with the endpoint 09/20/23 12:53:10.765
    Sep 20 12:53:11.766: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Sep 20 12:53:11.952: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8143-crds.webhook.example.com via the AdmissionRegistration API 09/20/23 12:53:12.545
    STEP: Creating a custom resource that should be mutated by the webhook 09/20/23 12:53:12.6
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:53:15.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6151" for this suite. 09/20/23 12:53:16.229
    STEP: Destroying namespace "webhook-6151-markers" for this suite. 09/20/23 12:53:16.271
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:53:16.337
Sep 20 12:53:16.338: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename var-expansion 09/20/23 12:53:16.338
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:53:16.553
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:53:16.556
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 09/20/23 12:53:16.827
Sep 20 12:53:16.837: INFO: Waiting up to 2m0s for pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09" in namespace "var-expansion-8811" to be "running"
Sep 20 12:53:16.843: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 6.681799ms
Sep 20 12:53:18.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009997847s
Sep 20 12:53:20.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010433881s
Sep 20 12:53:22.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01031607s
Sep 20 12:53:24.849: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011829912s
Sep 20 12:53:26.852: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 10.014814077s
Sep 20 12:53:28.849: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 12.011832111s
Sep 20 12:53:31.499: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 14.661969418s
Sep 20 12:53:32.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 16.009939466s
Sep 20 12:53:34.846: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 18.00955892s
Sep 20 12:53:36.849: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 20.01273289s
Sep 20 12:53:38.848: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 22.011509047s
Sep 20 12:53:41.003: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 24.166385266s
Sep 20 12:53:42.854: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 26.017340176s
Sep 20 12:53:44.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 28.010228236s
Sep 20 12:53:46.849: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 30.012404509s
Sep 20 12:53:48.848: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 32.011627335s
Sep 20 12:53:50.848: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 34.011104181s
Sep 20 12:53:52.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 36.010564676s
Sep 20 12:53:54.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 38.010450772s
Sep 20 12:53:56.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 40.010497762s
Sep 20 12:53:58.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 42.01069213s
Sep 20 12:54:00.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 44.010026295s
Sep 20 12:54:02.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 46.010495522s
Sep 20 12:54:04.850: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 48.013734305s
Sep 20 12:54:06.848: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 50.011158872s
Sep 20 12:54:08.848: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 52.011557406s
Sep 20 12:54:10.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 54.010433899s
Sep 20 12:54:12.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 56.010189051s
Sep 20 12:54:14.853: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 58.015880866s
Sep 20 12:54:16.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.010300671s
Sep 20 12:54:18.950: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.113472196s
Sep 20 12:54:20.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.01071748s
Sep 20 12:54:22.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.010397397s
Sep 20 12:54:24.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.010472651s
Sep 20 12:54:26.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.010689175s
Sep 20 12:54:28.916: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.079753884s
Sep 20 12:54:30.848: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.011382709s
Sep 20 12:54:32.849: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.012497941s
Sep 20 12:54:35.128: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.291150578s
Sep 20 12:54:36.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.010307897s
Sep 20 12:54:38.849: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.011923792s
Sep 20 12:54:40.851: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.014061433s
Sep 20 12:54:42.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.009918346s
Sep 20 12:54:44.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.010696376s
Sep 20 12:54:46.846: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.009447642s
Sep 20 12:54:48.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.010445814s
Sep 20 12:54:50.849: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.011963056s
Sep 20 12:54:52.848: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.01084605s
Sep 20 12:54:54.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.010235762s
Sep 20 12:54:56.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.010251973s
Sep 20 12:54:58.848: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.011077489s
Sep 20 12:55:01.064: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.227278901s
Sep 20 12:55:03.122: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.285588361s
Sep 20 12:55:04.875: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.038757746s
Sep 20 12:55:06.846: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.009709378s
Sep 20 12:55:08.848: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.010924658s
Sep 20 12:55:10.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.01020242s
Sep 20 12:55:12.849: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.012183072s
Sep 20 12:55:14.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.010339539s
Sep 20 12:55:16.942: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.105301136s
Sep 20 12:55:16.944: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.10772373s
STEP: updating the pod 09/20/23 12:55:16.944
Sep 20 12:55:17.455: INFO: Successfully updated pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09"
STEP: waiting for pod running 09/20/23 12:55:17.455
Sep 20 12:55:17.455: INFO: Waiting up to 2m0s for pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09" in namespace "var-expansion-8811" to be "running"
Sep 20 12:55:17.457: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.214823ms
Sep 20 12:55:19.461: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Running", Reason="", readiness=true. Elapsed: 2.005670366s
Sep 20 12:55:19.461: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09" satisfied condition "running"
STEP: deleting the pod gracefully 09/20/23 12:55:19.461
Sep 20 12:55:19.461: INFO: Deleting pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09" in namespace "var-expansion-8811"
Sep 20 12:55:19.468: INFO: Wait up to 5m0s for pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep 20 12:55:51.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8811" for this suite. 09/20/23 12:55:51.478
------------------------------
â€¢ [SLOW TEST] [155.148 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:53:16.337
    Sep 20 12:53:16.338: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename var-expansion 09/20/23 12:53:16.338
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:53:16.553
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:53:16.556
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 09/20/23 12:53:16.827
    Sep 20 12:53:16.837: INFO: Waiting up to 2m0s for pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09" in namespace "var-expansion-8811" to be "running"
    Sep 20 12:53:16.843: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 6.681799ms
    Sep 20 12:53:18.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009997847s
    Sep 20 12:53:20.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010433881s
    Sep 20 12:53:22.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01031607s
    Sep 20 12:53:24.849: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011829912s
    Sep 20 12:53:26.852: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 10.014814077s
    Sep 20 12:53:28.849: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 12.011832111s
    Sep 20 12:53:31.499: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 14.661969418s
    Sep 20 12:53:32.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 16.009939466s
    Sep 20 12:53:34.846: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 18.00955892s
    Sep 20 12:53:36.849: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 20.01273289s
    Sep 20 12:53:38.848: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 22.011509047s
    Sep 20 12:53:41.003: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 24.166385266s
    Sep 20 12:53:42.854: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 26.017340176s
    Sep 20 12:53:44.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 28.010228236s
    Sep 20 12:53:46.849: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 30.012404509s
    Sep 20 12:53:48.848: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 32.011627335s
    Sep 20 12:53:50.848: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 34.011104181s
    Sep 20 12:53:52.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 36.010564676s
    Sep 20 12:53:54.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 38.010450772s
    Sep 20 12:53:56.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 40.010497762s
    Sep 20 12:53:58.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 42.01069213s
    Sep 20 12:54:00.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 44.010026295s
    Sep 20 12:54:02.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 46.010495522s
    Sep 20 12:54:04.850: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 48.013734305s
    Sep 20 12:54:06.848: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 50.011158872s
    Sep 20 12:54:08.848: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 52.011557406s
    Sep 20 12:54:10.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 54.010433899s
    Sep 20 12:54:12.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 56.010189051s
    Sep 20 12:54:14.853: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 58.015880866s
    Sep 20 12:54:16.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.010300671s
    Sep 20 12:54:18.950: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.113472196s
    Sep 20 12:54:20.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.01071748s
    Sep 20 12:54:22.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.010397397s
    Sep 20 12:54:24.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.010472651s
    Sep 20 12:54:26.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.010689175s
    Sep 20 12:54:28.916: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.079753884s
    Sep 20 12:54:30.848: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.011382709s
    Sep 20 12:54:32.849: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.012497941s
    Sep 20 12:54:35.128: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.291150578s
    Sep 20 12:54:36.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.010307897s
    Sep 20 12:54:38.849: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.011923792s
    Sep 20 12:54:40.851: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.014061433s
    Sep 20 12:54:42.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.009918346s
    Sep 20 12:54:44.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.010696376s
    Sep 20 12:54:46.846: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.009447642s
    Sep 20 12:54:48.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.010445814s
    Sep 20 12:54:50.849: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.011963056s
    Sep 20 12:54:52.848: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.01084605s
    Sep 20 12:54:54.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.010235762s
    Sep 20 12:54:56.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.010251973s
    Sep 20 12:54:58.848: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.011077489s
    Sep 20 12:55:01.064: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.227278901s
    Sep 20 12:55:03.122: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.285588361s
    Sep 20 12:55:04.875: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.038757746s
    Sep 20 12:55:06.846: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.009709378s
    Sep 20 12:55:08.848: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.010924658s
    Sep 20 12:55:10.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.01020242s
    Sep 20 12:55:12.849: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.012183072s
    Sep 20 12:55:14.847: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.010339539s
    Sep 20 12:55:16.942: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.105301136s
    Sep 20 12:55:16.944: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.10772373s
    STEP: updating the pod 09/20/23 12:55:16.944
    Sep 20 12:55:17.455: INFO: Successfully updated pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09"
    STEP: waiting for pod running 09/20/23 12:55:17.455
    Sep 20 12:55:17.455: INFO: Waiting up to 2m0s for pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09" in namespace "var-expansion-8811" to be "running"
    Sep 20 12:55:17.457: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.214823ms
    Sep 20 12:55:19.461: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09": Phase="Running", Reason="", readiness=true. Elapsed: 2.005670366s
    Sep 20 12:55:19.461: INFO: Pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09" satisfied condition "running"
    STEP: deleting the pod gracefully 09/20/23 12:55:19.461
    Sep 20 12:55:19.461: INFO: Deleting pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09" in namespace "var-expansion-8811"
    Sep 20 12:55:19.468: INFO: Wait up to 5m0s for pod "var-expansion-3a076982-6243-4332-be8e-f04373c86f09" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:55:51.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8811" for this suite. 09/20/23 12:55:51.478
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:55:51.487
Sep 20 12:55:51.487: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename webhook 09/20/23 12:55:51.488
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:55:51.509
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:55:51.511
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/20/23 12:55:52.576
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/20/23 12:55:52.816
STEP: Deploying the webhook pod 09/20/23 12:55:53.199
STEP: Wait for the deployment to be ready 09/20/23 12:55:53.336
Sep 20 12:55:53.345: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 12:55:55.352: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 55, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 55, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 55, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 55, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/20/23 12:55:57.357
STEP: Verifying the service has paired with the endpoint 09/20/23 12:55:57.37
Sep 20 12:55:58.370: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Sep 20 12:55:58.447: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7478-crds.webhook.example.com via the AdmissionRegistration API 09/20/23 12:55:59.049
STEP: Creating a custom resource that should be mutated by the webhook 09/20/23 12:55:59.166
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep 20 12:56:01.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5629" for this suite. 09/20/23 12:56:01.815
STEP: Destroying namespace "webhook-5629-markers" for this suite. 09/20/23 12:56:01.943
------------------------------
â€¢ [SLOW TEST] [10.485 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:55:51.487
    Sep 20 12:55:51.487: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename webhook 09/20/23 12:55:51.488
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:55:51.509
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:55:51.511
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/20/23 12:55:52.576
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/20/23 12:55:52.816
    STEP: Deploying the webhook pod 09/20/23 12:55:53.199
    STEP: Wait for the deployment to be ready 09/20/23 12:55:53.336
    Sep 20 12:55:53.345: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Sep 20 12:55:55.352: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 12, 55, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 55, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 12, 55, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 12, 55, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/20/23 12:55:57.357
    STEP: Verifying the service has paired with the endpoint 09/20/23 12:55:57.37
    Sep 20 12:55:58.370: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Sep 20 12:55:58.447: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7478-crds.webhook.example.com via the AdmissionRegistration API 09/20/23 12:55:59.049
    STEP: Creating a custom resource that should be mutated by the webhook 09/20/23 12:55:59.166
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:56:01.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5629" for this suite. 09/20/23 12:56:01.815
    STEP: Destroying namespace "webhook-5629-markers" for this suite. 09/20/23 12:56:01.943
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:56:01.974
Sep 20 12:56:01.974: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename crd-publish-openapi 09/20/23 12:56:01.974
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:56:02.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:56:02.551
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 09/20/23 12:56:02.554
Sep 20 12:56:02.554: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 09/20/23 12:56:12.231
Sep 20 12:56:12.231: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
Sep 20 12:56:15.088: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep 20 12:56:24.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3318" for this suite. 09/20/23 12:56:24.136
------------------------------
â€¢ [SLOW TEST] [22.215 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:56:01.974
    Sep 20 12:56:01.974: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename crd-publish-openapi 09/20/23 12:56:01.974
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:56:02.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:56:02.551
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 09/20/23 12:56:02.554
    Sep 20 12:56:02.554: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 09/20/23 12:56:12.231
    Sep 20 12:56:12.231: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    Sep 20 12:56:15.088: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep 20 12:56:24.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3318" for this suite. 09/20/23 12:56:24.136
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 12:56:24.192
Sep 20 12:56:24.192: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename sched-pred 09/20/23 12:56:24.193
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:56:24.271
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:56:24.275
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Sep 20 12:56:24.282: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 20 12:56:24.294: INFO: Waiting for terminating namespaces to be deleted...
Sep 20 12:56:24.300: INFO: 
Logging pods the apiserver thinks is on node mycluster-ww3cg64etuwi-node-0 before test
Sep 20 12:56:24.310: INFO: csi-cinder-nodeplugin-k6qp5 from kube-system started at 2023-09-20 11:51:32 +0000 UTC (3 container statuses recorded)
Sep 20 12:56:24.310: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Sep 20 12:56:24.310: INFO: 	Container liveness-probe ready: true, restart count 0
Sep 20 12:56:24.310: INFO: 	Container node-driver-registrar ready: true, restart count 0
Sep 20 12:56:24.310: INFO: kube-flannel-ds-chfqx from kube-system started at 2023-09-20 11:51:32 +0000 UTC (1 container statuses recorded)
Sep 20 12:56:24.310: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 20 12:56:24.310: INFO: npd-ntx42 from kube-system started at 2023-09-20 11:52:06 +0000 UTC (1 container statuses recorded)
Sep 20 12:56:24.310: INFO: 	Container node-problem-detector ready: true, restart count 0
Sep 20 12:56:24.310: INFO: sonobuoy-e2e-job-2c0bc69190d741e4 from sonobuoy started at 2023-09-20 12:04:03 +0000 UTC (2 container statuses recorded)
Sep 20 12:56:24.310: INFO: 	Container e2e ready: true, restart count 0
Sep 20 12:56:24.311: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 12:56:24.311: INFO: sonobuoy-systemd-logs-daemon-set-6128a9cd7eba4f1a-r9rqh from sonobuoy started at 2023-09-20 12:04:03 +0000 UTC (2 container statuses recorded)
Sep 20 12:56:24.311: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 12:56:24.311: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 20 12:56:24.311: INFO: 
Logging pods the apiserver thinks is on node mycluster-ww3cg64etuwi-node-1 before test
Sep 20 12:56:24.324: INFO: csi-cinder-nodeplugin-r6zgs from kube-system started at 2023-09-20 11:51:30 +0000 UTC (3 container statuses recorded)
Sep 20 12:56:24.324: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Sep 20 12:56:24.324: INFO: 	Container liveness-probe ready: true, restart count 0
Sep 20 12:56:24.324: INFO: 	Container node-driver-registrar ready: true, restart count 0
Sep 20 12:56:24.324: INFO: kube-flannel-ds-nc8g9 from kube-system started at 2023-09-20 12:44:25 +0000 UTC (1 container statuses recorded)
Sep 20 12:56:24.324: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 20 12:56:24.324: INFO: npd-dqxrp from kube-system started at 2023-09-20 11:51:59 +0000 UTC (1 container statuses recorded)
Sep 20 12:56:24.324: INFO: 	Container node-problem-detector ready: true, restart count 0
Sep 20 12:56:24.324: INFO: sonobuoy-systemd-logs-daemon-set-6128a9cd7eba4f1a-8k2ck from sonobuoy started at 2023-09-20 12:04:03 +0000 UTC (2 container statuses recorded)
Sep 20 12:56:24.324: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 12:56:24.324: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 20 12:56:24.324: INFO: 
Logging pods the apiserver thinks is on node mycluster-ww3cg64etuwi-node-2 before test
Sep 20 12:56:24.335: INFO: csi-cinder-nodeplugin-qcqrp from kube-system started at 2023-09-20 11:51:31 +0000 UTC (3 container statuses recorded)
Sep 20 12:56:24.335: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Sep 20 12:56:24.335: INFO: 	Container liveness-probe ready: true, restart count 0
Sep 20 12:56:24.335: INFO: 	Container node-driver-registrar ready: true, restart count 0
Sep 20 12:56:24.335: INFO: kube-dns-autoscaler-86977fd5fc-l9tnc from kube-system started at 2023-09-20 12:43:53 +0000 UTC (1 container statuses recorded)
Sep 20 12:56:24.335: INFO: 	Container autoscaler ready: true, restart count 0
Sep 20 12:56:24.335: INFO: kube-flannel-ds-ncx55 from kube-system started at 2023-09-20 11:51:30 +0000 UTC (1 container statuses recorded)
Sep 20 12:56:24.335: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 20 12:56:24.335: INFO: magnum-metrics-server-6b5dcd575f-gdlm2 from kube-system started at 2023-09-20 12:43:53 +0000 UTC (1 container statuses recorded)
Sep 20 12:56:24.335: INFO: 	Container metrics-server ready: true, restart count 0
Sep 20 12:56:24.335: INFO: npd-k978m from kube-system started at 2023-09-20 11:52:36 +0000 UTC (1 container statuses recorded)
Sep 20 12:56:24.335: INFO: 	Container node-problem-detector ready: true, restart count 0
Sep 20 12:56:24.335: INFO: sonobuoy from sonobuoy started at 2023-09-20 12:03:39 +0000 UTC (1 container statuses recorded)
Sep 20 12:56:24.335: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 20 12:56:24.335: INFO: sonobuoy-systemd-logs-daemon-set-6128a9cd7eba4f1a-bw4zv from sonobuoy started at 2023-09-20 12:04:03 +0000 UTC (2 container statuses recorded)
Sep 20 12:56:24.335: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 12:56:24.335: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 09/20/23 12:56:24.335
Sep 20 12:56:24.370: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7109" to be "running"
Sep 20 12:56:24.378: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 7.648099ms
Sep 20 12:56:26.389: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018970946s
Sep 20 12:56:28.382: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.011804152s
Sep 20 12:56:28.382: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 09/20/23 12:56:28.385
STEP: Trying to apply a random label on the found node. 09/20/23 12:56:28.626
STEP: verifying the node has the label kubernetes.io/e2e-1eace3b2-af44-483f-9dc4-2cfc0cda0e10 95 09/20/23 12:56:28.864
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 09/20/23 12:56:29.025
Sep 20 12:56:29.036: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-7109" to be "not pending"
Sep 20 12:56:29.046: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.629982ms
Sep 20 12:56:31.050: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01402317s
Sep 20 12:56:33.050: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 4.01432045s
Sep 20 12:56:33.050: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.10.64 on the node which pod4 resides and expect not scheduled 09/20/23 12:56:33.05
Sep 20 12:56:33.061: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-7109" to be "not pending"
Sep 20 12:56:33.067: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.815811ms
Sep 20 12:56:35.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011460712s
Sep 20 12:56:37.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011977804s
Sep 20 12:56:39.076: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014944792s
Sep 20 12:56:41.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013446178s
Sep 20 12:56:43.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.012906952s
Sep 20 12:56:45.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.012531754s
Sep 20 12:56:47.071: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010872637s
Sep 20 12:56:49.088: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.027153199s
Sep 20 12:56:51.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.0125425s
Sep 20 12:56:53.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.03080684s
Sep 20 12:56:55.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.01386484s
Sep 20 12:56:57.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.012103521s
Sep 20 12:56:59.076: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.01516082s
Sep 20 12:57:01.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.013719141s
Sep 20 12:57:03.077: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.016146435s
Sep 20 12:57:05.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.013678484s
Sep 20 12:57:07.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.0122227s
Sep 20 12:57:09.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.012364736s
Sep 20 12:57:11.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.012509037s
Sep 20 12:57:13.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.011587549s
Sep 20 12:57:15.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.011010612s
Sep 20 12:57:17.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.013198262s
Sep 20 12:57:19.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.012612448s
Sep 20 12:57:21.078: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.017099049s
Sep 20 12:57:23.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.012853179s
Sep 20 12:57:25.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.013082229s
Sep 20 12:57:27.071: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.010908903s
Sep 20 12:57:29.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.012479662s
Sep 20 12:57:31.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.012178083s
Sep 20 12:57:33.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.012610095s
Sep 20 12:57:35.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.029684693s
Sep 20 12:57:37.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.013012432s
Sep 20 12:57:39.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.011876821s
Sep 20 12:57:41.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.012923402s
Sep 20 12:57:43.075: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.01422907s
Sep 20 12:57:45.135: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.074042899s
Sep 20 12:57:47.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.011081489s
Sep 20 12:57:49.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.013367004s
Sep 20 12:57:51.076: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.015366781s
Sep 20 12:57:53.071: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.010511492s
Sep 20 12:57:55.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.011475077s
Sep 20 12:57:57.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.012815802s
Sep 20 12:57:59.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.012852631s
Sep 20 12:58:01.075: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.01413748s
Sep 20 12:58:03.415: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.3545707s
Sep 20 12:58:05.071: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.010439502s
Sep 20 12:58:07.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.011858083s
Sep 20 12:58:09.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.011503674s
Sep 20 12:58:11.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.012199454s
Sep 20 12:58:13.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.013908502s
Sep 20 12:58:15.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.020041686s
Sep 20 12:58:17.312: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.250956601s
Sep 20 12:58:19.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.011686171s
Sep 20 12:58:21.093: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.032320916s
Sep 20 12:58:23.206: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.145880502s
Sep 20 12:58:25.076: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.015767697s
Sep 20 12:58:27.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.01365234s
Sep 20 12:58:29.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.011466291s
Sep 20 12:58:31.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.012407475s
Sep 20 12:58:33.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.013702323s
Sep 20 12:58:35.152: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.091436955s
Sep 20 12:58:37.084: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.023216867s
Sep 20 12:58:39.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.012983827s
Sep 20 12:58:41.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.011098824s
Sep 20 12:58:43.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.012325394s
Sep 20 12:58:45.079: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.01830086s
Sep 20 12:58:47.088: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.026939835s
Sep 20 12:58:49.167: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.106706464s
Sep 20 12:58:51.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.012241852s
Sep 20 12:58:53.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.011418389s
Sep 20 12:58:55.079: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.018820664s
Sep 20 12:58:57.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.012583432s
Sep 20 12:58:59.078: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.017174561s
Sep 20 12:59:01.071: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.010903455s
Sep 20 12:59:03.075: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.013988195s
Sep 20 12:59:05.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.011162851s
Sep 20 12:59:07.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.012011759s
Sep 20 12:59:09.076: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.015031999s
Sep 20 12:59:11.075: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.013962503s
Sep 20 12:59:13.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.011579661s
Sep 20 12:59:15.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.080125623s
Sep 20 12:59:17.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.012676828s
Sep 20 12:59:19.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.012141318s
Sep 20 12:59:21.078: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.017375467s
Sep 20 12:59:23.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.013267998s
Sep 20 12:59:25.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.01389588s
Sep 20 12:59:27.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.012626808s
Sep 20 12:59:29.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.176645209s
Sep 20 12:59:31.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.012301022s
Sep 20 12:59:33.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.012451935s
Sep 20 12:59:35.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.012499733s
Sep 20 12:59:37.071: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.010722465s
Sep 20 12:59:39.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.013896463s
Sep 20 12:59:41.077: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.016274181s
Sep 20 12:59:43.082: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.021736791s
Sep 20 12:59:45.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.030422353s
Sep 20 12:59:47.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.012717477s
Sep 20 12:59:49.075: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.014281932s
Sep 20 12:59:51.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.013913657s
Sep 20 12:59:53.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.011652567s
Sep 20 12:59:55.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.013121844s
Sep 20 12:59:57.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.631761008s
Sep 20 12:59:59.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.013127703s
Sep 20 13:00:01.294: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.233123758s
Sep 20 13:00:03.076: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.015177794s
Sep 20 13:00:05.076: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.015143899s
Sep 20 13:00:07.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.011099939s
Sep 20 13:00:09.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.012874281s
Sep 20 13:00:11.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.011913298s
Sep 20 13:00:13.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.012709569s
Sep 20 13:00:15.075: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.014157056s
Sep 20 13:00:17.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.012784378s
Sep 20 13:00:19.071: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.010347446s
Sep 20 13:00:21.086: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.025822269s
Sep 20 13:00:23.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.012359364s
Sep 20 13:00:25.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.011774912s
Sep 20 13:00:27.071: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.010313628s
Sep 20 13:00:29.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.012340065s
Sep 20 13:00:31.075: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.014199697s
Sep 20 13:00:33.076: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.015158203s
Sep 20 13:00:35.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.011644051s
Sep 20 13:00:38.151: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m5.090328889s
Sep 20 13:00:39.473: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.412385341s
Sep 20 13:00:41.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.012212538s
Sep 20 13:00:43.201: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.140475857s
Sep 20 13:00:45.075: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.014019941s
Sep 20 13:00:47.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.013240521s
Sep 20 13:00:49.226: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.165395533s
Sep 20 13:00:51.108: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.047497991s
Sep 20 13:00:53.077: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.016022931s
Sep 20 13:00:55.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.019299644s
Sep 20 13:00:57.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.011305061s
Sep 20 13:00:59.075: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.014853605s
Sep 20 13:01:01.308: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.247338459s
Sep 20 13:01:03.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.082548101s
Sep 20 13:01:05.131: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.070344398s
Sep 20 13:01:07.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.011723283s
Sep 20 13:01:09.119: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.058876399s
Sep 20 13:01:11.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.011612253s
Sep 20 13:01:13.082: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.021056705s
Sep 20 13:01:15.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.013348802s
Sep 20 13:01:17.075: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.014067897s
Sep 20 13:01:19.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.011570129s
Sep 20 13:01:21.076: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.015295256s
Sep 20 13:01:23.137: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.076096918s
Sep 20 13:01:25.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.449515622s
Sep 20 13:01:27.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.012743396s
Sep 20 13:01:29.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.012474789s
Sep 20 13:01:31.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.012344402s
Sep 20 13:01:33.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.012162438s
Sep 20 13:01:33.075: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.014511053s
STEP: removing the label kubernetes.io/e2e-1eace3b2-af44-483f-9dc4-2cfc0cda0e10 off the node mycluster-ww3cg64etuwi-node-1 09/20/23 13:01:33.075
STEP: verifying the node doesn't have the label kubernetes.io/e2e-1eace3b2-af44-483f-9dc4-2cfc0cda0e10 09/20/23 13:01:33.592
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Sep 20 13:01:33.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-7109" for this suite. 09/20/23 13:01:33.606
------------------------------
â€¢ [SLOW TEST] [310.000 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 12:56:24.192
    Sep 20 12:56:24.192: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename sched-pred 09/20/23 12:56:24.193
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 12:56:24.271
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 12:56:24.275
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Sep 20 12:56:24.282: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Sep 20 12:56:24.294: INFO: Waiting for terminating namespaces to be deleted...
    Sep 20 12:56:24.300: INFO: 
    Logging pods the apiserver thinks is on node mycluster-ww3cg64etuwi-node-0 before test
    Sep 20 12:56:24.310: INFO: csi-cinder-nodeplugin-k6qp5 from kube-system started at 2023-09-20 11:51:32 +0000 UTC (3 container statuses recorded)
    Sep 20 12:56:24.310: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Sep 20 12:56:24.310: INFO: 	Container liveness-probe ready: true, restart count 0
    Sep 20 12:56:24.310: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Sep 20 12:56:24.310: INFO: kube-flannel-ds-chfqx from kube-system started at 2023-09-20 11:51:32 +0000 UTC (1 container statuses recorded)
    Sep 20 12:56:24.310: INFO: 	Container kube-flannel ready: true, restart count 0
    Sep 20 12:56:24.310: INFO: npd-ntx42 from kube-system started at 2023-09-20 11:52:06 +0000 UTC (1 container statuses recorded)
    Sep 20 12:56:24.310: INFO: 	Container node-problem-detector ready: true, restart count 0
    Sep 20 12:56:24.310: INFO: sonobuoy-e2e-job-2c0bc69190d741e4 from sonobuoy started at 2023-09-20 12:04:03 +0000 UTC (2 container statuses recorded)
    Sep 20 12:56:24.310: INFO: 	Container e2e ready: true, restart count 0
    Sep 20 12:56:24.311: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep 20 12:56:24.311: INFO: sonobuoy-systemd-logs-daemon-set-6128a9cd7eba4f1a-r9rqh from sonobuoy started at 2023-09-20 12:04:03 +0000 UTC (2 container statuses recorded)
    Sep 20 12:56:24.311: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep 20 12:56:24.311: INFO: 	Container systemd-logs ready: true, restart count 0
    Sep 20 12:56:24.311: INFO: 
    Logging pods the apiserver thinks is on node mycluster-ww3cg64etuwi-node-1 before test
    Sep 20 12:56:24.324: INFO: csi-cinder-nodeplugin-r6zgs from kube-system started at 2023-09-20 11:51:30 +0000 UTC (3 container statuses recorded)
    Sep 20 12:56:24.324: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Sep 20 12:56:24.324: INFO: 	Container liveness-probe ready: true, restart count 0
    Sep 20 12:56:24.324: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Sep 20 12:56:24.324: INFO: kube-flannel-ds-nc8g9 from kube-system started at 2023-09-20 12:44:25 +0000 UTC (1 container statuses recorded)
    Sep 20 12:56:24.324: INFO: 	Container kube-flannel ready: true, restart count 0
    Sep 20 12:56:24.324: INFO: npd-dqxrp from kube-system started at 2023-09-20 11:51:59 +0000 UTC (1 container statuses recorded)
    Sep 20 12:56:24.324: INFO: 	Container node-problem-detector ready: true, restart count 0
    Sep 20 12:56:24.324: INFO: sonobuoy-systemd-logs-daemon-set-6128a9cd7eba4f1a-8k2ck from sonobuoy started at 2023-09-20 12:04:03 +0000 UTC (2 container statuses recorded)
    Sep 20 12:56:24.324: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep 20 12:56:24.324: INFO: 	Container systemd-logs ready: true, restart count 0
    Sep 20 12:56:24.324: INFO: 
    Logging pods the apiserver thinks is on node mycluster-ww3cg64etuwi-node-2 before test
    Sep 20 12:56:24.335: INFO: csi-cinder-nodeplugin-qcqrp from kube-system started at 2023-09-20 11:51:31 +0000 UTC (3 container statuses recorded)
    Sep 20 12:56:24.335: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Sep 20 12:56:24.335: INFO: 	Container liveness-probe ready: true, restart count 0
    Sep 20 12:56:24.335: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Sep 20 12:56:24.335: INFO: kube-dns-autoscaler-86977fd5fc-l9tnc from kube-system started at 2023-09-20 12:43:53 +0000 UTC (1 container statuses recorded)
    Sep 20 12:56:24.335: INFO: 	Container autoscaler ready: true, restart count 0
    Sep 20 12:56:24.335: INFO: kube-flannel-ds-ncx55 from kube-system started at 2023-09-20 11:51:30 +0000 UTC (1 container statuses recorded)
    Sep 20 12:56:24.335: INFO: 	Container kube-flannel ready: true, restart count 0
    Sep 20 12:56:24.335: INFO: magnum-metrics-server-6b5dcd575f-gdlm2 from kube-system started at 2023-09-20 12:43:53 +0000 UTC (1 container statuses recorded)
    Sep 20 12:56:24.335: INFO: 	Container metrics-server ready: true, restart count 0
    Sep 20 12:56:24.335: INFO: npd-k978m from kube-system started at 2023-09-20 11:52:36 +0000 UTC (1 container statuses recorded)
    Sep 20 12:56:24.335: INFO: 	Container node-problem-detector ready: true, restart count 0
    Sep 20 12:56:24.335: INFO: sonobuoy from sonobuoy started at 2023-09-20 12:03:39 +0000 UTC (1 container statuses recorded)
    Sep 20 12:56:24.335: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Sep 20 12:56:24.335: INFO: sonobuoy-systemd-logs-daemon-set-6128a9cd7eba4f1a-bw4zv from sonobuoy started at 2023-09-20 12:04:03 +0000 UTC (2 container statuses recorded)
    Sep 20 12:56:24.335: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep 20 12:56:24.335: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 09/20/23 12:56:24.335
    Sep 20 12:56:24.370: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7109" to be "running"
    Sep 20 12:56:24.378: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 7.648099ms
    Sep 20 12:56:26.389: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018970946s
    Sep 20 12:56:28.382: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.011804152s
    Sep 20 12:56:28.382: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 09/20/23 12:56:28.385
    STEP: Trying to apply a random label on the found node. 09/20/23 12:56:28.626
    STEP: verifying the node has the label kubernetes.io/e2e-1eace3b2-af44-483f-9dc4-2cfc0cda0e10 95 09/20/23 12:56:28.864
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 09/20/23 12:56:29.025
    Sep 20 12:56:29.036: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-7109" to be "not pending"
    Sep 20 12:56:29.046: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.629982ms
    Sep 20 12:56:31.050: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01402317s
    Sep 20 12:56:33.050: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 4.01432045s
    Sep 20 12:56:33.050: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.10.64 on the node which pod4 resides and expect not scheduled 09/20/23 12:56:33.05
    Sep 20 12:56:33.061: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-7109" to be "not pending"
    Sep 20 12:56:33.067: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.815811ms
    Sep 20 12:56:35.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011460712s
    Sep 20 12:56:37.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011977804s
    Sep 20 12:56:39.076: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014944792s
    Sep 20 12:56:41.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013446178s
    Sep 20 12:56:43.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.012906952s
    Sep 20 12:56:45.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.012531754s
    Sep 20 12:56:47.071: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010872637s
    Sep 20 12:56:49.088: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.027153199s
    Sep 20 12:56:51.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.0125425s
    Sep 20 12:56:53.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.03080684s
    Sep 20 12:56:55.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.01386484s
    Sep 20 12:56:57.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.012103521s
    Sep 20 12:56:59.076: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.01516082s
    Sep 20 12:57:01.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.013719141s
    Sep 20 12:57:03.077: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.016146435s
    Sep 20 12:57:05.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.013678484s
    Sep 20 12:57:07.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.0122227s
    Sep 20 12:57:09.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.012364736s
    Sep 20 12:57:11.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.012509037s
    Sep 20 12:57:13.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.011587549s
    Sep 20 12:57:15.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.011010612s
    Sep 20 12:57:17.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.013198262s
    Sep 20 12:57:19.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.012612448s
    Sep 20 12:57:21.078: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.017099049s
    Sep 20 12:57:23.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.012853179s
    Sep 20 12:57:25.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.013082229s
    Sep 20 12:57:27.071: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.010908903s
    Sep 20 12:57:29.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.012479662s
    Sep 20 12:57:31.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.012178083s
    Sep 20 12:57:33.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.012610095s
    Sep 20 12:57:35.090: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.029684693s
    Sep 20 12:57:37.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.013012432s
    Sep 20 12:57:39.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.011876821s
    Sep 20 12:57:41.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.012923402s
    Sep 20 12:57:43.075: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.01422907s
    Sep 20 12:57:45.135: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.074042899s
    Sep 20 12:57:47.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.011081489s
    Sep 20 12:57:49.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.013367004s
    Sep 20 12:57:51.076: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.015366781s
    Sep 20 12:57:53.071: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.010511492s
    Sep 20 12:57:55.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.011475077s
    Sep 20 12:57:57.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.012815802s
    Sep 20 12:57:59.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.012852631s
    Sep 20 12:58:01.075: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.01413748s
    Sep 20 12:58:03.415: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.3545707s
    Sep 20 12:58:05.071: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.010439502s
    Sep 20 12:58:07.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.011858083s
    Sep 20 12:58:09.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.011503674s
    Sep 20 12:58:11.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.012199454s
    Sep 20 12:58:13.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.013908502s
    Sep 20 12:58:15.081: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.020041686s
    Sep 20 12:58:17.312: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.250956601s
    Sep 20 12:58:19.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.011686171s
    Sep 20 12:58:21.093: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.032320916s
    Sep 20 12:58:23.206: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.145880502s
    Sep 20 12:58:25.076: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.015767697s
    Sep 20 12:58:27.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.01365234s
    Sep 20 12:58:29.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.011466291s
    Sep 20 12:58:31.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.012407475s
    Sep 20 12:58:33.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.013702323s
    Sep 20 12:58:35.152: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.091436955s
    Sep 20 12:58:37.084: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.023216867s
    Sep 20 12:58:39.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.012983827s
    Sep 20 12:58:41.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.011098824s
    Sep 20 12:58:43.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.012325394s
    Sep 20 12:58:45.079: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.01830086s
    Sep 20 12:58:47.088: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.026939835s
    Sep 20 12:58:49.167: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.106706464s
    Sep 20 12:58:51.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.012241852s
    Sep 20 12:58:53.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.011418389s
    Sep 20 12:58:55.079: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.018820664s
    Sep 20 12:58:57.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.012583432s
    Sep 20 12:58:59.078: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.017174561s
    Sep 20 12:59:01.071: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.010903455s
    Sep 20 12:59:03.075: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.013988195s
    Sep 20 12:59:05.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.011162851s
    Sep 20 12:59:07.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.012011759s
    Sep 20 12:59:09.076: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.015031999s
    Sep 20 12:59:11.075: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.013962503s
    Sep 20 12:59:13.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.011579661s
    Sep 20 12:59:15.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.080125623s
    Sep 20 12:59:17.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.012676828s
    Sep 20 12:59:19.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.012141318s
    Sep 20 12:59:21.078: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.017375467s
    Sep 20 12:59:23.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.013267998s
    Sep 20 12:59:25.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.01389588s
    Sep 20 12:59:27.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.012626808s
    Sep 20 12:59:29.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.176645209s
    Sep 20 12:59:31.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.012301022s
    Sep 20 12:59:33.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.012451935s
    Sep 20 12:59:35.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.012499733s
    Sep 20 12:59:37.071: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.010722465s
    Sep 20 12:59:39.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.013896463s
    Sep 20 12:59:41.077: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.016274181s
    Sep 20 12:59:43.082: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.021736791s
    Sep 20 12:59:45.091: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.030422353s
    Sep 20 12:59:47.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.012717477s
    Sep 20 12:59:49.075: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.014281932s
    Sep 20 12:59:51.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.013913657s
    Sep 20 12:59:53.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.011652567s
    Sep 20 12:59:55.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.013121844s
    Sep 20 12:59:57.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.631761008s
    Sep 20 12:59:59.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.013127703s
    Sep 20 13:00:01.294: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.233123758s
    Sep 20 13:00:03.076: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.015177794s
    Sep 20 13:00:05.076: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.015143899s
    Sep 20 13:00:07.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.011099939s
    Sep 20 13:00:09.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.012874281s
    Sep 20 13:00:11.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.011913298s
    Sep 20 13:00:13.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.012709569s
    Sep 20 13:00:15.075: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.014157056s
    Sep 20 13:00:17.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.012784378s
    Sep 20 13:00:19.071: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.010347446s
    Sep 20 13:00:21.086: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.025822269s
    Sep 20 13:00:23.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.012359364s
    Sep 20 13:00:25.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.011774912s
    Sep 20 13:00:27.071: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.010313628s
    Sep 20 13:00:29.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.012340065s
    Sep 20 13:00:31.075: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.014199697s
    Sep 20 13:00:33.076: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.015158203s
    Sep 20 13:00:35.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.011644051s
    Sep 20 13:00:38.151: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m5.090328889s
    Sep 20 13:00:39.473: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.412385341s
    Sep 20 13:00:41.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.012212538s
    Sep 20 13:00:43.201: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.140475857s
    Sep 20 13:00:45.075: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.014019941s
    Sep 20 13:00:47.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.013240521s
    Sep 20 13:00:49.226: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.165395533s
    Sep 20 13:00:51.108: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.047497991s
    Sep 20 13:00:53.077: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.016022931s
    Sep 20 13:00:55.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.019299644s
    Sep 20 13:00:57.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.011305061s
    Sep 20 13:00:59.075: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.014853605s
    Sep 20 13:01:01.308: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.247338459s
    Sep 20 13:01:03.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.082548101s
    Sep 20 13:01:05.131: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.070344398s
    Sep 20 13:01:07.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.011723283s
    Sep 20 13:01:09.119: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.058876399s
    Sep 20 13:01:11.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.011612253s
    Sep 20 13:01:13.082: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.021056705s
    Sep 20 13:01:15.074: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.013348802s
    Sep 20 13:01:17.075: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.014067897s
    Sep 20 13:01:19.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.011570129s
    Sep 20 13:01:21.076: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.015295256s
    Sep 20 13:01:23.137: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.076096918s
    Sep 20 13:01:25.510: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.449515622s
    Sep 20 13:01:27.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.012743396s
    Sep 20 13:01:29.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.012474789s
    Sep 20 13:01:31.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.012344402s
    Sep 20 13:01:33.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.012162438s
    Sep 20 13:01:33.075: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.014511053s
    STEP: removing the label kubernetes.io/e2e-1eace3b2-af44-483f-9dc4-2cfc0cda0e10 off the node mycluster-ww3cg64etuwi-node-1 09/20/23 13:01:33.075
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-1eace3b2-af44-483f-9dc4-2cfc0cda0e10 09/20/23 13:01:33.592
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:01:33.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-7109" for this suite. 09/20/23 13:01:33.606
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:01:34.193
Sep 20 13:01:34.193: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename cronjob 09/20/23 13:01:34.194
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:01:34.251
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:01:34.255
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 09/20/23 13:01:34.264
STEP: Ensuring a job is scheduled 09/20/23 13:01:34.274
STEP: Ensuring exactly one is scheduled 09/20/23 13:02:00.669
STEP: Ensuring exactly one running job exists by listing jobs explicitly 09/20/23 13:02:00.95
STEP: Ensuring the job is replaced with a new one 09/20/23 13:02:00.955
STEP: Removing cronjob 09/20/23 13:03:00.962
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Sep 20 13:03:00.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-3027" for this suite. 09/20/23 13:03:00.977
------------------------------
â€¢ [SLOW TEST] [86.795 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:01:34.193
    Sep 20 13:01:34.193: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename cronjob 09/20/23 13:01:34.194
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:01:34.251
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:01:34.255
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 09/20/23 13:01:34.264
    STEP: Ensuring a job is scheduled 09/20/23 13:01:34.274
    STEP: Ensuring exactly one is scheduled 09/20/23 13:02:00.669
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 09/20/23 13:02:00.95
    STEP: Ensuring the job is replaced with a new one 09/20/23 13:02:00.955
    STEP: Removing cronjob 09/20/23 13:03:00.962
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:03:00.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-3027" for this suite. 09/20/23 13:03:00.977
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:03:00.99
Sep 20 13:03:00.990: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename proxy 09/20/23 13:03:00.991
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:03:02.041
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:03:02.044
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 09/20/23 13:03:02.069
STEP: creating replication controller proxy-service-kxxhg in namespace proxy-3534 09/20/23 13:03:02.069
I0920 13:03:02.101587      20 runners.go:193] Created replication controller with name: proxy-service-kxxhg, namespace: proxy-3534, replica count: 1
I0920 13:03:03.152815      20 runners.go:193] proxy-service-kxxhg Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0920 13:03:04.153071      20 runners.go:193] proxy-service-kxxhg Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0920 13:03:05.153371      20 runners.go:193] proxy-service-kxxhg Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0920 13:03:06.154488      20 runners.go:193] proxy-service-kxxhg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0920 13:03:07.155107      20 runners.go:193] proxy-service-kxxhg Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 20 13:03:07.213: INFO: setup took 5.163804884s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 09/20/23 13:03:07.213
Sep 20 13:03:07.234: INFO: (0) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 20.494863ms)
Sep 20 13:03:07.251: INFO: (0) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 36.933644ms)
Sep 20 13:03:07.251: INFO: (0) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 37.47322ms)
Sep 20 13:03:07.251: INFO: (0) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 37.565373ms)
Sep 20 13:03:07.251: INFO: (0) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 37.837537ms)
Sep 20 13:03:07.252: INFO: (0) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 37.866531ms)
Sep 20 13:03:07.252: INFO: (0) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 37.780218ms)
Sep 20 13:03:07.257: INFO: (0) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 43.167859ms)
Sep 20 13:03:07.257: INFO: (0) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 43.852839ms)
Sep 20 13:03:07.257: INFO: (0) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 43.631252ms)
Sep 20 13:03:07.261: INFO: (0) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 47.817859ms)
Sep 20 13:03:07.261: INFO: (0) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 47.904062ms)
Sep 20 13:03:07.262: INFO: (0) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 48.405787ms)
Sep 20 13:03:07.262: INFO: (0) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 48.332971ms)
Sep 20 13:03:07.262: INFO: (0) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 48.767088ms)
Sep 20 13:03:07.262: INFO: (0) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 48.64065ms)
Sep 20 13:03:07.270: INFO: (1) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 7.548601ms)
Sep 20 13:03:07.273: INFO: (1) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 10.943768ms)
Sep 20 13:03:07.274: INFO: (1) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 11.622647ms)
Sep 20 13:03:07.274: INFO: (1) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 11.501629ms)
Sep 20 13:03:07.274: INFO: (1) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 12.060001ms)
Sep 20 13:03:07.277: INFO: (1) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 14.945317ms)
Sep 20 13:03:07.278: INFO: (1) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 15.13832ms)
Sep 20 13:03:07.278: INFO: (1) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 15.22281ms)
Sep 20 13:03:07.278: INFO: (1) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 15.487759ms)
Sep 20 13:03:07.278: INFO: (1) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 15.44532ms)
Sep 20 13:03:07.278: INFO: (1) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 16.172729ms)
Sep 20 13:03:07.279: INFO: (1) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 16.266787ms)
Sep 20 13:03:07.279: INFO: (1) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 16.666199ms)
Sep 20 13:03:07.280: INFO: (1) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 17.749781ms)
Sep 20 13:03:07.280: INFO: (1) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 18.039286ms)
Sep 20 13:03:07.282: INFO: (1) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 19.31561ms)
Sep 20 13:03:07.293: INFO: (2) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 11.292716ms)
Sep 20 13:03:07.293: INFO: (2) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 11.476913ms)
Sep 20 13:03:07.293: INFO: (2) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 11.36938ms)
Sep 20 13:03:07.294: INFO: (2) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 11.444702ms)
Sep 20 13:03:07.294: INFO: (2) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 12.120725ms)
Sep 20 13:03:07.296: INFO: (2) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 13.461782ms)
Sep 20 13:03:07.296: INFO: (2) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 13.580997ms)
Sep 20 13:03:07.296: INFO: (2) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 14.491392ms)
Sep 20 13:03:07.298: INFO: (2) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 15.696242ms)
Sep 20 13:03:07.298: INFO: (2) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 15.236476ms)
Sep 20 13:03:07.298: INFO: (2) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 16.058644ms)
Sep 20 13:03:07.299: INFO: (2) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 16.294078ms)
Sep 20 13:03:07.299: INFO: (2) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 17.492817ms)
Sep 20 13:03:07.299: INFO: (2) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 16.95811ms)
Sep 20 13:03:07.300: INFO: (2) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 17.711619ms)
Sep 20 13:03:07.300: INFO: (2) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 17.830763ms)
Sep 20 13:03:07.309: INFO: (3) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 8.269861ms)
Sep 20 13:03:07.311: INFO: (3) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 10.23369ms)
Sep 20 13:03:07.313: INFO: (3) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 12.410651ms)
Sep 20 13:03:07.313: INFO: (3) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 12.470745ms)
Sep 20 13:03:07.314: INFO: (3) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 13.169801ms)
Sep 20 13:03:07.314: INFO: (3) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 13.50809ms)
Sep 20 13:03:07.314: INFO: (3) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 13.384156ms)
Sep 20 13:03:07.316: INFO: (3) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 15.006723ms)
Sep 20 13:03:07.316: INFO: (3) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 14.686189ms)
Sep 20 13:03:07.316: INFO: (3) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 14.794504ms)
Sep 20 13:03:07.316: INFO: (3) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 14.997306ms)
Sep 20 13:03:07.316: INFO: (3) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 15.961532ms)
Sep 20 13:03:07.317: INFO: (3) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 16.013689ms)
Sep 20 13:03:07.317: INFO: (3) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 16.389378ms)
Sep 20 13:03:07.318: INFO: (3) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 17.586142ms)
Sep 20 13:03:07.319: INFO: (3) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 18.066628ms)
Sep 20 13:03:07.330: INFO: (4) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 11.238413ms)
Sep 20 13:03:07.330: INFO: (4) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 10.967934ms)
Sep 20 13:03:07.331: INFO: (4) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 11.833124ms)
Sep 20 13:03:07.331: INFO: (4) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 11.541685ms)
Sep 20 13:03:07.334: INFO: (4) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 14.16106ms)
Sep 20 13:03:07.334: INFO: (4) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 14.751983ms)
Sep 20 13:03:07.334: INFO: (4) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 14.404398ms)
Sep 20 13:03:07.334: INFO: (4) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 15.059663ms)
Sep 20 13:03:07.334: INFO: (4) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 14.943264ms)
Sep 20 13:03:07.335: INFO: (4) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 15.83328ms)
Sep 20 13:03:07.335: INFO: (4) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 15.237759ms)
Sep 20 13:03:07.335: INFO: (4) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 15.522744ms)
Sep 20 13:03:07.336: INFO: (4) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 16.624961ms)
Sep 20 13:03:07.339: INFO: (4) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 19.619854ms)
Sep 20 13:03:07.339: INFO: (4) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 19.49549ms)
Sep 20 13:03:07.339: INFO: (4) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 19.613522ms)
Sep 20 13:03:07.347: INFO: (5) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 7.833518ms)
Sep 20 13:03:07.347: INFO: (5) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 7.857864ms)
Sep 20 13:03:07.348: INFO: (5) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 8.191824ms)
Sep 20 13:03:07.348: INFO: (5) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 8.375769ms)
Sep 20 13:03:07.348: INFO: (5) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 8.406919ms)
Sep 20 13:03:07.351: INFO: (5) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 10.975238ms)
Sep 20 13:03:07.351: INFO: (5) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 10.865271ms)
Sep 20 13:03:07.353: INFO: (5) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 12.995444ms)
Sep 20 13:03:07.353: INFO: (5) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 13.386671ms)
Sep 20 13:03:07.354: INFO: (5) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 14.132475ms)
Sep 20 13:03:07.354: INFO: (5) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 14.066591ms)
Sep 20 13:03:07.354: INFO: (5) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 14.168764ms)
Sep 20 13:03:07.354: INFO: (5) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 14.503735ms)
Sep 20 13:03:07.354: INFO: (5) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 14.373188ms)
Sep 20 13:03:07.356: INFO: (5) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 16.333373ms)
Sep 20 13:03:07.359: INFO: (5) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 18.721551ms)
Sep 20 13:03:07.372: INFO: (6) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 12.868584ms)
Sep 20 13:03:07.372: INFO: (6) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 12.862763ms)
Sep 20 13:03:07.372: INFO: (6) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 13.022825ms)
Sep 20 13:03:07.372: INFO: (6) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 13.451123ms)
Sep 20 13:03:07.373: INFO: (6) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 13.695543ms)
Sep 20 13:03:07.373: INFO: (6) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 14.647597ms)
Sep 20 13:03:07.374: INFO: (6) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 14.727768ms)
Sep 20 13:03:07.374: INFO: (6) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 14.601901ms)
Sep 20 13:03:07.374: INFO: (6) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 15.01553ms)
Sep 20 13:03:07.377: INFO: (6) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 17.787242ms)
Sep 20 13:03:07.377: INFO: (6) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 17.853877ms)
Sep 20 13:03:07.377: INFO: (6) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 17.954137ms)
Sep 20 13:03:07.377: INFO: (6) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 18.005733ms)
Sep 20 13:03:07.377: INFO: (6) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 18.180683ms)
Sep 20 13:03:07.377: INFO: (6) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 18.250525ms)
Sep 20 13:03:07.377: INFO: (6) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 18.21153ms)
Sep 20 13:03:07.385: INFO: (7) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 7.308469ms)
Sep 20 13:03:07.385: INFO: (7) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 8.114357ms)
Sep 20 13:03:07.391: INFO: (7) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 13.966052ms)
Sep 20 13:03:07.391: INFO: (7) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 14.160199ms)
Sep 20 13:03:07.392: INFO: (7) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 14.959994ms)
Sep 20 13:03:07.392: INFO: (7) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 14.936801ms)
Sep 20 13:03:07.394: INFO: (7) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 16.19443ms)
Sep 20 13:03:07.396: INFO: (7) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 18.602718ms)
Sep 20 13:03:07.396: INFO: (7) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 18.886253ms)
Sep 20 13:03:07.396: INFO: (7) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 18.816512ms)
Sep 20 13:03:07.396: INFO: (7) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 18.963708ms)
Sep 20 13:03:07.396: INFO: (7) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 19.084676ms)
Sep 20 13:03:07.398: INFO: (7) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 20.165893ms)
Sep 20 13:03:07.398: INFO: (7) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 20.165522ms)
Sep 20 13:03:07.398: INFO: (7) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 20.322417ms)
Sep 20 13:03:07.399: INFO: (7) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 21.32147ms)
Sep 20 13:03:07.406: INFO: (8) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 7.133069ms)
Sep 20 13:03:07.411: INFO: (8) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 11.678112ms)
Sep 20 13:03:07.411: INFO: (8) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 11.713659ms)
Sep 20 13:03:07.411: INFO: (8) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 11.861507ms)
Sep 20 13:03:07.411: INFO: (8) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 12.337324ms)
Sep 20 13:03:07.412: INFO: (8) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 12.910824ms)
Sep 20 13:03:07.412: INFO: (8) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 13.021913ms)
Sep 20 13:03:07.412: INFO: (8) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 13.016613ms)
Sep 20 13:03:07.413: INFO: (8) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 13.690613ms)
Sep 20 13:03:07.414: INFO: (8) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 14.886276ms)
Sep 20 13:03:07.414: INFO: (8) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 14.91995ms)
Sep 20 13:03:07.414: INFO: (8) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 14.831943ms)
Sep 20 13:03:07.414: INFO: (8) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 15.162396ms)
Sep 20 13:03:07.414: INFO: (8) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 15.26021ms)
Sep 20 13:03:07.416: INFO: (8) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 16.622007ms)
Sep 20 13:03:07.418: INFO: (8) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 19.01265ms)
Sep 20 13:03:07.431: INFO: (9) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 12.748718ms)
Sep 20 13:03:07.432: INFO: (9) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 14.192229ms)
Sep 20 13:03:07.432: INFO: (9) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 14.319759ms)
Sep 20 13:03:07.432: INFO: (9) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 14.502422ms)
Sep 20 13:03:07.437: INFO: (9) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 19.149748ms)
Sep 20 13:03:07.439: INFO: (9) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 20.40837ms)
Sep 20 13:03:07.439: INFO: (9) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 20.392149ms)
Sep 20 13:03:07.440: INFO: (9) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 21.658053ms)
Sep 20 13:03:07.440: INFO: (9) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 21.465452ms)
Sep 20 13:03:07.440: INFO: (9) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 21.866537ms)
Sep 20 13:03:07.441: INFO: (9) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 22.336131ms)
Sep 20 13:03:07.441: INFO: (9) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 22.334348ms)
Sep 20 13:03:07.441: INFO: (9) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 23.043114ms)
Sep 20 13:03:07.442: INFO: (9) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 23.756657ms)
Sep 20 13:03:07.442: INFO: (9) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 23.818514ms)
Sep 20 13:03:07.443: INFO: (9) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 24.465533ms)
Sep 20 13:03:07.450: INFO: (10) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 7.025937ms)
Sep 20 13:03:07.454: INFO: (10) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 10.898042ms)
Sep 20 13:03:07.454: INFO: (10) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 10.924412ms)
Sep 20 13:03:07.454: INFO: (10) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 10.715739ms)
Sep 20 13:03:07.454: INFO: (10) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 11.048325ms)
Sep 20 13:03:07.454: INFO: (10) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 11.116423ms)
Sep 20 13:03:07.455: INFO: (10) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 11.951567ms)
Sep 20 13:03:07.455: INFO: (10) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 12.321654ms)
Sep 20 13:03:07.458: INFO: (10) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 14.239658ms)
Sep 20 13:03:07.458: INFO: (10) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 14.769125ms)
Sep 20 13:03:07.458: INFO: (10) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 14.152674ms)
Sep 20 13:03:07.458: INFO: (10) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 14.808119ms)
Sep 20 13:03:07.459: INFO: (10) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 15.333138ms)
Sep 20 13:03:07.459: INFO: (10) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 15.415463ms)
Sep 20 13:03:07.459: INFO: (10) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 15.919934ms)
Sep 20 13:03:07.460: INFO: (10) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 16.676559ms)
Sep 20 13:03:07.466: INFO: (11) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 6.289089ms)
Sep 20 13:03:07.466: INFO: (11) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 6.387674ms)
Sep 20 13:03:07.471: INFO: (11) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 11.521666ms)
Sep 20 13:03:07.471: INFO: (11) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 11.656151ms)
Sep 20 13:03:07.472: INFO: (11) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 11.558476ms)
Sep 20 13:03:07.472: INFO: (11) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 11.548657ms)
Sep 20 13:03:07.473: INFO: (11) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 13.397201ms)
Sep 20 13:03:07.473: INFO: (11) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 13.508822ms)
Sep 20 13:03:07.474: INFO: (11) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 13.649966ms)
Sep 20 13:03:07.474: INFO: (11) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 14.000166ms)
Sep 20 13:03:07.474: INFO: (11) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 14.05502ms)
Sep 20 13:03:07.474: INFO: (11) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 14.245818ms)
Sep 20 13:03:07.475: INFO: (11) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 15.573982ms)
Sep 20 13:03:07.476: INFO: (11) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 15.634867ms)
Sep 20 13:03:07.476: INFO: (11) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 15.570395ms)
Sep 20 13:03:07.476: INFO: (11) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 15.791371ms)
Sep 20 13:03:07.484: INFO: (12) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 8.054645ms)
Sep 20 13:03:07.487: INFO: (12) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 10.533395ms)
Sep 20 13:03:07.487: INFO: (12) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 10.43532ms)
Sep 20 13:03:07.487: INFO: (12) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 10.550818ms)
Sep 20 13:03:07.487: INFO: (12) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 11.366124ms)
Sep 20 13:03:07.490: INFO: (12) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 13.853179ms)
Sep 20 13:03:07.490: INFO: (12) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 13.969991ms)
Sep 20 13:03:07.491: INFO: (12) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 14.546036ms)
Sep 20 13:03:07.491: INFO: (12) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 14.765249ms)
Sep 20 13:03:07.491: INFO: (12) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 15.037562ms)
Sep 20 13:03:07.491: INFO: (12) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 15.17963ms)
Sep 20 13:03:07.492: INFO: (12) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 16.154355ms)
Sep 20 13:03:07.493: INFO: (12) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 16.509745ms)
Sep 20 13:03:07.493: INFO: (12) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 17.078636ms)
Sep 20 13:03:07.493: INFO: (12) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 17.157074ms)
Sep 20 13:03:07.495: INFO: (12) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 18.769532ms)
Sep 20 13:03:07.503: INFO: (13) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 8.496658ms)
Sep 20 13:03:07.507: INFO: (13) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 12.546448ms)
Sep 20 13:03:07.508: INFO: (13) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 12.631056ms)
Sep 20 13:03:07.508: INFO: (13) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 12.700337ms)
Sep 20 13:03:07.509: INFO: (13) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 14.312635ms)
Sep 20 13:03:07.509: INFO: (13) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 14.46926ms)
Sep 20 13:03:07.511: INFO: (13) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 15.582528ms)
Sep 20 13:03:07.511: INFO: (13) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 15.871212ms)
Sep 20 13:03:07.511: INFO: (13) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 15.983373ms)
Sep 20 13:03:07.512: INFO: (13) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 17.312627ms)
Sep 20 13:03:07.513: INFO: (13) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 17.721477ms)
Sep 20 13:03:07.513: INFO: (13) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 17.5376ms)
Sep 20 13:03:07.513: INFO: (13) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 17.715427ms)
Sep 20 13:03:07.513: INFO: (13) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 18.231499ms)
Sep 20 13:03:07.514: INFO: (13) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 18.812243ms)
Sep 20 13:03:07.514: INFO: (13) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 18.70447ms)
Sep 20 13:03:07.524: INFO: (14) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 9.534062ms)
Sep 20 13:03:07.524: INFO: (14) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 10.13248ms)
Sep 20 13:03:07.524: INFO: (14) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 10.251333ms)
Sep 20 13:03:07.524: INFO: (14) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 10.328168ms)
Sep 20 13:03:07.525: INFO: (14) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 10.924822ms)
Sep 20 13:03:07.526: INFO: (14) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 11.585327ms)
Sep 20 13:03:07.526: INFO: (14) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 11.768542ms)
Sep 20 13:03:07.527: INFO: (14) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 12.645935ms)
Sep 20 13:03:07.527: INFO: (14) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 12.772984ms)
Sep 20 13:03:07.527: INFO: (14) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 12.752516ms)
Sep 20 13:03:07.527: INFO: (14) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 13.138053ms)
Sep 20 13:03:07.527: INFO: (14) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 13.263278ms)
Sep 20 13:03:07.528: INFO: (14) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 13.81064ms)
Sep 20 13:03:07.528: INFO: (14) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 14.100406ms)
Sep 20 13:03:07.529: INFO: (14) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 14.232034ms)
Sep 20 13:03:07.529: INFO: (14) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 15.022973ms)
Sep 20 13:03:07.535: INFO: (15) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 6.234917ms)
Sep 20 13:03:07.536: INFO: (15) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 6.728898ms)
Sep 20 13:03:07.539: INFO: (15) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 9.318004ms)
Sep 20 13:03:07.539: INFO: (15) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 9.364442ms)
Sep 20 13:03:07.539: INFO: (15) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 9.602221ms)
Sep 20 13:03:07.540: INFO: (15) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 10.777045ms)
Sep 20 13:03:07.540: INFO: (15) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 11.083382ms)
Sep 20 13:03:07.540: INFO: (15) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 10.878846ms)
Sep 20 13:03:07.541: INFO: (15) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 11.373698ms)
Sep 20 13:03:07.541: INFO: (15) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 11.674195ms)
Sep 20 13:03:07.541: INFO: (15) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 11.787208ms)
Sep 20 13:03:07.542: INFO: (15) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 12.665381ms)
Sep 20 13:03:07.542: INFO: (15) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 12.538292ms)
Sep 20 13:03:07.542: INFO: (15) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 12.6963ms)
Sep 20 13:03:07.542: INFO: (15) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 12.751915ms)
Sep 20 13:03:07.543: INFO: (15) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 13.347777ms)
Sep 20 13:03:07.548: INFO: (16) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 5.268756ms)
Sep 20 13:03:07.549: INFO: (16) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 6.054768ms)
Sep 20 13:03:07.552: INFO: (16) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 9.361828ms)
Sep 20 13:03:07.552: INFO: (16) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 9.412304ms)
Sep 20 13:03:07.552: INFO: (16) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 9.528882ms)
Sep 20 13:03:07.552: INFO: (16) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 9.32578ms)
Sep 20 13:03:07.552: INFO: (16) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 9.689005ms)
Sep 20 13:03:07.553: INFO: (16) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 10.174098ms)
Sep 20 13:03:07.554: INFO: (16) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 11.107988ms)
Sep 20 13:03:07.554: INFO: (16) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 11.377145ms)
Sep 20 13:03:07.555: INFO: (16) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 12.367741ms)
Sep 20 13:03:07.555: INFO: (16) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 12.27731ms)
Sep 20 13:03:07.556: INFO: (16) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 12.562077ms)
Sep 20 13:03:07.556: INFO: (16) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 12.838187ms)
Sep 20 13:03:07.556: INFO: (16) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 12.583527ms)
Sep 20 13:03:07.556: INFO: (16) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 12.89231ms)
Sep 20 13:03:07.574: INFO: (17) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 17.869466ms)
Sep 20 13:03:07.574: INFO: (17) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 17.821876ms)
Sep 20 13:03:07.574: INFO: (17) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 18.325425ms)
Sep 20 13:03:07.574: INFO: (17) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 18.254481ms)
Sep 20 13:03:07.574: INFO: (17) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 18.42921ms)
Sep 20 13:03:07.575: INFO: (17) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 18.331306ms)
Sep 20 13:03:07.575: INFO: (17) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 18.411728ms)
Sep 20 13:03:07.575: INFO: (17) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 18.607658ms)
Sep 20 13:03:07.575: INFO: (17) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 18.688919ms)
Sep 20 13:03:07.575: INFO: (17) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 18.52394ms)
Sep 20 13:03:07.575: INFO: (17) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 18.717173ms)
Sep 20 13:03:07.575: INFO: (17) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 18.809869ms)
Sep 20 13:03:07.575: INFO: (17) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 18.930344ms)
Sep 20 13:03:07.575: INFO: (17) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 18.68916ms)
Sep 20 13:03:07.575: INFO: (17) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 18.810109ms)
Sep 20 13:03:07.575: INFO: (17) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 18.85904ms)
Sep 20 13:03:07.581: INFO: (18) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 5.762376ms)
Sep 20 13:03:07.584: INFO: (18) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 8.660516ms)
Sep 20 13:03:07.589: INFO: (18) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 14.067614ms)
Sep 20 13:03:07.589: INFO: (18) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 14.055771ms)
Sep 20 13:03:07.589: INFO: (18) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 14.218117ms)
Sep 20 13:03:07.589: INFO: (18) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 14.317415ms)
Sep 20 13:03:07.589: INFO: (18) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 14.338995ms)
Sep 20 13:03:07.589: INFO: (18) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 14.194202ms)
Sep 20 13:03:07.593: INFO: (18) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 17.224071ms)
Sep 20 13:03:07.593: INFO: (18) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 17.958754ms)
Sep 20 13:03:07.593: INFO: (18) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 17.729503ms)
Sep 20 13:03:07.593: INFO: (18) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 18.079882ms)
Sep 20 13:03:07.593: INFO: (18) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 18.207132ms)
Sep 20 13:03:07.594: INFO: (18) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 18.286592ms)
Sep 20 13:03:07.594: INFO: (18) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 18.424392ms)
Sep 20 13:03:07.594: INFO: (18) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 19.351388ms)
Sep 20 13:03:07.603: INFO: (19) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 8.545409ms)
Sep 20 13:03:07.607: INFO: (19) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 11.821631ms)
Sep 20 13:03:07.609: INFO: (19) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 13.781615ms)
Sep 20 13:03:07.609: INFO: (19) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 13.784981ms)
Sep 20 13:03:07.609: INFO: (19) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 13.883777ms)
Sep 20 13:03:07.612: INFO: (19) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 16.911852ms)
Sep 20 13:03:07.612: INFO: (19) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 17.307548ms)
Sep 20 13:03:07.616: INFO: (19) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 20.927488ms)
Sep 20 13:03:07.616: INFO: (19) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 20.974126ms)
Sep 20 13:03:07.616: INFO: (19) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 21.211703ms)
Sep 20 13:03:07.616: INFO: (19) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 21.376915ms)
Sep 20 13:03:07.616: INFO: (19) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 21.440244ms)
Sep 20 13:03:07.616: INFO: (19) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 21.547476ms)
Sep 20 13:03:07.618: INFO: (19) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 23.260414ms)
Sep 20 13:03:07.619: INFO: (19) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 24.297777ms)
Sep 20 13:03:07.619: INFO: (19) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 24.285794ms)
STEP: deleting ReplicationController proxy-service-kxxhg in namespace proxy-3534, will wait for the garbage collector to delete the pods 09/20/23 13:03:07.619
Sep 20 13:03:07.694: INFO: Deleting ReplicationController proxy-service-kxxhg took: 19.663416ms
Sep 20 13:03:08.095: INFO: Terminating ReplicationController proxy-service-kxxhg pods took: 400.289901ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Sep 20 13:03:11.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-3534" for this suite. 09/20/23 13:03:11.502
------------------------------
â€¢ [SLOW TEST] [10.695 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:03:00.99
    Sep 20 13:03:00.990: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename proxy 09/20/23 13:03:00.991
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:03:02.041
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:03:02.044
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 09/20/23 13:03:02.069
    STEP: creating replication controller proxy-service-kxxhg in namespace proxy-3534 09/20/23 13:03:02.069
    I0920 13:03:02.101587      20 runners.go:193] Created replication controller with name: proxy-service-kxxhg, namespace: proxy-3534, replica count: 1
    I0920 13:03:03.152815      20 runners.go:193] proxy-service-kxxhg Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0920 13:03:04.153071      20 runners.go:193] proxy-service-kxxhg Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0920 13:03:05.153371      20 runners.go:193] proxy-service-kxxhg Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0920 13:03:06.154488      20 runners.go:193] proxy-service-kxxhg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0920 13:03:07.155107      20 runners.go:193] proxy-service-kxxhg Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep 20 13:03:07.213: INFO: setup took 5.163804884s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 09/20/23 13:03:07.213
    Sep 20 13:03:07.234: INFO: (0) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 20.494863ms)
    Sep 20 13:03:07.251: INFO: (0) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 36.933644ms)
    Sep 20 13:03:07.251: INFO: (0) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 37.47322ms)
    Sep 20 13:03:07.251: INFO: (0) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 37.565373ms)
    Sep 20 13:03:07.251: INFO: (0) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 37.837537ms)
    Sep 20 13:03:07.252: INFO: (0) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 37.866531ms)
    Sep 20 13:03:07.252: INFO: (0) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 37.780218ms)
    Sep 20 13:03:07.257: INFO: (0) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 43.167859ms)
    Sep 20 13:03:07.257: INFO: (0) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 43.852839ms)
    Sep 20 13:03:07.257: INFO: (0) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 43.631252ms)
    Sep 20 13:03:07.261: INFO: (0) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 47.817859ms)
    Sep 20 13:03:07.261: INFO: (0) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 47.904062ms)
    Sep 20 13:03:07.262: INFO: (0) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 48.405787ms)
    Sep 20 13:03:07.262: INFO: (0) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 48.332971ms)
    Sep 20 13:03:07.262: INFO: (0) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 48.767088ms)
    Sep 20 13:03:07.262: INFO: (0) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 48.64065ms)
    Sep 20 13:03:07.270: INFO: (1) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 7.548601ms)
    Sep 20 13:03:07.273: INFO: (1) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 10.943768ms)
    Sep 20 13:03:07.274: INFO: (1) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 11.622647ms)
    Sep 20 13:03:07.274: INFO: (1) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 11.501629ms)
    Sep 20 13:03:07.274: INFO: (1) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 12.060001ms)
    Sep 20 13:03:07.277: INFO: (1) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 14.945317ms)
    Sep 20 13:03:07.278: INFO: (1) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 15.13832ms)
    Sep 20 13:03:07.278: INFO: (1) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 15.22281ms)
    Sep 20 13:03:07.278: INFO: (1) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 15.487759ms)
    Sep 20 13:03:07.278: INFO: (1) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 15.44532ms)
    Sep 20 13:03:07.278: INFO: (1) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 16.172729ms)
    Sep 20 13:03:07.279: INFO: (1) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 16.266787ms)
    Sep 20 13:03:07.279: INFO: (1) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 16.666199ms)
    Sep 20 13:03:07.280: INFO: (1) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 17.749781ms)
    Sep 20 13:03:07.280: INFO: (1) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 18.039286ms)
    Sep 20 13:03:07.282: INFO: (1) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 19.31561ms)
    Sep 20 13:03:07.293: INFO: (2) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 11.292716ms)
    Sep 20 13:03:07.293: INFO: (2) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 11.476913ms)
    Sep 20 13:03:07.293: INFO: (2) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 11.36938ms)
    Sep 20 13:03:07.294: INFO: (2) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 11.444702ms)
    Sep 20 13:03:07.294: INFO: (2) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 12.120725ms)
    Sep 20 13:03:07.296: INFO: (2) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 13.461782ms)
    Sep 20 13:03:07.296: INFO: (2) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 13.580997ms)
    Sep 20 13:03:07.296: INFO: (2) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 14.491392ms)
    Sep 20 13:03:07.298: INFO: (2) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 15.696242ms)
    Sep 20 13:03:07.298: INFO: (2) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 15.236476ms)
    Sep 20 13:03:07.298: INFO: (2) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 16.058644ms)
    Sep 20 13:03:07.299: INFO: (2) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 16.294078ms)
    Sep 20 13:03:07.299: INFO: (2) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 17.492817ms)
    Sep 20 13:03:07.299: INFO: (2) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 16.95811ms)
    Sep 20 13:03:07.300: INFO: (2) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 17.711619ms)
    Sep 20 13:03:07.300: INFO: (2) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 17.830763ms)
    Sep 20 13:03:07.309: INFO: (3) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 8.269861ms)
    Sep 20 13:03:07.311: INFO: (3) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 10.23369ms)
    Sep 20 13:03:07.313: INFO: (3) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 12.410651ms)
    Sep 20 13:03:07.313: INFO: (3) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 12.470745ms)
    Sep 20 13:03:07.314: INFO: (3) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 13.169801ms)
    Sep 20 13:03:07.314: INFO: (3) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 13.50809ms)
    Sep 20 13:03:07.314: INFO: (3) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 13.384156ms)
    Sep 20 13:03:07.316: INFO: (3) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 15.006723ms)
    Sep 20 13:03:07.316: INFO: (3) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 14.686189ms)
    Sep 20 13:03:07.316: INFO: (3) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 14.794504ms)
    Sep 20 13:03:07.316: INFO: (3) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 14.997306ms)
    Sep 20 13:03:07.316: INFO: (3) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 15.961532ms)
    Sep 20 13:03:07.317: INFO: (3) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 16.013689ms)
    Sep 20 13:03:07.317: INFO: (3) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 16.389378ms)
    Sep 20 13:03:07.318: INFO: (3) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 17.586142ms)
    Sep 20 13:03:07.319: INFO: (3) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 18.066628ms)
    Sep 20 13:03:07.330: INFO: (4) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 11.238413ms)
    Sep 20 13:03:07.330: INFO: (4) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 10.967934ms)
    Sep 20 13:03:07.331: INFO: (4) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 11.833124ms)
    Sep 20 13:03:07.331: INFO: (4) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 11.541685ms)
    Sep 20 13:03:07.334: INFO: (4) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 14.16106ms)
    Sep 20 13:03:07.334: INFO: (4) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 14.751983ms)
    Sep 20 13:03:07.334: INFO: (4) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 14.404398ms)
    Sep 20 13:03:07.334: INFO: (4) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 15.059663ms)
    Sep 20 13:03:07.334: INFO: (4) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 14.943264ms)
    Sep 20 13:03:07.335: INFO: (4) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 15.83328ms)
    Sep 20 13:03:07.335: INFO: (4) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 15.237759ms)
    Sep 20 13:03:07.335: INFO: (4) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 15.522744ms)
    Sep 20 13:03:07.336: INFO: (4) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 16.624961ms)
    Sep 20 13:03:07.339: INFO: (4) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 19.619854ms)
    Sep 20 13:03:07.339: INFO: (4) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 19.49549ms)
    Sep 20 13:03:07.339: INFO: (4) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 19.613522ms)
    Sep 20 13:03:07.347: INFO: (5) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 7.833518ms)
    Sep 20 13:03:07.347: INFO: (5) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 7.857864ms)
    Sep 20 13:03:07.348: INFO: (5) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 8.191824ms)
    Sep 20 13:03:07.348: INFO: (5) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 8.375769ms)
    Sep 20 13:03:07.348: INFO: (5) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 8.406919ms)
    Sep 20 13:03:07.351: INFO: (5) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 10.975238ms)
    Sep 20 13:03:07.351: INFO: (5) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 10.865271ms)
    Sep 20 13:03:07.353: INFO: (5) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 12.995444ms)
    Sep 20 13:03:07.353: INFO: (5) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 13.386671ms)
    Sep 20 13:03:07.354: INFO: (5) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 14.132475ms)
    Sep 20 13:03:07.354: INFO: (5) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 14.066591ms)
    Sep 20 13:03:07.354: INFO: (5) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 14.168764ms)
    Sep 20 13:03:07.354: INFO: (5) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 14.503735ms)
    Sep 20 13:03:07.354: INFO: (5) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 14.373188ms)
    Sep 20 13:03:07.356: INFO: (5) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 16.333373ms)
    Sep 20 13:03:07.359: INFO: (5) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 18.721551ms)
    Sep 20 13:03:07.372: INFO: (6) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 12.868584ms)
    Sep 20 13:03:07.372: INFO: (6) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 12.862763ms)
    Sep 20 13:03:07.372: INFO: (6) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 13.022825ms)
    Sep 20 13:03:07.372: INFO: (6) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 13.451123ms)
    Sep 20 13:03:07.373: INFO: (6) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 13.695543ms)
    Sep 20 13:03:07.373: INFO: (6) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 14.647597ms)
    Sep 20 13:03:07.374: INFO: (6) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 14.727768ms)
    Sep 20 13:03:07.374: INFO: (6) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 14.601901ms)
    Sep 20 13:03:07.374: INFO: (6) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 15.01553ms)
    Sep 20 13:03:07.377: INFO: (6) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 17.787242ms)
    Sep 20 13:03:07.377: INFO: (6) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 17.853877ms)
    Sep 20 13:03:07.377: INFO: (6) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 17.954137ms)
    Sep 20 13:03:07.377: INFO: (6) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 18.005733ms)
    Sep 20 13:03:07.377: INFO: (6) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 18.180683ms)
    Sep 20 13:03:07.377: INFO: (6) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 18.250525ms)
    Sep 20 13:03:07.377: INFO: (6) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 18.21153ms)
    Sep 20 13:03:07.385: INFO: (7) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 7.308469ms)
    Sep 20 13:03:07.385: INFO: (7) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 8.114357ms)
    Sep 20 13:03:07.391: INFO: (7) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 13.966052ms)
    Sep 20 13:03:07.391: INFO: (7) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 14.160199ms)
    Sep 20 13:03:07.392: INFO: (7) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 14.959994ms)
    Sep 20 13:03:07.392: INFO: (7) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 14.936801ms)
    Sep 20 13:03:07.394: INFO: (7) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 16.19443ms)
    Sep 20 13:03:07.396: INFO: (7) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 18.602718ms)
    Sep 20 13:03:07.396: INFO: (7) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 18.886253ms)
    Sep 20 13:03:07.396: INFO: (7) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 18.816512ms)
    Sep 20 13:03:07.396: INFO: (7) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 18.963708ms)
    Sep 20 13:03:07.396: INFO: (7) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 19.084676ms)
    Sep 20 13:03:07.398: INFO: (7) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 20.165893ms)
    Sep 20 13:03:07.398: INFO: (7) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 20.165522ms)
    Sep 20 13:03:07.398: INFO: (7) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 20.322417ms)
    Sep 20 13:03:07.399: INFO: (7) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 21.32147ms)
    Sep 20 13:03:07.406: INFO: (8) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 7.133069ms)
    Sep 20 13:03:07.411: INFO: (8) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 11.678112ms)
    Sep 20 13:03:07.411: INFO: (8) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 11.713659ms)
    Sep 20 13:03:07.411: INFO: (8) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 11.861507ms)
    Sep 20 13:03:07.411: INFO: (8) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 12.337324ms)
    Sep 20 13:03:07.412: INFO: (8) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 12.910824ms)
    Sep 20 13:03:07.412: INFO: (8) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 13.021913ms)
    Sep 20 13:03:07.412: INFO: (8) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 13.016613ms)
    Sep 20 13:03:07.413: INFO: (8) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 13.690613ms)
    Sep 20 13:03:07.414: INFO: (8) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 14.886276ms)
    Sep 20 13:03:07.414: INFO: (8) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 14.91995ms)
    Sep 20 13:03:07.414: INFO: (8) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 14.831943ms)
    Sep 20 13:03:07.414: INFO: (8) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 15.162396ms)
    Sep 20 13:03:07.414: INFO: (8) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 15.26021ms)
    Sep 20 13:03:07.416: INFO: (8) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 16.622007ms)
    Sep 20 13:03:07.418: INFO: (8) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 19.01265ms)
    Sep 20 13:03:07.431: INFO: (9) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 12.748718ms)
    Sep 20 13:03:07.432: INFO: (9) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 14.192229ms)
    Sep 20 13:03:07.432: INFO: (9) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 14.319759ms)
    Sep 20 13:03:07.432: INFO: (9) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 14.502422ms)
    Sep 20 13:03:07.437: INFO: (9) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 19.149748ms)
    Sep 20 13:03:07.439: INFO: (9) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 20.40837ms)
    Sep 20 13:03:07.439: INFO: (9) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 20.392149ms)
    Sep 20 13:03:07.440: INFO: (9) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 21.658053ms)
    Sep 20 13:03:07.440: INFO: (9) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 21.465452ms)
    Sep 20 13:03:07.440: INFO: (9) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 21.866537ms)
    Sep 20 13:03:07.441: INFO: (9) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 22.336131ms)
    Sep 20 13:03:07.441: INFO: (9) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 22.334348ms)
    Sep 20 13:03:07.441: INFO: (9) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 23.043114ms)
    Sep 20 13:03:07.442: INFO: (9) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 23.756657ms)
    Sep 20 13:03:07.442: INFO: (9) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 23.818514ms)
    Sep 20 13:03:07.443: INFO: (9) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 24.465533ms)
    Sep 20 13:03:07.450: INFO: (10) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 7.025937ms)
    Sep 20 13:03:07.454: INFO: (10) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 10.898042ms)
    Sep 20 13:03:07.454: INFO: (10) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 10.924412ms)
    Sep 20 13:03:07.454: INFO: (10) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 10.715739ms)
    Sep 20 13:03:07.454: INFO: (10) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 11.048325ms)
    Sep 20 13:03:07.454: INFO: (10) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 11.116423ms)
    Sep 20 13:03:07.455: INFO: (10) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 11.951567ms)
    Sep 20 13:03:07.455: INFO: (10) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 12.321654ms)
    Sep 20 13:03:07.458: INFO: (10) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 14.239658ms)
    Sep 20 13:03:07.458: INFO: (10) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 14.769125ms)
    Sep 20 13:03:07.458: INFO: (10) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 14.152674ms)
    Sep 20 13:03:07.458: INFO: (10) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 14.808119ms)
    Sep 20 13:03:07.459: INFO: (10) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 15.333138ms)
    Sep 20 13:03:07.459: INFO: (10) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 15.415463ms)
    Sep 20 13:03:07.459: INFO: (10) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 15.919934ms)
    Sep 20 13:03:07.460: INFO: (10) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 16.676559ms)
    Sep 20 13:03:07.466: INFO: (11) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 6.289089ms)
    Sep 20 13:03:07.466: INFO: (11) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 6.387674ms)
    Sep 20 13:03:07.471: INFO: (11) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 11.521666ms)
    Sep 20 13:03:07.471: INFO: (11) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 11.656151ms)
    Sep 20 13:03:07.472: INFO: (11) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 11.558476ms)
    Sep 20 13:03:07.472: INFO: (11) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 11.548657ms)
    Sep 20 13:03:07.473: INFO: (11) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 13.397201ms)
    Sep 20 13:03:07.473: INFO: (11) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 13.508822ms)
    Sep 20 13:03:07.474: INFO: (11) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 13.649966ms)
    Sep 20 13:03:07.474: INFO: (11) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 14.000166ms)
    Sep 20 13:03:07.474: INFO: (11) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 14.05502ms)
    Sep 20 13:03:07.474: INFO: (11) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 14.245818ms)
    Sep 20 13:03:07.475: INFO: (11) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 15.573982ms)
    Sep 20 13:03:07.476: INFO: (11) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 15.634867ms)
    Sep 20 13:03:07.476: INFO: (11) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 15.570395ms)
    Sep 20 13:03:07.476: INFO: (11) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 15.791371ms)
    Sep 20 13:03:07.484: INFO: (12) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 8.054645ms)
    Sep 20 13:03:07.487: INFO: (12) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 10.533395ms)
    Sep 20 13:03:07.487: INFO: (12) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 10.43532ms)
    Sep 20 13:03:07.487: INFO: (12) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 10.550818ms)
    Sep 20 13:03:07.487: INFO: (12) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 11.366124ms)
    Sep 20 13:03:07.490: INFO: (12) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 13.853179ms)
    Sep 20 13:03:07.490: INFO: (12) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 13.969991ms)
    Sep 20 13:03:07.491: INFO: (12) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 14.546036ms)
    Sep 20 13:03:07.491: INFO: (12) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 14.765249ms)
    Sep 20 13:03:07.491: INFO: (12) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 15.037562ms)
    Sep 20 13:03:07.491: INFO: (12) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 15.17963ms)
    Sep 20 13:03:07.492: INFO: (12) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 16.154355ms)
    Sep 20 13:03:07.493: INFO: (12) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 16.509745ms)
    Sep 20 13:03:07.493: INFO: (12) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 17.078636ms)
    Sep 20 13:03:07.493: INFO: (12) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 17.157074ms)
    Sep 20 13:03:07.495: INFO: (12) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 18.769532ms)
    Sep 20 13:03:07.503: INFO: (13) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 8.496658ms)
    Sep 20 13:03:07.507: INFO: (13) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 12.546448ms)
    Sep 20 13:03:07.508: INFO: (13) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 12.631056ms)
    Sep 20 13:03:07.508: INFO: (13) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 12.700337ms)
    Sep 20 13:03:07.509: INFO: (13) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 14.312635ms)
    Sep 20 13:03:07.509: INFO: (13) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 14.46926ms)
    Sep 20 13:03:07.511: INFO: (13) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 15.582528ms)
    Sep 20 13:03:07.511: INFO: (13) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 15.871212ms)
    Sep 20 13:03:07.511: INFO: (13) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 15.983373ms)
    Sep 20 13:03:07.512: INFO: (13) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 17.312627ms)
    Sep 20 13:03:07.513: INFO: (13) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 17.721477ms)
    Sep 20 13:03:07.513: INFO: (13) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 17.5376ms)
    Sep 20 13:03:07.513: INFO: (13) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 17.715427ms)
    Sep 20 13:03:07.513: INFO: (13) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 18.231499ms)
    Sep 20 13:03:07.514: INFO: (13) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 18.812243ms)
    Sep 20 13:03:07.514: INFO: (13) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 18.70447ms)
    Sep 20 13:03:07.524: INFO: (14) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 9.534062ms)
    Sep 20 13:03:07.524: INFO: (14) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 10.13248ms)
    Sep 20 13:03:07.524: INFO: (14) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 10.251333ms)
    Sep 20 13:03:07.524: INFO: (14) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 10.328168ms)
    Sep 20 13:03:07.525: INFO: (14) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 10.924822ms)
    Sep 20 13:03:07.526: INFO: (14) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 11.585327ms)
    Sep 20 13:03:07.526: INFO: (14) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 11.768542ms)
    Sep 20 13:03:07.527: INFO: (14) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 12.645935ms)
    Sep 20 13:03:07.527: INFO: (14) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 12.772984ms)
    Sep 20 13:03:07.527: INFO: (14) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 12.752516ms)
    Sep 20 13:03:07.527: INFO: (14) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 13.138053ms)
    Sep 20 13:03:07.527: INFO: (14) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 13.263278ms)
    Sep 20 13:03:07.528: INFO: (14) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 13.81064ms)
    Sep 20 13:03:07.528: INFO: (14) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 14.100406ms)
    Sep 20 13:03:07.529: INFO: (14) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 14.232034ms)
    Sep 20 13:03:07.529: INFO: (14) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 15.022973ms)
    Sep 20 13:03:07.535: INFO: (15) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 6.234917ms)
    Sep 20 13:03:07.536: INFO: (15) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 6.728898ms)
    Sep 20 13:03:07.539: INFO: (15) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 9.318004ms)
    Sep 20 13:03:07.539: INFO: (15) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 9.364442ms)
    Sep 20 13:03:07.539: INFO: (15) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 9.602221ms)
    Sep 20 13:03:07.540: INFO: (15) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 10.777045ms)
    Sep 20 13:03:07.540: INFO: (15) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 11.083382ms)
    Sep 20 13:03:07.540: INFO: (15) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 10.878846ms)
    Sep 20 13:03:07.541: INFO: (15) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 11.373698ms)
    Sep 20 13:03:07.541: INFO: (15) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 11.674195ms)
    Sep 20 13:03:07.541: INFO: (15) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 11.787208ms)
    Sep 20 13:03:07.542: INFO: (15) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 12.665381ms)
    Sep 20 13:03:07.542: INFO: (15) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 12.538292ms)
    Sep 20 13:03:07.542: INFO: (15) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 12.6963ms)
    Sep 20 13:03:07.542: INFO: (15) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 12.751915ms)
    Sep 20 13:03:07.543: INFO: (15) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 13.347777ms)
    Sep 20 13:03:07.548: INFO: (16) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 5.268756ms)
    Sep 20 13:03:07.549: INFO: (16) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 6.054768ms)
    Sep 20 13:03:07.552: INFO: (16) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 9.361828ms)
    Sep 20 13:03:07.552: INFO: (16) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 9.412304ms)
    Sep 20 13:03:07.552: INFO: (16) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 9.528882ms)
    Sep 20 13:03:07.552: INFO: (16) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 9.32578ms)
    Sep 20 13:03:07.552: INFO: (16) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 9.689005ms)
    Sep 20 13:03:07.553: INFO: (16) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 10.174098ms)
    Sep 20 13:03:07.554: INFO: (16) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 11.107988ms)
    Sep 20 13:03:07.554: INFO: (16) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 11.377145ms)
    Sep 20 13:03:07.555: INFO: (16) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 12.367741ms)
    Sep 20 13:03:07.555: INFO: (16) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 12.27731ms)
    Sep 20 13:03:07.556: INFO: (16) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 12.562077ms)
    Sep 20 13:03:07.556: INFO: (16) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 12.838187ms)
    Sep 20 13:03:07.556: INFO: (16) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 12.583527ms)
    Sep 20 13:03:07.556: INFO: (16) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 12.89231ms)
    Sep 20 13:03:07.574: INFO: (17) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 17.869466ms)
    Sep 20 13:03:07.574: INFO: (17) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 17.821876ms)
    Sep 20 13:03:07.574: INFO: (17) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 18.325425ms)
    Sep 20 13:03:07.574: INFO: (17) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 18.254481ms)
    Sep 20 13:03:07.574: INFO: (17) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 18.42921ms)
    Sep 20 13:03:07.575: INFO: (17) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 18.331306ms)
    Sep 20 13:03:07.575: INFO: (17) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 18.411728ms)
    Sep 20 13:03:07.575: INFO: (17) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 18.607658ms)
    Sep 20 13:03:07.575: INFO: (17) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 18.688919ms)
    Sep 20 13:03:07.575: INFO: (17) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 18.52394ms)
    Sep 20 13:03:07.575: INFO: (17) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 18.717173ms)
    Sep 20 13:03:07.575: INFO: (17) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 18.809869ms)
    Sep 20 13:03:07.575: INFO: (17) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 18.930344ms)
    Sep 20 13:03:07.575: INFO: (17) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 18.68916ms)
    Sep 20 13:03:07.575: INFO: (17) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 18.810109ms)
    Sep 20 13:03:07.575: INFO: (17) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 18.85904ms)
    Sep 20 13:03:07.581: INFO: (18) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 5.762376ms)
    Sep 20 13:03:07.584: INFO: (18) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 8.660516ms)
    Sep 20 13:03:07.589: INFO: (18) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 14.067614ms)
    Sep 20 13:03:07.589: INFO: (18) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 14.055771ms)
    Sep 20 13:03:07.589: INFO: (18) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 14.218117ms)
    Sep 20 13:03:07.589: INFO: (18) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 14.317415ms)
    Sep 20 13:03:07.589: INFO: (18) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 14.338995ms)
    Sep 20 13:03:07.589: INFO: (18) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 14.194202ms)
    Sep 20 13:03:07.593: INFO: (18) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 17.224071ms)
    Sep 20 13:03:07.593: INFO: (18) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 17.958754ms)
    Sep 20 13:03:07.593: INFO: (18) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 17.729503ms)
    Sep 20 13:03:07.593: INFO: (18) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 18.079882ms)
    Sep 20 13:03:07.593: INFO: (18) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 18.207132ms)
    Sep 20 13:03:07.594: INFO: (18) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 18.286592ms)
    Sep 20 13:03:07.594: INFO: (18) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 18.424392ms)
    Sep 20 13:03:07.594: INFO: (18) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 19.351388ms)
    Sep 20 13:03:07.603: INFO: (19) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">... (200; 8.545409ms)
    Sep 20 13:03:07.607: INFO: (19) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4/proxy/rewriteme">test</a> (200; 11.821631ms)
    Sep 20 13:03:07.609: INFO: (19) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 13.781615ms)
    Sep 20 13:03:07.609: INFO: (19) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:460/proxy/: tls baz (200; 13.784981ms)
    Sep 20 13:03:07.609: INFO: (19) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:162/proxy/: bar (200; 13.883777ms)
    Sep 20 13:03:07.612: INFO: (19) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:443/proxy/tlsrewritem... (200; 16.911852ms)
    Sep 20 13:03:07.612: INFO: (19) /api/v1/namespaces/proxy-3534/pods/https:proxy-service-kxxhg-m6cj4:462/proxy/: tls qux (200; 17.307548ms)
    Sep 20 13:03:07.616: INFO: (19) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 20.927488ms)
    Sep 20 13:03:07.616: INFO: (19) /api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/: <a href="/api/v1/namespaces/proxy-3534/pods/proxy-service-kxxhg-m6cj4:1080/proxy/rewriteme">test<... (200; 20.974126ms)
    Sep 20 13:03:07.616: INFO: (19) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname2/proxy/: tls qux (200; 21.211703ms)
    Sep 20 13:03:07.616: INFO: (19) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname2/proxy/: bar (200; 21.376915ms)
    Sep 20 13:03:07.616: INFO: (19) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname1/proxy/: foo (200; 21.440244ms)
    Sep 20 13:03:07.616: INFO: (19) /api/v1/namespaces/proxy-3534/pods/http:proxy-service-kxxhg-m6cj4:160/proxy/: foo (200; 21.547476ms)
    Sep 20 13:03:07.618: INFO: (19) /api/v1/namespaces/proxy-3534/services/proxy-service-kxxhg:portname1/proxy/: foo (200; 23.260414ms)
    Sep 20 13:03:07.619: INFO: (19) /api/v1/namespaces/proxy-3534/services/http:proxy-service-kxxhg:portname2/proxy/: bar (200; 24.297777ms)
    Sep 20 13:03:07.619: INFO: (19) /api/v1/namespaces/proxy-3534/services/https:proxy-service-kxxhg:tlsportname1/proxy/: tls baz (200; 24.285794ms)
    STEP: deleting ReplicationController proxy-service-kxxhg in namespace proxy-3534, will wait for the garbage collector to delete the pods 09/20/23 13:03:07.619
    Sep 20 13:03:07.694: INFO: Deleting ReplicationController proxy-service-kxxhg took: 19.663416ms
    Sep 20 13:03:08.095: INFO: Terminating ReplicationController proxy-service-kxxhg pods took: 400.289901ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:03:11.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-3534" for this suite. 09/20/23 13:03:11.502
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:03:11.687
Sep 20 13:03:11.687: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename crd-publish-openapi 09/20/23 13:03:11.688
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:03:11.897
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:03:11.901
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 09/20/23 13:03:11.905
Sep 20 13:03:11.906: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
Sep 20 13:03:14.597: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep 20 13:03:24.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6864" for this suite. 09/20/23 13:03:24.721
------------------------------
â€¢ [SLOW TEST] [13.181 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:03:11.687
    Sep 20 13:03:11.687: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename crd-publish-openapi 09/20/23 13:03:11.688
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:03:11.897
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:03:11.901
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 09/20/23 13:03:11.905
    Sep 20 13:03:11.906: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    Sep 20 13:03:14.597: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:03:24.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6864" for this suite. 09/20/23 13:03:24.721
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:03:24.869
Sep 20 13:03:24.869: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename deployment 09/20/23 13:03:24.87
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:03:24.988
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:03:24.992
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Sep 20 13:03:24.997: INFO: Creating deployment "test-recreate-deployment"
Sep 20 13:03:25.034: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Sep 20 13:03:25.041: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Sep 20 13:03:27.048: INFO: Waiting deployment "test-recreate-deployment" to complete
Sep 20 13:03:27.052: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 3, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 3, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 3, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 3, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-795566c5cb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 13:03:29.059: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 3, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 3, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 3, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 3, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-795566c5cb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 13:03:31.057: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 3, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 3, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 3, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 3, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-795566c5cb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 13:03:41.878: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Sep 20 13:03:42.053: INFO: Updating deployment test-recreate-deployment
Sep 20 13:03:42.053: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep 20 13:03:42.621: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-2977  d545e9df-e4d3-46f8-94f8-d2ff981cf017 26145 2 2023-09-20 13:03:24 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-20 13:03:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 13:03:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00521a258 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-09-20 13:03:42 +0000 UTC,LastTransitionTime:2023-09-20 13:03:42 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-09-20 13:03:42 +0000 UTC,LastTransitionTime:2023-09-20 13:03:25 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Sep 20 13:03:42.624: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-2977  cfc16e12-0f89-4286-8a32-645494d6c2b4 26141 1 2023-09-20 13:03:42 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment d545e9df-e4d3-46f8-94f8-d2ff981cf017 0xc002bd0e90 0xc002bd0e91}] [] [{kube-controller-manager Update apps/v1 2023-09-20 13:03:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d545e9df-e4d3-46f8-94f8-d2ff981cf017\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 13:03:42 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002bd0f28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 20 13:03:42.624: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Sep 20 13:03:42.624: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-2977  4817bc9a-66de-4d41-91c2-a65752aa49af 26133 2 2023-09-20 13:03:25 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment d545e9df-e4d3-46f8-94f8-d2ff981cf017 0xc002bd0d77 0xc002bd0d78}] [] [{kube-controller-manager Update apps/v1 2023-09-20 13:03:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d545e9df-e4d3-46f8-94f8-d2ff981cf017\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 13:03:42 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002bd0e28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 20 13:03:42.630: INFO: Pod "test-recreate-deployment-cff6dc657-j6sz4" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-j6sz4 test-recreate-deployment-cff6dc657- deployment-2977  88aa650d-b19a-4aa9-82dd-94665117dfb7 26143 0 2023-09-20 13:03:42 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 cfc16e12-0f89-4286-8a32-645494d6c2b4 0xc00521a5f0 0xc00521a5f1}] [] [{kube-controller-manager Update v1 2023-09-20 13:03:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cfc16e12-0f89-4286-8a32-645494d6c2b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:03:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sdmtc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sdmtc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:03:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:03:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:03:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:03:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.64,PodIP:,StartTime:2023-09-20 13:03:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep 20 13:03:42.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2977" for this suite. 09/20/23 13:03:42.635
------------------------------
â€¢ [SLOW TEST] [17.771 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:03:24.869
    Sep 20 13:03:24.869: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename deployment 09/20/23 13:03:24.87
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:03:24.988
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:03:24.992
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Sep 20 13:03:24.997: INFO: Creating deployment "test-recreate-deployment"
    Sep 20 13:03:25.034: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Sep 20 13:03:25.041: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
    Sep 20 13:03:27.048: INFO: Waiting deployment "test-recreate-deployment" to complete
    Sep 20 13:03:27.052: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 3, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 3, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 3, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 3, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-795566c5cb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 13:03:29.059: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 3, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 3, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 3, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 3, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-795566c5cb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 13:03:31.057: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 3, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 3, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 3, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 3, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-795566c5cb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 13:03:41.878: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Sep 20 13:03:42.053: INFO: Updating deployment test-recreate-deployment
    Sep 20 13:03:42.053: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep 20 13:03:42.621: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-2977  d545e9df-e4d3-46f8-94f8-d2ff981cf017 26145 2 2023-09-20 13:03:24 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-20 13:03:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 13:03:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00521a258 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-09-20 13:03:42 +0000 UTC,LastTransitionTime:2023-09-20 13:03:42 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-09-20 13:03:42 +0000 UTC,LastTransitionTime:2023-09-20 13:03:25 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Sep 20 13:03:42.624: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-2977  cfc16e12-0f89-4286-8a32-645494d6c2b4 26141 1 2023-09-20 13:03:42 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment d545e9df-e4d3-46f8-94f8-d2ff981cf017 0xc002bd0e90 0xc002bd0e91}] [] [{kube-controller-manager Update apps/v1 2023-09-20 13:03:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d545e9df-e4d3-46f8-94f8-d2ff981cf017\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 13:03:42 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002bd0f28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep 20 13:03:42.624: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Sep 20 13:03:42.624: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-2977  4817bc9a-66de-4d41-91c2-a65752aa49af 26133 2 2023-09-20 13:03:25 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment d545e9df-e4d3-46f8-94f8-d2ff981cf017 0xc002bd0d77 0xc002bd0d78}] [] [{kube-controller-manager Update apps/v1 2023-09-20 13:03:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d545e9df-e4d3-46f8-94f8-d2ff981cf017\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 13:03:42 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002bd0e28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep 20 13:03:42.630: INFO: Pod "test-recreate-deployment-cff6dc657-j6sz4" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-j6sz4 test-recreate-deployment-cff6dc657- deployment-2977  88aa650d-b19a-4aa9-82dd-94665117dfb7 26143 0 2023-09-20 13:03:42 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 cfc16e12-0f89-4286-8a32-645494d6c2b4 0xc00521a5f0 0xc00521a5f1}] [] [{kube-controller-manager Update v1 2023-09-20 13:03:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cfc16e12-0f89-4286-8a32-645494d6c2b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:03:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sdmtc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sdmtc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:03:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:03:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:03:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:03:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.64,PodIP:,StartTime:2023-09-20 13:03:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:03:42.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2977" for this suite. 09/20/23 13:03:42.635
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:03:42.642
Sep 20 13:03:42.642: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename webhook 09/20/23 13:03:42.643
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:03:42.697
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:03:42.7
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/20/23 13:03:42.715
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/20/23 13:03:43.097
STEP: Deploying the webhook pod 09/20/23 13:03:43.303
STEP: Wait for the deployment to be ready 09/20/23 13:03:43.324
Sep 20 13:03:43.332: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep 20 13:03:45.407: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 3, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 3, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 3, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 3, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 13:03:47.903: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 3, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 3, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 3, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 3, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/20/23 13:03:49.413
STEP: Verifying the service has paired with the endpoint 09/20/23 13:03:49.535
Sep 20 13:03:50.535: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 09/20/23 13:03:50.798
STEP: Updating a mutating webhook configuration's rules to not include the create operation 09/20/23 13:03:50.979
STEP: Creating a configMap that should not be mutated 09/20/23 13:03:50.991
STEP: Patching a mutating webhook configuration's rules to include the create operation 09/20/23 13:03:51.025
STEP: Creating a configMap that should be mutated 09/20/23 13:03:51.295
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep 20 13:03:51.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9497" for this suite. 09/20/23 13:03:51.421
STEP: Destroying namespace "webhook-9497-markers" for this suite. 09/20/23 13:03:51.432
------------------------------
â€¢ [SLOW TEST] [8.805 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:03:42.642
    Sep 20 13:03:42.642: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename webhook 09/20/23 13:03:42.643
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:03:42.697
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:03:42.7
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/20/23 13:03:42.715
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/20/23 13:03:43.097
    STEP: Deploying the webhook pod 09/20/23 13:03:43.303
    STEP: Wait for the deployment to be ready 09/20/23 13:03:43.324
    Sep 20 13:03:43.332: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Sep 20 13:03:45.407: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 3, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 3, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 3, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 3, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 13:03:47.903: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 3, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 3, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 3, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 3, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/20/23 13:03:49.413
    STEP: Verifying the service has paired with the endpoint 09/20/23 13:03:49.535
    Sep 20 13:03:50.535: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 09/20/23 13:03:50.798
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 09/20/23 13:03:50.979
    STEP: Creating a configMap that should not be mutated 09/20/23 13:03:50.991
    STEP: Patching a mutating webhook configuration's rules to include the create operation 09/20/23 13:03:51.025
    STEP: Creating a configMap that should be mutated 09/20/23 13:03:51.295
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:03:51.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9497" for this suite. 09/20/23 13:03:51.421
    STEP: Destroying namespace "webhook-9497-markers" for this suite. 09/20/23 13:03:51.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:03:51.451
Sep 20 13:03:51.451: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename pods 09/20/23 13:03:51.451
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:03:51.491
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:03:51.498
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 09/20/23 13:03:51.505
STEP: submitting the pod to kubernetes 09/20/23 13:03:51.505
Sep 20 13:03:51.524: INFO: Waiting up to 5m0s for pod "pod-update-f3443f71-dd92-4086-b065-53ef521c8183" in namespace "pods-2750" to be "running and ready"
Sep 20 13:03:51.532: INFO: Pod "pod-update-f3443f71-dd92-4086-b065-53ef521c8183": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016323ms
Sep 20 13:03:51.532: INFO: The phase of Pod pod-update-f3443f71-dd92-4086-b065-53ef521c8183 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:03:54.024: INFO: Pod "pod-update-f3443f71-dd92-4086-b065-53ef521c8183": Phase="Pending", Reason="", readiness=false. Elapsed: 2.499676165s
Sep 20 13:03:54.024: INFO: The phase of Pod pod-update-f3443f71-dd92-4086-b065-53ef521c8183 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:03:55.657: INFO: Pod "pod-update-f3443f71-dd92-4086-b065-53ef521c8183": Phase="Running", Reason="", readiness=true. Elapsed: 4.132315311s
Sep 20 13:03:55.657: INFO: The phase of Pod pod-update-f3443f71-dd92-4086-b065-53ef521c8183 is Running (Ready = true)
Sep 20 13:03:55.657: INFO: Pod "pod-update-f3443f71-dd92-4086-b065-53ef521c8183" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 09/20/23 13:03:55.66
STEP: updating the pod 09/20/23 13:03:55.666
Sep 20 13:03:56.262: INFO: Successfully updated pod "pod-update-f3443f71-dd92-4086-b065-53ef521c8183"
Sep 20 13:03:56.262: INFO: Waiting up to 5m0s for pod "pod-update-f3443f71-dd92-4086-b065-53ef521c8183" in namespace "pods-2750" to be "running"
Sep 20 13:03:56.265: INFO: Pod "pod-update-f3443f71-dd92-4086-b065-53ef521c8183": Phase="Running", Reason="", readiness=true. Elapsed: 3.961644ms
Sep 20 13:03:56.266: INFO: Pod "pod-update-f3443f71-dd92-4086-b065-53ef521c8183" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 09/20/23 13:03:56.266
Sep 20 13:03:56.272: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep 20 13:03:56.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2750" for this suite. 09/20/23 13:03:56.279
------------------------------
â€¢ [4.874 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:03:51.451
    Sep 20 13:03:51.451: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename pods 09/20/23 13:03:51.451
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:03:51.491
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:03:51.498
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 09/20/23 13:03:51.505
    STEP: submitting the pod to kubernetes 09/20/23 13:03:51.505
    Sep 20 13:03:51.524: INFO: Waiting up to 5m0s for pod "pod-update-f3443f71-dd92-4086-b065-53ef521c8183" in namespace "pods-2750" to be "running and ready"
    Sep 20 13:03:51.532: INFO: Pod "pod-update-f3443f71-dd92-4086-b065-53ef521c8183": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016323ms
    Sep 20 13:03:51.532: INFO: The phase of Pod pod-update-f3443f71-dd92-4086-b065-53ef521c8183 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:03:54.024: INFO: Pod "pod-update-f3443f71-dd92-4086-b065-53ef521c8183": Phase="Pending", Reason="", readiness=false. Elapsed: 2.499676165s
    Sep 20 13:03:54.024: INFO: The phase of Pod pod-update-f3443f71-dd92-4086-b065-53ef521c8183 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:03:55.657: INFO: Pod "pod-update-f3443f71-dd92-4086-b065-53ef521c8183": Phase="Running", Reason="", readiness=true. Elapsed: 4.132315311s
    Sep 20 13:03:55.657: INFO: The phase of Pod pod-update-f3443f71-dd92-4086-b065-53ef521c8183 is Running (Ready = true)
    Sep 20 13:03:55.657: INFO: Pod "pod-update-f3443f71-dd92-4086-b065-53ef521c8183" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 09/20/23 13:03:55.66
    STEP: updating the pod 09/20/23 13:03:55.666
    Sep 20 13:03:56.262: INFO: Successfully updated pod "pod-update-f3443f71-dd92-4086-b065-53ef521c8183"
    Sep 20 13:03:56.262: INFO: Waiting up to 5m0s for pod "pod-update-f3443f71-dd92-4086-b065-53ef521c8183" in namespace "pods-2750" to be "running"
    Sep 20 13:03:56.265: INFO: Pod "pod-update-f3443f71-dd92-4086-b065-53ef521c8183": Phase="Running", Reason="", readiness=true. Elapsed: 3.961644ms
    Sep 20 13:03:56.266: INFO: Pod "pod-update-f3443f71-dd92-4086-b065-53ef521c8183" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 09/20/23 13:03:56.266
    Sep 20 13:03:56.272: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:03:56.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2750" for this suite. 09/20/23 13:03:56.279
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:03:56.325
Sep 20 13:03:56.325: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename var-expansion 09/20/23 13:03:56.326
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:03:58.359
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:03:58.363
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Sep 20 13:03:58.385: INFO: Waiting up to 2m0s for pod "var-expansion-f4c8df44-2f61-45e3-8aab-e81fa4ce58f3" in namespace "var-expansion-3882" to be "container 0 failed with reason CreateContainerConfigError"
Sep 20 13:03:58.388: INFO: Pod "var-expansion-f4c8df44-2f61-45e3-8aab-e81fa4ce58f3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.446624ms
Sep 20 13:04:01.234: INFO: Pod "var-expansion-f4c8df44-2f61-45e3-8aab-e81fa4ce58f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.849490621s
Sep 20 13:04:02.396: INFO: Pod "var-expansion-f4c8df44-2f61-45e3-8aab-e81fa4ce58f3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010881949s
Sep 20 13:04:02.396: INFO: Pod "var-expansion-f4c8df44-2f61-45e3-8aab-e81fa4ce58f3" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Sep 20 13:04:02.396: INFO: Deleting pod "var-expansion-f4c8df44-2f61-45e3-8aab-e81fa4ce58f3" in namespace "var-expansion-3882"
Sep 20 13:04:02.596: INFO: Wait up to 5m0s for pod "var-expansion-f4c8df44-2f61-45e3-8aab-e81fa4ce58f3" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep 20 13:04:04.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-3882" for this suite. 09/20/23 13:04:04.611
------------------------------
â€¢ [SLOW TEST] [9.557 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:03:56.325
    Sep 20 13:03:56.325: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename var-expansion 09/20/23 13:03:56.326
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:03:58.359
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:03:58.363
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Sep 20 13:03:58.385: INFO: Waiting up to 2m0s for pod "var-expansion-f4c8df44-2f61-45e3-8aab-e81fa4ce58f3" in namespace "var-expansion-3882" to be "container 0 failed with reason CreateContainerConfigError"
    Sep 20 13:03:58.388: INFO: Pod "var-expansion-f4c8df44-2f61-45e3-8aab-e81fa4ce58f3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.446624ms
    Sep 20 13:04:01.234: INFO: Pod "var-expansion-f4c8df44-2f61-45e3-8aab-e81fa4ce58f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.849490621s
    Sep 20 13:04:02.396: INFO: Pod "var-expansion-f4c8df44-2f61-45e3-8aab-e81fa4ce58f3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010881949s
    Sep 20 13:04:02.396: INFO: Pod "var-expansion-f4c8df44-2f61-45e3-8aab-e81fa4ce58f3" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Sep 20 13:04:02.396: INFO: Deleting pod "var-expansion-f4c8df44-2f61-45e3-8aab-e81fa4ce58f3" in namespace "var-expansion-3882"
    Sep 20 13:04:02.596: INFO: Wait up to 5m0s for pod "var-expansion-f4c8df44-2f61-45e3-8aab-e81fa4ce58f3" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:04:04.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-3882" for this suite. 09/20/23 13:04:04.611
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:04:05.883
Sep 20 13:04:05.883: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename namespaces 09/20/23 13:04:05.884
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:04:06.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:04:06.225
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-lffj6" 09/20/23 13:04:06.231
Sep 20 13:04:06.665: INFO: Namespace "e2e-ns-lffj6-9128" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-lffj6-9128" 09/20/23 13:04:06.665
Sep 20 13:04:06.681: INFO: Namespace "e2e-ns-lffj6-9128" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-lffj6-9128" 09/20/23 13:04:06.681
Sep 20 13:04:06.694: INFO: Namespace "e2e-ns-lffj6-9128" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep 20 13:04:06.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-5323" for this suite. 09/20/23 13:04:06.699
STEP: Destroying namespace "e2e-ns-lffj6-9128" for this suite. 09/20/23 13:04:06.709
------------------------------
â€¢ [0.838 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:04:05.883
    Sep 20 13:04:05.883: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename namespaces 09/20/23 13:04:05.884
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:04:06.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:04:06.225
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-lffj6" 09/20/23 13:04:06.231
    Sep 20 13:04:06.665: INFO: Namespace "e2e-ns-lffj6-9128" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-lffj6-9128" 09/20/23 13:04:06.665
    Sep 20 13:04:06.681: INFO: Namespace "e2e-ns-lffj6-9128" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-lffj6-9128" 09/20/23 13:04:06.681
    Sep 20 13:04:06.694: INFO: Namespace "e2e-ns-lffj6-9128" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:04:06.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-5323" for this suite. 09/20/23 13:04:06.699
    STEP: Destroying namespace "e2e-ns-lffj6-9128" for this suite. 09/20/23 13:04:06.709
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:04:06.721
Sep 20 13:04:06.721: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename daemonsets 09/20/23 13:04:06.722
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:04:06.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:04:06.753
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305
STEP: Creating a simple DaemonSet "daemon-set" 09/20/23 13:04:06.787
STEP: Check that daemon pods launch on every node of the cluster. 09/20/23 13:04:06.794
Sep 20 13:04:06.798: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:04:06.798: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:04:06.798: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:04:06.801: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 13:04:06.801: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 13:04:07.809: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:04:07.809: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:04:07.809: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:04:07.813: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 13:04:07.813: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 13:04:09.002: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:04:09.002: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:04:09.002: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:04:09.086: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 13:04:09.086: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 13:04:09.807: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:04:09.808: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:04:09.808: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:04:09.811: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep 20 13:04:09.811: INFO: Node mycluster-ww3cg64etuwi-node-1 is running 0 daemon pod, expected 1
Sep 20 13:04:10.808: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:04:10.808: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:04:10.808: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:04:10.812: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Sep 20 13:04:10.812: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 09/20/23 13:04:10.817
Sep 20 13:04:11.480: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:04:11.481: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:04:11.481: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:04:11.490: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Sep 20 13:04:11.490: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 09/20/23 13:04:11.49
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 09/20/23 13:04:12.503
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3719, will wait for the garbage collector to delete the pods 09/20/23 13:04:12.503
Sep 20 13:04:12.564: INFO: Deleting DaemonSet.extensions daemon-set took: 6.982396ms
Sep 20 13:04:12.865: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.514312ms
Sep 20 13:04:17.972: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 13:04:17.972: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep 20 13:04:17.975: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26493"},"items":null}

Sep 20 13:04:17.977: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26493"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep 20 13:04:17.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3719" for this suite. 09/20/23 13:04:17.992
------------------------------
â€¢ [SLOW TEST] [11.278 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:04:06.721
    Sep 20 13:04:06.721: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename daemonsets 09/20/23 13:04:06.722
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:04:06.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:04:06.753
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:305
    STEP: Creating a simple DaemonSet "daemon-set" 09/20/23 13:04:06.787
    STEP: Check that daemon pods launch on every node of the cluster. 09/20/23 13:04:06.794
    Sep 20 13:04:06.798: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:04:06.798: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:04:06.798: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:04:06.801: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 13:04:06.801: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 13:04:07.809: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:04:07.809: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:04:07.809: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:04:07.813: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 13:04:07.813: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 13:04:09.002: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:04:09.002: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:04:09.002: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:04:09.086: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 13:04:09.086: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 13:04:09.807: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:04:09.808: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:04:09.808: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:04:09.811: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep 20 13:04:09.811: INFO: Node mycluster-ww3cg64etuwi-node-1 is running 0 daemon pod, expected 1
    Sep 20 13:04:10.808: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:04:10.808: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:04:10.808: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:04:10.812: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Sep 20 13:04:10.812: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 09/20/23 13:04:10.817
    Sep 20 13:04:11.480: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:04:11.481: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:04:11.481: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:04:11.490: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Sep 20 13:04:11.490: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 09/20/23 13:04:11.49
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 09/20/23 13:04:12.503
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3719, will wait for the garbage collector to delete the pods 09/20/23 13:04:12.503
    Sep 20 13:04:12.564: INFO: Deleting DaemonSet.extensions daemon-set took: 6.982396ms
    Sep 20 13:04:12.865: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.514312ms
    Sep 20 13:04:17.972: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 13:04:17.972: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep 20 13:04:17.975: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26493"},"items":null}

    Sep 20 13:04:17.977: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26493"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:04:17.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3719" for this suite. 09/20/23 13:04:17.992
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:04:18.002
Sep 20 13:04:18.002: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename deployment 09/20/23 13:04:18.002
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:04:18.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:04:18.077
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Sep 20 13:04:18.081: INFO: Creating deployment "webserver-deployment"
Sep 20 13:04:18.088: INFO: Waiting for observed generation 1
Sep 20 13:04:20.207: INFO: Waiting for all required pods to come up
Sep 20 13:04:20.220: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 09/20/23 13:04:20.22
Sep 20 13:04:20.220: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-t5vjj" in namespace "deployment-5759" to be "running"
Sep 20 13:04:20.220: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-6kk4p" in namespace "deployment-5759" to be "running"
Sep 20 13:04:20.221: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-hwggb" in namespace "deployment-5759" to be "running"
Sep 20 13:04:20.221: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-jdmf4" in namespace "deployment-5759" to be "running"
Sep 20 13:04:20.221: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-92v9v" in namespace "deployment-5759" to be "running"
Sep 20 13:04:20.220: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-7rc9g" in namespace "deployment-5759" to be "running"
Sep 20 13:04:20.220: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-2x4t7" in namespace "deployment-5759" to be "running"
Sep 20 13:04:20.221: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-9tffr" in namespace "deployment-5759" to be "running"
Sep 20 13:04:20.220: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-4jt2s" in namespace "deployment-5759" to be "running"
Sep 20 13:04:20.220: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-5x74q" in namespace "deployment-5759" to be "running"
Sep 20 13:04:20.232: INFO: Pod "webserver-deployment-7f5969cbc7-hwggb": Phase="Pending", Reason="", readiness=false. Elapsed: 11.334605ms
Sep 20 13:04:20.232: INFO: Pod "webserver-deployment-7f5969cbc7-6kk4p": Phase="Pending", Reason="", readiness=false. Elapsed: 11.553236ms
Sep 20 13:04:20.237: INFO: Pod "webserver-deployment-7f5969cbc7-92v9v": Phase="Pending", Reason="", readiness=false. Elapsed: 16.235098ms
Sep 20 13:04:20.240: INFO: Pod "webserver-deployment-7f5969cbc7-jdmf4": Phase="Pending", Reason="", readiness=false. Elapsed: 19.597532ms
Sep 20 13:04:20.241: INFO: Pod "webserver-deployment-7f5969cbc7-9tffr": Phase="Pending", Reason="", readiness=false. Elapsed: 19.61692ms
Sep 20 13:04:20.241: INFO: Pod "webserver-deployment-7f5969cbc7-4jt2s": Phase="Pending", Reason="", readiness=false. Elapsed: 19.402225ms
Sep 20 13:04:20.241: INFO: Pod "webserver-deployment-7f5969cbc7-2x4t7": Phase="Pending", Reason="", readiness=false. Elapsed: 19.73488ms
Sep 20 13:04:20.241: INFO: Pod "webserver-deployment-7f5969cbc7-t5vjj": Phase="Pending", Reason="", readiness=false. Elapsed: 20.261724ms
Sep 20 13:04:20.241: INFO: Pod "webserver-deployment-7f5969cbc7-5x74q": Phase="Pending", Reason="", readiness=false. Elapsed: 19.345668ms
Sep 20 13:04:20.241: INFO: Pod "webserver-deployment-7f5969cbc7-7rc9g": Phase="Pending", Reason="", readiness=false. Elapsed: 20.138641ms
Sep 20 13:04:22.271: INFO: Pod "webserver-deployment-7f5969cbc7-7rc9g": Phase="Running", Reason="", readiness=true. Elapsed: 2.049730842s
Sep 20 13:04:22.271: INFO: Pod "webserver-deployment-7f5969cbc7-7rc9g" satisfied condition "running"
Sep 20 13:04:22.271: INFO: Pod "webserver-deployment-7f5969cbc7-6kk4p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050218501s
Sep 20 13:04:22.271: INFO: Pod "webserver-deployment-7f5969cbc7-hwggb": Phase="Running", Reason="", readiness=true. Elapsed: 2.050331495s
Sep 20 13:04:22.271: INFO: Pod "webserver-deployment-7f5969cbc7-hwggb" satisfied condition "running"
Sep 20 13:04:22.271: INFO: Pod "webserver-deployment-7f5969cbc7-4jt2s": Phase="Running", Reason="", readiness=true. Elapsed: 2.049705656s
Sep 20 13:04:22.271: INFO: Pod "webserver-deployment-7f5969cbc7-4jt2s" satisfied condition "running"
Sep 20 13:04:22.271: INFO: Pod "webserver-deployment-7f5969cbc7-jdmf4": Phase="Running", Reason="", readiness=true. Elapsed: 2.050352013s
Sep 20 13:04:22.271: INFO: Pod "webserver-deployment-7f5969cbc7-jdmf4" satisfied condition "running"
Sep 20 13:04:22.271: INFO: Pod "webserver-deployment-7f5969cbc7-t5vjj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0508175s
Sep 20 13:04:22.271: INFO: Pod "webserver-deployment-7f5969cbc7-2x4t7": Phase="Running", Reason="", readiness=true. Elapsed: 2.050144151s
Sep 20 13:04:22.271: INFO: Pod "webserver-deployment-7f5969cbc7-2x4t7" satisfied condition "running"
Sep 20 13:04:22.271: INFO: Pod "webserver-deployment-7f5969cbc7-9tffr": Phase="Running", Reason="", readiness=true. Elapsed: 2.0501749s
Sep 20 13:04:22.272: INFO: Pod "webserver-deployment-7f5969cbc7-9tffr" satisfied condition "running"
Sep 20 13:04:22.271: INFO: Pod "webserver-deployment-7f5969cbc7-5x74q": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050111019s
Sep 20 13:04:22.272: INFO: Pod "webserver-deployment-7f5969cbc7-92v9v": Phase="Running", Reason="", readiness=true. Elapsed: 2.050804275s
Sep 20 13:04:22.272: INFO: Pod "webserver-deployment-7f5969cbc7-92v9v" satisfied condition "running"
Sep 20 13:04:24.261: INFO: Pod "webserver-deployment-7f5969cbc7-6kk4p": Phase="Running", Reason="", readiness=true. Elapsed: 4.040947022s
Sep 20 13:04:24.261: INFO: Pod "webserver-deployment-7f5969cbc7-6kk4p" satisfied condition "running"
Sep 20 13:04:24.261: INFO: Pod "webserver-deployment-7f5969cbc7-t5vjj": Phase="Running", Reason="", readiness=true. Elapsed: 4.041036341s
Sep 20 13:04:24.261: INFO: Pod "webserver-deployment-7f5969cbc7-t5vjj" satisfied condition "running"
Sep 20 13:04:24.261: INFO: Pod "webserver-deployment-7f5969cbc7-5x74q": Phase="Running", Reason="", readiness=true. Elapsed: 4.040053108s
Sep 20 13:04:24.262: INFO: Pod "webserver-deployment-7f5969cbc7-5x74q" satisfied condition "running"
Sep 20 13:04:24.262: INFO: Waiting for deployment "webserver-deployment" to complete
Sep 20 13:04:24.264: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:10, UpdatedReplicas:10, ReadyReplicas:9, AvailableReplicas:9, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 4, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 4, 23, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 4, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 4, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"webserver-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 13:04:26.276: INFO: Updating deployment "webserver-deployment" with a non-existent image
Sep 20 13:04:26.318: INFO: Updating deployment webserver-deployment
Sep 20 13:04:26.318: INFO: Waiting for observed generation 2
Sep 20 13:04:28.326: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Sep 20 13:04:28.333: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Sep 20 13:04:28.335: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep 20 13:04:28.344: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Sep 20 13:04:28.344: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Sep 20 13:04:28.368: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep 20 13:04:28.376: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Sep 20 13:04:28.376: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Sep 20 13:04:28.420: INFO: Updating deployment webserver-deployment
Sep 20 13:04:28.420: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Sep 20 13:04:28.428: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Sep 20 13:04:30.441: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep 20 13:04:30.449: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-5759  1f51e9cd-e204-4e79-9bcd-cdc4f2049546 26825 3 2023-09-20 13:04:18 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005cc72c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-09-20 13:04:28 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-09-20 13:04:28 +0000 UTC,LastTransitionTime:2023-09-20 13:04:18 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Sep 20 13:04:30.452: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-5759  c2687f67-bb17-49b0-b83b-e0f4e4829ede 26819 3 2023-09-20 13:04:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 1f51e9cd-e204-4e79-9bcd-cdc4f2049546 0xc005cc7827 0xc005cc7828}] [] [{kube-controller-manager Update apps/v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1f51e9cd-e204-4e79-9bcd-cdc4f2049546\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005cc78d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 20 13:04:30.452: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Sep 20 13:04:30.452: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-5759  fa55794c-9e9a-432b-94c3-9ad1abda8938 26821 3 2023-09-20 13:04:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 1f51e9cd-e204-4e79-9bcd-cdc4f2049546 0xc005cc7727 0xc005cc7728}] [] [{kube-controller-manager Update apps/v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1f51e9cd-e204-4e79-9bcd-cdc4f2049546\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005cc77b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Sep 20 13:04:30.458: INFO: Pod "webserver-deployment-7f5969cbc7-2x4t7" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2x4t7 webserver-deployment-7f5969cbc7- deployment-5759  a3b31620-6506-43a4-aa01-0d6f9e925cc9 26584 0 2023-09-20 13:04:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc005cc7e27 0xc005cc7e28}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.5.95\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2fb98,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2fb98,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.173,PodIP:10.100.5.95,StartTime:2023-09-20 13:04:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-20 13:04:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://11c2d638c8c43ddd5048934cafe1530c3bcbcf6e454524e7712e3cb3dcf23fea,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.5.95,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 13:04:30.459: INFO: Pod "webserver-deployment-7f5969cbc7-4jt2s" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-4jt2s webserver-deployment-7f5969cbc7- deployment-5759  82028fbc-5570-4fda-8676-dee26034c1e0 26579 0 2023-09-20 13:04:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc003a94040 0xc003a94041}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.3.133\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zjgcq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zjgcq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.172,PodIP:10.100.3.133,StartTime:2023-09-20 13:04:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-20 13:04:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b9a86ae3fc14033374c94c64f9e045dced3310d35e2f5adf55042b7155615157,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.3.133,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 13:04:30.459: INFO: Pod "webserver-deployment-7f5969cbc7-4w8h5" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-4w8h5 webserver-deployment-7f5969cbc7- deployment-5759  952264b8-9d09-466e-b94e-4e1816356e03 26789 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc003a94237 0xc003a94238}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-khpm6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-khpm6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.173,PodIP:,StartTime:2023-09-20 13:04:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 13:04:30.459: INFO: Pod "webserver-deployment-7f5969cbc7-7rc9g" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7rc9g webserver-deployment-7f5969cbc7- deployment-5759  6d72578d-39ab-4529-bc34-497c4052972f 26610 0 2023-09-20 13:04:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc003a943f7 0xc003a943f8}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.5.96\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rkg47,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rkg47,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.173,PodIP:10.100.5.96,StartTime:2023-09-20 13:04:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-20 13:04:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f19397ea3fb34911d0733e07c77a0884f591e234450a7c9d7f42a16d10623e3f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.5.96,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 13:04:30.460: INFO: Pod "webserver-deployment-7f5969cbc7-7rl54" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7rl54 webserver-deployment-7f5969cbc7- deployment-5759  2d5d9745-65c8-4124-a8db-9899fb7f37ce 26761 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc003a945e0 0xc003a945e1}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8n48j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8n48j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.64,PodIP:,StartTime:2023-09-20 13:04:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 13:04:30.460: INFO: Pod "webserver-deployment-7f5969cbc7-826b5" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-826b5 webserver-deployment-7f5969cbc7- deployment-5759  b7f602b7-e8ce-48bb-a3e9-2fbacea78061 26853 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc003a947b7 0xc003a947b8}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-knfvg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-knfvg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.64,PodIP:,StartTime:2023-09-20 13:04:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 13:04:30.460: INFO: Pod "webserver-deployment-7f5969cbc7-8qrjh" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-8qrjh webserver-deployment-7f5969cbc7- deployment-5759  57eeebe3-033e-4b2e-9b18-179fcb7bec93 26829 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc003a94977 0xc003a94978}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-556mk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-556mk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.172,PodIP:,StartTime:2023-09-20 13:04:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 13:04:30.460: INFO: Pod "webserver-deployment-7f5969cbc7-92v9v" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-92v9v webserver-deployment-7f5969cbc7- deployment-5759  d98e0485-074d-4cbd-b3b5-bc8c8b748e6f 26594 0 2023-09-20 13:04:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc003a94b37 0xc003a94b38}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.4.169\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r58tm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r58tm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.64,PodIP:10.100.4.169,StartTime:2023-09-20 13:04:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-20 13:04:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0d6dd355fd8c95688e2389a91d3b11e15bfe9914cb31677d49ac3bda366bad31,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.4.169,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 13:04:30.461: INFO: Pod "webserver-deployment-7f5969cbc7-9tffr" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9tffr webserver-deployment-7f5969cbc7- deployment-5759  c8002e42-1145-4805-9ca0-18534c9dae46 26611 0 2023-09-20 13:04:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc003a94d17 0xc003a94d18}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.3.134\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9lcdd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9lcdd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.172,PodIP:10.100.3.134,StartTime:2023-09-20 13:04:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-20 13:04:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a7c4c712f469c682483a488c640b8c4b038da8556ee7e48486799f963b439e86,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.3.134,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 13:04:30.461: INFO: Pod "webserver-deployment-7f5969cbc7-bgndw" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bgndw webserver-deployment-7f5969cbc7- deployment-5759  f0c5bc2b-b36f-4fc2-aa13-a65f920db5db 26791 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc003a94ef7 0xc003a94ef8}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bcrq9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bcrq9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 13:04:30.461: INFO: Pod "webserver-deployment-7f5969cbc7-f8s29" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-f8s29 webserver-deployment-7f5969cbc7- deployment-5759  fad8ea29-8640-46e8-9635-7b849bd1ca0d 26817 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc003a95060 0xc003a95061}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4289k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4289k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.173,PodIP:,StartTime:2023-09-20 13:04:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 13:04:30.461: INFO: Pod "webserver-deployment-7f5969cbc7-hqs89" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hqs89 webserver-deployment-7f5969cbc7- deployment-5759  ac3ba0c7-6e72-4a76-a2eb-db182c251863 26760 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc003a95247 0xc003a95248}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zbf5s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zbf5s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.173,PodIP:,StartTime:2023-09-20 13:04:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 13:04:30.462: INFO: Pod "webserver-deployment-7f5969cbc7-hwggb" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hwggb webserver-deployment-7f5969cbc7- deployment-5759  1bd308b8-f124-4a3d-966e-fe0dfd112a36 26608 0 2023-09-20 13:04:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc003a95427 0xc003a95428}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.3.135\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cjtc2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cjtc2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.172,PodIP:10.100.3.135,StartTime:2023-09-20 13:04:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-20 13:04:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c1466d369d80552d31ecc735d70f5a3fb3ef850a9c1c333c8ddcea3224d98f8e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.3.135,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 13:04:30.462: INFO: Pod "webserver-deployment-7f5969cbc7-jdmf4" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jdmf4 webserver-deployment-7f5969cbc7- deployment-5759  30f2499b-3bd6-42f6-a09d-89bb8f9ab82d 26583 0 2023-09-20 13:04:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc003a95617 0xc003a95618}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.5.94\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lvb85,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lvb85,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.173,PodIP:10.100.5.94,StartTime:2023-09-20 13:04:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-20 13:04:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1ba46682255bdf1330caf415f842bbd276360a69cd4359af6aad2cab5096f0f7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.5.94,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 13:04:30.462: INFO: Pod "webserver-deployment-7f5969cbc7-lzbgw" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-lzbgw webserver-deployment-7f5969cbc7- deployment-5759  c40888b4-44cf-4838-8b9f-7241ee9b5e76 26809 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc003a95840 0xc003a95841}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g42gs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g42gs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.64,PodIP:,StartTime:2023-09-20 13:04:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 13:04:30.462: INFO: Pod "webserver-deployment-7f5969cbc7-n2g5z" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-n2g5z webserver-deployment-7f5969cbc7- deployment-5759  96938f2d-991b-47f3-be98-9050462be0a4 26798 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc003a95ec7 0xc003a95ec8}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7nt7l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7nt7l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 13:04:30.462: INFO: Pod "webserver-deployment-7f5969cbc7-t5vjj" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-t5vjj webserver-deployment-7f5969cbc7- deployment-5759  5193bc84-eb52-40eb-a1a0-fdb46106de6c 26633 0 2023-09-20 13:04:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc001e0c150 0xc001e0c151}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.4.170\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sz9vd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sz9vd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.64,PodIP:10.100.4.170,StartTime:2023-09-20 13:04:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-20 13:04:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://20d99e2404cbec511f597c00ae29a8bb296ae1b20a1a6aaef0b34ecc2c1e3013,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.4.170,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 13:04:30.463: INFO: Pod "webserver-deployment-7f5969cbc7-vv2z7" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vv2z7 webserver-deployment-7f5969cbc7- deployment-5759  ec9c536a-98da-4d55-9f8b-c6e0c025e9ec 26844 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc001e0c327 0xc001e0c328}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jxrx8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jxrx8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.173,PodIP:,StartTime:2023-09-20 13:04:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 13:04:30.463: INFO: Pod "webserver-deployment-7f5969cbc7-wwl59" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wwl59 webserver-deployment-7f5969cbc7- deployment-5759  8f3c62ac-7e68-4469-8a2b-8d09cdd0ea8c 26855 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc001e0c4f7 0xc001e0c4f8}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jbhnm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jbhnm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.172,PodIP:,StartTime:2023-09-20 13:04:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 13:04:30.463: INFO: Pod "webserver-deployment-7f5969cbc7-zgt9v" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-zgt9v webserver-deployment-7f5969cbc7- deployment-5759  e1e243b9-3fe5-4d73-a551-a20b2381be9a 26828 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc001e0c6b7 0xc001e0c6b8}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gg227,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gg227,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.172,PodIP:,StartTime:2023-09-20 13:04:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 13:04:30.463: INFO: Pod "webserver-deployment-d9f79cb5-282c2" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-282c2 webserver-deployment-d9f79cb5- deployment-5759  506d68bd-db64-4c8c-8e28-3151c882daba 26799 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c2687f67-bb17-49b0-b83b-e0f4e4829ede 0xc001e0c887 0xc001e0c888}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c2687f67-bb17-49b0-b83b-e0f4e4829ede\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-94pmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-94pmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.173,PodIP:,StartTime:2023-09-20 13:04:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 13:04:30.464: INFO: Pod "webserver-deployment-d9f79cb5-4lmh6" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-4lmh6 webserver-deployment-d9f79cb5- deployment-5759  dfbee06b-cc16-4d0f-b855-f8ea0f38b024 26724 0 2023-09-20 13:04:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c2687f67-bb17-49b0-b83b-e0f4e4829ede 0xc001e0ca77 0xc001e0ca78}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c2687f67-bb17-49b0-b83b-e0f4e4829ede\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9vf6m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9vf6m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.173,PodIP:,StartTime:2023-09-20 13:04:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 13:04:30.464: INFO: Pod "webserver-deployment-d9f79cb5-6r8qj" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-6r8qj webserver-deployment-d9f79cb5- deployment-5759  9374b7b8-a0d7-4582-acd4-a17bfcd85b07 26693 0 2023-09-20 13:04:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c2687f67-bb17-49b0-b83b-e0f4e4829ede 0xc001e0cc57 0xc001e0cc58}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c2687f67-bb17-49b0-b83b-e0f4e4829ede\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zmstr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zmstr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.172,PodIP:,StartTime:2023-09-20 13:04:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 13:04:30.464: INFO: Pod "webserver-deployment-d9f79cb5-6xkvw" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-6xkvw webserver-deployment-d9f79cb5- deployment-5759  8a531efb-7566-4191-9cae-b20bb5bce874 26797 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c2687f67-bb17-49b0-b83b-e0f4e4829ede 0xc001e0cf07 0xc001e0cf08}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c2687f67-bb17-49b0-b83b-e0f4e4829ede\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x8fz4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x8fz4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 13:04:30.464: INFO: Pod "webserver-deployment-d9f79cb5-9vp7r" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-9vp7r webserver-deployment-d9f79cb5- deployment-5759  7fe57d8f-6c7d-4ec7-8518-cf2053e72879 26695 0 2023-09-20 13:04:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c2687f67-bb17-49b0-b83b-e0f4e4829ede 0xc001e0d10f 0xc001e0d120}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c2687f67-bb17-49b0-b83b-e0f4e4829ede\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2n846,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2n846,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.173,PodIP:,StartTime:2023-09-20 13:04:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 13:04:30.464: INFO: Pod "webserver-deployment-d9f79cb5-br4kz" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-br4kz webserver-deployment-d9f79cb5- deployment-5759  858ef0e0-08a9-438e-ab9d-8ec13ff533b5 26786 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c2687f67-bb17-49b0-b83b-e0f4e4829ede 0xc001e0d817 0xc001e0d818}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c2687f67-bb17-49b0-b83b-e0f4e4829ede\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l72bp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l72bp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 13:04:30.465: INFO: Pod "webserver-deployment-d9f79cb5-gpk6v" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-gpk6v webserver-deployment-d9f79cb5- deployment-5759  69b7f15a-0da1-44ec-bc4a-a3d136aaff49 26700 0 2023-09-20 13:04:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c2687f67-bb17-49b0-b83b-e0f4e4829ede 0xc001e0de8f 0xc001e0dea0}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c2687f67-bb17-49b0-b83b-e0f4e4829ede\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9fpww,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9fpww,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.64,PodIP:,StartTime:2023-09-20 13:04:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 13:04:30.465: INFO: Pod "webserver-deployment-d9f79cb5-m2xjs" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-m2xjs webserver-deployment-d9f79cb5- deployment-5759  6f136183-368d-4b5f-a41a-7ab7db9432c6 26826 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c2687f67-bb17-49b0-b83b-e0f4e4829ede 0xc000d9c187 0xc000d9c188}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c2687f67-bb17-49b0-b83b-e0f4e4829ede\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7nrb6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7nrb6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.172,PodIP:,StartTime:2023-09-20 13:04:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 13:04:30.465: INFO: Pod "webserver-deployment-d9f79cb5-mw9w5" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-mw9w5 webserver-deployment-d9f79cb5- deployment-5759  fbfdb2d1-bc75-46af-be14-cc847d61e58d 26802 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c2687f67-bb17-49b0-b83b-e0f4e4829ede 0xc000d9c777 0xc000d9c778}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c2687f67-bb17-49b0-b83b-e0f4e4829ede\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mqh7m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mqh7m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.173,PodIP:,StartTime:2023-09-20 13:04:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 13:04:30.465: INFO: Pod "webserver-deployment-d9f79cb5-pk6fr" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-pk6fr webserver-deployment-d9f79cb5- deployment-5759  f8144768-8010-4617-8d60-6c2ad5f3370f 26725 0 2023-09-20 13:04:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c2687f67-bb17-49b0-b83b-e0f4e4829ede 0xc000d9cba7 0xc000d9cba8}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c2687f67-bb17-49b0-b83b-e0f4e4829ede\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bxdrd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bxdrd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.64,PodIP:,StartTime:2023-09-20 13:04:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 13:04:30.465: INFO: Pod "webserver-deployment-d9f79cb5-rlcjc" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-rlcjc webserver-deployment-d9f79cb5- deployment-5759  4ee72d3d-afe7-4bce-a600-c96aaaf48983 26820 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c2687f67-bb17-49b0-b83b-e0f4e4829ede 0xc000d9cde7 0xc000d9cde8}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c2687f67-bb17-49b0-b83b-e0f4e4829ede\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xs6pc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xs6pc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.64,PodIP:,StartTime:2023-09-20 13:04:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 13:04:30.466: INFO: Pod "webserver-deployment-d9f79cb5-thgh6" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-thgh6 webserver-deployment-d9f79cb5- deployment-5759  9735b4bf-2d68-4030-91b0-0f133ebedf6d 26759 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c2687f67-bb17-49b0-b83b-e0f4e4829ede 0xc000d9cff7 0xc000d9cff8}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c2687f67-bb17-49b0-b83b-e0f4e4829ede\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vxsdx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vxsdx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.172,PodIP:,StartTime:2023-09-20 13:04:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 13:04:30.466: INFO: Pod "webserver-deployment-d9f79cb5-xl29d" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-xl29d webserver-deployment-d9f79cb5- deployment-5759  e07d233c-a3db-40d7-967b-e57a787576ba 26831 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c2687f67-bb17-49b0-b83b-e0f4e4829ede 0xc000d9d727 0xc000d9d728}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c2687f67-bb17-49b0-b83b-e0f4e4829ede\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fcj6j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fcj6j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.172,PodIP:,StartTime:2023-09-20 13:04:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep 20 13:04:30.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5759" for this suite. 09/20/23 13:04:30.472
------------------------------
â€¢ [SLOW TEST] [12.481 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:04:18.002
    Sep 20 13:04:18.002: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename deployment 09/20/23 13:04:18.002
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:04:18.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:04:18.077
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Sep 20 13:04:18.081: INFO: Creating deployment "webserver-deployment"
    Sep 20 13:04:18.088: INFO: Waiting for observed generation 1
    Sep 20 13:04:20.207: INFO: Waiting for all required pods to come up
    Sep 20 13:04:20.220: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 09/20/23 13:04:20.22
    Sep 20 13:04:20.220: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-t5vjj" in namespace "deployment-5759" to be "running"
    Sep 20 13:04:20.220: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-6kk4p" in namespace "deployment-5759" to be "running"
    Sep 20 13:04:20.221: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-hwggb" in namespace "deployment-5759" to be "running"
    Sep 20 13:04:20.221: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-jdmf4" in namespace "deployment-5759" to be "running"
    Sep 20 13:04:20.221: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-92v9v" in namespace "deployment-5759" to be "running"
    Sep 20 13:04:20.220: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-7rc9g" in namespace "deployment-5759" to be "running"
    Sep 20 13:04:20.220: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-2x4t7" in namespace "deployment-5759" to be "running"
    Sep 20 13:04:20.221: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-9tffr" in namespace "deployment-5759" to be "running"
    Sep 20 13:04:20.220: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-4jt2s" in namespace "deployment-5759" to be "running"
    Sep 20 13:04:20.220: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-5x74q" in namespace "deployment-5759" to be "running"
    Sep 20 13:04:20.232: INFO: Pod "webserver-deployment-7f5969cbc7-hwggb": Phase="Pending", Reason="", readiness=false. Elapsed: 11.334605ms
    Sep 20 13:04:20.232: INFO: Pod "webserver-deployment-7f5969cbc7-6kk4p": Phase="Pending", Reason="", readiness=false. Elapsed: 11.553236ms
    Sep 20 13:04:20.237: INFO: Pod "webserver-deployment-7f5969cbc7-92v9v": Phase="Pending", Reason="", readiness=false. Elapsed: 16.235098ms
    Sep 20 13:04:20.240: INFO: Pod "webserver-deployment-7f5969cbc7-jdmf4": Phase="Pending", Reason="", readiness=false. Elapsed: 19.597532ms
    Sep 20 13:04:20.241: INFO: Pod "webserver-deployment-7f5969cbc7-9tffr": Phase="Pending", Reason="", readiness=false. Elapsed: 19.61692ms
    Sep 20 13:04:20.241: INFO: Pod "webserver-deployment-7f5969cbc7-4jt2s": Phase="Pending", Reason="", readiness=false. Elapsed: 19.402225ms
    Sep 20 13:04:20.241: INFO: Pod "webserver-deployment-7f5969cbc7-2x4t7": Phase="Pending", Reason="", readiness=false. Elapsed: 19.73488ms
    Sep 20 13:04:20.241: INFO: Pod "webserver-deployment-7f5969cbc7-t5vjj": Phase="Pending", Reason="", readiness=false. Elapsed: 20.261724ms
    Sep 20 13:04:20.241: INFO: Pod "webserver-deployment-7f5969cbc7-5x74q": Phase="Pending", Reason="", readiness=false. Elapsed: 19.345668ms
    Sep 20 13:04:20.241: INFO: Pod "webserver-deployment-7f5969cbc7-7rc9g": Phase="Pending", Reason="", readiness=false. Elapsed: 20.138641ms
    Sep 20 13:04:22.271: INFO: Pod "webserver-deployment-7f5969cbc7-7rc9g": Phase="Running", Reason="", readiness=true. Elapsed: 2.049730842s
    Sep 20 13:04:22.271: INFO: Pod "webserver-deployment-7f5969cbc7-7rc9g" satisfied condition "running"
    Sep 20 13:04:22.271: INFO: Pod "webserver-deployment-7f5969cbc7-6kk4p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050218501s
    Sep 20 13:04:22.271: INFO: Pod "webserver-deployment-7f5969cbc7-hwggb": Phase="Running", Reason="", readiness=true. Elapsed: 2.050331495s
    Sep 20 13:04:22.271: INFO: Pod "webserver-deployment-7f5969cbc7-hwggb" satisfied condition "running"
    Sep 20 13:04:22.271: INFO: Pod "webserver-deployment-7f5969cbc7-4jt2s": Phase="Running", Reason="", readiness=true. Elapsed: 2.049705656s
    Sep 20 13:04:22.271: INFO: Pod "webserver-deployment-7f5969cbc7-4jt2s" satisfied condition "running"
    Sep 20 13:04:22.271: INFO: Pod "webserver-deployment-7f5969cbc7-jdmf4": Phase="Running", Reason="", readiness=true. Elapsed: 2.050352013s
    Sep 20 13:04:22.271: INFO: Pod "webserver-deployment-7f5969cbc7-jdmf4" satisfied condition "running"
    Sep 20 13:04:22.271: INFO: Pod "webserver-deployment-7f5969cbc7-t5vjj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0508175s
    Sep 20 13:04:22.271: INFO: Pod "webserver-deployment-7f5969cbc7-2x4t7": Phase="Running", Reason="", readiness=true. Elapsed: 2.050144151s
    Sep 20 13:04:22.271: INFO: Pod "webserver-deployment-7f5969cbc7-2x4t7" satisfied condition "running"
    Sep 20 13:04:22.271: INFO: Pod "webserver-deployment-7f5969cbc7-9tffr": Phase="Running", Reason="", readiness=true. Elapsed: 2.0501749s
    Sep 20 13:04:22.272: INFO: Pod "webserver-deployment-7f5969cbc7-9tffr" satisfied condition "running"
    Sep 20 13:04:22.271: INFO: Pod "webserver-deployment-7f5969cbc7-5x74q": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050111019s
    Sep 20 13:04:22.272: INFO: Pod "webserver-deployment-7f5969cbc7-92v9v": Phase="Running", Reason="", readiness=true. Elapsed: 2.050804275s
    Sep 20 13:04:22.272: INFO: Pod "webserver-deployment-7f5969cbc7-92v9v" satisfied condition "running"
    Sep 20 13:04:24.261: INFO: Pod "webserver-deployment-7f5969cbc7-6kk4p": Phase="Running", Reason="", readiness=true. Elapsed: 4.040947022s
    Sep 20 13:04:24.261: INFO: Pod "webserver-deployment-7f5969cbc7-6kk4p" satisfied condition "running"
    Sep 20 13:04:24.261: INFO: Pod "webserver-deployment-7f5969cbc7-t5vjj": Phase="Running", Reason="", readiness=true. Elapsed: 4.041036341s
    Sep 20 13:04:24.261: INFO: Pod "webserver-deployment-7f5969cbc7-t5vjj" satisfied condition "running"
    Sep 20 13:04:24.261: INFO: Pod "webserver-deployment-7f5969cbc7-5x74q": Phase="Running", Reason="", readiness=true. Elapsed: 4.040053108s
    Sep 20 13:04:24.262: INFO: Pod "webserver-deployment-7f5969cbc7-5x74q" satisfied condition "running"
    Sep 20 13:04:24.262: INFO: Waiting for deployment "webserver-deployment" to complete
    Sep 20 13:04:24.264: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:10, UpdatedReplicas:10, ReadyReplicas:9, AvailableReplicas:9, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 4, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 4, 23, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 4, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 4, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"webserver-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 13:04:26.276: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Sep 20 13:04:26.318: INFO: Updating deployment webserver-deployment
    Sep 20 13:04:26.318: INFO: Waiting for observed generation 2
    Sep 20 13:04:28.326: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Sep 20 13:04:28.333: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Sep 20 13:04:28.335: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Sep 20 13:04:28.344: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Sep 20 13:04:28.344: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Sep 20 13:04:28.368: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Sep 20 13:04:28.376: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Sep 20 13:04:28.376: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Sep 20 13:04:28.420: INFO: Updating deployment webserver-deployment
    Sep 20 13:04:28.420: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Sep 20 13:04:28.428: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Sep 20 13:04:30.441: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep 20 13:04:30.449: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-5759  1f51e9cd-e204-4e79-9bcd-cdc4f2049546 26825 3 2023-09-20 13:04:18 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005cc72c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-09-20 13:04:28 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-09-20 13:04:28 +0000 UTC,LastTransitionTime:2023-09-20 13:04:18 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Sep 20 13:04:30.452: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-5759  c2687f67-bb17-49b0-b83b-e0f4e4829ede 26819 3 2023-09-20 13:04:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 1f51e9cd-e204-4e79-9bcd-cdc4f2049546 0xc005cc7827 0xc005cc7828}] [] [{kube-controller-manager Update apps/v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1f51e9cd-e204-4e79-9bcd-cdc4f2049546\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005cc78d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep 20 13:04:30.452: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Sep 20 13:04:30.452: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-5759  fa55794c-9e9a-432b-94c3-9ad1abda8938 26821 3 2023-09-20 13:04:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 1f51e9cd-e204-4e79-9bcd-cdc4f2049546 0xc005cc7727 0xc005cc7728}] [] [{kube-controller-manager Update apps/v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1f51e9cd-e204-4e79-9bcd-cdc4f2049546\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005cc77b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Sep 20 13:04:30.458: INFO: Pod "webserver-deployment-7f5969cbc7-2x4t7" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2x4t7 webserver-deployment-7f5969cbc7- deployment-5759  a3b31620-6506-43a4-aa01-0d6f9e925cc9 26584 0 2023-09-20 13:04:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc005cc7e27 0xc005cc7e28}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.5.95\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2fb98,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2fb98,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.173,PodIP:10.100.5.95,StartTime:2023-09-20 13:04:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-20 13:04:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://11c2d638c8c43ddd5048934cafe1530c3bcbcf6e454524e7712e3cb3dcf23fea,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.5.95,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep 20 13:04:30.459: INFO: Pod "webserver-deployment-7f5969cbc7-4jt2s" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-4jt2s webserver-deployment-7f5969cbc7- deployment-5759  82028fbc-5570-4fda-8676-dee26034c1e0 26579 0 2023-09-20 13:04:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc003a94040 0xc003a94041}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.3.133\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zjgcq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zjgcq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.172,PodIP:10.100.3.133,StartTime:2023-09-20 13:04:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-20 13:04:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b9a86ae3fc14033374c94c64f9e045dced3310d35e2f5adf55042b7155615157,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.3.133,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep 20 13:04:30.459: INFO: Pod "webserver-deployment-7f5969cbc7-4w8h5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-4w8h5 webserver-deployment-7f5969cbc7- deployment-5759  952264b8-9d09-466e-b94e-4e1816356e03 26789 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc003a94237 0xc003a94238}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-khpm6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-khpm6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.173,PodIP:,StartTime:2023-09-20 13:04:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep 20 13:04:30.459: INFO: Pod "webserver-deployment-7f5969cbc7-7rc9g" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7rc9g webserver-deployment-7f5969cbc7- deployment-5759  6d72578d-39ab-4529-bc34-497c4052972f 26610 0 2023-09-20 13:04:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc003a943f7 0xc003a943f8}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.5.96\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rkg47,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rkg47,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.173,PodIP:10.100.5.96,StartTime:2023-09-20 13:04:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-20 13:04:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f19397ea3fb34911d0733e07c77a0884f591e234450a7c9d7f42a16d10623e3f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.5.96,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep 20 13:04:30.460: INFO: Pod "webserver-deployment-7f5969cbc7-7rl54" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7rl54 webserver-deployment-7f5969cbc7- deployment-5759  2d5d9745-65c8-4124-a8db-9899fb7f37ce 26761 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc003a945e0 0xc003a945e1}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8n48j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8n48j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.64,PodIP:,StartTime:2023-09-20 13:04:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep 20 13:04:30.460: INFO: Pod "webserver-deployment-7f5969cbc7-826b5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-826b5 webserver-deployment-7f5969cbc7- deployment-5759  b7f602b7-e8ce-48bb-a3e9-2fbacea78061 26853 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc003a947b7 0xc003a947b8}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-knfvg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-knfvg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.64,PodIP:,StartTime:2023-09-20 13:04:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep 20 13:04:30.460: INFO: Pod "webserver-deployment-7f5969cbc7-8qrjh" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-8qrjh webserver-deployment-7f5969cbc7- deployment-5759  57eeebe3-033e-4b2e-9b18-179fcb7bec93 26829 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc003a94977 0xc003a94978}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-556mk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-556mk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.172,PodIP:,StartTime:2023-09-20 13:04:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep 20 13:04:30.460: INFO: Pod "webserver-deployment-7f5969cbc7-92v9v" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-92v9v webserver-deployment-7f5969cbc7- deployment-5759  d98e0485-074d-4cbd-b3b5-bc8c8b748e6f 26594 0 2023-09-20 13:04:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc003a94b37 0xc003a94b38}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.4.169\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r58tm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r58tm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.64,PodIP:10.100.4.169,StartTime:2023-09-20 13:04:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-20 13:04:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0d6dd355fd8c95688e2389a91d3b11e15bfe9914cb31677d49ac3bda366bad31,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.4.169,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep 20 13:04:30.461: INFO: Pod "webserver-deployment-7f5969cbc7-9tffr" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9tffr webserver-deployment-7f5969cbc7- deployment-5759  c8002e42-1145-4805-9ca0-18534c9dae46 26611 0 2023-09-20 13:04:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc003a94d17 0xc003a94d18}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.3.134\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9lcdd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9lcdd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.172,PodIP:10.100.3.134,StartTime:2023-09-20 13:04:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-20 13:04:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a7c4c712f469c682483a488c640b8c4b038da8556ee7e48486799f963b439e86,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.3.134,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep 20 13:04:30.461: INFO: Pod "webserver-deployment-7f5969cbc7-bgndw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bgndw webserver-deployment-7f5969cbc7- deployment-5759  f0c5bc2b-b36f-4fc2-aa13-a65f920db5db 26791 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc003a94ef7 0xc003a94ef8}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bcrq9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bcrq9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep 20 13:04:30.461: INFO: Pod "webserver-deployment-7f5969cbc7-f8s29" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-f8s29 webserver-deployment-7f5969cbc7- deployment-5759  fad8ea29-8640-46e8-9635-7b849bd1ca0d 26817 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc003a95060 0xc003a95061}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4289k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4289k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.173,PodIP:,StartTime:2023-09-20 13:04:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep 20 13:04:30.461: INFO: Pod "webserver-deployment-7f5969cbc7-hqs89" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hqs89 webserver-deployment-7f5969cbc7- deployment-5759  ac3ba0c7-6e72-4a76-a2eb-db182c251863 26760 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc003a95247 0xc003a95248}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zbf5s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zbf5s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.173,PodIP:,StartTime:2023-09-20 13:04:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep 20 13:04:30.462: INFO: Pod "webserver-deployment-7f5969cbc7-hwggb" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hwggb webserver-deployment-7f5969cbc7- deployment-5759  1bd308b8-f124-4a3d-966e-fe0dfd112a36 26608 0 2023-09-20 13:04:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc003a95427 0xc003a95428}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.3.135\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cjtc2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cjtc2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.172,PodIP:10.100.3.135,StartTime:2023-09-20 13:04:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-20 13:04:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c1466d369d80552d31ecc735d70f5a3fb3ef850a9c1c333c8ddcea3224d98f8e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.3.135,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep 20 13:04:30.462: INFO: Pod "webserver-deployment-7f5969cbc7-jdmf4" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jdmf4 webserver-deployment-7f5969cbc7- deployment-5759  30f2499b-3bd6-42f6-a09d-89bb8f9ab82d 26583 0 2023-09-20 13:04:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc003a95617 0xc003a95618}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.5.94\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lvb85,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lvb85,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.173,PodIP:10.100.5.94,StartTime:2023-09-20 13:04:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-20 13:04:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1ba46682255bdf1330caf415f842bbd276360a69cd4359af6aad2cab5096f0f7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.5.94,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep 20 13:04:30.462: INFO: Pod "webserver-deployment-7f5969cbc7-lzbgw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-lzbgw webserver-deployment-7f5969cbc7- deployment-5759  c40888b4-44cf-4838-8b9f-7241ee9b5e76 26809 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc003a95840 0xc003a95841}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g42gs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g42gs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.64,PodIP:,StartTime:2023-09-20 13:04:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep 20 13:04:30.462: INFO: Pod "webserver-deployment-7f5969cbc7-n2g5z" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-n2g5z webserver-deployment-7f5969cbc7- deployment-5759  96938f2d-991b-47f3-be98-9050462be0a4 26798 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc003a95ec7 0xc003a95ec8}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7nt7l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7nt7l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep 20 13:04:30.462: INFO: Pod "webserver-deployment-7f5969cbc7-t5vjj" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-t5vjj webserver-deployment-7f5969cbc7- deployment-5759  5193bc84-eb52-40eb-a1a0-fdb46106de6c 26633 0 2023-09-20 13:04:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc001e0c150 0xc001e0c151}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.4.170\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sz9vd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sz9vd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.64,PodIP:10.100.4.170,StartTime:2023-09-20 13:04:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-20 13:04:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://20d99e2404cbec511f597c00ae29a8bb296ae1b20a1a6aaef0b34ecc2c1e3013,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.4.170,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep 20 13:04:30.463: INFO: Pod "webserver-deployment-7f5969cbc7-vv2z7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vv2z7 webserver-deployment-7f5969cbc7- deployment-5759  ec9c536a-98da-4d55-9f8b-c6e0c025e9ec 26844 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc001e0c327 0xc001e0c328}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jxrx8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jxrx8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.173,PodIP:,StartTime:2023-09-20 13:04:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep 20 13:04:30.463: INFO: Pod "webserver-deployment-7f5969cbc7-wwl59" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wwl59 webserver-deployment-7f5969cbc7- deployment-5759  8f3c62ac-7e68-4469-8a2b-8d09cdd0ea8c 26855 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc001e0c4f7 0xc001e0c4f8}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jbhnm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jbhnm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.172,PodIP:,StartTime:2023-09-20 13:04:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep 20 13:04:30.463: INFO: Pod "webserver-deployment-7f5969cbc7-zgt9v" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-zgt9v webserver-deployment-7f5969cbc7- deployment-5759  e1e243b9-3fe5-4d73-a551-a20b2381be9a 26828 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 fa55794c-9e9a-432b-94c3-9ad1abda8938 0xc001e0c6b7 0xc001e0c6b8}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa55794c-9e9a-432b-94c3-9ad1abda8938\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gg227,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gg227,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.172,PodIP:,StartTime:2023-09-20 13:04:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep 20 13:04:30.463: INFO: Pod "webserver-deployment-d9f79cb5-282c2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-282c2 webserver-deployment-d9f79cb5- deployment-5759  506d68bd-db64-4c8c-8e28-3151c882daba 26799 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c2687f67-bb17-49b0-b83b-e0f4e4829ede 0xc001e0c887 0xc001e0c888}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c2687f67-bb17-49b0-b83b-e0f4e4829ede\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-94pmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-94pmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.173,PodIP:,StartTime:2023-09-20 13:04:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep 20 13:04:30.464: INFO: Pod "webserver-deployment-d9f79cb5-4lmh6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-4lmh6 webserver-deployment-d9f79cb5- deployment-5759  dfbee06b-cc16-4d0f-b855-f8ea0f38b024 26724 0 2023-09-20 13:04:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c2687f67-bb17-49b0-b83b-e0f4e4829ede 0xc001e0ca77 0xc001e0ca78}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c2687f67-bb17-49b0-b83b-e0f4e4829ede\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9vf6m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9vf6m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.173,PodIP:,StartTime:2023-09-20 13:04:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep 20 13:04:30.464: INFO: Pod "webserver-deployment-d9f79cb5-6r8qj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-6r8qj webserver-deployment-d9f79cb5- deployment-5759  9374b7b8-a0d7-4582-acd4-a17bfcd85b07 26693 0 2023-09-20 13:04:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c2687f67-bb17-49b0-b83b-e0f4e4829ede 0xc001e0cc57 0xc001e0cc58}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c2687f67-bb17-49b0-b83b-e0f4e4829ede\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zmstr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zmstr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.172,PodIP:,StartTime:2023-09-20 13:04:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep 20 13:04:30.464: INFO: Pod "webserver-deployment-d9f79cb5-6xkvw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-6xkvw webserver-deployment-d9f79cb5- deployment-5759  8a531efb-7566-4191-9cae-b20bb5bce874 26797 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c2687f67-bb17-49b0-b83b-e0f4e4829ede 0xc001e0cf07 0xc001e0cf08}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c2687f67-bb17-49b0-b83b-e0f4e4829ede\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x8fz4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x8fz4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep 20 13:04:30.464: INFO: Pod "webserver-deployment-d9f79cb5-9vp7r" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-9vp7r webserver-deployment-d9f79cb5- deployment-5759  7fe57d8f-6c7d-4ec7-8518-cf2053e72879 26695 0 2023-09-20 13:04:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c2687f67-bb17-49b0-b83b-e0f4e4829ede 0xc001e0d10f 0xc001e0d120}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c2687f67-bb17-49b0-b83b-e0f4e4829ede\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2n846,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2n846,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.173,PodIP:,StartTime:2023-09-20 13:04:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep 20 13:04:30.464: INFO: Pod "webserver-deployment-d9f79cb5-br4kz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-br4kz webserver-deployment-d9f79cb5- deployment-5759  858ef0e0-08a9-438e-ab9d-8ec13ff533b5 26786 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c2687f67-bb17-49b0-b83b-e0f4e4829ede 0xc001e0d817 0xc001e0d818}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c2687f67-bb17-49b0-b83b-e0f4e4829ede\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l72bp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l72bp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep 20 13:04:30.465: INFO: Pod "webserver-deployment-d9f79cb5-gpk6v" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-gpk6v webserver-deployment-d9f79cb5- deployment-5759  69b7f15a-0da1-44ec-bc4a-a3d136aaff49 26700 0 2023-09-20 13:04:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c2687f67-bb17-49b0-b83b-e0f4e4829ede 0xc001e0de8f 0xc001e0dea0}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c2687f67-bb17-49b0-b83b-e0f4e4829ede\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9fpww,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9fpww,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.64,PodIP:,StartTime:2023-09-20 13:04:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep 20 13:04:30.465: INFO: Pod "webserver-deployment-d9f79cb5-m2xjs" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-m2xjs webserver-deployment-d9f79cb5- deployment-5759  6f136183-368d-4b5f-a41a-7ab7db9432c6 26826 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c2687f67-bb17-49b0-b83b-e0f4e4829ede 0xc000d9c187 0xc000d9c188}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c2687f67-bb17-49b0-b83b-e0f4e4829ede\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7nrb6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7nrb6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.172,PodIP:,StartTime:2023-09-20 13:04:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep 20 13:04:30.465: INFO: Pod "webserver-deployment-d9f79cb5-mw9w5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-mw9w5 webserver-deployment-d9f79cb5- deployment-5759  fbfdb2d1-bc75-46af-be14-cc847d61e58d 26802 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c2687f67-bb17-49b0-b83b-e0f4e4829ede 0xc000d9c777 0xc000d9c778}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c2687f67-bb17-49b0-b83b-e0f4e4829ede\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mqh7m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mqh7m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.173,PodIP:,StartTime:2023-09-20 13:04:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep 20 13:04:30.465: INFO: Pod "webserver-deployment-d9f79cb5-pk6fr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-pk6fr webserver-deployment-d9f79cb5- deployment-5759  f8144768-8010-4617-8d60-6c2ad5f3370f 26725 0 2023-09-20 13:04:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c2687f67-bb17-49b0-b83b-e0f4e4829ede 0xc000d9cba7 0xc000d9cba8}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c2687f67-bb17-49b0-b83b-e0f4e4829ede\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bxdrd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bxdrd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.64,PodIP:,StartTime:2023-09-20 13:04:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep 20 13:04:30.465: INFO: Pod "webserver-deployment-d9f79cb5-rlcjc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-rlcjc webserver-deployment-d9f79cb5- deployment-5759  4ee72d3d-afe7-4bce-a600-c96aaaf48983 26820 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c2687f67-bb17-49b0-b83b-e0f4e4829ede 0xc000d9cde7 0xc000d9cde8}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c2687f67-bb17-49b0-b83b-e0f4e4829ede\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xs6pc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xs6pc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.64,PodIP:,StartTime:2023-09-20 13:04:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep 20 13:04:30.466: INFO: Pod "webserver-deployment-d9f79cb5-thgh6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-thgh6 webserver-deployment-d9f79cb5- deployment-5759  9735b4bf-2d68-4030-91b0-0f133ebedf6d 26759 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c2687f67-bb17-49b0-b83b-e0f4e4829ede 0xc000d9cff7 0xc000d9cff8}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c2687f67-bb17-49b0-b83b-e0f4e4829ede\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vxsdx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vxsdx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.172,PodIP:,StartTime:2023-09-20 13:04:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep 20 13:04:30.466: INFO: Pod "webserver-deployment-d9f79cb5-xl29d" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-xl29d webserver-deployment-d9f79cb5- deployment-5759  e07d233c-a3db-40d7-967b-e57a787576ba 26831 0 2023-09-20 13:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c2687f67-bb17-49b0-b83b-e0f4e4829ede 0xc000d9d727 0xc000d9d728}] [] [{kube-controller-manager Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c2687f67-bb17-49b0-b83b-e0f4e4829ede\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:04:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fcj6j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fcj6j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.172,PodIP:,StartTime:2023-09-20 13:04:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:04:30.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5759" for this suite. 09/20/23 13:04:30.472
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:04:30.489
Sep 20 13:04:30.489: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename configmap 09/20/23 13:04:30.49
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:04:30.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:04:30.586
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-21029bec-1da5-4650-bfd6-9bed1bd041ac 09/20/23 13:04:30.591
STEP: Creating a pod to test consume configMaps 09/20/23 13:04:30.596
Sep 20 13:04:30.614: INFO: Waiting up to 5m0s for pod "pod-configmaps-a7762e07-8cbf-4057-810b-c37d7223de66" in namespace "configmap-2439" to be "Succeeded or Failed"
Sep 20 13:04:30.623: INFO: Pod "pod-configmaps-a7762e07-8cbf-4057-810b-c37d7223de66": Phase="Pending", Reason="", readiness=false. Elapsed: 8.562683ms
Sep 20 13:04:33.024: INFO: Pod "pod-configmaps-a7762e07-8cbf-4057-810b-c37d7223de66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.409272583s
Sep 20 13:04:34.646: INFO: Pod "pod-configmaps-a7762e07-8cbf-4057-810b-c37d7223de66": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031307s
Sep 20 13:04:36.956: INFO: Pod "pod-configmaps-a7762e07-8cbf-4057-810b-c37d7223de66": Phase="Pending", Reason="", readiness=false. Elapsed: 6.341290133s
Sep 20 13:04:38.807: INFO: Pod "pod-configmaps-a7762e07-8cbf-4057-810b-c37d7223de66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.192385906s
STEP: Saw pod success 09/20/23 13:04:38.807
Sep 20 13:04:38.807: INFO: Pod "pod-configmaps-a7762e07-8cbf-4057-810b-c37d7223de66" satisfied condition "Succeeded or Failed"
Sep 20 13:04:38.810: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-configmaps-a7762e07-8cbf-4057-810b-c37d7223de66 container configmap-volume-test: <nil>
STEP: delete the pod 09/20/23 13:04:38.913
Sep 20 13:04:38.961: INFO: Waiting for pod pod-configmaps-a7762e07-8cbf-4057-810b-c37d7223de66 to disappear
Sep 20 13:04:38.966: INFO: Pod pod-configmaps-a7762e07-8cbf-4057-810b-c37d7223de66 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep 20 13:04:38.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2439" for this suite. 09/20/23 13:04:38.971
------------------------------
â€¢ [SLOW TEST] [8.622 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:04:30.489
    Sep 20 13:04:30.489: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename configmap 09/20/23 13:04:30.49
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:04:30.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:04:30.586
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-21029bec-1da5-4650-bfd6-9bed1bd041ac 09/20/23 13:04:30.591
    STEP: Creating a pod to test consume configMaps 09/20/23 13:04:30.596
    Sep 20 13:04:30.614: INFO: Waiting up to 5m0s for pod "pod-configmaps-a7762e07-8cbf-4057-810b-c37d7223de66" in namespace "configmap-2439" to be "Succeeded or Failed"
    Sep 20 13:04:30.623: INFO: Pod "pod-configmaps-a7762e07-8cbf-4057-810b-c37d7223de66": Phase="Pending", Reason="", readiness=false. Elapsed: 8.562683ms
    Sep 20 13:04:33.024: INFO: Pod "pod-configmaps-a7762e07-8cbf-4057-810b-c37d7223de66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.409272583s
    Sep 20 13:04:34.646: INFO: Pod "pod-configmaps-a7762e07-8cbf-4057-810b-c37d7223de66": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031307s
    Sep 20 13:04:36.956: INFO: Pod "pod-configmaps-a7762e07-8cbf-4057-810b-c37d7223de66": Phase="Pending", Reason="", readiness=false. Elapsed: 6.341290133s
    Sep 20 13:04:38.807: INFO: Pod "pod-configmaps-a7762e07-8cbf-4057-810b-c37d7223de66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.192385906s
    STEP: Saw pod success 09/20/23 13:04:38.807
    Sep 20 13:04:38.807: INFO: Pod "pod-configmaps-a7762e07-8cbf-4057-810b-c37d7223de66" satisfied condition "Succeeded or Failed"
    Sep 20 13:04:38.810: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-configmaps-a7762e07-8cbf-4057-810b-c37d7223de66 container configmap-volume-test: <nil>
    STEP: delete the pod 09/20/23 13:04:38.913
    Sep 20 13:04:38.961: INFO: Waiting for pod pod-configmaps-a7762e07-8cbf-4057-810b-c37d7223de66 to disappear
    Sep 20 13:04:38.966: INFO: Pod pod-configmaps-a7762e07-8cbf-4057-810b-c37d7223de66 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:04:38.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2439" for this suite. 09/20/23 13:04:38.971
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:04:39.112
Sep 20 13:04:39.112: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename subpath 09/20/23 13:04:39.112
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:04:39.702
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:04:39.714
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/20/23 13:04:39.724
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-l7gg 09/20/23 13:04:39.777
STEP: Creating a pod to test atomic-volume-subpath 09/20/23 13:04:39.777
Sep 20 13:04:39.791: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-l7gg" in namespace "subpath-9909" to be "Succeeded or Failed"
Sep 20 13:04:39.797: INFO: Pod "pod-subpath-test-configmap-l7gg": Phase="Pending", Reason="", readiness=false. Elapsed: 5.139983ms
Sep 20 13:04:41.802: INFO: Pod "pod-subpath-test-configmap-l7gg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010016539s
Sep 20 13:04:43.826: INFO: Pod "pod-subpath-test-configmap-l7gg": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03405139s
Sep 20 13:04:45.802: INFO: Pod "pod-subpath-test-configmap-l7gg": Phase="Running", Reason="", readiness=true. Elapsed: 6.010373328s
Sep 20 13:04:47.801: INFO: Pod "pod-subpath-test-configmap-l7gg": Phase="Running", Reason="", readiness=true. Elapsed: 8.009202352s
Sep 20 13:04:49.802: INFO: Pod "pod-subpath-test-configmap-l7gg": Phase="Running", Reason="", readiness=true. Elapsed: 10.010377805s
Sep 20 13:04:51.802: INFO: Pod "pod-subpath-test-configmap-l7gg": Phase="Running", Reason="", readiness=true. Elapsed: 12.010018627s
Sep 20 13:04:53.800: INFO: Pod "pod-subpath-test-configmap-l7gg": Phase="Running", Reason="", readiness=true. Elapsed: 14.008827863s
Sep 20 13:04:55.837: INFO: Pod "pod-subpath-test-configmap-l7gg": Phase="Running", Reason="", readiness=true. Elapsed: 16.045327577s
Sep 20 13:04:57.804: INFO: Pod "pod-subpath-test-configmap-l7gg": Phase="Running", Reason="", readiness=true. Elapsed: 18.012250137s
Sep 20 13:05:01.999: INFO: Pod "pod-subpath-test-configmap-l7gg": Phase="Running", Reason="", readiness=true. Elapsed: 22.207741944s
Sep 20 13:05:04.990: INFO: Pod "pod-subpath-test-configmap-l7gg": Phase="Running", Reason="", readiness=true. Elapsed: 25.198880183s
Sep 20 13:05:05.801: INFO: Pod "pod-subpath-test-configmap-l7gg": Phase="Running", Reason="", readiness=true. Elapsed: 26.009127149s
Sep 20 13:05:09.412: INFO: Pod "pod-subpath-test-configmap-l7gg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 29.620754146s
STEP: Saw pod success 09/20/23 13:05:09.412
Sep 20 13:05:09.413: INFO: Pod "pod-subpath-test-configmap-l7gg" satisfied condition "Succeeded or Failed"
Sep 20 13:05:09.643: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-subpath-test-configmap-l7gg container test-container-subpath-configmap-l7gg: <nil>
STEP: delete the pod 09/20/23 13:05:09.903
Sep 20 13:05:10.111: INFO: Waiting for pod pod-subpath-test-configmap-l7gg to disappear
Sep 20 13:05:10.116: INFO: Pod pod-subpath-test-configmap-l7gg no longer exists
STEP: Deleting pod pod-subpath-test-configmap-l7gg 09/20/23 13:05:10.116
Sep 20 13:05:10.116: INFO: Deleting pod "pod-subpath-test-configmap-l7gg" in namespace "subpath-9909"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Sep 20 13:05:10.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-9909" for this suite. 09/20/23 13:05:10.13
------------------------------
â€¢ [SLOW TEST] [31.031 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:04:39.112
    Sep 20 13:04:39.112: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename subpath 09/20/23 13:04:39.112
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:04:39.702
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:04:39.714
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/20/23 13:04:39.724
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-l7gg 09/20/23 13:04:39.777
    STEP: Creating a pod to test atomic-volume-subpath 09/20/23 13:04:39.777
    Sep 20 13:04:39.791: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-l7gg" in namespace "subpath-9909" to be "Succeeded or Failed"
    Sep 20 13:04:39.797: INFO: Pod "pod-subpath-test-configmap-l7gg": Phase="Pending", Reason="", readiness=false. Elapsed: 5.139983ms
    Sep 20 13:04:41.802: INFO: Pod "pod-subpath-test-configmap-l7gg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010016539s
    Sep 20 13:04:43.826: INFO: Pod "pod-subpath-test-configmap-l7gg": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03405139s
    Sep 20 13:04:45.802: INFO: Pod "pod-subpath-test-configmap-l7gg": Phase="Running", Reason="", readiness=true. Elapsed: 6.010373328s
    Sep 20 13:04:47.801: INFO: Pod "pod-subpath-test-configmap-l7gg": Phase="Running", Reason="", readiness=true. Elapsed: 8.009202352s
    Sep 20 13:04:49.802: INFO: Pod "pod-subpath-test-configmap-l7gg": Phase="Running", Reason="", readiness=true. Elapsed: 10.010377805s
    Sep 20 13:04:51.802: INFO: Pod "pod-subpath-test-configmap-l7gg": Phase="Running", Reason="", readiness=true. Elapsed: 12.010018627s
    Sep 20 13:04:53.800: INFO: Pod "pod-subpath-test-configmap-l7gg": Phase="Running", Reason="", readiness=true. Elapsed: 14.008827863s
    Sep 20 13:04:55.837: INFO: Pod "pod-subpath-test-configmap-l7gg": Phase="Running", Reason="", readiness=true. Elapsed: 16.045327577s
    Sep 20 13:04:57.804: INFO: Pod "pod-subpath-test-configmap-l7gg": Phase="Running", Reason="", readiness=true. Elapsed: 18.012250137s
    Sep 20 13:05:01.999: INFO: Pod "pod-subpath-test-configmap-l7gg": Phase="Running", Reason="", readiness=true. Elapsed: 22.207741944s
    Sep 20 13:05:04.990: INFO: Pod "pod-subpath-test-configmap-l7gg": Phase="Running", Reason="", readiness=true. Elapsed: 25.198880183s
    Sep 20 13:05:05.801: INFO: Pod "pod-subpath-test-configmap-l7gg": Phase="Running", Reason="", readiness=true. Elapsed: 26.009127149s
    Sep 20 13:05:09.412: INFO: Pod "pod-subpath-test-configmap-l7gg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 29.620754146s
    STEP: Saw pod success 09/20/23 13:05:09.412
    Sep 20 13:05:09.413: INFO: Pod "pod-subpath-test-configmap-l7gg" satisfied condition "Succeeded or Failed"
    Sep 20 13:05:09.643: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-subpath-test-configmap-l7gg container test-container-subpath-configmap-l7gg: <nil>
    STEP: delete the pod 09/20/23 13:05:09.903
    Sep 20 13:05:10.111: INFO: Waiting for pod pod-subpath-test-configmap-l7gg to disappear
    Sep 20 13:05:10.116: INFO: Pod pod-subpath-test-configmap-l7gg no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-l7gg 09/20/23 13:05:10.116
    Sep 20 13:05:10.116: INFO: Deleting pod "pod-subpath-test-configmap-l7gg" in namespace "subpath-9909"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:05:10.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-9909" for this suite. 09/20/23 13:05:10.13
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:05:10.147
Sep 20 13:05:10.147: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename configmap 09/20/23 13:05:10.148
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:05:10.94
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:05:10.944
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 09/20/23 13:05:10.949
STEP: fetching the ConfigMap 09/20/23 13:05:11.006
STEP: patching the ConfigMap 09/20/23 13:05:11.01
STEP: listing all ConfigMaps in all namespaces with a label selector 09/20/23 13:05:11.029
STEP: deleting the ConfigMap by collection with a label selector 09/20/23 13:05:11.034
STEP: listing all ConfigMaps in test namespace 09/20/23 13:05:11.044
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep 20 13:05:11.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1319" for this suite. 09/20/23 13:05:11.053
------------------------------
â€¢ [0.915 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:05:10.147
    Sep 20 13:05:10.147: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename configmap 09/20/23 13:05:10.148
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:05:10.94
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:05:10.944
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 09/20/23 13:05:10.949
    STEP: fetching the ConfigMap 09/20/23 13:05:11.006
    STEP: patching the ConfigMap 09/20/23 13:05:11.01
    STEP: listing all ConfigMaps in all namespaces with a label selector 09/20/23 13:05:11.029
    STEP: deleting the ConfigMap by collection with a label selector 09/20/23 13:05:11.034
    STEP: listing all ConfigMaps in test namespace 09/20/23 13:05:11.044
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:05:11.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1319" for this suite. 09/20/23 13:05:11.053
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:05:11.062
Sep 20 13:05:11.062: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename container-probe 09/20/23 13:05:11.063
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:05:11.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:05:11.096
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep 20 13:06:11.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-3699" for this suite. 09/20/23 13:06:11.447
------------------------------
â€¢ [SLOW TEST] [60.391 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:05:11.062
    Sep 20 13:05:11.062: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename container-probe 09/20/23 13:05:11.063
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:05:11.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:05:11.096
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:06:11.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-3699" for this suite. 09/20/23 13:06:11.447
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:06:11.454
Sep 20 13:06:11.454: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename gc 09/20/23 13:06:11.454
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:06:11.475
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:06:11.479
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 09/20/23 13:06:11.488
STEP: delete the rc 09/20/23 13:06:16.735
STEP: wait for the rc to be deleted 09/20/23 13:06:16.785
Sep 20 13:06:17.826: INFO: 100 pods remaining
Sep 20 13:06:17.826: INFO: 88 pods has nil DeletionTimestamp
Sep 20 13:06:17.826: INFO: 
Sep 20 13:06:18.796: INFO: 75 pods remaining
Sep 20 13:06:18.796: INFO: 75 pods has nil DeletionTimestamp
Sep 20 13:06:18.796: INFO: 
Sep 20 13:06:20.516: INFO: 55 pods remaining
Sep 20 13:06:20.516: INFO: 55 pods has nil DeletionTimestamp
Sep 20 13:06:20.516: INFO: 
Sep 20 13:06:22.243: INFO: 35 pods remaining
Sep 20 13:06:22.243: INFO: 35 pods has nil DeletionTimestamp
Sep 20 13:06:22.243: INFO: 
Sep 20 13:06:22.865: INFO: 35 pods remaining
Sep 20 13:06:22.865: INFO: 35 pods has nil DeletionTimestamp
Sep 20 13:06:22.865: INFO: 
Sep 20 13:06:23.801: INFO: 15 pods remaining
Sep 20 13:06:23.801: INFO: 14 pods has nil DeletionTimestamp
Sep 20 13:06:23.801: INFO: 
STEP: Gathering metrics 09/20/23 13:06:24.796
W0920 13:06:24.804447      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Sep 20 13:06:24.804: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep 20 13:06:24.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-7782" for this suite. 09/20/23 13:06:24.809
------------------------------
â€¢ [SLOW TEST] [13.367 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:06:11.454
    Sep 20 13:06:11.454: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename gc 09/20/23 13:06:11.454
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:06:11.475
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:06:11.479
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 09/20/23 13:06:11.488
    STEP: delete the rc 09/20/23 13:06:16.735
    STEP: wait for the rc to be deleted 09/20/23 13:06:16.785
    Sep 20 13:06:17.826: INFO: 100 pods remaining
    Sep 20 13:06:17.826: INFO: 88 pods has nil DeletionTimestamp
    Sep 20 13:06:17.826: INFO: 
    Sep 20 13:06:18.796: INFO: 75 pods remaining
    Sep 20 13:06:18.796: INFO: 75 pods has nil DeletionTimestamp
    Sep 20 13:06:18.796: INFO: 
    Sep 20 13:06:20.516: INFO: 55 pods remaining
    Sep 20 13:06:20.516: INFO: 55 pods has nil DeletionTimestamp
    Sep 20 13:06:20.516: INFO: 
    Sep 20 13:06:22.243: INFO: 35 pods remaining
    Sep 20 13:06:22.243: INFO: 35 pods has nil DeletionTimestamp
    Sep 20 13:06:22.243: INFO: 
    Sep 20 13:06:22.865: INFO: 35 pods remaining
    Sep 20 13:06:22.865: INFO: 35 pods has nil DeletionTimestamp
    Sep 20 13:06:22.865: INFO: 
    Sep 20 13:06:23.801: INFO: 15 pods remaining
    Sep 20 13:06:23.801: INFO: 14 pods has nil DeletionTimestamp
    Sep 20 13:06:23.801: INFO: 
    STEP: Gathering metrics 09/20/23 13:06:24.796
    W0920 13:06:24.804447      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Sep 20 13:06:24.804: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:06:24.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-7782" for this suite. 09/20/23 13:06:24.809
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:06:24.821
Sep 20 13:06:24.821: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename emptydir 09/20/23 13:06:24.822
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:06:24.842
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:06:24.844
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 09/20/23 13:06:24.848
Sep 20 13:06:24.856: INFO: Waiting up to 5m0s for pod "pod-fc163a22-3a2f-4f75-9b6f-82d6b3835a57" in namespace "emptydir-6351" to be "Succeeded or Failed"
Sep 20 13:06:24.859: INFO: Pod "pod-fc163a22-3a2f-4f75-9b6f-82d6b3835a57": Phase="Pending", Reason="", readiness=false. Elapsed: 3.562903ms
Sep 20 13:06:26.866: INFO: Pod "pod-fc163a22-3a2f-4f75-9b6f-82d6b3835a57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009667403s
Sep 20 13:06:29.227: INFO: Pod "pod-fc163a22-3a2f-4f75-9b6f-82d6b3835a57": Phase="Pending", Reason="", readiness=false. Elapsed: 4.371544443s
Sep 20 13:06:31.044: INFO: Pod "pod-fc163a22-3a2f-4f75-9b6f-82d6b3835a57": Phase="Pending", Reason="", readiness=false. Elapsed: 6.188130197s
Sep 20 13:06:32.864: INFO: Pod "pod-fc163a22-3a2f-4f75-9b6f-82d6b3835a57": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008503436s
Sep 20 13:06:35.152: INFO: Pod "pod-fc163a22-3a2f-4f75-9b6f-82d6b3835a57": Phase="Pending", Reason="", readiness=false. Elapsed: 10.296274641s
Sep 20 13:06:36.869: INFO: Pod "pod-fc163a22-3a2f-4f75-9b6f-82d6b3835a57": Phase="Pending", Reason="", readiness=false. Elapsed: 12.012777085s
Sep 20 13:06:38.864: INFO: Pod "pod-fc163a22-3a2f-4f75-9b6f-82d6b3835a57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.008509964s
STEP: Saw pod success 09/20/23 13:06:38.864
Sep 20 13:06:38.864: INFO: Pod "pod-fc163a22-3a2f-4f75-9b6f-82d6b3835a57" satisfied condition "Succeeded or Failed"
Sep 20 13:06:38.870: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-fc163a22-3a2f-4f75-9b6f-82d6b3835a57 container test-container: <nil>
STEP: delete the pod 09/20/23 13:06:38.882
Sep 20 13:06:39.014: INFO: Waiting for pod pod-fc163a22-3a2f-4f75-9b6f-82d6b3835a57 to disappear
Sep 20 13:06:39.026: INFO: Pod pod-fc163a22-3a2f-4f75-9b6f-82d6b3835a57 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep 20 13:06:39.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6351" for this suite. 09/20/23 13:06:39.046
------------------------------
â€¢ [SLOW TEST] [14.237 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:06:24.821
    Sep 20 13:06:24.821: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename emptydir 09/20/23 13:06:24.822
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:06:24.842
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:06:24.844
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 09/20/23 13:06:24.848
    Sep 20 13:06:24.856: INFO: Waiting up to 5m0s for pod "pod-fc163a22-3a2f-4f75-9b6f-82d6b3835a57" in namespace "emptydir-6351" to be "Succeeded or Failed"
    Sep 20 13:06:24.859: INFO: Pod "pod-fc163a22-3a2f-4f75-9b6f-82d6b3835a57": Phase="Pending", Reason="", readiness=false. Elapsed: 3.562903ms
    Sep 20 13:06:26.866: INFO: Pod "pod-fc163a22-3a2f-4f75-9b6f-82d6b3835a57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009667403s
    Sep 20 13:06:29.227: INFO: Pod "pod-fc163a22-3a2f-4f75-9b6f-82d6b3835a57": Phase="Pending", Reason="", readiness=false. Elapsed: 4.371544443s
    Sep 20 13:06:31.044: INFO: Pod "pod-fc163a22-3a2f-4f75-9b6f-82d6b3835a57": Phase="Pending", Reason="", readiness=false. Elapsed: 6.188130197s
    Sep 20 13:06:32.864: INFO: Pod "pod-fc163a22-3a2f-4f75-9b6f-82d6b3835a57": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008503436s
    Sep 20 13:06:35.152: INFO: Pod "pod-fc163a22-3a2f-4f75-9b6f-82d6b3835a57": Phase="Pending", Reason="", readiness=false. Elapsed: 10.296274641s
    Sep 20 13:06:36.869: INFO: Pod "pod-fc163a22-3a2f-4f75-9b6f-82d6b3835a57": Phase="Pending", Reason="", readiness=false. Elapsed: 12.012777085s
    Sep 20 13:06:38.864: INFO: Pod "pod-fc163a22-3a2f-4f75-9b6f-82d6b3835a57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.008509964s
    STEP: Saw pod success 09/20/23 13:06:38.864
    Sep 20 13:06:38.864: INFO: Pod "pod-fc163a22-3a2f-4f75-9b6f-82d6b3835a57" satisfied condition "Succeeded or Failed"
    Sep 20 13:06:38.870: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-fc163a22-3a2f-4f75-9b6f-82d6b3835a57 container test-container: <nil>
    STEP: delete the pod 09/20/23 13:06:38.882
    Sep 20 13:06:39.014: INFO: Waiting for pod pod-fc163a22-3a2f-4f75-9b6f-82d6b3835a57 to disappear
    Sep 20 13:06:39.026: INFO: Pod pod-fc163a22-3a2f-4f75-9b6f-82d6b3835a57 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:06:39.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6351" for this suite. 09/20/23 13:06:39.046
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:06:39.059
Sep 20 13:06:39.060: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename job 09/20/23 13:06:39.06
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:06:39.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:06:39.088
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 09/20/23 13:06:39.092
STEP: Ensuring job reaches completions 09/20/23 13:06:39.098
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep 20 13:06:57.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-7458" for this suite. 09/20/23 13:06:57.433
------------------------------
â€¢ [SLOW TEST] [18.390 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:06:39.059
    Sep 20 13:06:39.060: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename job 09/20/23 13:06:39.06
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:06:39.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:06:39.088
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 09/20/23 13:06:39.092
    STEP: Ensuring job reaches completions 09/20/23 13:06:39.098
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:06:57.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-7458" for this suite. 09/20/23 13:06:57.433
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:06:57.454
Sep 20 13:06:57.454: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename projected 09/20/23 13:06:57.455
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:06:57.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:06:57.628
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-57d52d8d-6bd6-4e69-aef2-e1a3b03ec3bc 09/20/23 13:06:57.649
STEP: Creating a pod to test consume secrets 09/20/23 13:06:57.661
Sep 20 13:06:57.675: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-944e45b1-8f73-4d41-b1c5-1fa3c72d2c73" in namespace "projected-3397" to be "Succeeded or Failed"
Sep 20 13:06:57.681: INFO: Pod "pod-projected-secrets-944e45b1-8f73-4d41-b1c5-1fa3c72d2c73": Phase="Pending", Reason="", readiness=false. Elapsed: 6.407852ms
Sep 20 13:06:59.687: INFO: Pod "pod-projected-secrets-944e45b1-8f73-4d41-b1c5-1fa3c72d2c73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011921719s
Sep 20 13:07:01.685: INFO: Pod "pod-projected-secrets-944e45b1-8f73-4d41-b1c5-1fa3c72d2c73": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010313058s
Sep 20 13:07:03.820: INFO: Pod "pod-projected-secrets-944e45b1-8f73-4d41-b1c5-1fa3c72d2c73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.144963769s
STEP: Saw pod success 09/20/23 13:07:03.82
Sep 20 13:07:03.820: INFO: Pod "pod-projected-secrets-944e45b1-8f73-4d41-b1c5-1fa3c72d2c73" satisfied condition "Succeeded or Failed"
Sep 20 13:07:03.824: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-projected-secrets-944e45b1-8f73-4d41-b1c5-1fa3c72d2c73 container projected-secret-volume-test: <nil>
STEP: delete the pod 09/20/23 13:07:03.842
Sep 20 13:07:04.218: INFO: Waiting for pod pod-projected-secrets-944e45b1-8f73-4d41-b1c5-1fa3c72d2c73 to disappear
Sep 20 13:07:04.225: INFO: Pod pod-projected-secrets-944e45b1-8f73-4d41-b1c5-1fa3c72d2c73 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep 20 13:07:04.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3397" for this suite. 09/20/23 13:07:04.229
------------------------------
â€¢ [SLOW TEST] [6.781 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:06:57.454
    Sep 20 13:06:57.454: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename projected 09/20/23 13:06:57.455
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:06:57.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:06:57.628
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-57d52d8d-6bd6-4e69-aef2-e1a3b03ec3bc 09/20/23 13:06:57.649
    STEP: Creating a pod to test consume secrets 09/20/23 13:06:57.661
    Sep 20 13:06:57.675: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-944e45b1-8f73-4d41-b1c5-1fa3c72d2c73" in namespace "projected-3397" to be "Succeeded or Failed"
    Sep 20 13:06:57.681: INFO: Pod "pod-projected-secrets-944e45b1-8f73-4d41-b1c5-1fa3c72d2c73": Phase="Pending", Reason="", readiness=false. Elapsed: 6.407852ms
    Sep 20 13:06:59.687: INFO: Pod "pod-projected-secrets-944e45b1-8f73-4d41-b1c5-1fa3c72d2c73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011921719s
    Sep 20 13:07:01.685: INFO: Pod "pod-projected-secrets-944e45b1-8f73-4d41-b1c5-1fa3c72d2c73": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010313058s
    Sep 20 13:07:03.820: INFO: Pod "pod-projected-secrets-944e45b1-8f73-4d41-b1c5-1fa3c72d2c73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.144963769s
    STEP: Saw pod success 09/20/23 13:07:03.82
    Sep 20 13:07:03.820: INFO: Pod "pod-projected-secrets-944e45b1-8f73-4d41-b1c5-1fa3c72d2c73" satisfied condition "Succeeded or Failed"
    Sep 20 13:07:03.824: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-projected-secrets-944e45b1-8f73-4d41-b1c5-1fa3c72d2c73 container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/20/23 13:07:03.842
    Sep 20 13:07:04.218: INFO: Waiting for pod pod-projected-secrets-944e45b1-8f73-4d41-b1c5-1fa3c72d2c73 to disappear
    Sep 20 13:07:04.225: INFO: Pod pod-projected-secrets-944e45b1-8f73-4d41-b1c5-1fa3c72d2c73 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:07:04.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3397" for this suite. 09/20/23 13:07:04.229
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:07:04.238
Sep 20 13:07:04.238: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename emptydir 09/20/23 13:07:04.238
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:07:04.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:07:04.873
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 09/20/23 13:07:04.878
Sep 20 13:07:04.891: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-c08c8c03-cc74-407d-8246-22d77d27ceb8" in namespace "emptydir-4436" to be "running"
Sep 20 13:07:04.895: INFO: Pod "pod-sharedvolume-c08c8c03-cc74-407d-8246-22d77d27ceb8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026146ms
Sep 20 13:07:06.907: INFO: Pod "pod-sharedvolume-c08c8c03-cc74-407d-8246-22d77d27ceb8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016010684s
Sep 20 13:07:08.909: INFO: Pod "pod-sharedvolume-c08c8c03-cc74-407d-8246-22d77d27ceb8": Phase="Running", Reason="", readiness=false. Elapsed: 4.018569414s
Sep 20 13:07:08.910: INFO: Pod "pod-sharedvolume-c08c8c03-cc74-407d-8246-22d77d27ceb8" satisfied condition "running"
STEP: Reading file content from the nginx-container 09/20/23 13:07:08.91
Sep 20 13:07:08.910: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4436 PodName:pod-sharedvolume-c08c8c03-cc74-407d-8246-22d77d27ceb8 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 20 13:07:08.910: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
Sep 20 13:07:08.911: INFO: ExecWithOptions: Clientset creation
Sep 20 13:07:08.911: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/emptydir-4436/pods/pod-sharedvolume-c08c8c03-cc74-407d-8246-22d77d27ceb8/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Sep 20 13:07:09.064: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep 20 13:07:09.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4436" for this suite. 09/20/23 13:07:09.165
------------------------------
â€¢ [4.943 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:07:04.238
    Sep 20 13:07:04.238: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename emptydir 09/20/23 13:07:04.238
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:07:04.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:07:04.873
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 09/20/23 13:07:04.878
    Sep 20 13:07:04.891: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-c08c8c03-cc74-407d-8246-22d77d27ceb8" in namespace "emptydir-4436" to be "running"
    Sep 20 13:07:04.895: INFO: Pod "pod-sharedvolume-c08c8c03-cc74-407d-8246-22d77d27ceb8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026146ms
    Sep 20 13:07:06.907: INFO: Pod "pod-sharedvolume-c08c8c03-cc74-407d-8246-22d77d27ceb8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016010684s
    Sep 20 13:07:08.909: INFO: Pod "pod-sharedvolume-c08c8c03-cc74-407d-8246-22d77d27ceb8": Phase="Running", Reason="", readiness=false. Elapsed: 4.018569414s
    Sep 20 13:07:08.910: INFO: Pod "pod-sharedvolume-c08c8c03-cc74-407d-8246-22d77d27ceb8" satisfied condition "running"
    STEP: Reading file content from the nginx-container 09/20/23 13:07:08.91
    Sep 20 13:07:08.910: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4436 PodName:pod-sharedvolume-c08c8c03-cc74-407d-8246-22d77d27ceb8 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep 20 13:07:08.910: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    Sep 20 13:07:08.911: INFO: ExecWithOptions: Clientset creation
    Sep 20 13:07:08.911: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/emptydir-4436/pods/pod-sharedvolume-c08c8c03-cc74-407d-8246-22d77d27ceb8/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Sep 20 13:07:09.064: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:07:09.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4436" for this suite. 09/20/23 13:07:09.165
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:07:09.181
Sep 20 13:07:09.181: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename endpointslice 09/20/23 13:07:09.182
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:07:09.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:07:09.424
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 09/20/23 13:07:09.429
STEP: getting /apis/discovery.k8s.io 09/20/23 13:07:09.434
STEP: getting /apis/discovery.k8s.iov1 09/20/23 13:07:09.438
STEP: creating 09/20/23 13:07:09.442
STEP: getting 09/20/23 13:07:09.494
STEP: listing 09/20/23 13:07:09.499
STEP: watching 09/20/23 13:07:09.505
Sep 20 13:07:09.505: INFO: starting watch
STEP: cluster-wide listing 09/20/23 13:07:09.508
STEP: cluster-wide watching 09/20/23 13:07:09.518
Sep 20 13:07:09.518: INFO: starting watch
STEP: patching 09/20/23 13:07:09.524
STEP: updating 09/20/23 13:07:09.537
Sep 20 13:07:09.553: INFO: waiting for watch events with expected annotations
Sep 20 13:07:09.553: INFO: saw patched and updated annotations
STEP: deleting 09/20/23 13:07:09.553
STEP: deleting a collection 09/20/23 13:07:09.575
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Sep 20 13:07:09.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-7647" for this suite. 09/20/23 13:07:09.615
------------------------------
â€¢ [0.448 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:07:09.181
    Sep 20 13:07:09.181: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename endpointslice 09/20/23 13:07:09.182
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:07:09.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:07:09.424
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 09/20/23 13:07:09.429
    STEP: getting /apis/discovery.k8s.io 09/20/23 13:07:09.434
    STEP: getting /apis/discovery.k8s.iov1 09/20/23 13:07:09.438
    STEP: creating 09/20/23 13:07:09.442
    STEP: getting 09/20/23 13:07:09.494
    STEP: listing 09/20/23 13:07:09.499
    STEP: watching 09/20/23 13:07:09.505
    Sep 20 13:07:09.505: INFO: starting watch
    STEP: cluster-wide listing 09/20/23 13:07:09.508
    STEP: cluster-wide watching 09/20/23 13:07:09.518
    Sep 20 13:07:09.518: INFO: starting watch
    STEP: patching 09/20/23 13:07:09.524
    STEP: updating 09/20/23 13:07:09.537
    Sep 20 13:07:09.553: INFO: waiting for watch events with expected annotations
    Sep 20 13:07:09.553: INFO: saw patched and updated annotations
    STEP: deleting 09/20/23 13:07:09.553
    STEP: deleting a collection 09/20/23 13:07:09.575
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:07:09.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-7647" for this suite. 09/20/23 13:07:09.615
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:07:09.63
Sep 20 13:07:09.630: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename emptydir 09/20/23 13:07:09.63
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:07:09.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:07:09.784
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 09/20/23 13:07:09.795
Sep 20 13:07:09.815: INFO: Waiting up to 5m0s for pod "pod-a3f2593b-4e25-4c14-9e8a-933624648329" in namespace "emptydir-67" to be "Succeeded or Failed"
Sep 20 13:07:09.822: INFO: Pod "pod-a3f2593b-4e25-4c14-9e8a-933624648329": Phase="Pending", Reason="", readiness=false. Elapsed: 7.180279ms
Sep 20 13:07:11.829: INFO: Pod "pod-a3f2593b-4e25-4c14-9e8a-933624648329": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013941325s
Sep 20 13:07:13.826: INFO: Pod "pod-a3f2593b-4e25-4c14-9e8a-933624648329": Phase="Running", Reason="", readiness=false. Elapsed: 4.01142793s
Sep 20 13:07:15.828: INFO: Pod "pod-a3f2593b-4e25-4c14-9e8a-933624648329": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012923446s
STEP: Saw pod success 09/20/23 13:07:15.828
Sep 20 13:07:15.828: INFO: Pod "pod-a3f2593b-4e25-4c14-9e8a-933624648329" satisfied condition "Succeeded or Failed"
Sep 20 13:07:15.830: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-a3f2593b-4e25-4c14-9e8a-933624648329 container test-container: <nil>
STEP: delete the pod 09/20/23 13:07:15.839
Sep 20 13:07:16.228: INFO: Waiting for pod pod-a3f2593b-4e25-4c14-9e8a-933624648329 to disappear
Sep 20 13:07:16.234: INFO: Pod pod-a3f2593b-4e25-4c14-9e8a-933624648329 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep 20 13:07:16.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-67" for this suite. 09/20/23 13:07:16.238
------------------------------
â€¢ [SLOW TEST] [6.615 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:07:09.63
    Sep 20 13:07:09.630: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename emptydir 09/20/23 13:07:09.63
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:07:09.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:07:09.784
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 09/20/23 13:07:09.795
    Sep 20 13:07:09.815: INFO: Waiting up to 5m0s for pod "pod-a3f2593b-4e25-4c14-9e8a-933624648329" in namespace "emptydir-67" to be "Succeeded or Failed"
    Sep 20 13:07:09.822: INFO: Pod "pod-a3f2593b-4e25-4c14-9e8a-933624648329": Phase="Pending", Reason="", readiness=false. Elapsed: 7.180279ms
    Sep 20 13:07:11.829: INFO: Pod "pod-a3f2593b-4e25-4c14-9e8a-933624648329": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013941325s
    Sep 20 13:07:13.826: INFO: Pod "pod-a3f2593b-4e25-4c14-9e8a-933624648329": Phase="Running", Reason="", readiness=false. Elapsed: 4.01142793s
    Sep 20 13:07:15.828: INFO: Pod "pod-a3f2593b-4e25-4c14-9e8a-933624648329": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012923446s
    STEP: Saw pod success 09/20/23 13:07:15.828
    Sep 20 13:07:15.828: INFO: Pod "pod-a3f2593b-4e25-4c14-9e8a-933624648329" satisfied condition "Succeeded or Failed"
    Sep 20 13:07:15.830: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-a3f2593b-4e25-4c14-9e8a-933624648329 container test-container: <nil>
    STEP: delete the pod 09/20/23 13:07:15.839
    Sep 20 13:07:16.228: INFO: Waiting for pod pod-a3f2593b-4e25-4c14-9e8a-933624648329 to disappear
    Sep 20 13:07:16.234: INFO: Pod pod-a3f2593b-4e25-4c14-9e8a-933624648329 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:07:16.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-67" for this suite. 09/20/23 13:07:16.238
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:07:16.246
Sep 20 13:07:16.246: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename secrets 09/20/23 13:07:16.247
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:07:16.272
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:07:16.275
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-c8b2e047-29ba-4633-8099-bde9c053fab4 09/20/23 13:07:16.28
STEP: Creating a pod to test consume secrets 09/20/23 13:07:16.287
Sep 20 13:07:16.302: INFO: Waiting up to 5m0s for pod "pod-secrets-28dbb4b2-1505-4ca8-a0f3-0e61b50ac32e" in namespace "secrets-9429" to be "Succeeded or Failed"
Sep 20 13:07:16.307: INFO: Pod "pod-secrets-28dbb4b2-1505-4ca8-a0f3-0e61b50ac32e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.137329ms
Sep 20 13:07:18.401: INFO: Pod "pod-secrets-28dbb4b2-1505-4ca8-a0f3-0e61b50ac32e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.09933175s
Sep 20 13:07:20.312: INFO: Pod "pod-secrets-28dbb4b2-1505-4ca8-a0f3-0e61b50ac32e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010208421s
Sep 20 13:07:22.521: INFO: Pod "pod-secrets-28dbb4b2-1505-4ca8-a0f3-0e61b50ac32e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.218935654s
STEP: Saw pod success 09/20/23 13:07:22.521
Sep 20 13:07:22.521: INFO: Pod "pod-secrets-28dbb4b2-1505-4ca8-a0f3-0e61b50ac32e" satisfied condition "Succeeded or Failed"
Sep 20 13:07:22.529: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-secrets-28dbb4b2-1505-4ca8-a0f3-0e61b50ac32e container secret-volume-test: <nil>
STEP: delete the pod 09/20/23 13:07:22.536
Sep 20 13:07:22.766: INFO: Waiting for pod pod-secrets-28dbb4b2-1505-4ca8-a0f3-0e61b50ac32e to disappear
Sep 20 13:07:22.774: INFO: Pod pod-secrets-28dbb4b2-1505-4ca8-a0f3-0e61b50ac32e no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep 20 13:07:22.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9429" for this suite. 09/20/23 13:07:22.782
------------------------------
â€¢ [SLOW TEST] [6.547 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:07:16.246
    Sep 20 13:07:16.246: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename secrets 09/20/23 13:07:16.247
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:07:16.272
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:07:16.275
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-c8b2e047-29ba-4633-8099-bde9c053fab4 09/20/23 13:07:16.28
    STEP: Creating a pod to test consume secrets 09/20/23 13:07:16.287
    Sep 20 13:07:16.302: INFO: Waiting up to 5m0s for pod "pod-secrets-28dbb4b2-1505-4ca8-a0f3-0e61b50ac32e" in namespace "secrets-9429" to be "Succeeded or Failed"
    Sep 20 13:07:16.307: INFO: Pod "pod-secrets-28dbb4b2-1505-4ca8-a0f3-0e61b50ac32e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.137329ms
    Sep 20 13:07:18.401: INFO: Pod "pod-secrets-28dbb4b2-1505-4ca8-a0f3-0e61b50ac32e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.09933175s
    Sep 20 13:07:20.312: INFO: Pod "pod-secrets-28dbb4b2-1505-4ca8-a0f3-0e61b50ac32e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010208421s
    Sep 20 13:07:22.521: INFO: Pod "pod-secrets-28dbb4b2-1505-4ca8-a0f3-0e61b50ac32e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.218935654s
    STEP: Saw pod success 09/20/23 13:07:22.521
    Sep 20 13:07:22.521: INFO: Pod "pod-secrets-28dbb4b2-1505-4ca8-a0f3-0e61b50ac32e" satisfied condition "Succeeded or Failed"
    Sep 20 13:07:22.529: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-secrets-28dbb4b2-1505-4ca8-a0f3-0e61b50ac32e container secret-volume-test: <nil>
    STEP: delete the pod 09/20/23 13:07:22.536
    Sep 20 13:07:22.766: INFO: Waiting for pod pod-secrets-28dbb4b2-1505-4ca8-a0f3-0e61b50ac32e to disappear
    Sep 20 13:07:22.774: INFO: Pod pod-secrets-28dbb4b2-1505-4ca8-a0f3-0e61b50ac32e no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:07:22.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9429" for this suite. 09/20/23 13:07:22.782
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:07:22.795
Sep 20 13:07:22.795: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename projected 09/20/23 13:07:22.796
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:07:22.84
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:07:22.845
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-579fed3c-a0e3-4268-8f02-7dab4c044f17 09/20/23 13:07:22.854
STEP: Creating secret with name secret-projected-all-test-volume-e92f144a-79ca-49aa-a46b-89ced06725ae 09/20/23 13:07:22.865
STEP: Creating a pod to test Check all projections for projected volume plugin 09/20/23 13:07:22.875
Sep 20 13:07:22.887: INFO: Waiting up to 5m0s for pod "projected-volume-5386b8ea-f5ff-4a24-aebb-9729d1372440" in namespace "projected-5864" to be "Succeeded or Failed"
Sep 20 13:07:22.892: INFO: Pod "projected-volume-5386b8ea-f5ff-4a24-aebb-9729d1372440": Phase="Pending", Reason="", readiness=false. Elapsed: 4.68198ms
Sep 20 13:07:24.899: INFO: Pod "projected-volume-5386b8ea-f5ff-4a24-aebb-9729d1372440": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01190106s
Sep 20 13:07:27.124: INFO: Pod "projected-volume-5386b8ea-f5ff-4a24-aebb-9729d1372440": Phase="Pending", Reason="", readiness=false. Elapsed: 4.23685205s
Sep 20 13:07:28.897: INFO: Pod "projected-volume-5386b8ea-f5ff-4a24-aebb-9729d1372440": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009685103s
STEP: Saw pod success 09/20/23 13:07:28.897
Sep 20 13:07:28.897: INFO: Pod "projected-volume-5386b8ea-f5ff-4a24-aebb-9729d1372440" satisfied condition "Succeeded or Failed"
Sep 20 13:07:28.900: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod projected-volume-5386b8ea-f5ff-4a24-aebb-9729d1372440 container projected-all-volume-test: <nil>
STEP: delete the pod 09/20/23 13:07:28.906
Sep 20 13:07:29.657: INFO: Waiting for pod projected-volume-5386b8ea-f5ff-4a24-aebb-9729d1372440 to disappear
Sep 20 13:07:29.661: INFO: Pod projected-volume-5386b8ea-f5ff-4a24-aebb-9729d1372440 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Sep 20 13:07:29.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5864" for this suite. 09/20/23 13:07:29.669
------------------------------
â€¢ [SLOW TEST] [6.883 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:07:22.795
    Sep 20 13:07:22.795: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename projected 09/20/23 13:07:22.796
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:07:22.84
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:07:22.845
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-579fed3c-a0e3-4268-8f02-7dab4c044f17 09/20/23 13:07:22.854
    STEP: Creating secret with name secret-projected-all-test-volume-e92f144a-79ca-49aa-a46b-89ced06725ae 09/20/23 13:07:22.865
    STEP: Creating a pod to test Check all projections for projected volume plugin 09/20/23 13:07:22.875
    Sep 20 13:07:22.887: INFO: Waiting up to 5m0s for pod "projected-volume-5386b8ea-f5ff-4a24-aebb-9729d1372440" in namespace "projected-5864" to be "Succeeded or Failed"
    Sep 20 13:07:22.892: INFO: Pod "projected-volume-5386b8ea-f5ff-4a24-aebb-9729d1372440": Phase="Pending", Reason="", readiness=false. Elapsed: 4.68198ms
    Sep 20 13:07:24.899: INFO: Pod "projected-volume-5386b8ea-f5ff-4a24-aebb-9729d1372440": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01190106s
    Sep 20 13:07:27.124: INFO: Pod "projected-volume-5386b8ea-f5ff-4a24-aebb-9729d1372440": Phase="Pending", Reason="", readiness=false. Elapsed: 4.23685205s
    Sep 20 13:07:28.897: INFO: Pod "projected-volume-5386b8ea-f5ff-4a24-aebb-9729d1372440": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009685103s
    STEP: Saw pod success 09/20/23 13:07:28.897
    Sep 20 13:07:28.897: INFO: Pod "projected-volume-5386b8ea-f5ff-4a24-aebb-9729d1372440" satisfied condition "Succeeded or Failed"
    Sep 20 13:07:28.900: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod projected-volume-5386b8ea-f5ff-4a24-aebb-9729d1372440 container projected-all-volume-test: <nil>
    STEP: delete the pod 09/20/23 13:07:28.906
    Sep 20 13:07:29.657: INFO: Waiting for pod projected-volume-5386b8ea-f5ff-4a24-aebb-9729d1372440 to disappear
    Sep 20 13:07:29.661: INFO: Pod projected-volume-5386b8ea-f5ff-4a24-aebb-9729d1372440 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:07:29.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5864" for this suite. 09/20/23 13:07:29.669
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:07:29.68
Sep 20 13:07:29.681: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename downward-api 09/20/23 13:07:29.681
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:07:29.744
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:07:29.748
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 09/20/23 13:07:29.755
Sep 20 13:07:29.765: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f0d5b555-040e-4f31-8418-7913dcca7962" in namespace "downward-api-6809" to be "Succeeded or Failed"
Sep 20 13:07:29.770: INFO: Pod "downwardapi-volume-f0d5b555-040e-4f31-8418-7913dcca7962": Phase="Pending", Reason="", readiness=false. Elapsed: 5.016181ms
Sep 20 13:07:32.021: INFO: Pod "downwardapi-volume-f0d5b555-040e-4f31-8418-7913dcca7962": Phase="Pending", Reason="", readiness=false. Elapsed: 2.255549027s
Sep 20 13:07:33.777: INFO: Pod "downwardapi-volume-f0d5b555-040e-4f31-8418-7913dcca7962": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012188159s
Sep 20 13:07:35.932: INFO: Pod "downwardapi-volume-f0d5b555-040e-4f31-8418-7913dcca7962": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.166907362s
STEP: Saw pod success 09/20/23 13:07:35.932
Sep 20 13:07:35.933: INFO: Pod "downwardapi-volume-f0d5b555-040e-4f31-8418-7913dcca7962" satisfied condition "Succeeded or Failed"
Sep 20 13:07:35.937: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downwardapi-volume-f0d5b555-040e-4f31-8418-7913dcca7962 container client-container: <nil>
STEP: delete the pod 09/20/23 13:07:35.946
Sep 20 13:07:35.980: INFO: Waiting for pod downwardapi-volume-f0d5b555-040e-4f31-8418-7913dcca7962 to disappear
Sep 20 13:07:35.984: INFO: Pod downwardapi-volume-f0d5b555-040e-4f31-8418-7913dcca7962 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep 20 13:07:35.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6809" for this suite. 09/20/23 13:07:35.99
------------------------------
â€¢ [SLOW TEST] [6.319 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:07:29.68
    Sep 20 13:07:29.681: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename downward-api 09/20/23 13:07:29.681
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:07:29.744
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:07:29.748
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 09/20/23 13:07:29.755
    Sep 20 13:07:29.765: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f0d5b555-040e-4f31-8418-7913dcca7962" in namespace "downward-api-6809" to be "Succeeded or Failed"
    Sep 20 13:07:29.770: INFO: Pod "downwardapi-volume-f0d5b555-040e-4f31-8418-7913dcca7962": Phase="Pending", Reason="", readiness=false. Elapsed: 5.016181ms
    Sep 20 13:07:32.021: INFO: Pod "downwardapi-volume-f0d5b555-040e-4f31-8418-7913dcca7962": Phase="Pending", Reason="", readiness=false. Elapsed: 2.255549027s
    Sep 20 13:07:33.777: INFO: Pod "downwardapi-volume-f0d5b555-040e-4f31-8418-7913dcca7962": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012188159s
    Sep 20 13:07:35.932: INFO: Pod "downwardapi-volume-f0d5b555-040e-4f31-8418-7913dcca7962": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.166907362s
    STEP: Saw pod success 09/20/23 13:07:35.932
    Sep 20 13:07:35.933: INFO: Pod "downwardapi-volume-f0d5b555-040e-4f31-8418-7913dcca7962" satisfied condition "Succeeded or Failed"
    Sep 20 13:07:35.937: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downwardapi-volume-f0d5b555-040e-4f31-8418-7913dcca7962 container client-container: <nil>
    STEP: delete the pod 09/20/23 13:07:35.946
    Sep 20 13:07:35.980: INFO: Waiting for pod downwardapi-volume-f0d5b555-040e-4f31-8418-7913dcca7962 to disappear
    Sep 20 13:07:35.984: INFO: Pod downwardapi-volume-f0d5b555-040e-4f31-8418-7913dcca7962 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:07:35.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6809" for this suite. 09/20/23 13:07:35.99
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:07:36.002
Sep 20 13:07:36.002: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename emptydir 09/20/23 13:07:36.002
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:07:36.029
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:07:36.035
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 09/20/23 13:07:36.04
Sep 20 13:07:36.262: INFO: Waiting up to 5m0s for pod "pod-eaed053f-233a-4085-a1c5-b64b7a55d4e5" in namespace "emptydir-4003" to be "Succeeded or Failed"
Sep 20 13:07:36.273: INFO: Pod "pod-eaed053f-233a-4085-a1c5-b64b7a55d4e5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.478351ms
Sep 20 13:07:38.327: INFO: Pod "pod-eaed053f-233a-4085-a1c5-b64b7a55d4e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065113234s
Sep 20 13:07:40.446: INFO: Pod "pod-eaed053f-233a-4085-a1c5-b64b7a55d4e5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.184037918s
Sep 20 13:07:42.677: INFO: Pod "pod-eaed053f-233a-4085-a1c5-b64b7a55d4e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.414866761s
STEP: Saw pod success 09/20/23 13:07:42.677
Sep 20 13:07:42.677: INFO: Pod "pod-eaed053f-233a-4085-a1c5-b64b7a55d4e5" satisfied condition "Succeeded or Failed"
Sep 20 13:07:42.946: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-eaed053f-233a-4085-a1c5-b64b7a55d4e5 container test-container: <nil>
STEP: delete the pod 09/20/23 13:07:42.956
Sep 20 13:07:43.662: INFO: Waiting for pod pod-eaed053f-233a-4085-a1c5-b64b7a55d4e5 to disappear
Sep 20 13:07:43.666: INFO: Pod pod-eaed053f-233a-4085-a1c5-b64b7a55d4e5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep 20 13:07:43.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4003" for this suite. 09/20/23 13:07:43.671
------------------------------
â€¢ [SLOW TEST] [7.676 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:07:36.002
    Sep 20 13:07:36.002: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename emptydir 09/20/23 13:07:36.002
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:07:36.029
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:07:36.035
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 09/20/23 13:07:36.04
    Sep 20 13:07:36.262: INFO: Waiting up to 5m0s for pod "pod-eaed053f-233a-4085-a1c5-b64b7a55d4e5" in namespace "emptydir-4003" to be "Succeeded or Failed"
    Sep 20 13:07:36.273: INFO: Pod "pod-eaed053f-233a-4085-a1c5-b64b7a55d4e5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.478351ms
    Sep 20 13:07:38.327: INFO: Pod "pod-eaed053f-233a-4085-a1c5-b64b7a55d4e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065113234s
    Sep 20 13:07:40.446: INFO: Pod "pod-eaed053f-233a-4085-a1c5-b64b7a55d4e5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.184037918s
    Sep 20 13:07:42.677: INFO: Pod "pod-eaed053f-233a-4085-a1c5-b64b7a55d4e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.414866761s
    STEP: Saw pod success 09/20/23 13:07:42.677
    Sep 20 13:07:42.677: INFO: Pod "pod-eaed053f-233a-4085-a1c5-b64b7a55d4e5" satisfied condition "Succeeded or Failed"
    Sep 20 13:07:42.946: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-eaed053f-233a-4085-a1c5-b64b7a55d4e5 container test-container: <nil>
    STEP: delete the pod 09/20/23 13:07:42.956
    Sep 20 13:07:43.662: INFO: Waiting for pod pod-eaed053f-233a-4085-a1c5-b64b7a55d4e5 to disappear
    Sep 20 13:07:43.666: INFO: Pod pod-eaed053f-233a-4085-a1c5-b64b7a55d4e5 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:07:43.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4003" for this suite. 09/20/23 13:07:43.671
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:07:43.678
Sep 20 13:07:43.678: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename init-container 09/20/23 13:07:43.679
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:07:44.492
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:07:44.496
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 09/20/23 13:07:44.501
Sep 20 13:07:44.502: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep 20 13:07:51.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-9836" for this suite. 09/20/23 13:07:51.435
------------------------------
â€¢ [SLOW TEST] [7.764 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:07:43.678
    Sep 20 13:07:43.678: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename init-container 09/20/23 13:07:43.679
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:07:44.492
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:07:44.496
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 09/20/23 13:07:44.501
    Sep 20 13:07:44.502: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:07:51.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-9836" for this suite. 09/20/23 13:07:51.435
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:07:51.445
Sep 20 13:07:51.445: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename replication-controller 09/20/23 13:07:51.446
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:07:52.998
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:07:53.004
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 09/20/23 13:07:53.009
Sep 20 13:07:53.135: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-3954" to be "running and ready"
Sep 20 13:07:53.139: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 4.416101ms
Sep 20 13:07:53.139: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:07:55.151: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015961732s
Sep 20 13:07:55.151: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:07:57.261: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 4.126754617s
Sep 20 13:07:57.262: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Sep 20 13:07:57.262: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 09/20/23 13:07:57.268
STEP: Then the orphan pod is adopted 09/20/23 13:07:57.303
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep 20 13:07:58.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-3954" for this suite. 09/20/23 13:07:58.369
------------------------------
â€¢ [SLOW TEST] [6.948 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:07:51.445
    Sep 20 13:07:51.445: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename replication-controller 09/20/23 13:07:51.446
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:07:52.998
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:07:53.004
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 09/20/23 13:07:53.009
    Sep 20 13:07:53.135: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-3954" to be "running and ready"
    Sep 20 13:07:53.139: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 4.416101ms
    Sep 20 13:07:53.139: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:07:55.151: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015961732s
    Sep 20 13:07:55.151: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:07:57.261: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 4.126754617s
    Sep 20 13:07:57.262: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Sep 20 13:07:57.262: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 09/20/23 13:07:57.268
    STEP: Then the orphan pod is adopted 09/20/23 13:07:57.303
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:07:58.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-3954" for this suite. 09/20/23 13:07:58.369
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:07:58.395
Sep 20 13:07:58.395: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename projected 09/20/23 13:07:58.395
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:07:58.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:07:58.74
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 09/20/23 13:07:58.75
Sep 20 13:07:59.245: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2eeca12c-e30b-4a4c-a01a-d71eb3cf4f33" in namespace "projected-4230" to be "Succeeded or Failed"
Sep 20 13:07:59.431: INFO: Pod "downwardapi-volume-2eeca12c-e30b-4a4c-a01a-d71eb3cf4f33": Phase="Pending", Reason="", readiness=false. Elapsed: 186.214612ms
Sep 20 13:08:01.435: INFO: Pod "downwardapi-volume-2eeca12c-e30b-4a4c-a01a-d71eb3cf4f33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.190072419s
Sep 20 13:08:03.437: INFO: Pod "downwardapi-volume-2eeca12c-e30b-4a4c-a01a-d71eb3cf4f33": Phase="Pending", Reason="", readiness=false. Elapsed: 4.191966846s
Sep 20 13:08:05.436: INFO: Pod "downwardapi-volume-2eeca12c-e30b-4a4c-a01a-d71eb3cf4f33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.191574697s
STEP: Saw pod success 09/20/23 13:08:05.437
Sep 20 13:08:05.437: INFO: Pod "downwardapi-volume-2eeca12c-e30b-4a4c-a01a-d71eb3cf4f33" satisfied condition "Succeeded or Failed"
Sep 20 13:08:05.440: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downwardapi-volume-2eeca12c-e30b-4a4c-a01a-d71eb3cf4f33 container client-container: <nil>
STEP: delete the pod 09/20/23 13:08:05.447
Sep 20 13:08:05.461: INFO: Waiting for pod downwardapi-volume-2eeca12c-e30b-4a4c-a01a-d71eb3cf4f33 to disappear
Sep 20 13:08:05.465: INFO: Pod downwardapi-volume-2eeca12c-e30b-4a4c-a01a-d71eb3cf4f33 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep 20 13:08:05.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4230" for this suite. 09/20/23 13:08:05.471
------------------------------
â€¢ [SLOW TEST] [7.097 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:07:58.395
    Sep 20 13:07:58.395: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename projected 09/20/23 13:07:58.395
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:07:58.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:07:58.74
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 09/20/23 13:07:58.75
    Sep 20 13:07:59.245: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2eeca12c-e30b-4a4c-a01a-d71eb3cf4f33" in namespace "projected-4230" to be "Succeeded or Failed"
    Sep 20 13:07:59.431: INFO: Pod "downwardapi-volume-2eeca12c-e30b-4a4c-a01a-d71eb3cf4f33": Phase="Pending", Reason="", readiness=false. Elapsed: 186.214612ms
    Sep 20 13:08:01.435: INFO: Pod "downwardapi-volume-2eeca12c-e30b-4a4c-a01a-d71eb3cf4f33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.190072419s
    Sep 20 13:08:03.437: INFO: Pod "downwardapi-volume-2eeca12c-e30b-4a4c-a01a-d71eb3cf4f33": Phase="Pending", Reason="", readiness=false. Elapsed: 4.191966846s
    Sep 20 13:08:05.436: INFO: Pod "downwardapi-volume-2eeca12c-e30b-4a4c-a01a-d71eb3cf4f33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.191574697s
    STEP: Saw pod success 09/20/23 13:08:05.437
    Sep 20 13:08:05.437: INFO: Pod "downwardapi-volume-2eeca12c-e30b-4a4c-a01a-d71eb3cf4f33" satisfied condition "Succeeded or Failed"
    Sep 20 13:08:05.440: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downwardapi-volume-2eeca12c-e30b-4a4c-a01a-d71eb3cf4f33 container client-container: <nil>
    STEP: delete the pod 09/20/23 13:08:05.447
    Sep 20 13:08:05.461: INFO: Waiting for pod downwardapi-volume-2eeca12c-e30b-4a4c-a01a-d71eb3cf4f33 to disappear
    Sep 20 13:08:05.465: INFO: Pod downwardapi-volume-2eeca12c-e30b-4a4c-a01a-d71eb3cf4f33 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:08:05.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4230" for this suite. 09/20/23 13:08:05.471
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:08:05.493
Sep 20 13:08:05.493: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename configmap 09/20/23 13:08:05.494
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:08:05.557
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:08:05.561
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-f2d0fbd1-89e6-49b1-aa31-958008d48248 09/20/23 13:08:05.565
STEP: Creating a pod to test consume configMaps 09/20/23 13:08:05.577
Sep 20 13:08:05.593: INFO: Waiting up to 5m0s for pod "pod-configmaps-dd871393-4fbb-4354-91c0-2314407f98e1" in namespace "configmap-5453" to be "Succeeded or Failed"
Sep 20 13:08:05.599: INFO: Pod "pod-configmaps-dd871393-4fbb-4354-91c0-2314407f98e1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.610931ms
Sep 20 13:08:07.604: INFO: Pod "pod-configmaps-dd871393-4fbb-4354-91c0-2314407f98e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011158332s
Sep 20 13:08:09.605: INFO: Pod "pod-configmaps-dd871393-4fbb-4354-91c0-2314407f98e1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01206594s
Sep 20 13:08:11.654: INFO: Pod "pod-configmaps-dd871393-4fbb-4354-91c0-2314407f98e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.06108983s
STEP: Saw pod success 09/20/23 13:08:11.654
Sep 20 13:08:11.654: INFO: Pod "pod-configmaps-dd871393-4fbb-4354-91c0-2314407f98e1" satisfied condition "Succeeded or Failed"
Sep 20 13:08:11.658: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-configmaps-dd871393-4fbb-4354-91c0-2314407f98e1 container agnhost-container: <nil>
STEP: delete the pod 09/20/23 13:08:11.664
Sep 20 13:08:12.587: INFO: Waiting for pod pod-configmaps-dd871393-4fbb-4354-91c0-2314407f98e1 to disappear
Sep 20 13:08:12.593: INFO: Pod pod-configmaps-dd871393-4fbb-4354-91c0-2314407f98e1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep 20 13:08:12.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5453" for this suite. 09/20/23 13:08:13.303
------------------------------
â€¢ [SLOW TEST] [7.816 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:08:05.493
    Sep 20 13:08:05.493: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename configmap 09/20/23 13:08:05.494
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:08:05.557
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:08:05.561
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-f2d0fbd1-89e6-49b1-aa31-958008d48248 09/20/23 13:08:05.565
    STEP: Creating a pod to test consume configMaps 09/20/23 13:08:05.577
    Sep 20 13:08:05.593: INFO: Waiting up to 5m0s for pod "pod-configmaps-dd871393-4fbb-4354-91c0-2314407f98e1" in namespace "configmap-5453" to be "Succeeded or Failed"
    Sep 20 13:08:05.599: INFO: Pod "pod-configmaps-dd871393-4fbb-4354-91c0-2314407f98e1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.610931ms
    Sep 20 13:08:07.604: INFO: Pod "pod-configmaps-dd871393-4fbb-4354-91c0-2314407f98e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011158332s
    Sep 20 13:08:09.605: INFO: Pod "pod-configmaps-dd871393-4fbb-4354-91c0-2314407f98e1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01206594s
    Sep 20 13:08:11.654: INFO: Pod "pod-configmaps-dd871393-4fbb-4354-91c0-2314407f98e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.06108983s
    STEP: Saw pod success 09/20/23 13:08:11.654
    Sep 20 13:08:11.654: INFO: Pod "pod-configmaps-dd871393-4fbb-4354-91c0-2314407f98e1" satisfied condition "Succeeded or Failed"
    Sep 20 13:08:11.658: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-configmaps-dd871393-4fbb-4354-91c0-2314407f98e1 container agnhost-container: <nil>
    STEP: delete the pod 09/20/23 13:08:11.664
    Sep 20 13:08:12.587: INFO: Waiting for pod pod-configmaps-dd871393-4fbb-4354-91c0-2314407f98e1 to disappear
    Sep 20 13:08:12.593: INFO: Pod pod-configmaps-dd871393-4fbb-4354-91c0-2314407f98e1 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:08:12.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5453" for this suite. 09/20/23 13:08:13.303
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:08:13.309
Sep 20 13:08:13.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename secrets 09/20/23 13:08:13.31
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:08:13.734
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:08:13.739
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-33179202-d9b2-4a29-a5b3-1d814e66a828 09/20/23 13:08:13.744
STEP: Creating a pod to test consume secrets 09/20/23 13:08:13.751
Sep 20 13:08:13.766: INFO: Waiting up to 5m0s for pod "pod-secrets-f2822530-b8a7-4b99-a60e-a2e7e8cfc7a5" in namespace "secrets-2298" to be "Succeeded or Failed"
Sep 20 13:08:13.790: INFO: Pod "pod-secrets-f2822530-b8a7-4b99-a60e-a2e7e8cfc7a5": Phase="Pending", Reason="", readiness=false. Elapsed: 23.457804ms
Sep 20 13:08:15.916: INFO: Pod "pod-secrets-f2822530-b8a7-4b99-a60e-a2e7e8cfc7a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.149568267s
Sep 20 13:08:17.797: INFO: Pod "pod-secrets-f2822530-b8a7-4b99-a60e-a2e7e8cfc7a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030661354s
Sep 20 13:08:19.794: INFO: Pod "pod-secrets-f2822530-b8a7-4b99-a60e-a2e7e8cfc7a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027817265s
STEP: Saw pod success 09/20/23 13:08:19.794
Sep 20 13:08:19.794: INFO: Pod "pod-secrets-f2822530-b8a7-4b99-a60e-a2e7e8cfc7a5" satisfied condition "Succeeded or Failed"
Sep 20 13:08:19.797: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-secrets-f2822530-b8a7-4b99-a60e-a2e7e8cfc7a5 container secret-volume-test: <nil>
STEP: delete the pod 09/20/23 13:08:19.805
Sep 20 13:08:19.824: INFO: Waiting for pod pod-secrets-f2822530-b8a7-4b99-a60e-a2e7e8cfc7a5 to disappear
Sep 20 13:08:19.829: INFO: Pod pod-secrets-f2822530-b8a7-4b99-a60e-a2e7e8cfc7a5 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep 20 13:08:19.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2298" for this suite. 09/20/23 13:08:19.838
------------------------------
â€¢ [SLOW TEST] [6.549 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:08:13.309
    Sep 20 13:08:13.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename secrets 09/20/23 13:08:13.31
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:08:13.734
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:08:13.739
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-33179202-d9b2-4a29-a5b3-1d814e66a828 09/20/23 13:08:13.744
    STEP: Creating a pod to test consume secrets 09/20/23 13:08:13.751
    Sep 20 13:08:13.766: INFO: Waiting up to 5m0s for pod "pod-secrets-f2822530-b8a7-4b99-a60e-a2e7e8cfc7a5" in namespace "secrets-2298" to be "Succeeded or Failed"
    Sep 20 13:08:13.790: INFO: Pod "pod-secrets-f2822530-b8a7-4b99-a60e-a2e7e8cfc7a5": Phase="Pending", Reason="", readiness=false. Elapsed: 23.457804ms
    Sep 20 13:08:15.916: INFO: Pod "pod-secrets-f2822530-b8a7-4b99-a60e-a2e7e8cfc7a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.149568267s
    Sep 20 13:08:17.797: INFO: Pod "pod-secrets-f2822530-b8a7-4b99-a60e-a2e7e8cfc7a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030661354s
    Sep 20 13:08:19.794: INFO: Pod "pod-secrets-f2822530-b8a7-4b99-a60e-a2e7e8cfc7a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027817265s
    STEP: Saw pod success 09/20/23 13:08:19.794
    Sep 20 13:08:19.794: INFO: Pod "pod-secrets-f2822530-b8a7-4b99-a60e-a2e7e8cfc7a5" satisfied condition "Succeeded or Failed"
    Sep 20 13:08:19.797: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-secrets-f2822530-b8a7-4b99-a60e-a2e7e8cfc7a5 container secret-volume-test: <nil>
    STEP: delete the pod 09/20/23 13:08:19.805
    Sep 20 13:08:19.824: INFO: Waiting for pod pod-secrets-f2822530-b8a7-4b99-a60e-a2e7e8cfc7a5 to disappear
    Sep 20 13:08:19.829: INFO: Pod pod-secrets-f2822530-b8a7-4b99-a60e-a2e7e8cfc7a5 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:08:19.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2298" for this suite. 09/20/23 13:08:19.838
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:08:19.865
Sep 20 13:08:19.865: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename hostport 09/20/23 13:08:19.866
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:08:20.001
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:08:20.004
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 09/20/23 13:08:20.052
Sep 20 13:08:20.096: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-4100" to be "running and ready"
Sep 20 13:08:20.101: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.617669ms
Sep 20 13:08:20.101: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:08:22.107: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010952513s
Sep 20 13:08:22.107: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:08:24.445: INFO: Pod "pod1": Phase="Running", Reason="", readiness=false. Elapsed: 4.348541065s
Sep 20 13:08:24.445: INFO: The phase of Pod pod1 is Running (Ready = false)
Sep 20 13:08:26.140: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 6.044467523s
Sep 20 13:08:26.141: INFO: The phase of Pod pod1 is Running (Ready = true)
Sep 20 13:08:26.141: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.10.173 on the node which pod1 resides and expect scheduled 09/20/23 13:08:26.141
Sep 20 13:08:26.149: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-4100" to be "running and ready"
Sep 20 13:08:26.154: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.869729ms
Sep 20 13:08:26.154: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:08:28.160: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011137171s
Sep 20 13:08:28.160: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:08:30.162: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.013038863s
Sep 20 13:08:30.162: INFO: The phase of Pod pod2 is Running (Ready = true)
Sep 20 13:08:30.162: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.10.173 but use UDP protocol on the node which pod2 resides 09/20/23 13:08:30.162
Sep 20 13:08:30.422: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-4100" to be "running and ready"
Sep 20 13:08:30.428: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.62085ms
Sep 20 13:08:30.428: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:08:32.434: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011353238s
Sep 20 13:08:32.434: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:08:34.437: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.014328041s
Sep 20 13:08:34.437: INFO: The phase of Pod pod3 is Running (Ready = true)
Sep 20 13:08:34.437: INFO: Pod "pod3" satisfied condition "running and ready"
Sep 20 13:08:34.450: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-4100" to be "running and ready"
Sep 20 13:08:34.456: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 5.932667ms
Sep 20 13:08:34.456: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:08:36.572: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.1216394s
Sep 20 13:08:36.572: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Sep 20 13:08:36.572: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 09/20/23 13:08:36.589
Sep 20 13:08:36.589: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.10.173 http://127.0.0.1:54323/hostname] Namespace:hostport-4100 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 20 13:08:36.589: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
Sep 20 13:08:36.590: INFO: ExecWithOptions: Clientset creation
Sep 20 13:08:36.590: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/hostport-4100/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.10.173+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.10.173, port: 54323 09/20/23 13:08:37.277
Sep 20 13:08:37.277: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.10.173:54323/hostname] Namespace:hostport-4100 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 20 13:08:37.277: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
Sep 20 13:08:37.278: INFO: ExecWithOptions: Clientset creation
Sep 20 13:08:37.278: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/hostport-4100/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.10.173%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.10.173, port: 54323 UDP 09/20/23 13:08:37.384
Sep 20 13:08:37.384: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.10.173 54323] Namespace:hostport-4100 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 20 13:08:37.384: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
Sep 20 13:08:37.384: INFO: ExecWithOptions: Clientset creation
Sep 20 13:08:37.384: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/hostport-4100/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.10.173+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Sep 20 13:08:42.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-4100" for this suite. 09/20/23 13:08:42.826
------------------------------
â€¢ [SLOW TEST] [23.268 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:08:19.865
    Sep 20 13:08:19.865: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename hostport 09/20/23 13:08:19.866
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:08:20.001
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:08:20.004
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 09/20/23 13:08:20.052
    Sep 20 13:08:20.096: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-4100" to be "running and ready"
    Sep 20 13:08:20.101: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.617669ms
    Sep 20 13:08:20.101: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:08:22.107: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010952513s
    Sep 20 13:08:22.107: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:08:24.445: INFO: Pod "pod1": Phase="Running", Reason="", readiness=false. Elapsed: 4.348541065s
    Sep 20 13:08:24.445: INFO: The phase of Pod pod1 is Running (Ready = false)
    Sep 20 13:08:26.140: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 6.044467523s
    Sep 20 13:08:26.141: INFO: The phase of Pod pod1 is Running (Ready = true)
    Sep 20 13:08:26.141: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.10.173 on the node which pod1 resides and expect scheduled 09/20/23 13:08:26.141
    Sep 20 13:08:26.149: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-4100" to be "running and ready"
    Sep 20 13:08:26.154: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.869729ms
    Sep 20 13:08:26.154: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:08:28.160: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011137171s
    Sep 20 13:08:28.160: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:08:30.162: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.013038863s
    Sep 20 13:08:30.162: INFO: The phase of Pod pod2 is Running (Ready = true)
    Sep 20 13:08:30.162: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.10.173 but use UDP protocol on the node which pod2 resides 09/20/23 13:08:30.162
    Sep 20 13:08:30.422: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-4100" to be "running and ready"
    Sep 20 13:08:30.428: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.62085ms
    Sep 20 13:08:30.428: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:08:32.434: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011353238s
    Sep 20 13:08:32.434: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:08:34.437: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.014328041s
    Sep 20 13:08:34.437: INFO: The phase of Pod pod3 is Running (Ready = true)
    Sep 20 13:08:34.437: INFO: Pod "pod3" satisfied condition "running and ready"
    Sep 20 13:08:34.450: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-4100" to be "running and ready"
    Sep 20 13:08:34.456: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 5.932667ms
    Sep 20 13:08:34.456: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:08:36.572: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.1216394s
    Sep 20 13:08:36.572: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Sep 20 13:08:36.572: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 09/20/23 13:08:36.589
    Sep 20 13:08:36.589: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.10.173 http://127.0.0.1:54323/hostname] Namespace:hostport-4100 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep 20 13:08:36.589: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    Sep 20 13:08:36.590: INFO: ExecWithOptions: Clientset creation
    Sep 20 13:08:36.590: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/hostport-4100/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.10.173+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.10.173, port: 54323 09/20/23 13:08:37.277
    Sep 20 13:08:37.277: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.10.173:54323/hostname] Namespace:hostport-4100 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep 20 13:08:37.277: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    Sep 20 13:08:37.278: INFO: ExecWithOptions: Clientset creation
    Sep 20 13:08:37.278: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/hostport-4100/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.10.173%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.10.173, port: 54323 UDP 09/20/23 13:08:37.384
    Sep 20 13:08:37.384: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.10.173 54323] Namespace:hostport-4100 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep 20 13:08:37.384: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    Sep 20 13:08:37.384: INFO: ExecWithOptions: Clientset creation
    Sep 20 13:08:37.384: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/hostport-4100/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.10.173+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:08:42.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-4100" for this suite. 09/20/23 13:08:42.826
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:08:43.134
Sep 20 13:08:43.134: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename services 09/20/23 13:08:43.135
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:08:43.164
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:08:43.166
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-1794 09/20/23 13:08:43.171
STEP: creating service affinity-clusterip in namespace services-1794 09/20/23 13:08:43.171
STEP: creating replication controller affinity-clusterip in namespace services-1794 09/20/23 13:08:43.213
I0920 13:08:43.488842      20 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-1794, replica count: 3
I0920 13:08:46.540269      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0920 13:08:49.541225      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0920 13:08:52.542308      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 20 13:08:52.550: INFO: Creating new exec pod
Sep 20 13:08:52.563: INFO: Waiting up to 5m0s for pod "execpod-affinityczvdx" in namespace "services-1794" to be "running"
Sep 20 13:08:52.567: INFO: Pod "execpod-affinityczvdx": Phase="Pending", Reason="", readiness=false. Elapsed: 3.863869ms
Sep 20 13:08:54.777: INFO: Pod "execpod-affinityczvdx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.214629935s
Sep 20 13:08:56.607: INFO: Pod "execpod-affinityczvdx": Phase="Running", Reason="", readiness=true. Elapsed: 4.044450503s
Sep 20 13:08:56.607: INFO: Pod "execpod-affinityczvdx" satisfied condition "running"
Sep 20 13:08:57.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-1794 exec execpod-affinityczvdx -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Sep 20 13:08:57.831: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Sep 20 13:08:57.831: INFO: stdout: ""
Sep 20 13:08:57.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-1794 exec execpod-affinityczvdx -- /bin/sh -x -c nc -v -z -w 2 10.254.119.172 80'
Sep 20 13:08:58.040: INFO: stderr: "+ nc -v -z -w 2 10.254.119.172 80\nConnection to 10.254.119.172 80 port [tcp/http] succeeded!\n"
Sep 20 13:08:58.040: INFO: stdout: ""
Sep 20 13:08:58.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-1794 exec execpod-affinityczvdx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.119.172:80/ ; done'
Sep 20 13:08:58.351: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.119.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.119.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.119.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.119.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.119.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.119.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.119.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.119.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.119.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.119.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.119.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.119.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.119.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.119.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.119.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.119.172:80/\n"
Sep 20 13:08:58.351: INFO: stdout: "\naffinity-clusterip-wccc9\naffinity-clusterip-wccc9\naffinity-clusterip-wccc9\naffinity-clusterip-wccc9\naffinity-clusterip-wccc9\naffinity-clusterip-wccc9\naffinity-clusterip-wccc9\naffinity-clusterip-wccc9\naffinity-clusterip-wccc9\naffinity-clusterip-wccc9\naffinity-clusterip-wccc9\naffinity-clusterip-wccc9\naffinity-clusterip-wccc9\naffinity-clusterip-wccc9\naffinity-clusterip-wccc9\naffinity-clusterip-wccc9"
Sep 20 13:08:58.351: INFO: Received response from host: affinity-clusterip-wccc9
Sep 20 13:08:58.351: INFO: Received response from host: affinity-clusterip-wccc9
Sep 20 13:08:58.351: INFO: Received response from host: affinity-clusterip-wccc9
Sep 20 13:08:58.351: INFO: Received response from host: affinity-clusterip-wccc9
Sep 20 13:08:58.351: INFO: Received response from host: affinity-clusterip-wccc9
Sep 20 13:08:58.351: INFO: Received response from host: affinity-clusterip-wccc9
Sep 20 13:08:58.351: INFO: Received response from host: affinity-clusterip-wccc9
Sep 20 13:08:58.351: INFO: Received response from host: affinity-clusterip-wccc9
Sep 20 13:08:58.351: INFO: Received response from host: affinity-clusterip-wccc9
Sep 20 13:08:58.351: INFO: Received response from host: affinity-clusterip-wccc9
Sep 20 13:08:58.351: INFO: Received response from host: affinity-clusterip-wccc9
Sep 20 13:08:58.351: INFO: Received response from host: affinity-clusterip-wccc9
Sep 20 13:08:58.351: INFO: Received response from host: affinity-clusterip-wccc9
Sep 20 13:08:58.351: INFO: Received response from host: affinity-clusterip-wccc9
Sep 20 13:08:58.351: INFO: Received response from host: affinity-clusterip-wccc9
Sep 20 13:08:58.351: INFO: Received response from host: affinity-clusterip-wccc9
Sep 20 13:08:58.351: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-1794, will wait for the garbage collector to delete the pods 09/20/23 13:08:58.67
Sep 20 13:08:58.835: INFO: Deleting ReplicationController affinity-clusterip took: 93.782878ms
Sep 20 13:08:59.437: INFO: Terminating ReplicationController affinity-clusterip pods took: 601.909184ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep 20 13:09:03.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1794" for this suite. 09/20/23 13:09:03.416
------------------------------
â€¢ [SLOW TEST] [20.358 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:08:43.134
    Sep 20 13:08:43.134: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename services 09/20/23 13:08:43.135
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:08:43.164
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:08:43.166
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-1794 09/20/23 13:08:43.171
    STEP: creating service affinity-clusterip in namespace services-1794 09/20/23 13:08:43.171
    STEP: creating replication controller affinity-clusterip in namespace services-1794 09/20/23 13:08:43.213
    I0920 13:08:43.488842      20 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-1794, replica count: 3
    I0920 13:08:46.540269      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0920 13:08:49.541225      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0920 13:08:52.542308      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep 20 13:08:52.550: INFO: Creating new exec pod
    Sep 20 13:08:52.563: INFO: Waiting up to 5m0s for pod "execpod-affinityczvdx" in namespace "services-1794" to be "running"
    Sep 20 13:08:52.567: INFO: Pod "execpod-affinityczvdx": Phase="Pending", Reason="", readiness=false. Elapsed: 3.863869ms
    Sep 20 13:08:54.777: INFO: Pod "execpod-affinityczvdx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.214629935s
    Sep 20 13:08:56.607: INFO: Pod "execpod-affinityczvdx": Phase="Running", Reason="", readiness=true. Elapsed: 4.044450503s
    Sep 20 13:08:56.607: INFO: Pod "execpod-affinityczvdx" satisfied condition "running"
    Sep 20 13:08:57.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-1794 exec execpod-affinityczvdx -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Sep 20 13:08:57.831: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Sep 20 13:08:57.831: INFO: stdout: ""
    Sep 20 13:08:57.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-1794 exec execpod-affinityczvdx -- /bin/sh -x -c nc -v -z -w 2 10.254.119.172 80'
    Sep 20 13:08:58.040: INFO: stderr: "+ nc -v -z -w 2 10.254.119.172 80\nConnection to 10.254.119.172 80 port [tcp/http] succeeded!\n"
    Sep 20 13:08:58.040: INFO: stdout: ""
    Sep 20 13:08:58.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-1794 exec execpod-affinityczvdx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.119.172:80/ ; done'
    Sep 20 13:08:58.351: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.119.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.119.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.119.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.119.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.119.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.119.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.119.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.119.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.119.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.119.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.119.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.119.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.119.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.119.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.119.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.119.172:80/\n"
    Sep 20 13:08:58.351: INFO: stdout: "\naffinity-clusterip-wccc9\naffinity-clusterip-wccc9\naffinity-clusterip-wccc9\naffinity-clusterip-wccc9\naffinity-clusterip-wccc9\naffinity-clusterip-wccc9\naffinity-clusterip-wccc9\naffinity-clusterip-wccc9\naffinity-clusterip-wccc9\naffinity-clusterip-wccc9\naffinity-clusterip-wccc9\naffinity-clusterip-wccc9\naffinity-clusterip-wccc9\naffinity-clusterip-wccc9\naffinity-clusterip-wccc9\naffinity-clusterip-wccc9"
    Sep 20 13:08:58.351: INFO: Received response from host: affinity-clusterip-wccc9
    Sep 20 13:08:58.351: INFO: Received response from host: affinity-clusterip-wccc9
    Sep 20 13:08:58.351: INFO: Received response from host: affinity-clusterip-wccc9
    Sep 20 13:08:58.351: INFO: Received response from host: affinity-clusterip-wccc9
    Sep 20 13:08:58.351: INFO: Received response from host: affinity-clusterip-wccc9
    Sep 20 13:08:58.351: INFO: Received response from host: affinity-clusterip-wccc9
    Sep 20 13:08:58.351: INFO: Received response from host: affinity-clusterip-wccc9
    Sep 20 13:08:58.351: INFO: Received response from host: affinity-clusterip-wccc9
    Sep 20 13:08:58.351: INFO: Received response from host: affinity-clusterip-wccc9
    Sep 20 13:08:58.351: INFO: Received response from host: affinity-clusterip-wccc9
    Sep 20 13:08:58.351: INFO: Received response from host: affinity-clusterip-wccc9
    Sep 20 13:08:58.351: INFO: Received response from host: affinity-clusterip-wccc9
    Sep 20 13:08:58.351: INFO: Received response from host: affinity-clusterip-wccc9
    Sep 20 13:08:58.351: INFO: Received response from host: affinity-clusterip-wccc9
    Sep 20 13:08:58.351: INFO: Received response from host: affinity-clusterip-wccc9
    Sep 20 13:08:58.351: INFO: Received response from host: affinity-clusterip-wccc9
    Sep 20 13:08:58.351: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-1794, will wait for the garbage collector to delete the pods 09/20/23 13:08:58.67
    Sep 20 13:08:58.835: INFO: Deleting ReplicationController affinity-clusterip took: 93.782878ms
    Sep 20 13:08:59.437: INFO: Terminating ReplicationController affinity-clusterip pods took: 601.909184ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:09:03.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1794" for this suite. 09/20/23 13:09:03.416
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:09:03.493
Sep 20 13:09:03.493: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename configmap 09/20/23 13:09:03.493
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:09:03.524
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:09:03.53
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-5690/configmap-test-995647d5-7151-491a-a708-6c5b953e7424 09/20/23 13:09:03.542
STEP: Creating a pod to test consume configMaps 09/20/23 13:09:03.553
Sep 20 13:09:03.569: INFO: Waiting up to 5m0s for pod "pod-configmaps-c94b0ca7-69a2-4834-89b4-ee7082da340a" in namespace "configmap-5690" to be "Succeeded or Failed"
Sep 20 13:09:03.575: INFO: Pod "pod-configmaps-c94b0ca7-69a2-4834-89b4-ee7082da340a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013299ms
Sep 20 13:09:05.580: INFO: Pod "pod-configmaps-c94b0ca7-69a2-4834-89b4-ee7082da340a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011189931s
Sep 20 13:09:08.243: INFO: Pod "pod-configmaps-c94b0ca7-69a2-4834-89b4-ee7082da340a": Phase="Running", Reason="", readiness=false. Elapsed: 4.674171096s
Sep 20 13:09:09.581: INFO: Pod "pod-configmaps-c94b0ca7-69a2-4834-89b4-ee7082da340a": Phase="Running", Reason="", readiness=false. Elapsed: 6.011751307s
Sep 20 13:09:11.619: INFO: Pod "pod-configmaps-c94b0ca7-69a2-4834-89b4-ee7082da340a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.050047335s
STEP: Saw pod success 09/20/23 13:09:11.619
Sep 20 13:09:11.619: INFO: Pod "pod-configmaps-c94b0ca7-69a2-4834-89b4-ee7082da340a" satisfied condition "Succeeded or Failed"
Sep 20 13:09:11.626: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-configmaps-c94b0ca7-69a2-4834-89b4-ee7082da340a container env-test: <nil>
STEP: delete the pod 09/20/23 13:09:11.635
Sep 20 13:09:11.823: INFO: Waiting for pod pod-configmaps-c94b0ca7-69a2-4834-89b4-ee7082da340a to disappear
Sep 20 13:09:11.829: INFO: Pod pod-configmaps-c94b0ca7-69a2-4834-89b4-ee7082da340a no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep 20 13:09:11.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5690" for this suite. 09/20/23 13:09:11.835
------------------------------
â€¢ [SLOW TEST] [8.365 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:09:03.493
    Sep 20 13:09:03.493: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename configmap 09/20/23 13:09:03.493
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:09:03.524
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:09:03.53
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-5690/configmap-test-995647d5-7151-491a-a708-6c5b953e7424 09/20/23 13:09:03.542
    STEP: Creating a pod to test consume configMaps 09/20/23 13:09:03.553
    Sep 20 13:09:03.569: INFO: Waiting up to 5m0s for pod "pod-configmaps-c94b0ca7-69a2-4834-89b4-ee7082da340a" in namespace "configmap-5690" to be "Succeeded or Failed"
    Sep 20 13:09:03.575: INFO: Pod "pod-configmaps-c94b0ca7-69a2-4834-89b4-ee7082da340a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013299ms
    Sep 20 13:09:05.580: INFO: Pod "pod-configmaps-c94b0ca7-69a2-4834-89b4-ee7082da340a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011189931s
    Sep 20 13:09:08.243: INFO: Pod "pod-configmaps-c94b0ca7-69a2-4834-89b4-ee7082da340a": Phase="Running", Reason="", readiness=false. Elapsed: 4.674171096s
    Sep 20 13:09:09.581: INFO: Pod "pod-configmaps-c94b0ca7-69a2-4834-89b4-ee7082da340a": Phase="Running", Reason="", readiness=false. Elapsed: 6.011751307s
    Sep 20 13:09:11.619: INFO: Pod "pod-configmaps-c94b0ca7-69a2-4834-89b4-ee7082da340a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.050047335s
    STEP: Saw pod success 09/20/23 13:09:11.619
    Sep 20 13:09:11.619: INFO: Pod "pod-configmaps-c94b0ca7-69a2-4834-89b4-ee7082da340a" satisfied condition "Succeeded or Failed"
    Sep 20 13:09:11.626: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-configmaps-c94b0ca7-69a2-4834-89b4-ee7082da340a container env-test: <nil>
    STEP: delete the pod 09/20/23 13:09:11.635
    Sep 20 13:09:11.823: INFO: Waiting for pod pod-configmaps-c94b0ca7-69a2-4834-89b4-ee7082da340a to disappear
    Sep 20 13:09:11.829: INFO: Pod pod-configmaps-c94b0ca7-69a2-4834-89b4-ee7082da340a no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:09:11.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5690" for this suite. 09/20/23 13:09:11.835
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:09:11.86
Sep 20 13:09:11.860: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename emptydir 09/20/23 13:09:11.86
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:09:11.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:09:11.904
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 09/20/23 13:09:11.909
Sep 20 13:09:11.920: INFO: Waiting up to 5m0s for pod "pod-9111c817-a47d-44fd-b181-5e6aee59d90e" in namespace "emptydir-627" to be "Succeeded or Failed"
Sep 20 13:09:11.924: INFO: Pod "pod-9111c817-a47d-44fd-b181-5e6aee59d90e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.423754ms
Sep 20 13:09:14.841: INFO: Pod "pod-9111c817-a47d-44fd-b181-5e6aee59d90e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.920719107s
Sep 20 13:09:16.369: INFO: Pod "pod-9111c817-a47d-44fd-b181-5e6aee59d90e": Phase="Running", Reason="", readiness=true. Elapsed: 4.448542608s
Sep 20 13:09:17.930: INFO: Pod "pod-9111c817-a47d-44fd-b181-5e6aee59d90e": Phase="Running", Reason="", readiness=false. Elapsed: 6.009844685s
Sep 20 13:09:19.929: INFO: Pod "pod-9111c817-a47d-44fd-b181-5e6aee59d90e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009246737s
STEP: Saw pod success 09/20/23 13:09:19.929
Sep 20 13:09:19.930: INFO: Pod "pod-9111c817-a47d-44fd-b181-5e6aee59d90e" satisfied condition "Succeeded or Failed"
Sep 20 13:09:19.932: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-9111c817-a47d-44fd-b181-5e6aee59d90e container test-container: <nil>
STEP: delete the pod 09/20/23 13:09:19.939
Sep 20 13:09:20.909: INFO: Waiting for pod pod-9111c817-a47d-44fd-b181-5e6aee59d90e to disappear
Sep 20 13:09:20.916: INFO: Pod pod-9111c817-a47d-44fd-b181-5e6aee59d90e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep 20 13:09:20.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-627" for this suite. 09/20/23 13:09:20.922
------------------------------
â€¢ [SLOW TEST] [9.074 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:09:11.86
    Sep 20 13:09:11.860: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename emptydir 09/20/23 13:09:11.86
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:09:11.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:09:11.904
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 09/20/23 13:09:11.909
    Sep 20 13:09:11.920: INFO: Waiting up to 5m0s for pod "pod-9111c817-a47d-44fd-b181-5e6aee59d90e" in namespace "emptydir-627" to be "Succeeded or Failed"
    Sep 20 13:09:11.924: INFO: Pod "pod-9111c817-a47d-44fd-b181-5e6aee59d90e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.423754ms
    Sep 20 13:09:14.841: INFO: Pod "pod-9111c817-a47d-44fd-b181-5e6aee59d90e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.920719107s
    Sep 20 13:09:16.369: INFO: Pod "pod-9111c817-a47d-44fd-b181-5e6aee59d90e": Phase="Running", Reason="", readiness=true. Elapsed: 4.448542608s
    Sep 20 13:09:17.930: INFO: Pod "pod-9111c817-a47d-44fd-b181-5e6aee59d90e": Phase="Running", Reason="", readiness=false. Elapsed: 6.009844685s
    Sep 20 13:09:19.929: INFO: Pod "pod-9111c817-a47d-44fd-b181-5e6aee59d90e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009246737s
    STEP: Saw pod success 09/20/23 13:09:19.929
    Sep 20 13:09:19.930: INFO: Pod "pod-9111c817-a47d-44fd-b181-5e6aee59d90e" satisfied condition "Succeeded or Failed"
    Sep 20 13:09:19.932: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-9111c817-a47d-44fd-b181-5e6aee59d90e container test-container: <nil>
    STEP: delete the pod 09/20/23 13:09:19.939
    Sep 20 13:09:20.909: INFO: Waiting for pod pod-9111c817-a47d-44fd-b181-5e6aee59d90e to disappear
    Sep 20 13:09:20.916: INFO: Pod pod-9111c817-a47d-44fd-b181-5e6aee59d90e no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:09:20.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-627" for this suite. 09/20/23 13:09:20.922
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:09:20.936
Sep 20 13:09:20.936: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename pods 09/20/23 13:09:20.937
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:09:20.985
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:09:20.989
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 09/20/23 13:09:20.996
STEP: submitting the pod to kubernetes 09/20/23 13:09:20.996
Sep 20 13:09:21.063: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-343a1c28-79f0-4720-8788-881cc1e6c4fa" in namespace "pods-8060" to be "running and ready"
Sep 20 13:09:21.070: INFO: Pod "pod-update-activedeadlineseconds-343a1c28-79f0-4720-8788-881cc1e6c4fa": Phase="Pending", Reason="", readiness=false. Elapsed: 7.847805ms
Sep 20 13:09:21.071: INFO: The phase of Pod pod-update-activedeadlineseconds-343a1c28-79f0-4720-8788-881cc1e6c4fa is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:09:23.299: INFO: Pod "pod-update-activedeadlineseconds-343a1c28-79f0-4720-8788-881cc1e6c4fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.236563317s
Sep 20 13:09:23.299: INFO: The phase of Pod pod-update-activedeadlineseconds-343a1c28-79f0-4720-8788-881cc1e6c4fa is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:09:25.078: INFO: Pod "pod-update-activedeadlineseconds-343a1c28-79f0-4720-8788-881cc1e6c4fa": Phase="Running", Reason="", readiness=true. Elapsed: 4.015320912s
Sep 20 13:09:25.078: INFO: The phase of Pod pod-update-activedeadlineseconds-343a1c28-79f0-4720-8788-881cc1e6c4fa is Running (Ready = true)
Sep 20 13:09:25.078: INFO: Pod "pod-update-activedeadlineseconds-343a1c28-79f0-4720-8788-881cc1e6c4fa" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 09/20/23 13:09:25.082
STEP: updating the pod 09/20/23 13:09:25.088
Sep 20 13:09:25.605: INFO: Successfully updated pod "pod-update-activedeadlineseconds-343a1c28-79f0-4720-8788-881cc1e6c4fa"
Sep 20 13:09:25.605: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-343a1c28-79f0-4720-8788-881cc1e6c4fa" in namespace "pods-8060" to be "terminated with reason DeadlineExceeded"
Sep 20 13:09:25.608: INFO: Pod "pod-update-activedeadlineseconds-343a1c28-79f0-4720-8788-881cc1e6c4fa": Phase="Running", Reason="", readiness=true. Elapsed: 3.201922ms
Sep 20 13:09:27.614: INFO: Pod "pod-update-activedeadlineseconds-343a1c28-79f0-4720-8788-881cc1e6c4fa": Phase="Running", Reason="", readiness=false. Elapsed: 2.009334777s
Sep 20 13:09:29.835: INFO: Pod "pod-update-activedeadlineseconds-343a1c28-79f0-4720-8788-881cc1e6c4fa": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.229843116s
Sep 20 13:09:29.835: INFO: Pod "pod-update-activedeadlineseconds-343a1c28-79f0-4720-8788-881cc1e6c4fa" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep 20 13:09:29.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8060" for this suite. 09/20/23 13:09:29.845
------------------------------
â€¢ [SLOW TEST] [8.917 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:09:20.936
    Sep 20 13:09:20.936: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename pods 09/20/23 13:09:20.937
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:09:20.985
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:09:20.989
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 09/20/23 13:09:20.996
    STEP: submitting the pod to kubernetes 09/20/23 13:09:20.996
    Sep 20 13:09:21.063: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-343a1c28-79f0-4720-8788-881cc1e6c4fa" in namespace "pods-8060" to be "running and ready"
    Sep 20 13:09:21.070: INFO: Pod "pod-update-activedeadlineseconds-343a1c28-79f0-4720-8788-881cc1e6c4fa": Phase="Pending", Reason="", readiness=false. Elapsed: 7.847805ms
    Sep 20 13:09:21.071: INFO: The phase of Pod pod-update-activedeadlineseconds-343a1c28-79f0-4720-8788-881cc1e6c4fa is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:09:23.299: INFO: Pod "pod-update-activedeadlineseconds-343a1c28-79f0-4720-8788-881cc1e6c4fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.236563317s
    Sep 20 13:09:23.299: INFO: The phase of Pod pod-update-activedeadlineseconds-343a1c28-79f0-4720-8788-881cc1e6c4fa is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:09:25.078: INFO: Pod "pod-update-activedeadlineseconds-343a1c28-79f0-4720-8788-881cc1e6c4fa": Phase="Running", Reason="", readiness=true. Elapsed: 4.015320912s
    Sep 20 13:09:25.078: INFO: The phase of Pod pod-update-activedeadlineseconds-343a1c28-79f0-4720-8788-881cc1e6c4fa is Running (Ready = true)
    Sep 20 13:09:25.078: INFO: Pod "pod-update-activedeadlineseconds-343a1c28-79f0-4720-8788-881cc1e6c4fa" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 09/20/23 13:09:25.082
    STEP: updating the pod 09/20/23 13:09:25.088
    Sep 20 13:09:25.605: INFO: Successfully updated pod "pod-update-activedeadlineseconds-343a1c28-79f0-4720-8788-881cc1e6c4fa"
    Sep 20 13:09:25.605: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-343a1c28-79f0-4720-8788-881cc1e6c4fa" in namespace "pods-8060" to be "terminated with reason DeadlineExceeded"
    Sep 20 13:09:25.608: INFO: Pod "pod-update-activedeadlineseconds-343a1c28-79f0-4720-8788-881cc1e6c4fa": Phase="Running", Reason="", readiness=true. Elapsed: 3.201922ms
    Sep 20 13:09:27.614: INFO: Pod "pod-update-activedeadlineseconds-343a1c28-79f0-4720-8788-881cc1e6c4fa": Phase="Running", Reason="", readiness=false. Elapsed: 2.009334777s
    Sep 20 13:09:29.835: INFO: Pod "pod-update-activedeadlineseconds-343a1c28-79f0-4720-8788-881cc1e6c4fa": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.229843116s
    Sep 20 13:09:29.835: INFO: Pod "pod-update-activedeadlineseconds-343a1c28-79f0-4720-8788-881cc1e6c4fa" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:09:29.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8060" for this suite. 09/20/23 13:09:29.845
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:09:29.853
Sep 20 13:09:29.853: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename runtimeclass 09/20/23 13:09:29.854
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:09:30.223
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:09:30.231
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 09/20/23 13:09:30.239
STEP: getting /apis/node.k8s.io 09/20/23 13:09:30.248
STEP: getting /apis/node.k8s.io/v1 09/20/23 13:09:30.251
STEP: creating 09/20/23 13:09:30.253
STEP: watching 09/20/23 13:09:30.516
Sep 20 13:09:30.517: INFO: starting watch
STEP: getting 09/20/23 13:09:31.225
STEP: listing 09/20/23 13:09:31.444
STEP: patching 09/20/23 13:09:31.463
STEP: updating 09/20/23 13:09:31.531
Sep 20 13:09:31.628: INFO: waiting for watch events with expected annotations
STEP: deleting 09/20/23 13:09:31.628
STEP: deleting a collection 09/20/23 13:09:31.644
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Sep 20 13:09:31.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-7109" for this suite. 09/20/23 13:09:31.664
------------------------------
â€¢ [1.820 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:09:29.853
    Sep 20 13:09:29.853: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename runtimeclass 09/20/23 13:09:29.854
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:09:30.223
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:09:30.231
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 09/20/23 13:09:30.239
    STEP: getting /apis/node.k8s.io 09/20/23 13:09:30.248
    STEP: getting /apis/node.k8s.io/v1 09/20/23 13:09:30.251
    STEP: creating 09/20/23 13:09:30.253
    STEP: watching 09/20/23 13:09:30.516
    Sep 20 13:09:30.517: INFO: starting watch
    STEP: getting 09/20/23 13:09:31.225
    STEP: listing 09/20/23 13:09:31.444
    STEP: patching 09/20/23 13:09:31.463
    STEP: updating 09/20/23 13:09:31.531
    Sep 20 13:09:31.628: INFO: waiting for watch events with expected annotations
    STEP: deleting 09/20/23 13:09:31.628
    STEP: deleting a collection 09/20/23 13:09:31.644
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:09:31.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-7109" for this suite. 09/20/23 13:09:31.664
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:09:31.676
Sep 20 13:09:31.676: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename configmap 09/20/23 13:09:31.677
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:09:31.695
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:09:31.698
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-5171/configmap-test-bcdbe6ed-d00b-4db9-a587-724ba67461e3 09/20/23 13:09:31.703
STEP: Creating a pod to test consume configMaps 09/20/23 13:09:31.709
Sep 20 13:09:31.720: INFO: Waiting up to 5m0s for pod "pod-configmaps-ce0054ca-a7f3-4326-9440-270587e79278" in namespace "configmap-5171" to be "Succeeded or Failed"
Sep 20 13:09:31.732: INFO: Pod "pod-configmaps-ce0054ca-a7f3-4326-9440-270587e79278": Phase="Pending", Reason="", readiness=false. Elapsed: 12.596211ms
Sep 20 13:09:33.751: INFO: Pod "pod-configmaps-ce0054ca-a7f3-4326-9440-270587e79278": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031128526s
Sep 20 13:09:35.740: INFO: Pod "pod-configmaps-ce0054ca-a7f3-4326-9440-270587e79278": Phase="Running", Reason="", readiness=true. Elapsed: 4.02012305s
Sep 20 13:09:37.738: INFO: Pod "pod-configmaps-ce0054ca-a7f3-4326-9440-270587e79278": Phase="Running", Reason="", readiness=false. Elapsed: 6.018421803s
Sep 20 13:09:39.739: INFO: Pod "pod-configmaps-ce0054ca-a7f3-4326-9440-270587e79278": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.01909454s
STEP: Saw pod success 09/20/23 13:09:39.739
Sep 20 13:09:39.739: INFO: Pod "pod-configmaps-ce0054ca-a7f3-4326-9440-270587e79278" satisfied condition "Succeeded or Failed"
Sep 20 13:09:39.742: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-configmaps-ce0054ca-a7f3-4326-9440-270587e79278 container env-test: <nil>
STEP: delete the pod 09/20/23 13:09:39.754
Sep 20 13:09:40.101: INFO: Waiting for pod pod-configmaps-ce0054ca-a7f3-4326-9440-270587e79278 to disappear
Sep 20 13:09:40.109: INFO: Pod pod-configmaps-ce0054ca-a7f3-4326-9440-270587e79278 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep 20 13:09:40.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5171" for this suite. 09/20/23 13:09:40.117
------------------------------
â€¢ [SLOW TEST] [8.452 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:09:31.676
    Sep 20 13:09:31.676: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename configmap 09/20/23 13:09:31.677
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:09:31.695
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:09:31.698
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-5171/configmap-test-bcdbe6ed-d00b-4db9-a587-724ba67461e3 09/20/23 13:09:31.703
    STEP: Creating a pod to test consume configMaps 09/20/23 13:09:31.709
    Sep 20 13:09:31.720: INFO: Waiting up to 5m0s for pod "pod-configmaps-ce0054ca-a7f3-4326-9440-270587e79278" in namespace "configmap-5171" to be "Succeeded or Failed"
    Sep 20 13:09:31.732: INFO: Pod "pod-configmaps-ce0054ca-a7f3-4326-9440-270587e79278": Phase="Pending", Reason="", readiness=false. Elapsed: 12.596211ms
    Sep 20 13:09:33.751: INFO: Pod "pod-configmaps-ce0054ca-a7f3-4326-9440-270587e79278": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031128526s
    Sep 20 13:09:35.740: INFO: Pod "pod-configmaps-ce0054ca-a7f3-4326-9440-270587e79278": Phase="Running", Reason="", readiness=true. Elapsed: 4.02012305s
    Sep 20 13:09:37.738: INFO: Pod "pod-configmaps-ce0054ca-a7f3-4326-9440-270587e79278": Phase="Running", Reason="", readiness=false. Elapsed: 6.018421803s
    Sep 20 13:09:39.739: INFO: Pod "pod-configmaps-ce0054ca-a7f3-4326-9440-270587e79278": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.01909454s
    STEP: Saw pod success 09/20/23 13:09:39.739
    Sep 20 13:09:39.739: INFO: Pod "pod-configmaps-ce0054ca-a7f3-4326-9440-270587e79278" satisfied condition "Succeeded or Failed"
    Sep 20 13:09:39.742: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-configmaps-ce0054ca-a7f3-4326-9440-270587e79278 container env-test: <nil>
    STEP: delete the pod 09/20/23 13:09:39.754
    Sep 20 13:09:40.101: INFO: Waiting for pod pod-configmaps-ce0054ca-a7f3-4326-9440-270587e79278 to disappear
    Sep 20 13:09:40.109: INFO: Pod pod-configmaps-ce0054ca-a7f3-4326-9440-270587e79278 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:09:40.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5171" for this suite. 09/20/23 13:09:40.117
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:09:40.131
Sep 20 13:09:40.131: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename crd-publish-openapi 09/20/23 13:09:40.132
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:09:40.54
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:09:40.545
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Sep 20 13:09:40.551: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/20/23 13:09:42.8
Sep 20 13:09:42.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-1592 --namespace=crd-publish-openapi-1592 create -f -'
Sep 20 13:09:43.449: INFO: stderr: ""
Sep 20 13:09:43.449: INFO: stdout: "e2e-test-crd-publish-openapi-1085-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep 20 13:09:43.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-1592 --namespace=crd-publish-openapi-1592 delete e2e-test-crd-publish-openapi-1085-crds test-cr'
Sep 20 13:09:43.538: INFO: stderr: ""
Sep 20 13:09:43.538: INFO: stdout: "e2e-test-crd-publish-openapi-1085-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Sep 20 13:09:43.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-1592 --namespace=crd-publish-openapi-1592 apply -f -'
Sep 20 13:09:44.176: INFO: stderr: ""
Sep 20 13:09:44.176: INFO: stdout: "e2e-test-crd-publish-openapi-1085-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep 20 13:09:44.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-1592 --namespace=crd-publish-openapi-1592 delete e2e-test-crd-publish-openapi-1085-crds test-cr'
Sep 20 13:09:44.260: INFO: stderr: ""
Sep 20 13:09:44.260: INFO: stdout: "e2e-test-crd-publish-openapi-1085-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 09/20/23 13:09:44.26
Sep 20 13:09:44.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-1592 explain e2e-test-crd-publish-openapi-1085-crds'
Sep 20 13:09:44.777: INFO: stderr: ""
Sep 20 13:09:44.777: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1085-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep 20 13:09:47.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1592" for this suite. 09/20/23 13:09:47.982
------------------------------
â€¢ [SLOW TEST] [7.858 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:09:40.131
    Sep 20 13:09:40.131: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename crd-publish-openapi 09/20/23 13:09:40.132
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:09:40.54
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:09:40.545
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Sep 20 13:09:40.551: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/20/23 13:09:42.8
    Sep 20 13:09:42.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-1592 --namespace=crd-publish-openapi-1592 create -f -'
    Sep 20 13:09:43.449: INFO: stderr: ""
    Sep 20 13:09:43.449: INFO: stdout: "e2e-test-crd-publish-openapi-1085-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Sep 20 13:09:43.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-1592 --namespace=crd-publish-openapi-1592 delete e2e-test-crd-publish-openapi-1085-crds test-cr'
    Sep 20 13:09:43.538: INFO: stderr: ""
    Sep 20 13:09:43.538: INFO: stdout: "e2e-test-crd-publish-openapi-1085-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Sep 20 13:09:43.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-1592 --namespace=crd-publish-openapi-1592 apply -f -'
    Sep 20 13:09:44.176: INFO: stderr: ""
    Sep 20 13:09:44.176: INFO: stdout: "e2e-test-crd-publish-openapi-1085-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Sep 20 13:09:44.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-1592 --namespace=crd-publish-openapi-1592 delete e2e-test-crd-publish-openapi-1085-crds test-cr'
    Sep 20 13:09:44.260: INFO: stderr: ""
    Sep 20 13:09:44.260: INFO: stdout: "e2e-test-crd-publish-openapi-1085-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 09/20/23 13:09:44.26
    Sep 20 13:09:44.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-1592 explain e2e-test-crd-publish-openapi-1085-crds'
    Sep 20 13:09:44.777: INFO: stderr: ""
    Sep 20 13:09:44.777: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1085-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:09:47.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1592" for this suite. 09/20/23 13:09:47.982
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:09:47.994
Sep 20 13:09:47.994: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename emptydir 09/20/23 13:09:47.995
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:09:48.308
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:09:48.313
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 09/20/23 13:09:48.318
Sep 20 13:09:48.329: INFO: Waiting up to 5m0s for pod "pod-ea2ba011-3a7c-4af3-bf90-670b204be4b4" in namespace "emptydir-244" to be "Succeeded or Failed"
Sep 20 13:09:48.353: INFO: Pod "pod-ea2ba011-3a7c-4af3-bf90-670b204be4b4": Phase="Pending", Reason="", readiness=false. Elapsed: 24.764456ms
Sep 20 13:09:50.674: INFO: Pod "pod-ea2ba011-3a7c-4af3-bf90-670b204be4b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.344905707s
Sep 20 13:09:52.359: INFO: Pod "pod-ea2ba011-3a7c-4af3-bf90-670b204be4b4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030355808s
Sep 20 13:09:54.550: INFO: Pod "pod-ea2ba011-3a7c-4af3-bf90-670b204be4b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.221373477s
STEP: Saw pod success 09/20/23 13:09:54.55
Sep 20 13:09:54.550: INFO: Pod "pod-ea2ba011-3a7c-4af3-bf90-670b204be4b4" satisfied condition "Succeeded or Failed"
Sep 20 13:09:54.558: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-ea2ba011-3a7c-4af3-bf90-670b204be4b4 container test-container: <nil>
STEP: delete the pod 09/20/23 13:09:54.62
Sep 20 13:09:55.193: INFO: Waiting for pod pod-ea2ba011-3a7c-4af3-bf90-670b204be4b4 to disappear
Sep 20 13:09:55.200: INFO: Pod pod-ea2ba011-3a7c-4af3-bf90-670b204be4b4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep 20 13:09:55.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-244" for this suite. 09/20/23 13:09:55.335
------------------------------
â€¢ [SLOW TEST] [7.361 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:09:47.994
    Sep 20 13:09:47.994: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename emptydir 09/20/23 13:09:47.995
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:09:48.308
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:09:48.313
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 09/20/23 13:09:48.318
    Sep 20 13:09:48.329: INFO: Waiting up to 5m0s for pod "pod-ea2ba011-3a7c-4af3-bf90-670b204be4b4" in namespace "emptydir-244" to be "Succeeded or Failed"
    Sep 20 13:09:48.353: INFO: Pod "pod-ea2ba011-3a7c-4af3-bf90-670b204be4b4": Phase="Pending", Reason="", readiness=false. Elapsed: 24.764456ms
    Sep 20 13:09:50.674: INFO: Pod "pod-ea2ba011-3a7c-4af3-bf90-670b204be4b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.344905707s
    Sep 20 13:09:52.359: INFO: Pod "pod-ea2ba011-3a7c-4af3-bf90-670b204be4b4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030355808s
    Sep 20 13:09:54.550: INFO: Pod "pod-ea2ba011-3a7c-4af3-bf90-670b204be4b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.221373477s
    STEP: Saw pod success 09/20/23 13:09:54.55
    Sep 20 13:09:54.550: INFO: Pod "pod-ea2ba011-3a7c-4af3-bf90-670b204be4b4" satisfied condition "Succeeded or Failed"
    Sep 20 13:09:54.558: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-ea2ba011-3a7c-4af3-bf90-670b204be4b4 container test-container: <nil>
    STEP: delete the pod 09/20/23 13:09:54.62
    Sep 20 13:09:55.193: INFO: Waiting for pod pod-ea2ba011-3a7c-4af3-bf90-670b204be4b4 to disappear
    Sep 20 13:09:55.200: INFO: Pod pod-ea2ba011-3a7c-4af3-bf90-670b204be4b4 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:09:55.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-244" for this suite. 09/20/23 13:09:55.335
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:09:55.355
Sep 20 13:09:55.355: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename deployment 09/20/23 13:09:55.356
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:09:55.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:09:55.39
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 09/20/23 13:09:55.401
Sep 20 13:09:55.401: INFO: Creating simple deployment test-deployment-fts6j
Sep 20 13:09:55.804: INFO: deployment "test-deployment-fts6j" doesn't have the required revision set
Sep 20 13:09:57.823: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 9, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 9, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 9, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 9, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-fts6j-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status 09/20/23 13:09:59.831
Sep 20 13:09:59.836: INFO: Deployment test-deployment-fts6j has Conditions: [{Available True 2023-09-20 13:09:58 +0000 UTC 2023-09-20 13:09:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-09-20 13:09:58 +0000 UTC 2023-09-20 13:09:55 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-fts6j-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 09/20/23 13:09:59.836
Sep 20 13:09:59.956: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 9, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 9, 58, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 9, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 9, 55, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-fts6j-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 09/20/23 13:09:59.956
Sep 20 13:09:59.958: INFO: Observed &Deployment event: ADDED
Sep 20 13:09:59.958: INFO: Observed Deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-20 13:09:55 +0000 UTC 2023-09-20 13:09:55 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-fts6j-54bc444df"}
Sep 20 13:09:59.959: INFO: Observed &Deployment event: MODIFIED
Sep 20 13:09:59.959: INFO: Observed Deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-20 13:09:55 +0000 UTC 2023-09-20 13:09:55 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-fts6j-54bc444df"}
Sep 20 13:09:59.959: INFO: Observed Deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-20 13:09:55 +0000 UTC 2023-09-20 13:09:55 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep 20 13:09:59.959: INFO: Observed &Deployment event: MODIFIED
Sep 20 13:09:59.959: INFO: Observed Deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-20 13:09:55 +0000 UTC 2023-09-20 13:09:55 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep 20 13:09:59.959: INFO: Observed Deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-20 13:09:55 +0000 UTC 2023-09-20 13:09:55 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-fts6j-54bc444df" is progressing.}
Sep 20 13:09:59.959: INFO: Observed &Deployment event: MODIFIED
Sep 20 13:09:59.959: INFO: Observed Deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-20 13:09:58 +0000 UTC 2023-09-20 13:09:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep 20 13:09:59.959: INFO: Observed Deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-20 13:09:58 +0000 UTC 2023-09-20 13:09:55 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-fts6j-54bc444df" has successfully progressed.}
Sep 20 13:09:59.959: INFO: Observed &Deployment event: MODIFIED
Sep 20 13:09:59.959: INFO: Observed Deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-20 13:09:58 +0000 UTC 2023-09-20 13:09:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep 20 13:09:59.959: INFO: Observed Deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-20 13:09:58 +0000 UTC 2023-09-20 13:09:55 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-fts6j-54bc444df" has successfully progressed.}
Sep 20 13:09:59.959: INFO: Found Deployment test-deployment-fts6j in namespace deployment-1462 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep 20 13:09:59.959: INFO: Deployment test-deployment-fts6j has an updated status
STEP: patching the Statefulset Status 09/20/23 13:09:59.959
Sep 20 13:09:59.959: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Sep 20 13:10:00.194: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 09/20/23 13:10:00.194
Sep 20 13:10:00.196: INFO: Observed &Deployment event: ADDED
Sep 20 13:10:00.196: INFO: Observed deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-20 13:09:55 +0000 UTC 2023-09-20 13:09:55 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-fts6j-54bc444df"}
Sep 20 13:10:00.196: INFO: Observed &Deployment event: MODIFIED
Sep 20 13:10:00.196: INFO: Observed deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-20 13:09:55 +0000 UTC 2023-09-20 13:09:55 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-fts6j-54bc444df"}
Sep 20 13:10:00.196: INFO: Observed deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-20 13:09:55 +0000 UTC 2023-09-20 13:09:55 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep 20 13:10:00.196: INFO: Observed &Deployment event: MODIFIED
Sep 20 13:10:00.197: INFO: Observed deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-20 13:09:55 +0000 UTC 2023-09-20 13:09:55 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep 20 13:10:00.197: INFO: Observed deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-20 13:09:55 +0000 UTC 2023-09-20 13:09:55 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-fts6j-54bc444df" is progressing.}
Sep 20 13:10:00.197: INFO: Observed &Deployment event: MODIFIED
Sep 20 13:10:00.197: INFO: Observed deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-20 13:09:58 +0000 UTC 2023-09-20 13:09:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep 20 13:10:00.197: INFO: Observed deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-20 13:09:58 +0000 UTC 2023-09-20 13:09:55 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-fts6j-54bc444df" has successfully progressed.}
Sep 20 13:10:00.197: INFO: Observed &Deployment event: MODIFIED
Sep 20 13:10:00.197: INFO: Observed deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-20 13:09:58 +0000 UTC 2023-09-20 13:09:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep 20 13:10:00.197: INFO: Observed deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-20 13:09:58 +0000 UTC 2023-09-20 13:09:55 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-fts6j-54bc444df" has successfully progressed.}
Sep 20 13:10:00.197: INFO: Observed deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep 20 13:10:00.197: INFO: Observed &Deployment event: MODIFIED
Sep 20 13:10:00.198: INFO: Found deployment test-deployment-fts6j in namespace deployment-1462 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Sep 20 13:10:00.198: INFO: Deployment test-deployment-fts6j has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep 20 13:10:01.397: INFO: Deployment "test-deployment-fts6j":
&Deployment{ObjectMeta:{test-deployment-fts6j  deployment-1462  66d9c33c-1f5d-49d5-9bcd-a6cd1e3a9c40 30169 1 2023-09-20 13:09:55 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-09-20 13:09:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-09-20 13:09:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-09-20 13:10:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003894348 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-fts6j-54bc444df",LastUpdateTime:2023-09-20 13:10:00 +0000 UTC,LastTransitionTime:2023-09-20 13:10:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep 20 13:10:01.801: INFO: New ReplicaSet "test-deployment-fts6j-54bc444df" of Deployment "test-deployment-fts6j":
&ReplicaSet{ObjectMeta:{test-deployment-fts6j-54bc444df  deployment-1462  5886c5d2-7658-41c6-b42d-2fd917fe8247 30163 1 2023-09-20 13:09:55 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-fts6j 66d9c33c-1f5d-49d5-9bcd-a6cd1e3a9c40 0xc003894720 0xc003894721}] [] [{kube-controller-manager Update apps/v1 2023-09-20 13:09:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66d9c33c-1f5d-49d5-9bcd-a6cd1e3a9c40\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 13:09:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038947d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 20 13:10:01.823: INFO: Pod "test-deployment-fts6j-54bc444df-vcpj5" is available:
&Pod{ObjectMeta:{test-deployment-fts6j-54bc444df-vcpj5 test-deployment-fts6j-54bc444df- deployment-1462  6e98b3b0-dfda-44cd-9f17-fee0c1e75195 30162 0 2023-09-20 13:09:55 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [{apps/v1 ReplicaSet test-deployment-fts6j-54bc444df 5886c5d2-7658-41c6-b42d-2fd917fe8247 0xc003894b70 0xc003894b71}] [] [{kube-controller-manager Update v1 2023-09-20 13:09:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5886c5d2-7658-41c6-b42d-2fd917fe8247\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:09:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.4.243\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lm62v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lm62v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:09:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:09:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:09:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:09:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.64,PodIP:10.100.4.243,StartTime:2023-09-20 13:09:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-20 13:09:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6d3cf90fe851f40bebcbb689e11765f425477b9a10ffe3faacc9e3005287a259,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.4.243,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep 20 13:10:01.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1462" for this suite. 09/20/23 13:10:02.042
------------------------------
â€¢ [SLOW TEST] [6.959 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:09:55.355
    Sep 20 13:09:55.355: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename deployment 09/20/23 13:09:55.356
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:09:55.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:09:55.39
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 09/20/23 13:09:55.401
    Sep 20 13:09:55.401: INFO: Creating simple deployment test-deployment-fts6j
    Sep 20 13:09:55.804: INFO: deployment "test-deployment-fts6j" doesn't have the required revision set
    Sep 20 13:09:57.823: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 9, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 9, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 9, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 9, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-fts6j-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Getting /status 09/20/23 13:09:59.831
    Sep 20 13:09:59.836: INFO: Deployment test-deployment-fts6j has Conditions: [{Available True 2023-09-20 13:09:58 +0000 UTC 2023-09-20 13:09:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-09-20 13:09:58 +0000 UTC 2023-09-20 13:09:55 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-fts6j-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 09/20/23 13:09:59.836
    Sep 20 13:09:59.956: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 9, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 9, 58, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 9, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 9, 55, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-fts6j-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 09/20/23 13:09:59.956
    Sep 20 13:09:59.958: INFO: Observed &Deployment event: ADDED
    Sep 20 13:09:59.958: INFO: Observed Deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-20 13:09:55 +0000 UTC 2023-09-20 13:09:55 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-fts6j-54bc444df"}
    Sep 20 13:09:59.959: INFO: Observed &Deployment event: MODIFIED
    Sep 20 13:09:59.959: INFO: Observed Deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-20 13:09:55 +0000 UTC 2023-09-20 13:09:55 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-fts6j-54bc444df"}
    Sep 20 13:09:59.959: INFO: Observed Deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-20 13:09:55 +0000 UTC 2023-09-20 13:09:55 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Sep 20 13:09:59.959: INFO: Observed &Deployment event: MODIFIED
    Sep 20 13:09:59.959: INFO: Observed Deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-20 13:09:55 +0000 UTC 2023-09-20 13:09:55 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Sep 20 13:09:59.959: INFO: Observed Deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-20 13:09:55 +0000 UTC 2023-09-20 13:09:55 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-fts6j-54bc444df" is progressing.}
    Sep 20 13:09:59.959: INFO: Observed &Deployment event: MODIFIED
    Sep 20 13:09:59.959: INFO: Observed Deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-20 13:09:58 +0000 UTC 2023-09-20 13:09:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Sep 20 13:09:59.959: INFO: Observed Deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-20 13:09:58 +0000 UTC 2023-09-20 13:09:55 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-fts6j-54bc444df" has successfully progressed.}
    Sep 20 13:09:59.959: INFO: Observed &Deployment event: MODIFIED
    Sep 20 13:09:59.959: INFO: Observed Deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-20 13:09:58 +0000 UTC 2023-09-20 13:09:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Sep 20 13:09:59.959: INFO: Observed Deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-20 13:09:58 +0000 UTC 2023-09-20 13:09:55 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-fts6j-54bc444df" has successfully progressed.}
    Sep 20 13:09:59.959: INFO: Found Deployment test-deployment-fts6j in namespace deployment-1462 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Sep 20 13:09:59.959: INFO: Deployment test-deployment-fts6j has an updated status
    STEP: patching the Statefulset Status 09/20/23 13:09:59.959
    Sep 20 13:09:59.959: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Sep 20 13:10:00.194: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 09/20/23 13:10:00.194
    Sep 20 13:10:00.196: INFO: Observed &Deployment event: ADDED
    Sep 20 13:10:00.196: INFO: Observed deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-20 13:09:55 +0000 UTC 2023-09-20 13:09:55 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-fts6j-54bc444df"}
    Sep 20 13:10:00.196: INFO: Observed &Deployment event: MODIFIED
    Sep 20 13:10:00.196: INFO: Observed deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-20 13:09:55 +0000 UTC 2023-09-20 13:09:55 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-fts6j-54bc444df"}
    Sep 20 13:10:00.196: INFO: Observed deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-20 13:09:55 +0000 UTC 2023-09-20 13:09:55 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Sep 20 13:10:00.196: INFO: Observed &Deployment event: MODIFIED
    Sep 20 13:10:00.197: INFO: Observed deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-20 13:09:55 +0000 UTC 2023-09-20 13:09:55 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Sep 20 13:10:00.197: INFO: Observed deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-20 13:09:55 +0000 UTC 2023-09-20 13:09:55 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-fts6j-54bc444df" is progressing.}
    Sep 20 13:10:00.197: INFO: Observed &Deployment event: MODIFIED
    Sep 20 13:10:00.197: INFO: Observed deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-20 13:09:58 +0000 UTC 2023-09-20 13:09:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Sep 20 13:10:00.197: INFO: Observed deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-20 13:09:58 +0000 UTC 2023-09-20 13:09:55 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-fts6j-54bc444df" has successfully progressed.}
    Sep 20 13:10:00.197: INFO: Observed &Deployment event: MODIFIED
    Sep 20 13:10:00.197: INFO: Observed deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-20 13:09:58 +0000 UTC 2023-09-20 13:09:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Sep 20 13:10:00.197: INFO: Observed deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-20 13:09:58 +0000 UTC 2023-09-20 13:09:55 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-fts6j-54bc444df" has successfully progressed.}
    Sep 20 13:10:00.197: INFO: Observed deployment test-deployment-fts6j in namespace deployment-1462 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Sep 20 13:10:00.197: INFO: Observed &Deployment event: MODIFIED
    Sep 20 13:10:00.198: INFO: Found deployment test-deployment-fts6j in namespace deployment-1462 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Sep 20 13:10:00.198: INFO: Deployment test-deployment-fts6j has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep 20 13:10:01.397: INFO: Deployment "test-deployment-fts6j":
    &Deployment{ObjectMeta:{test-deployment-fts6j  deployment-1462  66d9c33c-1f5d-49d5-9bcd-a6cd1e3a9c40 30169 1 2023-09-20 13:09:55 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-09-20 13:09:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-09-20 13:09:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-09-20 13:10:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003894348 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-fts6j-54bc444df",LastUpdateTime:2023-09-20 13:10:00 +0000 UTC,LastTransitionTime:2023-09-20 13:10:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Sep 20 13:10:01.801: INFO: New ReplicaSet "test-deployment-fts6j-54bc444df" of Deployment "test-deployment-fts6j":
    &ReplicaSet{ObjectMeta:{test-deployment-fts6j-54bc444df  deployment-1462  5886c5d2-7658-41c6-b42d-2fd917fe8247 30163 1 2023-09-20 13:09:55 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-fts6j 66d9c33c-1f5d-49d5-9bcd-a6cd1e3a9c40 0xc003894720 0xc003894721}] [] [{kube-controller-manager Update apps/v1 2023-09-20 13:09:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66d9c33c-1f5d-49d5-9bcd-a6cd1e3a9c40\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 13:09:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038947d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep 20 13:10:01.823: INFO: Pod "test-deployment-fts6j-54bc444df-vcpj5" is available:
    &Pod{ObjectMeta:{test-deployment-fts6j-54bc444df-vcpj5 test-deployment-fts6j-54bc444df- deployment-1462  6e98b3b0-dfda-44cd-9f17-fee0c1e75195 30162 0 2023-09-20 13:09:55 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [{apps/v1 ReplicaSet test-deployment-fts6j-54bc444df 5886c5d2-7658-41c6-b42d-2fd917fe8247 0xc003894b70 0xc003894b71}] [] [{kube-controller-manager Update v1 2023-09-20 13:09:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5886c5d2-7658-41c6-b42d-2fd917fe8247\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:09:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.4.243\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lm62v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lm62v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:09:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:09:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:09:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:09:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.64,PodIP:10.100.4.243,StartTime:2023-09-20 13:09:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-20 13:09:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6d3cf90fe851f40bebcbb689e11765f425477b9a10ffe3faacc9e3005287a259,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.4.243,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:10:01.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1462" for this suite. 09/20/23 13:10:02.042
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:10:02.317
Sep 20 13:10:02.317: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename downward-api 09/20/23 13:10:02.317
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:10:02.833
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:10:02.838
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 09/20/23 13:10:02.843
Sep 20 13:10:03.287: INFO: Waiting up to 5m0s for pod "downward-api-ece795ea-2d65-42ed-9829-bbb9f28fcfec" in namespace "downward-api-2153" to be "Succeeded or Failed"
Sep 20 13:10:03.347: INFO: Pod "downward-api-ece795ea-2d65-42ed-9829-bbb9f28fcfec": Phase="Pending", Reason="", readiness=false. Elapsed: 59.495791ms
Sep 20 13:10:05.352: INFO: Pod "downward-api-ece795ea-2d65-42ed-9829-bbb9f28fcfec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.064619053s
Sep 20 13:10:07.361: INFO: Pod "downward-api-ece795ea-2d65-42ed-9829-bbb9f28fcfec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.073354869s
Sep 20 13:10:10.046: INFO: Pod "downward-api-ece795ea-2d65-42ed-9829-bbb9f28fcfec": Phase="Pending", Reason="", readiness=false. Elapsed: 6.758851104s
Sep 20 13:10:11.351: INFO: Pod "downward-api-ece795ea-2d65-42ed-9829-bbb9f28fcfec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.064173631s
STEP: Saw pod success 09/20/23 13:10:11.352
Sep 20 13:10:11.352: INFO: Pod "downward-api-ece795ea-2d65-42ed-9829-bbb9f28fcfec" satisfied condition "Succeeded or Failed"
Sep 20 13:10:11.355: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downward-api-ece795ea-2d65-42ed-9829-bbb9f28fcfec container dapi-container: <nil>
STEP: delete the pod 09/20/23 13:10:11.363
Sep 20 13:10:11.427: INFO: Waiting for pod downward-api-ece795ea-2d65-42ed-9829-bbb9f28fcfec to disappear
Sep 20 13:10:11.432: INFO: Pod downward-api-ece795ea-2d65-42ed-9829-bbb9f28fcfec no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Sep 20 13:10:11.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2153" for this suite. 09/20/23 13:10:11.437
------------------------------
â€¢ [SLOW TEST] [9.288 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:10:02.317
    Sep 20 13:10:02.317: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename downward-api 09/20/23 13:10:02.317
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:10:02.833
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:10:02.838
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 09/20/23 13:10:02.843
    Sep 20 13:10:03.287: INFO: Waiting up to 5m0s for pod "downward-api-ece795ea-2d65-42ed-9829-bbb9f28fcfec" in namespace "downward-api-2153" to be "Succeeded or Failed"
    Sep 20 13:10:03.347: INFO: Pod "downward-api-ece795ea-2d65-42ed-9829-bbb9f28fcfec": Phase="Pending", Reason="", readiness=false. Elapsed: 59.495791ms
    Sep 20 13:10:05.352: INFO: Pod "downward-api-ece795ea-2d65-42ed-9829-bbb9f28fcfec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.064619053s
    Sep 20 13:10:07.361: INFO: Pod "downward-api-ece795ea-2d65-42ed-9829-bbb9f28fcfec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.073354869s
    Sep 20 13:10:10.046: INFO: Pod "downward-api-ece795ea-2d65-42ed-9829-bbb9f28fcfec": Phase="Pending", Reason="", readiness=false. Elapsed: 6.758851104s
    Sep 20 13:10:11.351: INFO: Pod "downward-api-ece795ea-2d65-42ed-9829-bbb9f28fcfec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.064173631s
    STEP: Saw pod success 09/20/23 13:10:11.352
    Sep 20 13:10:11.352: INFO: Pod "downward-api-ece795ea-2d65-42ed-9829-bbb9f28fcfec" satisfied condition "Succeeded or Failed"
    Sep 20 13:10:11.355: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downward-api-ece795ea-2d65-42ed-9829-bbb9f28fcfec container dapi-container: <nil>
    STEP: delete the pod 09/20/23 13:10:11.363
    Sep 20 13:10:11.427: INFO: Waiting for pod downward-api-ece795ea-2d65-42ed-9829-bbb9f28fcfec to disappear
    Sep 20 13:10:11.432: INFO: Pod downward-api-ece795ea-2d65-42ed-9829-bbb9f28fcfec no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:10:11.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2153" for this suite. 09/20/23 13:10:11.437
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:10:11.613
Sep 20 13:10:11.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename statefulset 09/20/23 13:10:11.614
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:10:13.164
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:10:13.172
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2775 09/20/23 13:10:13.176
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 09/20/23 13:10:13.19
STEP: Creating stateful set ss in namespace statefulset-2775 09/20/23 13:10:13.235
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2775 09/20/23 13:10:13.301
Sep 20 13:10:13.310: INFO: Found 0 stateful pods, waiting for 1
Sep 20 13:10:23.316: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 09/20/23 13:10:23.316
Sep 20 13:10:23.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-2775 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 20 13:10:23.529: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 20 13:10:23.529: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 20 13:10:23.529: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 20 13:10:23.534: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep 20 13:10:33.585: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 20 13:10:33.585: INFO: Waiting for statefulset status.replicas updated to 0
Sep 20 13:10:33.764: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999859s
Sep 20 13:10:35.475: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.976160572s
Sep 20 13:10:36.562: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.265590112s
Sep 20 13:10:37.570: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.178614364s
Sep 20 13:10:38.898: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.171256999s
Sep 20 13:10:39.903: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.843353658s
Sep 20 13:10:41.147: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.838298531s
Sep 20 13:10:42.344: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.592930841s
Sep 20 13:10:43.351: INFO: Verifying statefulset ss doesn't scale past 1 for another 396.508955ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2775 09/20/23 13:10:44.351
Sep 20 13:10:44.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-2775 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 13:10:44.626: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 20 13:10:44.626: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 20 13:10:44.626: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 20 13:10:44.631: INFO: Found 1 stateful pods, waiting for 3
Sep 20 13:10:54.639: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 13:10:54.639: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 20 13:10:54.639: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 09/20/23 13:10:54.639
STEP: Scale down will halt with unhealthy stateful pod 09/20/23 13:10:54.639
Sep 20 13:10:54.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-2775 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 20 13:10:54.836: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 20 13:10:54.836: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 20 13:10:54.836: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 20 13:10:54.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-2775 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 20 13:10:55.020: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 20 13:10:55.020: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 20 13:10:55.020: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 20 13:10:55.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-2775 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 20 13:10:55.489: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 20 13:10:55.489: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 20 13:10:55.489: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 20 13:10:55.489: INFO: Waiting for statefulset status.replicas updated to 0
Sep 20 13:10:55.493: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Sep 20 13:11:05.502: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 20 13:11:05.502: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep 20 13:11:05.502: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep 20 13:11:05.747: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999599s
Sep 20 13:11:06.754: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.904787504s
Sep 20 13:11:07.758: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.898578432s
Sep 20 13:11:08.765: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.893556245s
Sep 20 13:11:09.769: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.887726127s
Sep 20 13:11:10.777: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.882981122s
Sep 20 13:11:11.781: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.875463505s
Sep 20 13:11:12.787: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.870354452s
Sep 20 13:11:13.792: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.865398329s
Sep 20 13:11:14.798: INFO: Verifying statefulset ss doesn't scale past 3 for another 859.042593ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2775 09/20/23 13:11:15.799
Sep 20 13:11:16.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-2775 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 13:11:16.210: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 20 13:11:16.210: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 20 13:11:16.210: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 20 13:11:16.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-2775 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 13:11:16.443: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 20 13:11:16.443: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 20 13:11:16.443: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 20 13:11:16.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-2775 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 20 13:11:16.613: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 20 13:11:16.613: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 20 13:11:16.613: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 20 13:11:16.613: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 09/20/23 13:11:26.841
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep 20 13:11:26.842: INFO: Deleting all statefulset in ns statefulset-2775
Sep 20 13:11:26.845: INFO: Scaling statefulset ss to 0
Sep 20 13:11:26.876: INFO: Waiting for statefulset status.replicas updated to 0
Sep 20 13:11:26.879: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep 20 13:11:27.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2775" for this suite. 09/20/23 13:11:27.08
------------------------------
â€¢ [SLOW TEST] [75.662 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:10:11.613
    Sep 20 13:10:11.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename statefulset 09/20/23 13:10:11.614
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:10:13.164
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:10:13.172
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2775 09/20/23 13:10:13.176
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 09/20/23 13:10:13.19
    STEP: Creating stateful set ss in namespace statefulset-2775 09/20/23 13:10:13.235
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2775 09/20/23 13:10:13.301
    Sep 20 13:10:13.310: INFO: Found 0 stateful pods, waiting for 1
    Sep 20 13:10:23.316: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 09/20/23 13:10:23.316
    Sep 20 13:10:23.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-2775 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep 20 13:10:23.529: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep 20 13:10:23.529: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep 20 13:10:23.529: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep 20 13:10:23.534: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Sep 20 13:10:33.585: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Sep 20 13:10:33.585: INFO: Waiting for statefulset status.replicas updated to 0
    Sep 20 13:10:33.764: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999859s
    Sep 20 13:10:35.475: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.976160572s
    Sep 20 13:10:36.562: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.265590112s
    Sep 20 13:10:37.570: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.178614364s
    Sep 20 13:10:38.898: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.171256999s
    Sep 20 13:10:39.903: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.843353658s
    Sep 20 13:10:41.147: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.838298531s
    Sep 20 13:10:42.344: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.592930841s
    Sep 20 13:10:43.351: INFO: Verifying statefulset ss doesn't scale past 1 for another 396.508955ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2775 09/20/23 13:10:44.351
    Sep 20 13:10:44.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-2775 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep 20 13:10:44.626: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep 20 13:10:44.626: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep 20 13:10:44.626: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep 20 13:10:44.631: INFO: Found 1 stateful pods, waiting for 3
    Sep 20 13:10:54.639: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep 20 13:10:54.639: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep 20 13:10:54.639: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 09/20/23 13:10:54.639
    STEP: Scale down will halt with unhealthy stateful pod 09/20/23 13:10:54.639
    Sep 20 13:10:54.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-2775 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep 20 13:10:54.836: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep 20 13:10:54.836: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep 20 13:10:54.836: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep 20 13:10:54.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-2775 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep 20 13:10:55.020: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep 20 13:10:55.020: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep 20 13:10:55.020: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep 20 13:10:55.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-2775 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep 20 13:10:55.489: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep 20 13:10:55.489: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep 20 13:10:55.489: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep 20 13:10:55.489: INFO: Waiting for statefulset status.replicas updated to 0
    Sep 20 13:10:55.493: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
    Sep 20 13:11:05.502: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Sep 20 13:11:05.502: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Sep 20 13:11:05.502: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Sep 20 13:11:05.747: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999599s
    Sep 20 13:11:06.754: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.904787504s
    Sep 20 13:11:07.758: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.898578432s
    Sep 20 13:11:08.765: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.893556245s
    Sep 20 13:11:09.769: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.887726127s
    Sep 20 13:11:10.777: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.882981122s
    Sep 20 13:11:11.781: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.875463505s
    Sep 20 13:11:12.787: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.870354452s
    Sep 20 13:11:13.792: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.865398329s
    Sep 20 13:11:14.798: INFO: Verifying statefulset ss doesn't scale past 3 for another 859.042593ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2775 09/20/23 13:11:15.799
    Sep 20 13:11:16.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-2775 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep 20 13:11:16.210: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep 20 13:11:16.210: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep 20 13:11:16.210: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep 20 13:11:16.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-2775 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep 20 13:11:16.443: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep 20 13:11:16.443: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep 20 13:11:16.443: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep 20 13:11:16.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=statefulset-2775 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep 20 13:11:16.613: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep 20 13:11:16.613: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep 20 13:11:16.613: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep 20 13:11:16.613: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 09/20/23 13:11:26.841
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep 20 13:11:26.842: INFO: Deleting all statefulset in ns statefulset-2775
    Sep 20 13:11:26.845: INFO: Scaling statefulset ss to 0
    Sep 20 13:11:26.876: INFO: Waiting for statefulset status.replicas updated to 0
    Sep 20 13:11:26.879: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:11:27.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2775" for this suite. 09/20/23 13:11:27.08
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:11:27.278
Sep 20 13:11:27.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename pods 09/20/23 13:11:27.279
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:11:27.787
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:11:27.791
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 09/20/23 13:11:27.796
Sep 20 13:11:27.804: INFO: Waiting up to 5m0s for pod "pod-n7th8" in namespace "pods-2692" to be "running"
Sep 20 13:11:27.809: INFO: Pod "pod-n7th8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.778823ms
Sep 20 13:11:29.814: INFO: Pod "pod-n7th8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010056645s
Sep 20 13:11:31.878: INFO: Pod "pod-n7th8": Phase="Running", Reason="", readiness=true. Elapsed: 4.07406119s
Sep 20 13:11:31.878: INFO: Pod "pod-n7th8" satisfied condition "running"
STEP: patching /status 09/20/23 13:11:31.878
Sep 20 13:11:31.919: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep 20 13:11:31.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2692" for this suite. 09/20/23 13:11:31.926
------------------------------
â€¢ [4.654 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:11:27.278
    Sep 20 13:11:27.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename pods 09/20/23 13:11:27.279
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:11:27.787
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:11:27.791
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 09/20/23 13:11:27.796
    Sep 20 13:11:27.804: INFO: Waiting up to 5m0s for pod "pod-n7th8" in namespace "pods-2692" to be "running"
    Sep 20 13:11:27.809: INFO: Pod "pod-n7th8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.778823ms
    Sep 20 13:11:29.814: INFO: Pod "pod-n7th8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010056645s
    Sep 20 13:11:31.878: INFO: Pod "pod-n7th8": Phase="Running", Reason="", readiness=true. Elapsed: 4.07406119s
    Sep 20 13:11:31.878: INFO: Pod "pod-n7th8" satisfied condition "running"
    STEP: patching /status 09/20/23 13:11:31.878
    Sep 20 13:11:31.919: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:11:31.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2692" for this suite. 09/20/23 13:11:31.926
  << End Captured GinkgoWriter Output
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:11:31.933
Sep 20 13:11:31.933: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename events 09/20/23 13:11:31.933
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:11:31.948
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:11:31.952
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 09/20/23 13:11:31.956
STEP: listing all events in all namespaces 09/20/23 13:11:31.961
STEP: patching the test event 09/20/23 13:11:32.02
STEP: fetching the test event 09/20/23 13:11:32.037
STEP: updating the test event 09/20/23 13:11:32.042
STEP: getting the test event 09/20/23 13:11:32.056
STEP: deleting the test event 09/20/23 13:11:32.059
STEP: listing all events in all namespaces 09/20/23 13:11:32.066
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Sep 20 13:11:32.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-7745" for this suite. 09/20/23 13:11:32.081
------------------------------
â€¢ [0.155 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:11:31.933
    Sep 20 13:11:31.933: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename events 09/20/23 13:11:31.933
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:11:31.948
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:11:31.952
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 09/20/23 13:11:31.956
    STEP: listing all events in all namespaces 09/20/23 13:11:31.961
    STEP: patching the test event 09/20/23 13:11:32.02
    STEP: fetching the test event 09/20/23 13:11:32.037
    STEP: updating the test event 09/20/23 13:11:32.042
    STEP: getting the test event 09/20/23 13:11:32.056
    STEP: deleting the test event 09/20/23 13:11:32.059
    STEP: listing all events in all namespaces 09/20/23 13:11:32.066
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:11:32.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-7745" for this suite. 09/20/23 13:11:32.081
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:11:32.089
Sep 20 13:11:32.089: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename custom-resource-definition 09/20/23 13:11:32.09
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:11:32.101
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:11:32.104
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Sep 20 13:11:32.120: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep 20 13:11:32.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-2274" for this suite. 09/20/23 13:11:32.767
------------------------------
â€¢ [0.694 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:11:32.089
    Sep 20 13:11:32.089: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename custom-resource-definition 09/20/23 13:11:32.09
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:11:32.101
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:11:32.104
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Sep 20 13:11:32.120: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:11:32.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-2274" for this suite. 09/20/23 13:11:32.767
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:11:32.784
Sep 20 13:11:32.784: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename downward-api 09/20/23 13:11:32.785
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:11:32.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:11:32.832
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 09/20/23 13:11:32.838
Sep 20 13:11:32.864: INFO: Waiting up to 5m0s for pod "downwardapi-volume-559c5dea-ddd9-417d-ab57-2a51b46f3ca1" in namespace "downward-api-746" to be "Succeeded or Failed"
Sep 20 13:11:32.942: INFO: Pod "downwardapi-volume-559c5dea-ddd9-417d-ab57-2a51b46f3ca1": Phase="Pending", Reason="", readiness=false. Elapsed: 78.055806ms
Sep 20 13:11:35.116: INFO: Pod "downwardapi-volume-559c5dea-ddd9-417d-ab57-2a51b46f3ca1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.251575249s
Sep 20 13:11:36.963: INFO: Pod "downwardapi-volume-559c5dea-ddd9-417d-ab57-2a51b46f3ca1": Phase="Running", Reason="", readiness=false. Elapsed: 4.098611172s
Sep 20 13:11:38.946: INFO: Pod "downwardapi-volume-559c5dea-ddd9-417d-ab57-2a51b46f3ca1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.081630298s
STEP: Saw pod success 09/20/23 13:11:38.946
Sep 20 13:11:38.946: INFO: Pod "downwardapi-volume-559c5dea-ddd9-417d-ab57-2a51b46f3ca1" satisfied condition "Succeeded or Failed"
Sep 20 13:11:38.948: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downwardapi-volume-559c5dea-ddd9-417d-ab57-2a51b46f3ca1 container client-container: <nil>
STEP: delete the pod 09/20/23 13:11:38.957
Sep 20 13:11:39.311: INFO: Waiting for pod downwardapi-volume-559c5dea-ddd9-417d-ab57-2a51b46f3ca1 to disappear
Sep 20 13:11:39.507: INFO: Pod downwardapi-volume-559c5dea-ddd9-417d-ab57-2a51b46f3ca1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep 20 13:11:39.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-746" for this suite. 09/20/23 13:11:39.517
------------------------------
â€¢ [SLOW TEST] [6.781 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:11:32.784
    Sep 20 13:11:32.784: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename downward-api 09/20/23 13:11:32.785
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:11:32.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:11:32.832
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 09/20/23 13:11:32.838
    Sep 20 13:11:32.864: INFO: Waiting up to 5m0s for pod "downwardapi-volume-559c5dea-ddd9-417d-ab57-2a51b46f3ca1" in namespace "downward-api-746" to be "Succeeded or Failed"
    Sep 20 13:11:32.942: INFO: Pod "downwardapi-volume-559c5dea-ddd9-417d-ab57-2a51b46f3ca1": Phase="Pending", Reason="", readiness=false. Elapsed: 78.055806ms
    Sep 20 13:11:35.116: INFO: Pod "downwardapi-volume-559c5dea-ddd9-417d-ab57-2a51b46f3ca1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.251575249s
    Sep 20 13:11:36.963: INFO: Pod "downwardapi-volume-559c5dea-ddd9-417d-ab57-2a51b46f3ca1": Phase="Running", Reason="", readiness=false. Elapsed: 4.098611172s
    Sep 20 13:11:38.946: INFO: Pod "downwardapi-volume-559c5dea-ddd9-417d-ab57-2a51b46f3ca1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.081630298s
    STEP: Saw pod success 09/20/23 13:11:38.946
    Sep 20 13:11:38.946: INFO: Pod "downwardapi-volume-559c5dea-ddd9-417d-ab57-2a51b46f3ca1" satisfied condition "Succeeded or Failed"
    Sep 20 13:11:38.948: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downwardapi-volume-559c5dea-ddd9-417d-ab57-2a51b46f3ca1 container client-container: <nil>
    STEP: delete the pod 09/20/23 13:11:38.957
    Sep 20 13:11:39.311: INFO: Waiting for pod downwardapi-volume-559c5dea-ddd9-417d-ab57-2a51b46f3ca1 to disappear
    Sep 20 13:11:39.507: INFO: Pod downwardapi-volume-559c5dea-ddd9-417d-ab57-2a51b46f3ca1 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:11:39.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-746" for this suite. 09/20/23 13:11:39.517
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:11:39.567
Sep 20 13:11:39.567: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename runtimeclass 09/20/23 13:11:39.567
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:11:39.638
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:11:39.644
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Sep 20 13:11:39.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-6480" for this suite. 09/20/23 13:11:39.715
------------------------------
â€¢ [0.160 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:11:39.567
    Sep 20 13:11:39.567: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename runtimeclass 09/20/23 13:11:39.567
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:11:39.638
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:11:39.644
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:11:39.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-6480" for this suite. 09/20/23 13:11:39.715
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:11:39.727
Sep 20 13:11:39.727: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename deployment 09/20/23 13:11:39.728
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:11:39.748
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:11:39.754
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Sep 20 13:11:39.798: INFO: Creating simple deployment test-new-deployment
Sep 20 13:11:39.831: INFO: deployment "test-new-deployment" doesn't have the required revision set
Sep 20 13:11:41.840: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 11, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 11, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 11, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 11, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 13:11:43.887: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 11, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 11, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 11, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 11, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource 09/20/23 13:11:45.848
STEP: updating a scale subresource 09/20/23 13:11:45.854
STEP: verifying the deployment Spec.Replicas was modified 09/20/23 13:11:45.865
STEP: Patch a scale subresource 09/20/23 13:11:45.875
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep 20 13:11:46.177: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-8758  6ccc4c8e-db7c-46a6-a7fa-97ff06c7473c 30919 3 2023-09-20 13:11:39 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-09-20 13:11:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 13:11:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045cc078 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-20 13:11:43 +0000 UTC,LastTransitionTime:2023-09-20 13:11:43 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-09-20 13:11:43 +0000 UTC,LastTransitionTime:2023-09-20 13:11:39 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep 20 13:11:46.182: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-8758  5931fc99-2562-4df4-9913-56ac4e62e969 30923 3 2023-09-20 13:11:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 6ccc4c8e-db7c-46a6-a7fa-97ff06c7473c 0xc0045cc497 0xc0045cc498}] [] [{kube-controller-manager Update apps/v1 2023-09-20 13:11:43 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-09-20 13:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6ccc4c8e-db7c-46a6-a7fa-97ff06c7473c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045cc528 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 20 13:11:46.196: INFO: Pod "test-new-deployment-7f5969cbc7-j8nvv" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-j8nvv test-new-deployment-7f5969cbc7- deployment-8758  4544961e-7b02-46f8-b708-c76eaf08c946 30924 0 2023-09-20 13:11:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 5931fc99-2562-4df4-9913-56ac4e62e969 0xc00046efb7 0xc00046efb8}] [] [{kube-controller-manager Update v1 2023-09-20 13:11:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5931fc99-2562-4df4-9913-56ac4e62e969\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5kqjc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5kqjc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:11:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 20 13:11:46.196: INFO: Pod "test-new-deployment-7f5969cbc7-lspd6" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-lspd6 test-new-deployment-7f5969cbc7- deployment-8758  eda38edb-8c44-474b-8344-6dcaf3db942b 30891 0 2023-09-20 13:11:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 5931fc99-2562-4df4-9913-56ac4e62e969 0xc004bec0b0 0xc004bec0b1}] [] [{kube-controller-manager Update v1 2023-09-20 13:11:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5931fc99-2562-4df4-9913-56ac4e62e969\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:11:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.4.248\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cc886,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cc886,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:11:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:11:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:11:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:11:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.64,PodIP:10.100.4.248,StartTime:2023-09-20 13:11:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-20 13:11:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7d31d7bb25be4bbefee49cfcccbd85a6ed92f4e0b850a6aebb746b7f8a187e54,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.4.248,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep 20 13:11:46.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8758" for this suite. 09/20/23 13:11:46.202
------------------------------
â€¢ [SLOW TEST] [7.178 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:11:39.727
    Sep 20 13:11:39.727: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename deployment 09/20/23 13:11:39.728
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:11:39.748
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:11:39.754
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Sep 20 13:11:39.798: INFO: Creating simple deployment test-new-deployment
    Sep 20 13:11:39.831: INFO: deployment "test-new-deployment" doesn't have the required revision set
    Sep 20 13:11:41.840: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 11, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 11, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 11, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 11, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 13:11:43.887: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 11, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 11, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 11, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 11, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: getting scale subresource 09/20/23 13:11:45.848
    STEP: updating a scale subresource 09/20/23 13:11:45.854
    STEP: verifying the deployment Spec.Replicas was modified 09/20/23 13:11:45.865
    STEP: Patch a scale subresource 09/20/23 13:11:45.875
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep 20 13:11:46.177: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-8758  6ccc4c8e-db7c-46a6-a7fa-97ff06c7473c 30919 3 2023-09-20 13:11:39 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-09-20 13:11:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 13:11:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045cc078 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-20 13:11:43 +0000 UTC,LastTransitionTime:2023-09-20 13:11:43 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-09-20 13:11:43 +0000 UTC,LastTransitionTime:2023-09-20 13:11:39 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Sep 20 13:11:46.182: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-8758  5931fc99-2562-4df4-9913-56ac4e62e969 30923 3 2023-09-20 13:11:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 6ccc4c8e-db7c-46a6-a7fa-97ff06c7473c 0xc0045cc497 0xc0045cc498}] [] [{kube-controller-manager Update apps/v1 2023-09-20 13:11:43 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-09-20 13:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6ccc4c8e-db7c-46a6-a7fa-97ff06c7473c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045cc528 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep 20 13:11:46.196: INFO: Pod "test-new-deployment-7f5969cbc7-j8nvv" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-j8nvv test-new-deployment-7f5969cbc7- deployment-8758  4544961e-7b02-46f8-b708-c76eaf08c946 30924 0 2023-09-20 13:11:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 5931fc99-2562-4df4-9913-56ac4e62e969 0xc00046efb7 0xc00046efb8}] [] [{kube-controller-manager Update v1 2023-09-20 13:11:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5931fc99-2562-4df4-9913-56ac4e62e969\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5kqjc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5kqjc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:11:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep 20 13:11:46.196: INFO: Pod "test-new-deployment-7f5969cbc7-lspd6" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-lspd6 test-new-deployment-7f5969cbc7- deployment-8758  eda38edb-8c44-474b-8344-6dcaf3db942b 30891 0 2023-09-20 13:11:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 5931fc99-2562-4df4-9913-56ac4e62e969 0xc004bec0b0 0xc004bec0b1}] [] [{kube-controller-manager Update v1 2023-09-20 13:11:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5931fc99-2562-4df4-9913-56ac4e62e969\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:11:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.4.248\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cc886,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cc886,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:11:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:11:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:11:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:11:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.64,PodIP:10.100.4.248,StartTime:2023-09-20 13:11:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-20 13:11:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7d31d7bb25be4bbefee49cfcccbd85a6ed92f4e0b850a6aebb746b7f8a187e54,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.4.248,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:11:46.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8758" for this suite. 09/20/23 13:11:46.202
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:11:46.905
Sep 20 13:11:46.905: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename pods 09/20/23 13:11:46.906
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:11:47.024
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:11:47.028
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Sep 20 13:11:47.035: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: creating the pod 09/20/23 13:11:47.036
STEP: submitting the pod to kubernetes 09/20/23 13:11:47.036
Sep 20 13:11:47.264: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-8fea62f2-76bf-45c4-815d-c393a74b7daa" in namespace "pods-9061" to be "running and ready"
Sep 20 13:11:47.270: INFO: Pod "pod-exec-websocket-8fea62f2-76bf-45c4-815d-c393a74b7daa": Phase="Pending", Reason="", readiness=false. Elapsed: 5.847497ms
Sep 20 13:11:47.270: INFO: The phase of Pod pod-exec-websocket-8fea62f2-76bf-45c4-815d-c393a74b7daa is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:11:49.274: INFO: Pod "pod-exec-websocket-8fea62f2-76bf-45c4-815d-c393a74b7daa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010672277s
Sep 20 13:11:49.274: INFO: The phase of Pod pod-exec-websocket-8fea62f2-76bf-45c4-815d-c393a74b7daa is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:11:51.639: INFO: Pod "pod-exec-websocket-8fea62f2-76bf-45c4-815d-c393a74b7daa": Phase="Running", Reason="", readiness=true. Elapsed: 4.37490858s
Sep 20 13:11:51.639: INFO: The phase of Pod pod-exec-websocket-8fea62f2-76bf-45c4-815d-c393a74b7daa is Running (Ready = true)
Sep 20 13:11:51.639: INFO: Pod "pod-exec-websocket-8fea62f2-76bf-45c4-815d-c393a74b7daa" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep 20 13:11:51.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9061" for this suite. 09/20/23 13:11:51.83
------------------------------
â€¢ [4.936 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:11:46.905
    Sep 20 13:11:46.905: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename pods 09/20/23 13:11:46.906
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:11:47.024
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:11:47.028
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Sep 20 13:11:47.035: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: creating the pod 09/20/23 13:11:47.036
    STEP: submitting the pod to kubernetes 09/20/23 13:11:47.036
    Sep 20 13:11:47.264: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-8fea62f2-76bf-45c4-815d-c393a74b7daa" in namespace "pods-9061" to be "running and ready"
    Sep 20 13:11:47.270: INFO: Pod "pod-exec-websocket-8fea62f2-76bf-45c4-815d-c393a74b7daa": Phase="Pending", Reason="", readiness=false. Elapsed: 5.847497ms
    Sep 20 13:11:47.270: INFO: The phase of Pod pod-exec-websocket-8fea62f2-76bf-45c4-815d-c393a74b7daa is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:11:49.274: INFO: Pod "pod-exec-websocket-8fea62f2-76bf-45c4-815d-c393a74b7daa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010672277s
    Sep 20 13:11:49.274: INFO: The phase of Pod pod-exec-websocket-8fea62f2-76bf-45c4-815d-c393a74b7daa is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:11:51.639: INFO: Pod "pod-exec-websocket-8fea62f2-76bf-45c4-815d-c393a74b7daa": Phase="Running", Reason="", readiness=true. Elapsed: 4.37490858s
    Sep 20 13:11:51.639: INFO: The phase of Pod pod-exec-websocket-8fea62f2-76bf-45c4-815d-c393a74b7daa is Running (Ready = true)
    Sep 20 13:11:51.639: INFO: Pod "pod-exec-websocket-8fea62f2-76bf-45c4-815d-c393a74b7daa" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:11:51.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9061" for this suite. 09/20/23 13:11:51.83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:11:51.842
Sep 20 13:11:51.843: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename resourcequota 09/20/23 13:11:51.843
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:11:52.191
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:11:52.196
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 09/20/23 13:11:52.204
STEP: Creating a ResourceQuota 09/20/23 13:11:57.211
STEP: Ensuring resource quota status is calculated 09/20/23 13:11:57.231
STEP: Creating a Pod that fits quota 09/20/23 13:11:59.238
STEP: Ensuring ResourceQuota status captures the pod usage 09/20/23 13:11:59.396
STEP: Not allowing a pod to be created that exceeds remaining quota 09/20/23 13:12:01.403
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 09/20/23 13:12:01.407
STEP: Ensuring a pod cannot update its resource requirements 09/20/23 13:12:01.411
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 09/20/23 13:12:01.42
STEP: Deleting the pod 09/20/23 13:12:03.874
STEP: Ensuring resource quota status released the pod usage 09/20/23 13:12:04.13
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep 20 13:12:06.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1904" for this suite. 09/20/23 13:12:06.143
------------------------------
â€¢ [SLOW TEST] [14.409 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:11:51.842
    Sep 20 13:11:51.843: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename resourcequota 09/20/23 13:11:51.843
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:11:52.191
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:11:52.196
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 09/20/23 13:11:52.204
    STEP: Creating a ResourceQuota 09/20/23 13:11:57.211
    STEP: Ensuring resource quota status is calculated 09/20/23 13:11:57.231
    STEP: Creating a Pod that fits quota 09/20/23 13:11:59.238
    STEP: Ensuring ResourceQuota status captures the pod usage 09/20/23 13:11:59.396
    STEP: Not allowing a pod to be created that exceeds remaining quota 09/20/23 13:12:01.403
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 09/20/23 13:12:01.407
    STEP: Ensuring a pod cannot update its resource requirements 09/20/23 13:12:01.411
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 09/20/23 13:12:01.42
    STEP: Deleting the pod 09/20/23 13:12:03.874
    STEP: Ensuring resource quota status released the pod usage 09/20/23 13:12:04.13
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:12:06.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1904" for this suite. 09/20/23 13:12:06.143
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:12:06.252
Sep 20 13:12:06.252: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename projected 09/20/23 13:12:06.253
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:12:06.401
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:12:06.405
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 09/20/23 13:12:06.409
Sep 20 13:12:06.421: INFO: Waiting up to 5m0s for pod "downwardapi-volume-262d7824-9a8c-42e6-bfb9-d79014717857" in namespace "projected-7019" to be "Succeeded or Failed"
Sep 20 13:12:06.426: INFO: Pod "downwardapi-volume-262d7824-9a8c-42e6-bfb9-d79014717857": Phase="Pending", Reason="", readiness=false. Elapsed: 4.913727ms
Sep 20 13:12:08.752: INFO: Pod "downwardapi-volume-262d7824-9a8c-42e6-bfb9-d79014717857": Phase="Pending", Reason="", readiness=false. Elapsed: 2.331711186s
Sep 20 13:12:10.431: INFO: Pod "downwardapi-volume-262d7824-9a8c-42e6-bfb9-d79014717857": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01006511s
Sep 20 13:12:12.430: INFO: Pod "downwardapi-volume-262d7824-9a8c-42e6-bfb9-d79014717857": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009275811s
STEP: Saw pod success 09/20/23 13:12:12.43
Sep 20 13:12:12.430: INFO: Pod "downwardapi-volume-262d7824-9a8c-42e6-bfb9-d79014717857" satisfied condition "Succeeded or Failed"
Sep 20 13:12:12.433: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downwardapi-volume-262d7824-9a8c-42e6-bfb9-d79014717857 container client-container: <nil>
STEP: delete the pod 09/20/23 13:12:12.441
Sep 20 13:12:12.489: INFO: Waiting for pod downwardapi-volume-262d7824-9a8c-42e6-bfb9-d79014717857 to disappear
Sep 20 13:12:12.493: INFO: Pod downwardapi-volume-262d7824-9a8c-42e6-bfb9-d79014717857 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep 20 13:12:12.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7019" for this suite. 09/20/23 13:12:12.498
------------------------------
â€¢ [SLOW TEST] [6.255 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:12:06.252
    Sep 20 13:12:06.252: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename projected 09/20/23 13:12:06.253
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:12:06.401
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:12:06.405
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 09/20/23 13:12:06.409
    Sep 20 13:12:06.421: INFO: Waiting up to 5m0s for pod "downwardapi-volume-262d7824-9a8c-42e6-bfb9-d79014717857" in namespace "projected-7019" to be "Succeeded or Failed"
    Sep 20 13:12:06.426: INFO: Pod "downwardapi-volume-262d7824-9a8c-42e6-bfb9-d79014717857": Phase="Pending", Reason="", readiness=false. Elapsed: 4.913727ms
    Sep 20 13:12:08.752: INFO: Pod "downwardapi-volume-262d7824-9a8c-42e6-bfb9-d79014717857": Phase="Pending", Reason="", readiness=false. Elapsed: 2.331711186s
    Sep 20 13:12:10.431: INFO: Pod "downwardapi-volume-262d7824-9a8c-42e6-bfb9-d79014717857": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01006511s
    Sep 20 13:12:12.430: INFO: Pod "downwardapi-volume-262d7824-9a8c-42e6-bfb9-d79014717857": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009275811s
    STEP: Saw pod success 09/20/23 13:12:12.43
    Sep 20 13:12:12.430: INFO: Pod "downwardapi-volume-262d7824-9a8c-42e6-bfb9-d79014717857" satisfied condition "Succeeded or Failed"
    Sep 20 13:12:12.433: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downwardapi-volume-262d7824-9a8c-42e6-bfb9-d79014717857 container client-container: <nil>
    STEP: delete the pod 09/20/23 13:12:12.441
    Sep 20 13:12:12.489: INFO: Waiting for pod downwardapi-volume-262d7824-9a8c-42e6-bfb9-d79014717857 to disappear
    Sep 20 13:12:12.493: INFO: Pod downwardapi-volume-262d7824-9a8c-42e6-bfb9-d79014717857 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:12:12.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7019" for this suite. 09/20/23 13:12:12.498
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:12:12.508
Sep 20 13:12:12.508: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename security-context-test 09/20/23 13:12:12.509
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:12:12.53
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:12:12.534
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Sep 20 13:12:12.551: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-7ad22554-50c8-42b2-afe0-821e13a4702d" in namespace "security-context-test-5155" to be "Succeeded or Failed"
Sep 20 13:12:12.556: INFO: Pod "busybox-readonly-false-7ad22554-50c8-42b2-afe0-821e13a4702d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.245543ms
Sep 20 13:12:14.561: INFO: Pod "busybox-readonly-false-7ad22554-50c8-42b2-afe0-821e13a4702d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010186371s
Sep 20 13:12:16.562: INFO: Pod "busybox-readonly-false-7ad22554-50c8-42b2-afe0-821e13a4702d": Phase="Running", Reason="", readiness=true. Elapsed: 4.010963312s
Sep 20 13:12:18.565: INFO: Pod "busybox-readonly-false-7ad22554-50c8-42b2-afe0-821e13a4702d": Phase="Running", Reason="", readiness=false. Elapsed: 6.013492407s
Sep 20 13:12:20.569: INFO: Pod "busybox-readonly-false-7ad22554-50c8-42b2-afe0-821e13a4702d": Phase="Running", Reason="", readiness=false. Elapsed: 8.018025084s
Sep 20 13:12:22.562: INFO: Pod "busybox-readonly-false-7ad22554-50c8-42b2-afe0-821e13a4702d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.010629531s
Sep 20 13:12:22.562: INFO: Pod "busybox-readonly-false-7ad22554-50c8-42b2-afe0-821e13a4702d" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep 20 13:12:22.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-5155" for this suite. 09/20/23 13:12:22.568
------------------------------
â€¢ [SLOW TEST] [10.100 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:12:12.508
    Sep 20 13:12:12.508: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename security-context-test 09/20/23 13:12:12.509
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:12:12.53
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:12:12.534
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Sep 20 13:12:12.551: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-7ad22554-50c8-42b2-afe0-821e13a4702d" in namespace "security-context-test-5155" to be "Succeeded or Failed"
    Sep 20 13:12:12.556: INFO: Pod "busybox-readonly-false-7ad22554-50c8-42b2-afe0-821e13a4702d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.245543ms
    Sep 20 13:12:14.561: INFO: Pod "busybox-readonly-false-7ad22554-50c8-42b2-afe0-821e13a4702d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010186371s
    Sep 20 13:12:16.562: INFO: Pod "busybox-readonly-false-7ad22554-50c8-42b2-afe0-821e13a4702d": Phase="Running", Reason="", readiness=true. Elapsed: 4.010963312s
    Sep 20 13:12:18.565: INFO: Pod "busybox-readonly-false-7ad22554-50c8-42b2-afe0-821e13a4702d": Phase="Running", Reason="", readiness=false. Elapsed: 6.013492407s
    Sep 20 13:12:20.569: INFO: Pod "busybox-readonly-false-7ad22554-50c8-42b2-afe0-821e13a4702d": Phase="Running", Reason="", readiness=false. Elapsed: 8.018025084s
    Sep 20 13:12:22.562: INFO: Pod "busybox-readonly-false-7ad22554-50c8-42b2-afe0-821e13a4702d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.010629531s
    Sep 20 13:12:22.562: INFO: Pod "busybox-readonly-false-7ad22554-50c8-42b2-afe0-821e13a4702d" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:12:22.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-5155" for this suite. 09/20/23 13:12:22.568
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:12:22.61
Sep 20 13:12:22.610: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename sched-pred 09/20/23 13:12:22.611
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:12:22.66
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:12:22.675
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Sep 20 13:12:22.684: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 20 13:12:22.698: INFO: Waiting for terminating namespaces to be deleted...
Sep 20 13:12:22.704: INFO: 
Logging pods the apiserver thinks is on node mycluster-ww3cg64etuwi-node-0 before test
Sep 20 13:12:22.714: INFO: csi-cinder-nodeplugin-k6qp5 from kube-system started at 2023-09-20 11:51:32 +0000 UTC (3 container statuses recorded)
Sep 20 13:12:22.714: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Sep 20 13:12:22.714: INFO: 	Container liveness-probe ready: true, restart count 0
Sep 20 13:12:22.714: INFO: 	Container node-driver-registrar ready: true, restart count 0
Sep 20 13:12:22.714: INFO: kube-flannel-ds-chfqx from kube-system started at 2023-09-20 11:51:32 +0000 UTC (1 container statuses recorded)
Sep 20 13:12:22.714: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 20 13:12:22.714: INFO: npd-ntx42 from kube-system started at 2023-09-20 11:52:06 +0000 UTC (1 container statuses recorded)
Sep 20 13:12:22.714: INFO: 	Container node-problem-detector ready: true, restart count 0
Sep 20 13:12:22.714: INFO: sonobuoy-e2e-job-2c0bc69190d741e4 from sonobuoy started at 2023-09-20 12:04:03 +0000 UTC (2 container statuses recorded)
Sep 20 13:12:22.714: INFO: 	Container e2e ready: true, restart count 0
Sep 20 13:12:22.714: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 13:12:22.714: INFO: sonobuoy-systemd-logs-daemon-set-6128a9cd7eba4f1a-r9rqh from sonobuoy started at 2023-09-20 12:04:03 +0000 UTC (2 container statuses recorded)
Sep 20 13:12:22.714: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 13:12:22.714: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 20 13:12:22.714: INFO: 
Logging pods the apiserver thinks is on node mycluster-ww3cg64etuwi-node-1 before test
Sep 20 13:12:22.726: INFO: csi-cinder-nodeplugin-r6zgs from kube-system started at 2023-09-20 11:51:30 +0000 UTC (3 container statuses recorded)
Sep 20 13:12:22.726: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Sep 20 13:12:22.726: INFO: 	Container liveness-probe ready: true, restart count 0
Sep 20 13:12:22.726: INFO: 	Container node-driver-registrar ready: true, restart count 0
Sep 20 13:12:22.726: INFO: kube-flannel-ds-nc8g9 from kube-system started at 2023-09-20 12:44:25 +0000 UTC (1 container statuses recorded)
Sep 20 13:12:22.726: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 20 13:12:22.726: INFO: npd-dqxrp from kube-system started at 2023-09-20 11:51:59 +0000 UTC (1 container statuses recorded)
Sep 20 13:12:22.726: INFO: 	Container node-problem-detector ready: true, restart count 0
Sep 20 13:12:22.726: INFO: pod-exec-websocket-8fea62f2-76bf-45c4-815d-c393a74b7daa from pods-9061 started at 2023-09-20 13:11:47 +0000 UTC (1 container statuses recorded)
Sep 20 13:12:22.726: INFO: 	Container main ready: true, restart count 0
Sep 20 13:12:22.726: INFO: busybox-readonly-false-7ad22554-50c8-42b2-afe0-821e13a4702d from security-context-test-5155 started at 2023-09-20 13:12:12 +0000 UTC (1 container statuses recorded)
Sep 20 13:12:22.726: INFO: 	Container busybox-readonly-false-7ad22554-50c8-42b2-afe0-821e13a4702d ready: false, restart count 0
Sep 20 13:12:22.726: INFO: sonobuoy-systemd-logs-daemon-set-6128a9cd7eba4f1a-8k2ck from sonobuoy started at 2023-09-20 12:04:03 +0000 UTC (2 container statuses recorded)
Sep 20 13:12:22.726: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 13:12:22.726: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 20 13:12:22.726: INFO: 
Logging pods the apiserver thinks is on node mycluster-ww3cg64etuwi-node-2 before test
Sep 20 13:12:22.735: INFO: csi-cinder-nodeplugin-qcqrp from kube-system started at 2023-09-20 11:51:31 +0000 UTC (3 container statuses recorded)
Sep 20 13:12:22.735: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Sep 20 13:12:22.736: INFO: 	Container liveness-probe ready: true, restart count 0
Sep 20 13:12:22.736: INFO: 	Container node-driver-registrar ready: true, restart count 0
Sep 20 13:12:22.736: INFO: kube-dns-autoscaler-86977fd5fc-l9tnc from kube-system started at 2023-09-20 12:43:53 +0000 UTC (1 container statuses recorded)
Sep 20 13:12:22.736: INFO: 	Container autoscaler ready: true, restart count 0
Sep 20 13:12:22.736: INFO: kube-flannel-ds-ncx55 from kube-system started at 2023-09-20 11:51:30 +0000 UTC (1 container statuses recorded)
Sep 20 13:12:22.736: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 20 13:12:22.736: INFO: magnum-metrics-server-6b5dcd575f-gdlm2 from kube-system started at 2023-09-20 12:43:53 +0000 UTC (1 container statuses recorded)
Sep 20 13:12:22.736: INFO: 	Container metrics-server ready: true, restart count 0
Sep 20 13:12:22.736: INFO: npd-k978m from kube-system started at 2023-09-20 11:52:36 +0000 UTC (1 container statuses recorded)
Sep 20 13:12:22.736: INFO: 	Container node-problem-detector ready: true, restart count 0
Sep 20 13:12:22.736: INFO: sonobuoy from sonobuoy started at 2023-09-20 12:03:39 +0000 UTC (1 container statuses recorded)
Sep 20 13:12:22.736: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 20 13:12:22.736: INFO: sonobuoy-systemd-logs-daemon-set-6128a9cd7eba4f1a-bw4zv from sonobuoy started at 2023-09-20 12:04:03 +0000 UTC (2 container statuses recorded)
Sep 20 13:12:22.736: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 13:12:22.736: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 09/20/23 13:12:22.736
Sep 20 13:12:22.757: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6565" to be "running"
Sep 20 13:12:22.779: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 22.677145ms
Sep 20 13:12:25.009: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.252526052s
Sep 20 13:12:26.803: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.045935552s
Sep 20 13:12:26.803: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 09/20/23 13:12:26.811
STEP: Trying to apply a random label on the found node. 09/20/23 13:12:26.835
STEP: verifying the node has the label kubernetes.io/e2e-70ad1307-9636-41fc-87a0-a45e6ee5b73d 42 09/20/23 13:12:26.852
STEP: Trying to relaunch the pod, now with labels. 09/20/23 13:12:26.86
Sep 20 13:12:26.872: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-6565" to be "not pending"
Sep 20 13:12:26.881: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 8.938401ms
Sep 20 13:12:28.984: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.11191063s
Sep 20 13:12:30.885: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 4.012507041s
Sep 20 13:12:30.885: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-70ad1307-9636-41fc-87a0-a45e6ee5b73d off the node mycluster-ww3cg64etuwi-node-1 09/20/23 13:12:30.888
STEP: verifying the node doesn't have the label kubernetes.io/e2e-70ad1307-9636-41fc-87a0-a45e6ee5b73d 09/20/23 13:12:30.904
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Sep 20 13:12:30.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-6565" for this suite. 09/20/23 13:12:30.917
------------------------------
â€¢ [SLOW TEST] [8.314 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:12:22.61
    Sep 20 13:12:22.610: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename sched-pred 09/20/23 13:12:22.611
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:12:22.66
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:12:22.675
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Sep 20 13:12:22.684: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Sep 20 13:12:22.698: INFO: Waiting for terminating namespaces to be deleted...
    Sep 20 13:12:22.704: INFO: 
    Logging pods the apiserver thinks is on node mycluster-ww3cg64etuwi-node-0 before test
    Sep 20 13:12:22.714: INFO: csi-cinder-nodeplugin-k6qp5 from kube-system started at 2023-09-20 11:51:32 +0000 UTC (3 container statuses recorded)
    Sep 20 13:12:22.714: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Sep 20 13:12:22.714: INFO: 	Container liveness-probe ready: true, restart count 0
    Sep 20 13:12:22.714: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Sep 20 13:12:22.714: INFO: kube-flannel-ds-chfqx from kube-system started at 2023-09-20 11:51:32 +0000 UTC (1 container statuses recorded)
    Sep 20 13:12:22.714: INFO: 	Container kube-flannel ready: true, restart count 0
    Sep 20 13:12:22.714: INFO: npd-ntx42 from kube-system started at 2023-09-20 11:52:06 +0000 UTC (1 container statuses recorded)
    Sep 20 13:12:22.714: INFO: 	Container node-problem-detector ready: true, restart count 0
    Sep 20 13:12:22.714: INFO: sonobuoy-e2e-job-2c0bc69190d741e4 from sonobuoy started at 2023-09-20 12:04:03 +0000 UTC (2 container statuses recorded)
    Sep 20 13:12:22.714: INFO: 	Container e2e ready: true, restart count 0
    Sep 20 13:12:22.714: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep 20 13:12:22.714: INFO: sonobuoy-systemd-logs-daemon-set-6128a9cd7eba4f1a-r9rqh from sonobuoy started at 2023-09-20 12:04:03 +0000 UTC (2 container statuses recorded)
    Sep 20 13:12:22.714: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep 20 13:12:22.714: INFO: 	Container systemd-logs ready: true, restart count 0
    Sep 20 13:12:22.714: INFO: 
    Logging pods the apiserver thinks is on node mycluster-ww3cg64etuwi-node-1 before test
    Sep 20 13:12:22.726: INFO: csi-cinder-nodeplugin-r6zgs from kube-system started at 2023-09-20 11:51:30 +0000 UTC (3 container statuses recorded)
    Sep 20 13:12:22.726: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Sep 20 13:12:22.726: INFO: 	Container liveness-probe ready: true, restart count 0
    Sep 20 13:12:22.726: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Sep 20 13:12:22.726: INFO: kube-flannel-ds-nc8g9 from kube-system started at 2023-09-20 12:44:25 +0000 UTC (1 container statuses recorded)
    Sep 20 13:12:22.726: INFO: 	Container kube-flannel ready: true, restart count 0
    Sep 20 13:12:22.726: INFO: npd-dqxrp from kube-system started at 2023-09-20 11:51:59 +0000 UTC (1 container statuses recorded)
    Sep 20 13:12:22.726: INFO: 	Container node-problem-detector ready: true, restart count 0
    Sep 20 13:12:22.726: INFO: pod-exec-websocket-8fea62f2-76bf-45c4-815d-c393a74b7daa from pods-9061 started at 2023-09-20 13:11:47 +0000 UTC (1 container statuses recorded)
    Sep 20 13:12:22.726: INFO: 	Container main ready: true, restart count 0
    Sep 20 13:12:22.726: INFO: busybox-readonly-false-7ad22554-50c8-42b2-afe0-821e13a4702d from security-context-test-5155 started at 2023-09-20 13:12:12 +0000 UTC (1 container statuses recorded)
    Sep 20 13:12:22.726: INFO: 	Container busybox-readonly-false-7ad22554-50c8-42b2-afe0-821e13a4702d ready: false, restart count 0
    Sep 20 13:12:22.726: INFO: sonobuoy-systemd-logs-daemon-set-6128a9cd7eba4f1a-8k2ck from sonobuoy started at 2023-09-20 12:04:03 +0000 UTC (2 container statuses recorded)
    Sep 20 13:12:22.726: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep 20 13:12:22.726: INFO: 	Container systemd-logs ready: true, restart count 0
    Sep 20 13:12:22.726: INFO: 
    Logging pods the apiserver thinks is on node mycluster-ww3cg64etuwi-node-2 before test
    Sep 20 13:12:22.735: INFO: csi-cinder-nodeplugin-qcqrp from kube-system started at 2023-09-20 11:51:31 +0000 UTC (3 container statuses recorded)
    Sep 20 13:12:22.735: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Sep 20 13:12:22.736: INFO: 	Container liveness-probe ready: true, restart count 0
    Sep 20 13:12:22.736: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Sep 20 13:12:22.736: INFO: kube-dns-autoscaler-86977fd5fc-l9tnc from kube-system started at 2023-09-20 12:43:53 +0000 UTC (1 container statuses recorded)
    Sep 20 13:12:22.736: INFO: 	Container autoscaler ready: true, restart count 0
    Sep 20 13:12:22.736: INFO: kube-flannel-ds-ncx55 from kube-system started at 2023-09-20 11:51:30 +0000 UTC (1 container statuses recorded)
    Sep 20 13:12:22.736: INFO: 	Container kube-flannel ready: true, restart count 0
    Sep 20 13:12:22.736: INFO: magnum-metrics-server-6b5dcd575f-gdlm2 from kube-system started at 2023-09-20 12:43:53 +0000 UTC (1 container statuses recorded)
    Sep 20 13:12:22.736: INFO: 	Container metrics-server ready: true, restart count 0
    Sep 20 13:12:22.736: INFO: npd-k978m from kube-system started at 2023-09-20 11:52:36 +0000 UTC (1 container statuses recorded)
    Sep 20 13:12:22.736: INFO: 	Container node-problem-detector ready: true, restart count 0
    Sep 20 13:12:22.736: INFO: sonobuoy from sonobuoy started at 2023-09-20 12:03:39 +0000 UTC (1 container statuses recorded)
    Sep 20 13:12:22.736: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Sep 20 13:12:22.736: INFO: sonobuoy-systemd-logs-daemon-set-6128a9cd7eba4f1a-bw4zv from sonobuoy started at 2023-09-20 12:04:03 +0000 UTC (2 container statuses recorded)
    Sep 20 13:12:22.736: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep 20 13:12:22.736: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 09/20/23 13:12:22.736
    Sep 20 13:12:22.757: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6565" to be "running"
    Sep 20 13:12:22.779: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 22.677145ms
    Sep 20 13:12:25.009: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.252526052s
    Sep 20 13:12:26.803: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.045935552s
    Sep 20 13:12:26.803: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 09/20/23 13:12:26.811
    STEP: Trying to apply a random label on the found node. 09/20/23 13:12:26.835
    STEP: verifying the node has the label kubernetes.io/e2e-70ad1307-9636-41fc-87a0-a45e6ee5b73d 42 09/20/23 13:12:26.852
    STEP: Trying to relaunch the pod, now with labels. 09/20/23 13:12:26.86
    Sep 20 13:12:26.872: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-6565" to be "not pending"
    Sep 20 13:12:26.881: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 8.938401ms
    Sep 20 13:12:28.984: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.11191063s
    Sep 20 13:12:30.885: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 4.012507041s
    Sep 20 13:12:30.885: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-70ad1307-9636-41fc-87a0-a45e6ee5b73d off the node mycluster-ww3cg64etuwi-node-1 09/20/23 13:12:30.888
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-70ad1307-9636-41fc-87a0-a45e6ee5b73d 09/20/23 13:12:30.904
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:12:30.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-6565" for this suite. 09/20/23 13:12:30.917
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:12:30.927
Sep 20 13:12:30.927: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename var-expansion 09/20/23 13:12:30.928
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:12:30.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:12:30.977
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 09/20/23 13:12:30.981
Sep 20 13:12:30.990: INFO: Waiting up to 5m0s for pod "var-expansion-96050cdc-ed31-412c-8796-e847e3af4542" in namespace "var-expansion-3477" to be "Succeeded or Failed"
Sep 20 13:12:31.001: INFO: Pod "var-expansion-96050cdc-ed31-412c-8796-e847e3af4542": Phase="Pending", Reason="", readiness=false. Elapsed: 10.739123ms
Sep 20 13:12:33.090: INFO: Pod "var-expansion-96050cdc-ed31-412c-8796-e847e3af4542": Phase="Pending", Reason="", readiness=false. Elapsed: 2.100111441s
Sep 20 13:12:35.005: INFO: Pod "var-expansion-96050cdc-ed31-412c-8796-e847e3af4542": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015134813s
Sep 20 13:12:37.060: INFO: Pod "var-expansion-96050cdc-ed31-412c-8796-e847e3af4542": Phase="Pending", Reason="", readiness=false. Elapsed: 6.070083025s
Sep 20 13:12:39.271: INFO: Pod "var-expansion-96050cdc-ed31-412c-8796-e847e3af4542": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.281027776s
STEP: Saw pod success 09/20/23 13:12:39.271
Sep 20 13:12:39.271: INFO: Pod "var-expansion-96050cdc-ed31-412c-8796-e847e3af4542" satisfied condition "Succeeded or Failed"
Sep 20 13:12:39.275: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod var-expansion-96050cdc-ed31-412c-8796-e847e3af4542 container dapi-container: <nil>
STEP: delete the pod 09/20/23 13:12:39.317
Sep 20 13:12:39.335: INFO: Waiting for pod var-expansion-96050cdc-ed31-412c-8796-e847e3af4542 to disappear
Sep 20 13:12:39.339: INFO: Pod var-expansion-96050cdc-ed31-412c-8796-e847e3af4542 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep 20 13:12:39.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-3477" for this suite. 09/20/23 13:12:39.343
------------------------------
â€¢ [SLOW TEST] [8.422 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:12:30.927
    Sep 20 13:12:30.927: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename var-expansion 09/20/23 13:12:30.928
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:12:30.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:12:30.977
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 09/20/23 13:12:30.981
    Sep 20 13:12:30.990: INFO: Waiting up to 5m0s for pod "var-expansion-96050cdc-ed31-412c-8796-e847e3af4542" in namespace "var-expansion-3477" to be "Succeeded or Failed"
    Sep 20 13:12:31.001: INFO: Pod "var-expansion-96050cdc-ed31-412c-8796-e847e3af4542": Phase="Pending", Reason="", readiness=false. Elapsed: 10.739123ms
    Sep 20 13:12:33.090: INFO: Pod "var-expansion-96050cdc-ed31-412c-8796-e847e3af4542": Phase="Pending", Reason="", readiness=false. Elapsed: 2.100111441s
    Sep 20 13:12:35.005: INFO: Pod "var-expansion-96050cdc-ed31-412c-8796-e847e3af4542": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015134813s
    Sep 20 13:12:37.060: INFO: Pod "var-expansion-96050cdc-ed31-412c-8796-e847e3af4542": Phase="Pending", Reason="", readiness=false. Elapsed: 6.070083025s
    Sep 20 13:12:39.271: INFO: Pod "var-expansion-96050cdc-ed31-412c-8796-e847e3af4542": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.281027776s
    STEP: Saw pod success 09/20/23 13:12:39.271
    Sep 20 13:12:39.271: INFO: Pod "var-expansion-96050cdc-ed31-412c-8796-e847e3af4542" satisfied condition "Succeeded or Failed"
    Sep 20 13:12:39.275: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod var-expansion-96050cdc-ed31-412c-8796-e847e3af4542 container dapi-container: <nil>
    STEP: delete the pod 09/20/23 13:12:39.317
    Sep 20 13:12:39.335: INFO: Waiting for pod var-expansion-96050cdc-ed31-412c-8796-e847e3af4542 to disappear
    Sep 20 13:12:39.339: INFO: Pod var-expansion-96050cdc-ed31-412c-8796-e847e3af4542 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:12:39.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-3477" for this suite. 09/20/23 13:12:39.343
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:12:39.351
Sep 20 13:12:39.351: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename cronjob 09/20/23 13:12:39.352
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:12:39.389
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:12:39.392
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 09/20/23 13:12:39.397
STEP: Ensuring a job is scheduled 09/20/23 13:12:39.404
STEP: Ensuring exactly one is scheduled 09/20/23 13:13:01.411
STEP: Ensuring exactly one running job exists by listing jobs explicitly 09/20/23 13:13:01.416
STEP: Ensuring no more jobs are scheduled 09/20/23 13:13:01.421
STEP: Removing cronjob 09/20/23 13:18:01.429
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Sep 20 13:18:01.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-1561" for this suite. 09/20/23 13:18:01.447
------------------------------
â€¢ [SLOW TEST] [322.132 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:12:39.351
    Sep 20 13:12:39.351: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename cronjob 09/20/23 13:12:39.352
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:12:39.389
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:12:39.392
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 09/20/23 13:12:39.397
    STEP: Ensuring a job is scheduled 09/20/23 13:12:39.404
    STEP: Ensuring exactly one is scheduled 09/20/23 13:13:01.411
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 09/20/23 13:13:01.416
    STEP: Ensuring no more jobs are scheduled 09/20/23 13:13:01.421
    STEP: Removing cronjob 09/20/23 13:18:01.429
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:18:01.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-1561" for this suite. 09/20/23 13:18:01.447
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:18:01.486
Sep 20 13:18:01.486: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename services 09/20/23 13:18:01.486
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:18:01.524
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:18:01.528
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-7258 09/20/23 13:18:01.534
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7258 to expose endpoints map[] 09/20/23 13:18:01.547
Sep 20 13:18:01.553: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Sep 20 13:18:02.570: INFO: successfully validated that service multi-endpoint-test in namespace services-7258 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7258 09/20/23 13:18:02.57
Sep 20 13:18:02.585: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7258" to be "running and ready"
Sep 20 13:18:02.589: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.976861ms
Sep 20 13:18:02.589: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:18:04.594: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009126706s
Sep 20 13:18:04.594: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:18:06.596: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.010662581s
Sep 20 13:18:06.596: INFO: The phase of Pod pod1 is Running (Ready = true)
Sep 20 13:18:06.596: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7258 to expose endpoints map[pod1:[100]] 09/20/23 13:18:06.6
Sep 20 13:18:06.613: INFO: successfully validated that service multi-endpoint-test in namespace services-7258 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-7258 09/20/23 13:18:06.613
Sep 20 13:18:07.226: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7258" to be "running and ready"
Sep 20 13:18:07.244: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 18.509531ms
Sep 20 13:18:07.244: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:18:09.250: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024132859s
Sep 20 13:18:09.250: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:18:11.251: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.024847106s
Sep 20 13:18:11.251: INFO: The phase of Pod pod2 is Running (Ready = true)
Sep 20 13:18:11.251: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7258 to expose endpoints map[pod1:[100] pod2:[101]] 09/20/23 13:18:11.254
Sep 20 13:18:11.265: INFO: successfully validated that service multi-endpoint-test in namespace services-7258 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 09/20/23 13:18:11.265
Sep 20 13:18:11.265: INFO: Creating new exec pod
Sep 20 13:18:11.443: INFO: Waiting up to 5m0s for pod "execpoddqvbn" in namespace "services-7258" to be "running"
Sep 20 13:18:11.456: INFO: Pod "execpoddqvbn": Phase="Pending", Reason="", readiness=false. Elapsed: 13.616322ms
Sep 20 13:18:13.462: INFO: Pod "execpoddqvbn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019212827s
Sep 20 13:18:15.550: INFO: Pod "execpoddqvbn": Phase="Running", Reason="", readiness=true. Elapsed: 4.107023139s
Sep 20 13:18:15.550: INFO: Pod "execpoddqvbn" satisfied condition "running"
Sep 20 13:18:16.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-7258 exec execpoddqvbn -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Sep 20 13:18:17.035: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Sep 20 13:18:17.035: INFO: stdout: ""
Sep 20 13:18:17.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-7258 exec execpoddqvbn -- /bin/sh -x -c nc -v -z -w 2 10.254.86.173 80'
Sep 20 13:18:17.255: INFO: stderr: "+ nc -v -z -w 2 10.254.86.173 80\nConnection to 10.254.86.173 80 port [tcp/http] succeeded!\n"
Sep 20 13:18:17.255: INFO: stdout: ""
Sep 20 13:18:17.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-7258 exec execpoddqvbn -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Sep 20 13:18:17.477: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Sep 20 13:18:17.477: INFO: stdout: ""
Sep 20 13:18:17.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-7258 exec execpoddqvbn -- /bin/sh -x -c nc -v -z -w 2 10.254.86.173 81'
Sep 20 13:18:17.673: INFO: stderr: "+ nc -v -z -w 2 10.254.86.173 81\nConnection to 10.254.86.173 81 port [tcp/*] succeeded!\n"
Sep 20 13:18:17.673: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-7258 09/20/23 13:18:17.673
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7258 to expose endpoints map[pod2:[101]] 09/20/23 13:18:17.717
Sep 20 13:18:17.732: INFO: successfully validated that service multi-endpoint-test in namespace services-7258 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-7258 09/20/23 13:18:17.732
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7258 to expose endpoints map[] 09/20/23 13:18:17.887
Sep 20 13:18:18.488: INFO: successfully validated that service multi-endpoint-test in namespace services-7258 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep 20 13:18:18.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7258" for this suite. 09/20/23 13:18:18.522
------------------------------
â€¢ [SLOW TEST] [17.056 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:18:01.486
    Sep 20 13:18:01.486: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename services 09/20/23 13:18:01.486
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:18:01.524
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:18:01.528
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-7258 09/20/23 13:18:01.534
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7258 to expose endpoints map[] 09/20/23 13:18:01.547
    Sep 20 13:18:01.553: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Sep 20 13:18:02.570: INFO: successfully validated that service multi-endpoint-test in namespace services-7258 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-7258 09/20/23 13:18:02.57
    Sep 20 13:18:02.585: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7258" to be "running and ready"
    Sep 20 13:18:02.589: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.976861ms
    Sep 20 13:18:02.589: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:18:04.594: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009126706s
    Sep 20 13:18:04.594: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:18:06.596: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.010662581s
    Sep 20 13:18:06.596: INFO: The phase of Pod pod1 is Running (Ready = true)
    Sep 20 13:18:06.596: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7258 to expose endpoints map[pod1:[100]] 09/20/23 13:18:06.6
    Sep 20 13:18:06.613: INFO: successfully validated that service multi-endpoint-test in namespace services-7258 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-7258 09/20/23 13:18:06.613
    Sep 20 13:18:07.226: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7258" to be "running and ready"
    Sep 20 13:18:07.244: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 18.509531ms
    Sep 20 13:18:07.244: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:18:09.250: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024132859s
    Sep 20 13:18:09.250: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:18:11.251: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.024847106s
    Sep 20 13:18:11.251: INFO: The phase of Pod pod2 is Running (Ready = true)
    Sep 20 13:18:11.251: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7258 to expose endpoints map[pod1:[100] pod2:[101]] 09/20/23 13:18:11.254
    Sep 20 13:18:11.265: INFO: successfully validated that service multi-endpoint-test in namespace services-7258 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 09/20/23 13:18:11.265
    Sep 20 13:18:11.265: INFO: Creating new exec pod
    Sep 20 13:18:11.443: INFO: Waiting up to 5m0s for pod "execpoddqvbn" in namespace "services-7258" to be "running"
    Sep 20 13:18:11.456: INFO: Pod "execpoddqvbn": Phase="Pending", Reason="", readiness=false. Elapsed: 13.616322ms
    Sep 20 13:18:13.462: INFO: Pod "execpoddqvbn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019212827s
    Sep 20 13:18:15.550: INFO: Pod "execpoddqvbn": Phase="Running", Reason="", readiness=true. Elapsed: 4.107023139s
    Sep 20 13:18:15.550: INFO: Pod "execpoddqvbn" satisfied condition "running"
    Sep 20 13:18:16.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-7258 exec execpoddqvbn -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Sep 20 13:18:17.035: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Sep 20 13:18:17.035: INFO: stdout: ""
    Sep 20 13:18:17.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-7258 exec execpoddqvbn -- /bin/sh -x -c nc -v -z -w 2 10.254.86.173 80'
    Sep 20 13:18:17.255: INFO: stderr: "+ nc -v -z -w 2 10.254.86.173 80\nConnection to 10.254.86.173 80 port [tcp/http] succeeded!\n"
    Sep 20 13:18:17.255: INFO: stdout: ""
    Sep 20 13:18:17.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-7258 exec execpoddqvbn -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Sep 20 13:18:17.477: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Sep 20 13:18:17.477: INFO: stdout: ""
    Sep 20 13:18:17.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-7258 exec execpoddqvbn -- /bin/sh -x -c nc -v -z -w 2 10.254.86.173 81'
    Sep 20 13:18:17.673: INFO: stderr: "+ nc -v -z -w 2 10.254.86.173 81\nConnection to 10.254.86.173 81 port [tcp/*] succeeded!\n"
    Sep 20 13:18:17.673: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-7258 09/20/23 13:18:17.673
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7258 to expose endpoints map[pod2:[101]] 09/20/23 13:18:17.717
    Sep 20 13:18:17.732: INFO: successfully validated that service multi-endpoint-test in namespace services-7258 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-7258 09/20/23 13:18:17.732
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7258 to expose endpoints map[] 09/20/23 13:18:17.887
    Sep 20 13:18:18.488: INFO: successfully validated that service multi-endpoint-test in namespace services-7258 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:18:18.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7258" for this suite. 09/20/23 13:18:18.522
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:18:18.541
Sep 20 13:18:18.542: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename kubectl 09/20/23 13:18:18.542
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:18:18.575
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:18:18.578
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 09/20/23 13:18:18.583
Sep 20 13:18:18.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-3319 create -f -'
Sep 20 13:18:19.364: INFO: stderr: ""
Sep 20 13:18:19.364: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 09/20/23 13:18:19.364
Sep 20 13:18:20.372: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 20 13:18:20.372: INFO: Found 0 / 1
Sep 20 13:18:21.368: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 20 13:18:21.368: INFO: Found 0 / 1
Sep 20 13:18:22.496: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 20 13:18:22.496: INFO: Found 0 / 1
Sep 20 13:18:23.369: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 20 13:18:23.369: INFO: Found 1 / 1
Sep 20 13:18:23.369: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 09/20/23 13:18:23.369
Sep 20 13:18:23.373: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 20 13:18:23.373: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 20 13:18:23.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-3319 patch pod agnhost-primary-cpz58 -p {"metadata":{"annotations":{"x":"y"}}}'
Sep 20 13:18:23.479: INFO: stderr: ""
Sep 20 13:18:23.479: INFO: stdout: "pod/agnhost-primary-cpz58 patched\n"
STEP: checking annotations 09/20/23 13:18:23.479
Sep 20 13:18:23.484: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 20 13:18:23.484: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep 20 13:18:23.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3319" for this suite. 09/20/23 13:18:23.489
------------------------------
â€¢ [SLOW TEST] [5.203 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:18:18.541
    Sep 20 13:18:18.542: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename kubectl 09/20/23 13:18:18.542
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:18:18.575
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:18:18.578
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 09/20/23 13:18:18.583
    Sep 20 13:18:18.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-3319 create -f -'
    Sep 20 13:18:19.364: INFO: stderr: ""
    Sep 20 13:18:19.364: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 09/20/23 13:18:19.364
    Sep 20 13:18:20.372: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep 20 13:18:20.372: INFO: Found 0 / 1
    Sep 20 13:18:21.368: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep 20 13:18:21.368: INFO: Found 0 / 1
    Sep 20 13:18:22.496: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep 20 13:18:22.496: INFO: Found 0 / 1
    Sep 20 13:18:23.369: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep 20 13:18:23.369: INFO: Found 1 / 1
    Sep 20 13:18:23.369: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 09/20/23 13:18:23.369
    Sep 20 13:18:23.373: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep 20 13:18:23.373: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Sep 20 13:18:23.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-3319 patch pod agnhost-primary-cpz58 -p {"metadata":{"annotations":{"x":"y"}}}'
    Sep 20 13:18:23.479: INFO: stderr: ""
    Sep 20 13:18:23.479: INFO: stdout: "pod/agnhost-primary-cpz58 patched\n"
    STEP: checking annotations 09/20/23 13:18:23.479
    Sep 20 13:18:23.484: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep 20 13:18:23.484: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:18:23.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3319" for this suite. 09/20/23 13:18:23.489
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:18:23.745
Sep 20 13:18:23.746: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename watch 09/20/23 13:18:23.747
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:18:23.797
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:18:23.801
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 09/20/23 13:18:23.807
STEP: creating a new configmap 09/20/23 13:18:23.808
STEP: modifying the configmap once 09/20/23 13:18:24.114
STEP: changing the label value of the configmap 09/20/23 13:18:24.474
STEP: Expecting to observe a delete notification for the watched object 09/20/23 13:18:24.493
Sep 20 13:18:24.493: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3133  9a8b0189-6056-4299-8c5a-d78c3b83fb42 32624 0 2023-09-20 13:18:23 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-20 13:18:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 20 13:18:24.493: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3133  9a8b0189-6056-4299-8c5a-d78c3b83fb42 32628 0 2023-09-20 13:18:23 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-20 13:18:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 20 13:18:24.493: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3133  9a8b0189-6056-4299-8c5a-d78c3b83fb42 32629 0 2023-09-20 13:18:23 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-20 13:18:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 09/20/23 13:18:24.493
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 09/20/23 13:18:24.506
STEP: changing the label value of the configmap back 09/20/23 13:18:34.507
STEP: modifying the configmap a third time 09/20/23 13:18:34.704
STEP: deleting the configmap 09/20/23 13:18:34.736
STEP: Expecting to observe an add notification for the watched object when the label value was restored 09/20/23 13:18:34.746
Sep 20 13:18:34.746: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3133  9a8b0189-6056-4299-8c5a-d78c3b83fb42 32697 0 2023-09-20 13:18:23 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-20 13:18:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 20 13:18:34.746: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3133  9a8b0189-6056-4299-8c5a-d78c3b83fb42 32700 0 2023-09-20 13:18:23 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-20 13:18:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 20 13:18:34.746: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3133  9a8b0189-6056-4299-8c5a-d78c3b83fb42 32701 0 2023-09-20 13:18:23 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-20 13:18:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Sep 20 13:18:34.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-3133" for this suite. 09/20/23 13:18:34.752
------------------------------
â€¢ [SLOW TEST] [11.015 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:18:23.745
    Sep 20 13:18:23.746: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename watch 09/20/23 13:18:23.747
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:18:23.797
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:18:23.801
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 09/20/23 13:18:23.807
    STEP: creating a new configmap 09/20/23 13:18:23.808
    STEP: modifying the configmap once 09/20/23 13:18:24.114
    STEP: changing the label value of the configmap 09/20/23 13:18:24.474
    STEP: Expecting to observe a delete notification for the watched object 09/20/23 13:18:24.493
    Sep 20 13:18:24.493: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3133  9a8b0189-6056-4299-8c5a-d78c3b83fb42 32624 0 2023-09-20 13:18:23 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-20 13:18:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep 20 13:18:24.493: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3133  9a8b0189-6056-4299-8c5a-d78c3b83fb42 32628 0 2023-09-20 13:18:23 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-20 13:18:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep 20 13:18:24.493: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3133  9a8b0189-6056-4299-8c5a-d78c3b83fb42 32629 0 2023-09-20 13:18:23 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-20 13:18:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 09/20/23 13:18:24.493
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 09/20/23 13:18:24.506
    STEP: changing the label value of the configmap back 09/20/23 13:18:34.507
    STEP: modifying the configmap a third time 09/20/23 13:18:34.704
    STEP: deleting the configmap 09/20/23 13:18:34.736
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 09/20/23 13:18:34.746
    Sep 20 13:18:34.746: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3133  9a8b0189-6056-4299-8c5a-d78c3b83fb42 32697 0 2023-09-20 13:18:23 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-20 13:18:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep 20 13:18:34.746: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3133  9a8b0189-6056-4299-8c5a-d78c3b83fb42 32700 0 2023-09-20 13:18:23 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-20 13:18:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep 20 13:18:34.746: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3133  9a8b0189-6056-4299-8c5a-d78c3b83fb42 32701 0 2023-09-20 13:18:23 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-20 13:18:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:18:34.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-3133" for this suite. 09/20/23 13:18:34.752
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:18:34.763
Sep 20 13:18:34.764: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename services 09/20/23 13:18:34.764
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:18:35.216
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:18:35.223
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-5769 09/20/23 13:18:35.263
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5769 to expose endpoints map[] 09/20/23 13:18:35.473
Sep 20 13:18:35.761: INFO: successfully validated that service endpoint-test2 in namespace services-5769 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5769 09/20/23 13:18:35.761
Sep 20 13:18:36.087: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5769" to be "running and ready"
Sep 20 13:18:36.098: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.567278ms
Sep 20 13:18:36.098: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:18:38.105: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017499858s
Sep 20 13:18:38.105: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:18:40.104: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.016283389s
Sep 20 13:18:40.104: INFO: The phase of Pod pod1 is Running (Ready = true)
Sep 20 13:18:40.104: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5769 to expose endpoints map[pod1:[80]] 09/20/23 13:18:40.107
Sep 20 13:18:40.115: INFO: successfully validated that service endpoint-test2 in namespace services-5769 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 09/20/23 13:18:40.115
Sep 20 13:18:40.116: INFO: Creating new exec pod
Sep 20 13:18:40.122: INFO: Waiting up to 5m0s for pod "execpodq9mvf" in namespace "services-5769" to be "running"
Sep 20 13:18:40.126: INFO: Pod "execpodq9mvf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.181707ms
Sep 20 13:18:42.130: INFO: Pod "execpodq9mvf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008216671s
Sep 20 13:18:44.131: INFO: Pod "execpodq9mvf": Phase="Running", Reason="", readiness=true. Elapsed: 4.009262192s
Sep 20 13:18:44.131: INFO: Pod "execpodq9mvf" satisfied condition "running"
Sep 20 13:18:45.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-5769 exec execpodq9mvf -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Sep 20 13:18:45.347: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Sep 20 13:18:45.347: INFO: stdout: ""
Sep 20 13:18:45.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-5769 exec execpodq9mvf -- /bin/sh -x -c nc -v -z -w 2 10.254.229.250 80'
Sep 20 13:18:45.545: INFO: stderr: "+ nc -v -z -w 2 10.254.229.250 80\nConnection to 10.254.229.250 80 port [tcp/http] succeeded!\n"
Sep 20 13:18:45.545: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-5769 09/20/23 13:18:45.545
Sep 20 13:18:45.859: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5769" to be "running and ready"
Sep 20 13:18:45.865: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.14917ms
Sep 20 13:18:45.865: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:18:47.904: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044760147s
Sep 20 13:18:47.904: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:18:49.871: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.011334686s
Sep 20 13:18:49.871: INFO: The phase of Pod pod2 is Running (Ready = true)
Sep 20 13:18:49.871: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5769 to expose endpoints map[pod1:[80] pod2:[80]] 09/20/23 13:18:49.874
Sep 20 13:18:49.891: INFO: successfully validated that service endpoint-test2 in namespace services-5769 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 09/20/23 13:18:49.891
Sep 20 13:18:50.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-5769 exec execpodq9mvf -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Sep 20 13:18:51.132: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Sep 20 13:18:51.132: INFO: stdout: ""
Sep 20 13:18:51.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-5769 exec execpodq9mvf -- /bin/sh -x -c nc -v -z -w 2 10.254.229.250 80'
Sep 20 13:18:51.369: INFO: stderr: "+ nc -v -z -w 2 10.254.229.250 80\nConnection to 10.254.229.250 80 port [tcp/http] succeeded!\n"
Sep 20 13:18:51.369: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-5769 09/20/23 13:18:51.369
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5769 to expose endpoints map[pod2:[80]] 09/20/23 13:18:51.405
Sep 20 13:18:51.439: INFO: successfully validated that service endpoint-test2 in namespace services-5769 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 09/20/23 13:18:51.439
Sep 20 13:18:52.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-5769 exec execpodq9mvf -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Sep 20 13:18:52.632: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Sep 20 13:18:52.632: INFO: stdout: ""
Sep 20 13:18:52.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-5769 exec execpodq9mvf -- /bin/sh -x -c nc -v -z -w 2 10.254.229.250 80'
Sep 20 13:18:52.862: INFO: stderr: "+ nc -v -z -w 2 10.254.229.250 80\nConnection to 10.254.229.250 80 port [tcp/http] succeeded!\n"
Sep 20 13:18:52.862: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-5769 09/20/23 13:18:52.862
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5769 to expose endpoints map[] 09/20/23 13:18:52.915
Sep 20 13:18:54.083: INFO: successfully validated that service endpoint-test2 in namespace services-5769 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep 20 13:18:54.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5769" for this suite. 09/20/23 13:18:54.179
------------------------------
â€¢ [SLOW TEST] [19.428 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:18:34.763
    Sep 20 13:18:34.764: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename services 09/20/23 13:18:34.764
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:18:35.216
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:18:35.223
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-5769 09/20/23 13:18:35.263
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5769 to expose endpoints map[] 09/20/23 13:18:35.473
    Sep 20 13:18:35.761: INFO: successfully validated that service endpoint-test2 in namespace services-5769 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-5769 09/20/23 13:18:35.761
    Sep 20 13:18:36.087: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5769" to be "running and ready"
    Sep 20 13:18:36.098: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.567278ms
    Sep 20 13:18:36.098: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:18:38.105: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017499858s
    Sep 20 13:18:38.105: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:18:40.104: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.016283389s
    Sep 20 13:18:40.104: INFO: The phase of Pod pod1 is Running (Ready = true)
    Sep 20 13:18:40.104: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5769 to expose endpoints map[pod1:[80]] 09/20/23 13:18:40.107
    Sep 20 13:18:40.115: INFO: successfully validated that service endpoint-test2 in namespace services-5769 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 09/20/23 13:18:40.115
    Sep 20 13:18:40.116: INFO: Creating new exec pod
    Sep 20 13:18:40.122: INFO: Waiting up to 5m0s for pod "execpodq9mvf" in namespace "services-5769" to be "running"
    Sep 20 13:18:40.126: INFO: Pod "execpodq9mvf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.181707ms
    Sep 20 13:18:42.130: INFO: Pod "execpodq9mvf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008216671s
    Sep 20 13:18:44.131: INFO: Pod "execpodq9mvf": Phase="Running", Reason="", readiness=true. Elapsed: 4.009262192s
    Sep 20 13:18:44.131: INFO: Pod "execpodq9mvf" satisfied condition "running"
    Sep 20 13:18:45.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-5769 exec execpodq9mvf -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Sep 20 13:18:45.347: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Sep 20 13:18:45.347: INFO: stdout: ""
    Sep 20 13:18:45.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-5769 exec execpodq9mvf -- /bin/sh -x -c nc -v -z -w 2 10.254.229.250 80'
    Sep 20 13:18:45.545: INFO: stderr: "+ nc -v -z -w 2 10.254.229.250 80\nConnection to 10.254.229.250 80 port [tcp/http] succeeded!\n"
    Sep 20 13:18:45.545: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-5769 09/20/23 13:18:45.545
    Sep 20 13:18:45.859: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5769" to be "running and ready"
    Sep 20 13:18:45.865: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.14917ms
    Sep 20 13:18:45.865: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:18:47.904: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044760147s
    Sep 20 13:18:47.904: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:18:49.871: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.011334686s
    Sep 20 13:18:49.871: INFO: The phase of Pod pod2 is Running (Ready = true)
    Sep 20 13:18:49.871: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5769 to expose endpoints map[pod1:[80] pod2:[80]] 09/20/23 13:18:49.874
    Sep 20 13:18:49.891: INFO: successfully validated that service endpoint-test2 in namespace services-5769 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 09/20/23 13:18:49.891
    Sep 20 13:18:50.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-5769 exec execpodq9mvf -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Sep 20 13:18:51.132: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Sep 20 13:18:51.132: INFO: stdout: ""
    Sep 20 13:18:51.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-5769 exec execpodq9mvf -- /bin/sh -x -c nc -v -z -w 2 10.254.229.250 80'
    Sep 20 13:18:51.369: INFO: stderr: "+ nc -v -z -w 2 10.254.229.250 80\nConnection to 10.254.229.250 80 port [tcp/http] succeeded!\n"
    Sep 20 13:18:51.369: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-5769 09/20/23 13:18:51.369
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5769 to expose endpoints map[pod2:[80]] 09/20/23 13:18:51.405
    Sep 20 13:18:51.439: INFO: successfully validated that service endpoint-test2 in namespace services-5769 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 09/20/23 13:18:51.439
    Sep 20 13:18:52.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-5769 exec execpodq9mvf -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Sep 20 13:18:52.632: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Sep 20 13:18:52.632: INFO: stdout: ""
    Sep 20 13:18:52.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-5769 exec execpodq9mvf -- /bin/sh -x -c nc -v -z -w 2 10.254.229.250 80'
    Sep 20 13:18:52.862: INFO: stderr: "+ nc -v -z -w 2 10.254.229.250 80\nConnection to 10.254.229.250 80 port [tcp/http] succeeded!\n"
    Sep 20 13:18:52.862: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-5769 09/20/23 13:18:52.862
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5769 to expose endpoints map[] 09/20/23 13:18:52.915
    Sep 20 13:18:54.083: INFO: successfully validated that service endpoint-test2 in namespace services-5769 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:18:54.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5769" for this suite. 09/20/23 13:18:54.179
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:18:54.192
Sep 20 13:18:54.192: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename resourcequota 09/20/23 13:18:54.193
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:18:54.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:18:54.525
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 09/20/23 13:19:11.534
STEP: Creating a ResourceQuota 09/20/23 13:19:16.54
STEP: Ensuring resource quota status is calculated 09/20/23 13:19:16.552
STEP: Creating a ConfigMap 09/20/23 13:19:18.557
STEP: Ensuring resource quota status captures configMap creation 09/20/23 13:19:18.594
STEP: Deleting a ConfigMap 09/20/23 13:19:20.598
STEP: Ensuring resource quota status released usage 09/20/23 13:19:20.606
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep 20 13:19:22.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2357" for this suite. 09/20/23 13:19:22.617
------------------------------
â€¢ [SLOW TEST] [28.433 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:18:54.192
    Sep 20 13:18:54.192: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename resourcequota 09/20/23 13:18:54.193
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:18:54.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:18:54.525
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 09/20/23 13:19:11.534
    STEP: Creating a ResourceQuota 09/20/23 13:19:16.54
    STEP: Ensuring resource quota status is calculated 09/20/23 13:19:16.552
    STEP: Creating a ConfigMap 09/20/23 13:19:18.557
    STEP: Ensuring resource quota status captures configMap creation 09/20/23 13:19:18.594
    STEP: Deleting a ConfigMap 09/20/23 13:19:20.598
    STEP: Ensuring resource quota status released usage 09/20/23 13:19:20.606
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:19:22.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2357" for this suite. 09/20/23 13:19:22.617
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:19:22.627
Sep 20 13:19:22.627: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename svcaccounts 09/20/23 13:19:22.628
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:19:22.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:19:22.83
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Sep 20 13:19:22.960: INFO: Waiting up to 5m0s for pod "pod-service-account-3254ab6d-d3a0-4965-9cdd-dd98bb6ceeb4" in namespace "svcaccounts-2942" to be "running"
Sep 20 13:19:22.967: INFO: Pod "pod-service-account-3254ab6d-d3a0-4965-9cdd-dd98bb6ceeb4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.570008ms
Sep 20 13:19:24.973: INFO: Pod "pod-service-account-3254ab6d-d3a0-4965-9cdd-dd98bb6ceeb4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012729935s
Sep 20 13:19:26.984: INFO: Pod "pod-service-account-3254ab6d-d3a0-4965-9cdd-dd98bb6ceeb4": Phase="Running", Reason="", readiness=true. Elapsed: 4.023876405s
Sep 20 13:19:26.984: INFO: Pod "pod-service-account-3254ab6d-d3a0-4965-9cdd-dd98bb6ceeb4" satisfied condition "running"
STEP: reading a file in the container 09/20/23 13:19:26.984
Sep 20 13:19:26.984: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2942 pod-service-account-3254ab6d-d3a0-4965-9cdd-dd98bb6ceeb4 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 09/20/23 13:19:27.206
Sep 20 13:19:27.206: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2942 pod-service-account-3254ab6d-d3a0-4965-9cdd-dd98bb6ceeb4 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 09/20/23 13:19:27.393
Sep 20 13:19:27.394: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2942 pod-service-account-3254ab6d-d3a0-4965-9cdd-dd98bb6ceeb4 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Sep 20 13:19:27.591: INFO: Got root ca configmap in namespace "svcaccounts-2942"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep 20 13:19:27.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2942" for this suite. 09/20/23 13:19:27.6
------------------------------
â€¢ [4.982 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:19:22.627
    Sep 20 13:19:22.627: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename svcaccounts 09/20/23 13:19:22.628
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:19:22.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:19:22.83
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Sep 20 13:19:22.960: INFO: Waiting up to 5m0s for pod "pod-service-account-3254ab6d-d3a0-4965-9cdd-dd98bb6ceeb4" in namespace "svcaccounts-2942" to be "running"
    Sep 20 13:19:22.967: INFO: Pod "pod-service-account-3254ab6d-d3a0-4965-9cdd-dd98bb6ceeb4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.570008ms
    Sep 20 13:19:24.973: INFO: Pod "pod-service-account-3254ab6d-d3a0-4965-9cdd-dd98bb6ceeb4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012729935s
    Sep 20 13:19:26.984: INFO: Pod "pod-service-account-3254ab6d-d3a0-4965-9cdd-dd98bb6ceeb4": Phase="Running", Reason="", readiness=true. Elapsed: 4.023876405s
    Sep 20 13:19:26.984: INFO: Pod "pod-service-account-3254ab6d-d3a0-4965-9cdd-dd98bb6ceeb4" satisfied condition "running"
    STEP: reading a file in the container 09/20/23 13:19:26.984
    Sep 20 13:19:26.984: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2942 pod-service-account-3254ab6d-d3a0-4965-9cdd-dd98bb6ceeb4 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 09/20/23 13:19:27.206
    Sep 20 13:19:27.206: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2942 pod-service-account-3254ab6d-d3a0-4965-9cdd-dd98bb6ceeb4 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 09/20/23 13:19:27.393
    Sep 20 13:19:27.394: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2942 pod-service-account-3254ab6d-d3a0-4965-9cdd-dd98bb6ceeb4 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Sep 20 13:19:27.591: INFO: Got root ca configmap in namespace "svcaccounts-2942"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:19:27.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2942" for this suite. 09/20/23 13:19:27.6
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:19:27.609
Sep 20 13:19:27.609: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename emptydir-wrapper 09/20/23 13:19:27.61
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:19:27.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:19:27.669
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 09/20/23 13:19:27.692
STEP: Creating RC which spawns configmap-volume pods 09/20/23 13:19:31.973
Sep 20 13:19:31.999: INFO: Pod name wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad: Found 0 pods out of 5
Sep 20 13:19:37.149: INFO: Pod name wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad: Found 5 pods out of 5
STEP: Ensuring each pod is running 09/20/23 13:19:37.15
Sep 20 13:19:37.150: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-8b7qd" in namespace "emptydir-wrapper-1989" to be "running"
Sep 20 13:19:37.155: INFO: Pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-8b7qd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.160542ms
Sep 20 13:19:39.160: INFO: Pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-8b7qd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010199457s
Sep 20 13:19:41.163: INFO: Pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-8b7qd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013246478s
Sep 20 13:19:43.161: INFO: Pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-8b7qd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011588336s
Sep 20 13:19:45.239: INFO: Pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-8b7qd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.08938989s
Sep 20 13:19:47.165: INFO: Pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-8b7qd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.015096699s
Sep 20 13:19:49.174: INFO: Pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-8b7qd": Phase="Running", Reason="", readiness=true. Elapsed: 12.024319666s
Sep 20 13:19:49.174: INFO: Pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-8b7qd" satisfied condition "running"
Sep 20 13:19:49.174: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-c2kwn" in namespace "emptydir-wrapper-1989" to be "running"
Sep 20 13:19:49.642: INFO: Pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-c2kwn": Phase="Pending", Reason="", readiness=false. Elapsed: 467.898323ms
Sep 20 13:19:51.691: INFO: Pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-c2kwn": Phase="Running", Reason="", readiness=true. Elapsed: 2.516413213s
Sep 20 13:19:51.691: INFO: Pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-c2kwn" satisfied condition "running"
Sep 20 13:19:51.691: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-fr52w" in namespace "emptydir-wrapper-1989" to be "running"
Sep 20 13:19:51.750: INFO: Pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-fr52w": Phase="Running", Reason="", readiness=true. Elapsed: 59.584424ms
Sep 20 13:19:51.750: INFO: Pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-fr52w" satisfied condition "running"
Sep 20 13:19:51.750: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-g66kw" in namespace "emptydir-wrapper-1989" to be "running"
Sep 20 13:19:51.755: INFO: Pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-g66kw": Phase="Running", Reason="", readiness=true. Elapsed: 4.586681ms
Sep 20 13:19:51.755: INFO: Pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-g66kw" satisfied condition "running"
Sep 20 13:19:51.755: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-h4ks4" in namespace "emptydir-wrapper-1989" to be "running"
Sep 20 13:19:51.758: INFO: Pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-h4ks4": Phase="Running", Reason="", readiness=true. Elapsed: 3.575465ms
Sep 20 13:19:51.759: INFO: Pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-h4ks4" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad in namespace emptydir-wrapper-1989, will wait for the garbage collector to delete the pods 09/20/23 13:19:51.759
Sep 20 13:19:51.821: INFO: Deleting ReplicationController wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad took: 8.215137ms
Sep 20 13:19:52.022: INFO: Terminating ReplicationController wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad pods took: 200.494541ms
STEP: Creating RC which spawns configmap-volume pods 09/20/23 13:19:57.337
Sep 20 13:19:57.367: INFO: Pod name wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257: Found 0 pods out of 5
Sep 20 13:20:02.380: INFO: Pod name wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257: Found 5 pods out of 5
STEP: Ensuring each pod is running 09/20/23 13:20:02.38
Sep 20 13:20:02.380: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-4hjrr" in namespace "emptydir-wrapper-1989" to be "running"
Sep 20 13:20:02.390: INFO: Pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-4hjrr": Phase="Pending", Reason="", readiness=false. Elapsed: 9.222023ms
Sep 20 13:20:04.396: INFO: Pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-4hjrr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015629367s
Sep 20 13:20:06.396: INFO: Pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-4hjrr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015111723s
Sep 20 13:20:08.399: INFO: Pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-4hjrr": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018104504s
Sep 20 13:20:10.395: INFO: Pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-4hjrr": Phase="Pending", Reason="", readiness=false. Elapsed: 8.014254792s
Sep 20 13:20:12.394: INFO: Pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-4hjrr": Phase="Pending", Reason="", readiness=false. Elapsed: 10.013917077s
Sep 20 13:20:14.398: INFO: Pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-4hjrr": Phase="Running", Reason="", readiness=true. Elapsed: 12.017185636s
Sep 20 13:20:14.398: INFO: Pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-4hjrr" satisfied condition "running"
Sep 20 13:20:14.398: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-c8k67" in namespace "emptydir-wrapper-1989" to be "running"
Sep 20 13:20:14.404: INFO: Pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-c8k67": Phase="Running", Reason="", readiness=true. Elapsed: 5.714856ms
Sep 20 13:20:14.404: INFO: Pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-c8k67" satisfied condition "running"
Sep 20 13:20:14.404: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-f9snp" in namespace "emptydir-wrapper-1989" to be "running"
Sep 20 13:20:14.409: INFO: Pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-f9snp": Phase="Running", Reason="", readiness=true. Elapsed: 5.272552ms
Sep 20 13:20:14.409: INFO: Pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-f9snp" satisfied condition "running"
Sep 20 13:20:14.409: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-gzjkz" in namespace "emptydir-wrapper-1989" to be "running"
Sep 20 13:20:14.415: INFO: Pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-gzjkz": Phase="Running", Reason="", readiness=true. Elapsed: 5.75369ms
Sep 20 13:20:14.415: INFO: Pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-gzjkz" satisfied condition "running"
Sep 20 13:20:14.415: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-ncg9q" in namespace "emptydir-wrapper-1989" to be "running"
Sep 20 13:20:14.420: INFO: Pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-ncg9q": Phase="Running", Reason="", readiness=true. Elapsed: 4.754468ms
Sep 20 13:20:14.420: INFO: Pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-ncg9q" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257 in namespace emptydir-wrapper-1989, will wait for the garbage collector to delete the pods 09/20/23 13:20:14.42
Sep 20 13:20:14.486: INFO: Deleting ReplicationController wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257 took: 9.484568ms
Sep 20 13:20:14.586: INFO: Terminating ReplicationController wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257 pods took: 100.27337ms
STEP: Creating RC which spawns configmap-volume pods 09/20/23 13:20:19.709
Sep 20 13:20:19.738: INFO: Pod name wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6: Found 0 pods out of 5
Sep 20 13:20:24.746: INFO: Pod name wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6: Found 5 pods out of 5
STEP: Ensuring each pod is running 09/20/23 13:20:24.746
Sep 20 13:20:24.747: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-6kfrq" in namespace "emptydir-wrapper-1989" to be "running"
Sep 20 13:20:24.752: INFO: Pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-6kfrq": Phase="Pending", Reason="", readiness=false. Elapsed: 5.805517ms
Sep 20 13:20:26.871: INFO: Pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-6kfrq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.12443046s
Sep 20 13:20:28.758: INFO: Pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-6kfrq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011696568s
Sep 20 13:20:31.151: INFO: Pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-6kfrq": Phase="Pending", Reason="", readiness=false. Elapsed: 6.404052947s
Sep 20 13:20:34.083: INFO: Pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-6kfrq": Phase="Pending", Reason="", readiness=false. Elapsed: 9.336181723s
Sep 20 13:20:35.145: INFO: Pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-6kfrq": Phase="Pending", Reason="", readiness=false. Elapsed: 10.398802317s
Sep 20 13:20:36.808: INFO: Pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-6kfrq": Phase="Pending", Reason="", readiness=false. Elapsed: 12.061312847s
Sep 20 13:20:39.010: INFO: Pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-6kfrq": Phase="Running", Reason="", readiness=true. Elapsed: 14.263330699s
Sep 20 13:20:39.010: INFO: Pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-6kfrq" satisfied condition "running"
Sep 20 13:20:39.010: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-jq4q2" in namespace "emptydir-wrapper-1989" to be "running"
Sep 20 13:20:39.063: INFO: Pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-jq4q2": Phase="Running", Reason="", readiness=true. Elapsed: 52.963591ms
Sep 20 13:20:39.063: INFO: Pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-jq4q2" satisfied condition "running"
Sep 20 13:20:39.063: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-qbmcz" in namespace "emptydir-wrapper-1989" to be "running"
Sep 20 13:20:39.071: INFO: Pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-qbmcz": Phase="Running", Reason="", readiness=true. Elapsed: 7.480753ms
Sep 20 13:20:39.071: INFO: Pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-qbmcz" satisfied condition "running"
Sep 20 13:20:39.071: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-vrktc" in namespace "emptydir-wrapper-1989" to be "running"
Sep 20 13:20:39.079: INFO: Pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-vrktc": Phase="Running", Reason="", readiness=true. Elapsed: 8.276864ms
Sep 20 13:20:39.079: INFO: Pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-vrktc" satisfied condition "running"
Sep 20 13:20:39.079: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-wpfr6" in namespace "emptydir-wrapper-1989" to be "running"
Sep 20 13:20:39.086: INFO: Pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-wpfr6": Phase="Running", Reason="", readiness=true. Elapsed: 6.34805ms
Sep 20 13:20:39.086: INFO: Pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-wpfr6" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6 in namespace emptydir-wrapper-1989, will wait for the garbage collector to delete the pods 09/20/23 13:20:39.086
Sep 20 13:20:39.156: INFO: Deleting ReplicationController wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6 took: 12.599226ms
Sep 20 13:20:40.056: INFO: Terminating ReplicationController wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6 pods took: 900.371625ms
STEP: Cleaning up the configMaps 09/20/23 13:20:43.957
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Sep 20 13:20:45.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-1989" for this suite. 09/20/23 13:20:45.915
------------------------------
â€¢ [SLOW TEST] [78.313 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:19:27.609
    Sep 20 13:19:27.609: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename emptydir-wrapper 09/20/23 13:19:27.61
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:19:27.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:19:27.669
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 09/20/23 13:19:27.692
    STEP: Creating RC which spawns configmap-volume pods 09/20/23 13:19:31.973
    Sep 20 13:19:31.999: INFO: Pod name wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad: Found 0 pods out of 5
    Sep 20 13:19:37.149: INFO: Pod name wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad: Found 5 pods out of 5
    STEP: Ensuring each pod is running 09/20/23 13:19:37.15
    Sep 20 13:19:37.150: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-8b7qd" in namespace "emptydir-wrapper-1989" to be "running"
    Sep 20 13:19:37.155: INFO: Pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-8b7qd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.160542ms
    Sep 20 13:19:39.160: INFO: Pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-8b7qd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010199457s
    Sep 20 13:19:41.163: INFO: Pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-8b7qd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013246478s
    Sep 20 13:19:43.161: INFO: Pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-8b7qd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011588336s
    Sep 20 13:19:45.239: INFO: Pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-8b7qd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.08938989s
    Sep 20 13:19:47.165: INFO: Pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-8b7qd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.015096699s
    Sep 20 13:19:49.174: INFO: Pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-8b7qd": Phase="Running", Reason="", readiness=true. Elapsed: 12.024319666s
    Sep 20 13:19:49.174: INFO: Pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-8b7qd" satisfied condition "running"
    Sep 20 13:19:49.174: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-c2kwn" in namespace "emptydir-wrapper-1989" to be "running"
    Sep 20 13:19:49.642: INFO: Pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-c2kwn": Phase="Pending", Reason="", readiness=false. Elapsed: 467.898323ms
    Sep 20 13:19:51.691: INFO: Pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-c2kwn": Phase="Running", Reason="", readiness=true. Elapsed: 2.516413213s
    Sep 20 13:19:51.691: INFO: Pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-c2kwn" satisfied condition "running"
    Sep 20 13:19:51.691: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-fr52w" in namespace "emptydir-wrapper-1989" to be "running"
    Sep 20 13:19:51.750: INFO: Pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-fr52w": Phase="Running", Reason="", readiness=true. Elapsed: 59.584424ms
    Sep 20 13:19:51.750: INFO: Pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-fr52w" satisfied condition "running"
    Sep 20 13:19:51.750: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-g66kw" in namespace "emptydir-wrapper-1989" to be "running"
    Sep 20 13:19:51.755: INFO: Pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-g66kw": Phase="Running", Reason="", readiness=true. Elapsed: 4.586681ms
    Sep 20 13:19:51.755: INFO: Pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-g66kw" satisfied condition "running"
    Sep 20 13:19:51.755: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-h4ks4" in namespace "emptydir-wrapper-1989" to be "running"
    Sep 20 13:19:51.758: INFO: Pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-h4ks4": Phase="Running", Reason="", readiness=true. Elapsed: 3.575465ms
    Sep 20 13:19:51.759: INFO: Pod "wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad-h4ks4" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad in namespace emptydir-wrapper-1989, will wait for the garbage collector to delete the pods 09/20/23 13:19:51.759
    Sep 20 13:19:51.821: INFO: Deleting ReplicationController wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad took: 8.215137ms
    Sep 20 13:19:52.022: INFO: Terminating ReplicationController wrapped-volume-race-fed623eb-dc6c-4134-8363-e0c8e6f06aad pods took: 200.494541ms
    STEP: Creating RC which spawns configmap-volume pods 09/20/23 13:19:57.337
    Sep 20 13:19:57.367: INFO: Pod name wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257: Found 0 pods out of 5
    Sep 20 13:20:02.380: INFO: Pod name wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257: Found 5 pods out of 5
    STEP: Ensuring each pod is running 09/20/23 13:20:02.38
    Sep 20 13:20:02.380: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-4hjrr" in namespace "emptydir-wrapper-1989" to be "running"
    Sep 20 13:20:02.390: INFO: Pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-4hjrr": Phase="Pending", Reason="", readiness=false. Elapsed: 9.222023ms
    Sep 20 13:20:04.396: INFO: Pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-4hjrr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015629367s
    Sep 20 13:20:06.396: INFO: Pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-4hjrr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015111723s
    Sep 20 13:20:08.399: INFO: Pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-4hjrr": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018104504s
    Sep 20 13:20:10.395: INFO: Pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-4hjrr": Phase="Pending", Reason="", readiness=false. Elapsed: 8.014254792s
    Sep 20 13:20:12.394: INFO: Pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-4hjrr": Phase="Pending", Reason="", readiness=false. Elapsed: 10.013917077s
    Sep 20 13:20:14.398: INFO: Pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-4hjrr": Phase="Running", Reason="", readiness=true. Elapsed: 12.017185636s
    Sep 20 13:20:14.398: INFO: Pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-4hjrr" satisfied condition "running"
    Sep 20 13:20:14.398: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-c8k67" in namespace "emptydir-wrapper-1989" to be "running"
    Sep 20 13:20:14.404: INFO: Pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-c8k67": Phase="Running", Reason="", readiness=true. Elapsed: 5.714856ms
    Sep 20 13:20:14.404: INFO: Pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-c8k67" satisfied condition "running"
    Sep 20 13:20:14.404: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-f9snp" in namespace "emptydir-wrapper-1989" to be "running"
    Sep 20 13:20:14.409: INFO: Pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-f9snp": Phase="Running", Reason="", readiness=true. Elapsed: 5.272552ms
    Sep 20 13:20:14.409: INFO: Pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-f9snp" satisfied condition "running"
    Sep 20 13:20:14.409: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-gzjkz" in namespace "emptydir-wrapper-1989" to be "running"
    Sep 20 13:20:14.415: INFO: Pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-gzjkz": Phase="Running", Reason="", readiness=true. Elapsed: 5.75369ms
    Sep 20 13:20:14.415: INFO: Pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-gzjkz" satisfied condition "running"
    Sep 20 13:20:14.415: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-ncg9q" in namespace "emptydir-wrapper-1989" to be "running"
    Sep 20 13:20:14.420: INFO: Pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-ncg9q": Phase="Running", Reason="", readiness=true. Elapsed: 4.754468ms
    Sep 20 13:20:14.420: INFO: Pod "wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257-ncg9q" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257 in namespace emptydir-wrapper-1989, will wait for the garbage collector to delete the pods 09/20/23 13:20:14.42
    Sep 20 13:20:14.486: INFO: Deleting ReplicationController wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257 took: 9.484568ms
    Sep 20 13:20:14.586: INFO: Terminating ReplicationController wrapped-volume-race-f72aa999-19d0-4e2f-98bc-e9f31c82b257 pods took: 100.27337ms
    STEP: Creating RC which spawns configmap-volume pods 09/20/23 13:20:19.709
    Sep 20 13:20:19.738: INFO: Pod name wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6: Found 0 pods out of 5
    Sep 20 13:20:24.746: INFO: Pod name wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6: Found 5 pods out of 5
    STEP: Ensuring each pod is running 09/20/23 13:20:24.746
    Sep 20 13:20:24.747: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-6kfrq" in namespace "emptydir-wrapper-1989" to be "running"
    Sep 20 13:20:24.752: INFO: Pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-6kfrq": Phase="Pending", Reason="", readiness=false. Elapsed: 5.805517ms
    Sep 20 13:20:26.871: INFO: Pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-6kfrq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.12443046s
    Sep 20 13:20:28.758: INFO: Pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-6kfrq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011696568s
    Sep 20 13:20:31.151: INFO: Pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-6kfrq": Phase="Pending", Reason="", readiness=false. Elapsed: 6.404052947s
    Sep 20 13:20:34.083: INFO: Pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-6kfrq": Phase="Pending", Reason="", readiness=false. Elapsed: 9.336181723s
    Sep 20 13:20:35.145: INFO: Pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-6kfrq": Phase="Pending", Reason="", readiness=false. Elapsed: 10.398802317s
    Sep 20 13:20:36.808: INFO: Pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-6kfrq": Phase="Pending", Reason="", readiness=false. Elapsed: 12.061312847s
    Sep 20 13:20:39.010: INFO: Pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-6kfrq": Phase="Running", Reason="", readiness=true. Elapsed: 14.263330699s
    Sep 20 13:20:39.010: INFO: Pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-6kfrq" satisfied condition "running"
    Sep 20 13:20:39.010: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-jq4q2" in namespace "emptydir-wrapper-1989" to be "running"
    Sep 20 13:20:39.063: INFO: Pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-jq4q2": Phase="Running", Reason="", readiness=true. Elapsed: 52.963591ms
    Sep 20 13:20:39.063: INFO: Pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-jq4q2" satisfied condition "running"
    Sep 20 13:20:39.063: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-qbmcz" in namespace "emptydir-wrapper-1989" to be "running"
    Sep 20 13:20:39.071: INFO: Pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-qbmcz": Phase="Running", Reason="", readiness=true. Elapsed: 7.480753ms
    Sep 20 13:20:39.071: INFO: Pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-qbmcz" satisfied condition "running"
    Sep 20 13:20:39.071: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-vrktc" in namespace "emptydir-wrapper-1989" to be "running"
    Sep 20 13:20:39.079: INFO: Pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-vrktc": Phase="Running", Reason="", readiness=true. Elapsed: 8.276864ms
    Sep 20 13:20:39.079: INFO: Pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-vrktc" satisfied condition "running"
    Sep 20 13:20:39.079: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-wpfr6" in namespace "emptydir-wrapper-1989" to be "running"
    Sep 20 13:20:39.086: INFO: Pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-wpfr6": Phase="Running", Reason="", readiness=true. Elapsed: 6.34805ms
    Sep 20 13:20:39.086: INFO: Pod "wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6-wpfr6" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6 in namespace emptydir-wrapper-1989, will wait for the garbage collector to delete the pods 09/20/23 13:20:39.086
    Sep 20 13:20:39.156: INFO: Deleting ReplicationController wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6 took: 12.599226ms
    Sep 20 13:20:40.056: INFO: Terminating ReplicationController wrapped-volume-race-339cc5da-ba45-4d75-addd-4d95ee7e72a6 pods took: 900.371625ms
    STEP: Cleaning up the configMaps 09/20/23 13:20:43.957
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:20:45.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-1989" for this suite. 09/20/23 13:20:45.915
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:20:45.928
Sep 20 13:20:45.928: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename sched-pred 09/20/23 13:20:45.928
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:20:45.97
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:20:45.973
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Sep 20 13:20:46.175: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 20 13:20:46.182: INFO: Waiting for terminating namespaces to be deleted...
Sep 20 13:20:46.186: INFO: 
Logging pods the apiserver thinks is on node mycluster-ww3cg64etuwi-node-0 before test
Sep 20 13:20:46.191: INFO: csi-cinder-nodeplugin-k6qp5 from kube-system started at 2023-09-20 11:51:32 +0000 UTC (3 container statuses recorded)
Sep 20 13:20:46.191: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Sep 20 13:20:46.191: INFO: 	Container liveness-probe ready: true, restart count 0
Sep 20 13:20:46.191: INFO: 	Container node-driver-registrar ready: true, restart count 0
Sep 20 13:20:46.191: INFO: kube-flannel-ds-chfqx from kube-system started at 2023-09-20 11:51:32 +0000 UTC (1 container statuses recorded)
Sep 20 13:20:46.191: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 20 13:20:46.191: INFO: npd-ntx42 from kube-system started at 2023-09-20 11:52:06 +0000 UTC (1 container statuses recorded)
Sep 20 13:20:46.191: INFO: 	Container node-problem-detector ready: true, restart count 0
Sep 20 13:20:46.191: INFO: sonobuoy-e2e-job-2c0bc69190d741e4 from sonobuoy started at 2023-09-20 12:04:03 +0000 UTC (2 container statuses recorded)
Sep 20 13:20:46.191: INFO: 	Container e2e ready: true, restart count 0
Sep 20 13:20:46.191: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 13:20:46.191: INFO: sonobuoy-systemd-logs-daemon-set-6128a9cd7eba4f1a-r9rqh from sonobuoy started at 2023-09-20 12:04:03 +0000 UTC (2 container statuses recorded)
Sep 20 13:20:46.191: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 13:20:46.191: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 20 13:20:46.191: INFO: 
Logging pods the apiserver thinks is on node mycluster-ww3cg64etuwi-node-1 before test
Sep 20 13:20:46.197: INFO: csi-cinder-nodeplugin-r6zgs from kube-system started at 2023-09-20 11:51:30 +0000 UTC (3 container statuses recorded)
Sep 20 13:20:46.197: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Sep 20 13:20:46.197: INFO: 	Container liveness-probe ready: true, restart count 0
Sep 20 13:20:46.197: INFO: 	Container node-driver-registrar ready: true, restart count 0
Sep 20 13:20:46.197: INFO: kube-flannel-ds-nc8g9 from kube-system started at 2023-09-20 12:44:25 +0000 UTC (1 container statuses recorded)
Sep 20 13:20:46.197: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 20 13:20:46.197: INFO: npd-dqxrp from kube-system started at 2023-09-20 11:51:59 +0000 UTC (1 container statuses recorded)
Sep 20 13:20:46.197: INFO: 	Container node-problem-detector ready: true, restart count 0
Sep 20 13:20:46.197: INFO: sonobuoy-systemd-logs-daemon-set-6128a9cd7eba4f1a-8k2ck from sonobuoy started at 2023-09-20 12:04:03 +0000 UTC (2 container statuses recorded)
Sep 20 13:20:46.197: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 13:20:46.197: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 20 13:20:46.197: INFO: 
Logging pods the apiserver thinks is on node mycluster-ww3cg64etuwi-node-2 before test
Sep 20 13:20:46.203: INFO: csi-cinder-nodeplugin-qcqrp from kube-system started at 2023-09-20 11:51:31 +0000 UTC (3 container statuses recorded)
Sep 20 13:20:46.203: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Sep 20 13:20:46.203: INFO: 	Container liveness-probe ready: true, restart count 0
Sep 20 13:20:46.203: INFO: 	Container node-driver-registrar ready: true, restart count 0
Sep 20 13:20:46.203: INFO: kube-dns-autoscaler-86977fd5fc-l9tnc from kube-system started at 2023-09-20 12:43:53 +0000 UTC (1 container statuses recorded)
Sep 20 13:20:46.203: INFO: 	Container autoscaler ready: true, restart count 0
Sep 20 13:20:46.203: INFO: kube-flannel-ds-ncx55 from kube-system started at 2023-09-20 11:51:30 +0000 UTC (1 container statuses recorded)
Sep 20 13:20:46.203: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 20 13:20:46.203: INFO: magnum-metrics-server-6b5dcd575f-gdlm2 from kube-system started at 2023-09-20 12:43:53 +0000 UTC (1 container statuses recorded)
Sep 20 13:20:46.203: INFO: 	Container metrics-server ready: true, restart count 0
Sep 20 13:20:46.203: INFO: npd-k978m from kube-system started at 2023-09-20 11:52:36 +0000 UTC (1 container statuses recorded)
Sep 20 13:20:46.203: INFO: 	Container node-problem-detector ready: true, restart count 0
Sep 20 13:20:46.203: INFO: sonobuoy from sonobuoy started at 2023-09-20 12:03:39 +0000 UTC (1 container statuses recorded)
Sep 20 13:20:46.203: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 20 13:20:46.203: INFO: sonobuoy-systemd-logs-daemon-set-6128a9cd7eba4f1a-bw4zv from sonobuoy started at 2023-09-20 12:04:03 +0000 UTC (2 container statuses recorded)
Sep 20 13:20:46.203: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 13:20:46.203: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 09/20/23 13:20:46.204
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.17869e032d1ce1d1], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match Pod's node affinity/selector, 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }. preemption: 0/6 nodes are available: 6 Preemption is not helpful for scheduling..] 09/20/23 13:20:46.243
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Sep 20 13:20:47.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-9399" for this suite. 09/20/23 13:20:47.243
------------------------------
â€¢ [1.327 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:20:45.928
    Sep 20 13:20:45.928: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename sched-pred 09/20/23 13:20:45.928
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:20:45.97
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:20:45.973
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Sep 20 13:20:46.175: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Sep 20 13:20:46.182: INFO: Waiting for terminating namespaces to be deleted...
    Sep 20 13:20:46.186: INFO: 
    Logging pods the apiserver thinks is on node mycluster-ww3cg64etuwi-node-0 before test
    Sep 20 13:20:46.191: INFO: csi-cinder-nodeplugin-k6qp5 from kube-system started at 2023-09-20 11:51:32 +0000 UTC (3 container statuses recorded)
    Sep 20 13:20:46.191: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Sep 20 13:20:46.191: INFO: 	Container liveness-probe ready: true, restart count 0
    Sep 20 13:20:46.191: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Sep 20 13:20:46.191: INFO: kube-flannel-ds-chfqx from kube-system started at 2023-09-20 11:51:32 +0000 UTC (1 container statuses recorded)
    Sep 20 13:20:46.191: INFO: 	Container kube-flannel ready: true, restart count 0
    Sep 20 13:20:46.191: INFO: npd-ntx42 from kube-system started at 2023-09-20 11:52:06 +0000 UTC (1 container statuses recorded)
    Sep 20 13:20:46.191: INFO: 	Container node-problem-detector ready: true, restart count 0
    Sep 20 13:20:46.191: INFO: sonobuoy-e2e-job-2c0bc69190d741e4 from sonobuoy started at 2023-09-20 12:04:03 +0000 UTC (2 container statuses recorded)
    Sep 20 13:20:46.191: INFO: 	Container e2e ready: true, restart count 0
    Sep 20 13:20:46.191: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep 20 13:20:46.191: INFO: sonobuoy-systemd-logs-daemon-set-6128a9cd7eba4f1a-r9rqh from sonobuoy started at 2023-09-20 12:04:03 +0000 UTC (2 container statuses recorded)
    Sep 20 13:20:46.191: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep 20 13:20:46.191: INFO: 	Container systemd-logs ready: true, restart count 0
    Sep 20 13:20:46.191: INFO: 
    Logging pods the apiserver thinks is on node mycluster-ww3cg64etuwi-node-1 before test
    Sep 20 13:20:46.197: INFO: csi-cinder-nodeplugin-r6zgs from kube-system started at 2023-09-20 11:51:30 +0000 UTC (3 container statuses recorded)
    Sep 20 13:20:46.197: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Sep 20 13:20:46.197: INFO: 	Container liveness-probe ready: true, restart count 0
    Sep 20 13:20:46.197: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Sep 20 13:20:46.197: INFO: kube-flannel-ds-nc8g9 from kube-system started at 2023-09-20 12:44:25 +0000 UTC (1 container statuses recorded)
    Sep 20 13:20:46.197: INFO: 	Container kube-flannel ready: true, restart count 0
    Sep 20 13:20:46.197: INFO: npd-dqxrp from kube-system started at 2023-09-20 11:51:59 +0000 UTC (1 container statuses recorded)
    Sep 20 13:20:46.197: INFO: 	Container node-problem-detector ready: true, restart count 0
    Sep 20 13:20:46.197: INFO: sonobuoy-systemd-logs-daemon-set-6128a9cd7eba4f1a-8k2ck from sonobuoy started at 2023-09-20 12:04:03 +0000 UTC (2 container statuses recorded)
    Sep 20 13:20:46.197: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep 20 13:20:46.197: INFO: 	Container systemd-logs ready: true, restart count 0
    Sep 20 13:20:46.197: INFO: 
    Logging pods the apiserver thinks is on node mycluster-ww3cg64etuwi-node-2 before test
    Sep 20 13:20:46.203: INFO: csi-cinder-nodeplugin-qcqrp from kube-system started at 2023-09-20 11:51:31 +0000 UTC (3 container statuses recorded)
    Sep 20 13:20:46.203: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Sep 20 13:20:46.203: INFO: 	Container liveness-probe ready: true, restart count 0
    Sep 20 13:20:46.203: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Sep 20 13:20:46.203: INFO: kube-dns-autoscaler-86977fd5fc-l9tnc from kube-system started at 2023-09-20 12:43:53 +0000 UTC (1 container statuses recorded)
    Sep 20 13:20:46.203: INFO: 	Container autoscaler ready: true, restart count 0
    Sep 20 13:20:46.203: INFO: kube-flannel-ds-ncx55 from kube-system started at 2023-09-20 11:51:30 +0000 UTC (1 container statuses recorded)
    Sep 20 13:20:46.203: INFO: 	Container kube-flannel ready: true, restart count 0
    Sep 20 13:20:46.203: INFO: magnum-metrics-server-6b5dcd575f-gdlm2 from kube-system started at 2023-09-20 12:43:53 +0000 UTC (1 container statuses recorded)
    Sep 20 13:20:46.203: INFO: 	Container metrics-server ready: true, restart count 0
    Sep 20 13:20:46.203: INFO: npd-k978m from kube-system started at 2023-09-20 11:52:36 +0000 UTC (1 container statuses recorded)
    Sep 20 13:20:46.203: INFO: 	Container node-problem-detector ready: true, restart count 0
    Sep 20 13:20:46.203: INFO: sonobuoy from sonobuoy started at 2023-09-20 12:03:39 +0000 UTC (1 container statuses recorded)
    Sep 20 13:20:46.203: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Sep 20 13:20:46.203: INFO: sonobuoy-systemd-logs-daemon-set-6128a9cd7eba4f1a-bw4zv from sonobuoy started at 2023-09-20 12:04:03 +0000 UTC (2 container statuses recorded)
    Sep 20 13:20:46.203: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep 20 13:20:46.203: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 09/20/23 13:20:46.204
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.17869e032d1ce1d1], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match Pod's node affinity/selector, 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }. preemption: 0/6 nodes are available: 6 Preemption is not helpful for scheduling..] 09/20/23 13:20:46.243
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:20:47.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-9399" for this suite. 09/20/23 13:20:47.243
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:20:47.256
Sep 20 13:20:47.256: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename projected 09/20/23 13:20:47.257
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:20:47.744
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:20:47.748
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-9579c5ed-4f54-480f-8a94-5bf172daa41b 09/20/23 13:20:48.423
STEP: Creating the pod 09/20/23 13:20:48.432
Sep 20 13:20:48.468: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1a4d3882-e096-4509-b91c-a4f504c20b84" in namespace "projected-2030" to be "running and ready"
Sep 20 13:20:48.474: INFO: Pod "pod-projected-configmaps-1a4d3882-e096-4509-b91c-a4f504c20b84": Phase="Pending", Reason="", readiness=false. Elapsed: 5.772294ms
Sep 20 13:20:48.474: INFO: The phase of Pod pod-projected-configmaps-1a4d3882-e096-4509-b91c-a4f504c20b84 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:20:50.608: INFO: Pod "pod-projected-configmaps-1a4d3882-e096-4509-b91c-a4f504c20b84": Phase="Pending", Reason="", readiness=false. Elapsed: 2.139087966s
Sep 20 13:20:50.608: INFO: The phase of Pod pod-projected-configmaps-1a4d3882-e096-4509-b91c-a4f504c20b84 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:20:52.556: INFO: Pod "pod-projected-configmaps-1a4d3882-e096-4509-b91c-a4f504c20b84": Phase="Running", Reason="", readiness=true. Elapsed: 4.087751922s
Sep 20 13:20:52.556: INFO: The phase of Pod pod-projected-configmaps-1a4d3882-e096-4509-b91c-a4f504c20b84 is Running (Ready = true)
Sep 20 13:20:52.556: INFO: Pod "pod-projected-configmaps-1a4d3882-e096-4509-b91c-a4f504c20b84" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-9579c5ed-4f54-480f-8a94-5bf172daa41b 09/20/23 13:20:52.768
STEP: waiting to observe update in volume 09/20/23 13:20:52.996
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep 20 13:21:58.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2030" for this suite. 09/20/23 13:21:58.905
------------------------------
â€¢ [SLOW TEST] [71.870 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:20:47.256
    Sep 20 13:20:47.256: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename projected 09/20/23 13:20:47.257
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:20:47.744
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:20:47.748
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-9579c5ed-4f54-480f-8a94-5bf172daa41b 09/20/23 13:20:48.423
    STEP: Creating the pod 09/20/23 13:20:48.432
    Sep 20 13:20:48.468: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1a4d3882-e096-4509-b91c-a4f504c20b84" in namespace "projected-2030" to be "running and ready"
    Sep 20 13:20:48.474: INFO: Pod "pod-projected-configmaps-1a4d3882-e096-4509-b91c-a4f504c20b84": Phase="Pending", Reason="", readiness=false. Elapsed: 5.772294ms
    Sep 20 13:20:48.474: INFO: The phase of Pod pod-projected-configmaps-1a4d3882-e096-4509-b91c-a4f504c20b84 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:20:50.608: INFO: Pod "pod-projected-configmaps-1a4d3882-e096-4509-b91c-a4f504c20b84": Phase="Pending", Reason="", readiness=false. Elapsed: 2.139087966s
    Sep 20 13:20:50.608: INFO: The phase of Pod pod-projected-configmaps-1a4d3882-e096-4509-b91c-a4f504c20b84 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:20:52.556: INFO: Pod "pod-projected-configmaps-1a4d3882-e096-4509-b91c-a4f504c20b84": Phase="Running", Reason="", readiness=true. Elapsed: 4.087751922s
    Sep 20 13:20:52.556: INFO: The phase of Pod pod-projected-configmaps-1a4d3882-e096-4509-b91c-a4f504c20b84 is Running (Ready = true)
    Sep 20 13:20:52.556: INFO: Pod "pod-projected-configmaps-1a4d3882-e096-4509-b91c-a4f504c20b84" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-9579c5ed-4f54-480f-8a94-5bf172daa41b 09/20/23 13:20:52.768
    STEP: waiting to observe update in volume 09/20/23 13:20:52.996
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:21:58.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2030" for this suite. 09/20/23 13:21:58.905
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:21:59.129
Sep 20 13:21:59.129: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename dns 09/20/23 13:21:59.129
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:21:59.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:21:59.168
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 09/20/23 13:21:59.173
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5152.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5152.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 09/20/23 13:21:59.182
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5152.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5152.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 09/20/23 13:21:59.182
STEP: creating a pod to probe DNS 09/20/23 13:21:59.182
STEP: submitting the pod to kubernetes 09/20/23 13:21:59.182
Sep 20 13:21:59.219: INFO: Waiting up to 15m0s for pod "dns-test-b1b4c147-259b-4d01-98ca-2389b08f6f62" in namespace "dns-5152" to be "running"
Sep 20 13:21:59.233: INFO: Pod "dns-test-b1b4c147-259b-4d01-98ca-2389b08f6f62": Phase="Pending", Reason="", readiness=false. Elapsed: 13.900438ms
Sep 20 13:22:01.238: INFO: Pod "dns-test-b1b4c147-259b-4d01-98ca-2389b08f6f62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018922252s
Sep 20 13:22:03.239: INFO: Pod "dns-test-b1b4c147-259b-4d01-98ca-2389b08f6f62": Phase="Running", Reason="", readiness=true. Elapsed: 4.019917518s
Sep 20 13:22:03.239: INFO: Pod "dns-test-b1b4c147-259b-4d01-98ca-2389b08f6f62" satisfied condition "running"
STEP: retrieving the pod 09/20/23 13:22:03.239
STEP: looking for the results for each expected name from probers 09/20/23 13:22:03.244
Sep 20 13:22:04.049: INFO: DNS probes using dns-5152/dns-test-b1b4c147-259b-4d01-98ca-2389b08f6f62 succeeded

STEP: deleting the pod 09/20/23 13:22:04.049
STEP: deleting the test headless service 09/20/23 13:22:04.199
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep 20 13:22:04.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5152" for this suite. 09/20/23 13:22:04.301
------------------------------
â€¢ [SLOW TEST] [5.462 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:21:59.129
    Sep 20 13:21:59.129: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename dns 09/20/23 13:21:59.129
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:21:59.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:21:59.168
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 09/20/23 13:21:59.173
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5152.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5152.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     09/20/23 13:21:59.182
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5152.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5152.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     09/20/23 13:21:59.182
    STEP: creating a pod to probe DNS 09/20/23 13:21:59.182
    STEP: submitting the pod to kubernetes 09/20/23 13:21:59.182
    Sep 20 13:21:59.219: INFO: Waiting up to 15m0s for pod "dns-test-b1b4c147-259b-4d01-98ca-2389b08f6f62" in namespace "dns-5152" to be "running"
    Sep 20 13:21:59.233: INFO: Pod "dns-test-b1b4c147-259b-4d01-98ca-2389b08f6f62": Phase="Pending", Reason="", readiness=false. Elapsed: 13.900438ms
    Sep 20 13:22:01.238: INFO: Pod "dns-test-b1b4c147-259b-4d01-98ca-2389b08f6f62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018922252s
    Sep 20 13:22:03.239: INFO: Pod "dns-test-b1b4c147-259b-4d01-98ca-2389b08f6f62": Phase="Running", Reason="", readiness=true. Elapsed: 4.019917518s
    Sep 20 13:22:03.239: INFO: Pod "dns-test-b1b4c147-259b-4d01-98ca-2389b08f6f62" satisfied condition "running"
    STEP: retrieving the pod 09/20/23 13:22:03.239
    STEP: looking for the results for each expected name from probers 09/20/23 13:22:03.244
    Sep 20 13:22:04.049: INFO: DNS probes using dns-5152/dns-test-b1b4c147-259b-4d01-98ca-2389b08f6f62 succeeded

    STEP: deleting the pod 09/20/23 13:22:04.049
    STEP: deleting the test headless service 09/20/23 13:22:04.199
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:22:04.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5152" for this suite. 09/20/23 13:22:04.301
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:22:04.591
Sep 20 13:22:04.591: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename kubelet-test 09/20/23 13:22:04.592
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:22:05.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:22:05.753
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Sep 20 13:22:05.969: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs639e9533-acf8-4882-900f-bbd3679df6d2" in namespace "kubelet-test-1440" to be "running and ready"
Sep 20 13:22:05.985: INFO: Pod "busybox-readonly-fs639e9533-acf8-4882-900f-bbd3679df6d2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.153331ms
Sep 20 13:22:05.985: INFO: The phase of Pod busybox-readonly-fs639e9533-acf8-4882-900f-bbd3679df6d2 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:22:07.993: INFO: Pod "busybox-readonly-fs639e9533-acf8-4882-900f-bbd3679df6d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024743484s
Sep 20 13:22:07.994: INFO: The phase of Pod busybox-readonly-fs639e9533-acf8-4882-900f-bbd3679df6d2 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:22:10.004: INFO: Pod "busybox-readonly-fs639e9533-acf8-4882-900f-bbd3679df6d2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034816333s
Sep 20 13:22:10.004: INFO: The phase of Pod busybox-readonly-fs639e9533-acf8-4882-900f-bbd3679df6d2 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:22:12.038: INFO: Pod "busybox-readonly-fs639e9533-acf8-4882-900f-bbd3679df6d2": Phase="Running", Reason="", readiness=true. Elapsed: 6.069054117s
Sep 20 13:22:12.038: INFO: The phase of Pod busybox-readonly-fs639e9533-acf8-4882-900f-bbd3679df6d2 is Running (Ready = true)
Sep 20 13:22:12.038: INFO: Pod "busybox-readonly-fs639e9533-acf8-4882-900f-bbd3679df6d2" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Sep 20 13:22:12.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-1440" for this suite. 09/20/23 13:22:12.303
------------------------------
â€¢ [SLOW TEST] [7.810 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:22:04.591
    Sep 20 13:22:04.591: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename kubelet-test 09/20/23 13:22:04.592
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:22:05.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:22:05.753
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Sep 20 13:22:05.969: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs639e9533-acf8-4882-900f-bbd3679df6d2" in namespace "kubelet-test-1440" to be "running and ready"
    Sep 20 13:22:05.985: INFO: Pod "busybox-readonly-fs639e9533-acf8-4882-900f-bbd3679df6d2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.153331ms
    Sep 20 13:22:05.985: INFO: The phase of Pod busybox-readonly-fs639e9533-acf8-4882-900f-bbd3679df6d2 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:22:07.993: INFO: Pod "busybox-readonly-fs639e9533-acf8-4882-900f-bbd3679df6d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024743484s
    Sep 20 13:22:07.994: INFO: The phase of Pod busybox-readonly-fs639e9533-acf8-4882-900f-bbd3679df6d2 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:22:10.004: INFO: Pod "busybox-readonly-fs639e9533-acf8-4882-900f-bbd3679df6d2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034816333s
    Sep 20 13:22:10.004: INFO: The phase of Pod busybox-readonly-fs639e9533-acf8-4882-900f-bbd3679df6d2 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:22:12.038: INFO: Pod "busybox-readonly-fs639e9533-acf8-4882-900f-bbd3679df6d2": Phase="Running", Reason="", readiness=true. Elapsed: 6.069054117s
    Sep 20 13:22:12.038: INFO: The phase of Pod busybox-readonly-fs639e9533-acf8-4882-900f-bbd3679df6d2 is Running (Ready = true)
    Sep 20 13:22:12.038: INFO: Pod "busybox-readonly-fs639e9533-acf8-4882-900f-bbd3679df6d2" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:22:12.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-1440" for this suite. 09/20/23 13:22:12.303
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:22:12.404
Sep 20 13:22:12.404: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename container-runtime 09/20/23 13:22:12.405
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:22:12.459
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:22:12.464
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 09/20/23 13:22:12.468
STEP: wait for the container to reach Succeeded 09/20/23 13:22:12.482
STEP: get the container status 09/20/23 13:22:18.934
STEP: the container should be terminated 09/20/23 13:22:18.937
STEP: the termination message should be set 09/20/23 13:22:18.937
Sep 20 13:22:18.937: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 09/20/23 13:22:18.937
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Sep 20 13:22:18.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-3907" for this suite. 09/20/23 13:22:19
------------------------------
â€¢ [SLOW TEST] [6.602 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:22:12.404
    Sep 20 13:22:12.404: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename container-runtime 09/20/23 13:22:12.405
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:22:12.459
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:22:12.464
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 09/20/23 13:22:12.468
    STEP: wait for the container to reach Succeeded 09/20/23 13:22:12.482
    STEP: get the container status 09/20/23 13:22:18.934
    STEP: the container should be terminated 09/20/23 13:22:18.937
    STEP: the termination message should be set 09/20/23 13:22:18.937
    Sep 20 13:22:18.937: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 09/20/23 13:22:18.937
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:22:18.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-3907" for this suite. 09/20/23 13:22:19
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:22:19.007
Sep 20 13:22:19.007: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename dns 09/20/23 13:22:19.008
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:22:19.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:22:19.041
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 09/20/23 13:22:19.045
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8698.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8698.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8698.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8698.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8698.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8698.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8698.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8698.svc.cluster.local;sleep 1; done
 09/20/23 13:22:19.051
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8698.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8698.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8698.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8698.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8698.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8698.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8698.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8698.svc.cluster.local;sleep 1; done
 09/20/23 13:22:19.051
STEP: creating a pod to probe DNS 09/20/23 13:22:19.051
STEP: submitting the pod to kubernetes 09/20/23 13:22:19.052
Sep 20 13:22:19.064: INFO: Waiting up to 15m0s for pod "dns-test-ac02cc16-b78a-4f46-a324-c310ed860474" in namespace "dns-8698" to be "running"
Sep 20 13:22:19.070: INFO: Pod "dns-test-ac02cc16-b78a-4f46-a324-c310ed860474": Phase="Pending", Reason="", readiness=false. Elapsed: 6.032856ms
Sep 20 13:22:21.074: INFO: Pod "dns-test-ac02cc16-b78a-4f46-a324-c310ed860474": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010069472s
Sep 20 13:22:23.481: INFO: Pod "dns-test-ac02cc16-b78a-4f46-a324-c310ed860474": Phase="Pending", Reason="", readiness=false. Elapsed: 4.416378625s
Sep 20 13:22:25.075: INFO: Pod "dns-test-ac02cc16-b78a-4f46-a324-c310ed860474": Phase="Running", Reason="", readiness=true. Elapsed: 6.010786584s
Sep 20 13:22:25.075: INFO: Pod "dns-test-ac02cc16-b78a-4f46-a324-c310ed860474" satisfied condition "running"
STEP: retrieving the pod 09/20/23 13:22:25.075
STEP: looking for the results for each expected name from probers 09/20/23 13:22:25.078
Sep 20 13:22:25.083: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8698.svc.cluster.local from pod dns-8698/dns-test-ac02cc16-b78a-4f46-a324-c310ed860474: the server could not find the requested resource (get pods dns-test-ac02cc16-b78a-4f46-a324-c310ed860474)
Sep 20 13:22:25.086: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8698.svc.cluster.local from pod dns-8698/dns-test-ac02cc16-b78a-4f46-a324-c310ed860474: the server could not find the requested resource (get pods dns-test-ac02cc16-b78a-4f46-a324-c310ed860474)
Sep 20 13:22:25.089: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8698.svc.cluster.local from pod dns-8698/dns-test-ac02cc16-b78a-4f46-a324-c310ed860474: the server could not find the requested resource (get pods dns-test-ac02cc16-b78a-4f46-a324-c310ed860474)
Sep 20 13:22:25.091: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8698.svc.cluster.local from pod dns-8698/dns-test-ac02cc16-b78a-4f46-a324-c310ed860474: the server could not find the requested resource (get pods dns-test-ac02cc16-b78a-4f46-a324-c310ed860474)
Sep 20 13:22:25.094: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8698.svc.cluster.local from pod dns-8698/dns-test-ac02cc16-b78a-4f46-a324-c310ed860474: the server could not find the requested resource (get pods dns-test-ac02cc16-b78a-4f46-a324-c310ed860474)
Sep 20 13:22:25.097: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8698.svc.cluster.local from pod dns-8698/dns-test-ac02cc16-b78a-4f46-a324-c310ed860474: the server could not find the requested resource (get pods dns-test-ac02cc16-b78a-4f46-a324-c310ed860474)
Sep 20 13:22:25.100: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8698.svc.cluster.local from pod dns-8698/dns-test-ac02cc16-b78a-4f46-a324-c310ed860474: the server could not find the requested resource (get pods dns-test-ac02cc16-b78a-4f46-a324-c310ed860474)
Sep 20 13:22:25.103: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8698.svc.cluster.local from pod dns-8698/dns-test-ac02cc16-b78a-4f46-a324-c310ed860474: the server could not find the requested resource (get pods dns-test-ac02cc16-b78a-4f46-a324-c310ed860474)
Sep 20 13:22:25.103: INFO: Lookups using dns-8698/dns-test-ac02cc16-b78a-4f46-a324-c310ed860474 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8698.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8698.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8698.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8698.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8698.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8698.svc.cluster.local jessie_udp@dns-test-service-2.dns-8698.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8698.svc.cluster.local]

Sep 20 13:22:30.160: INFO: DNS probes using dns-8698/dns-test-ac02cc16-b78a-4f46-a324-c310ed860474 succeeded

STEP: deleting the pod 09/20/23 13:22:30.16
STEP: deleting the test headless service 09/20/23 13:22:30.186
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep 20 13:22:30.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8698" for this suite. 09/20/23 13:22:30.267
------------------------------
â€¢ [SLOW TEST] [11.273 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:22:19.007
    Sep 20 13:22:19.007: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename dns 09/20/23 13:22:19.008
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:22:19.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:22:19.041
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 09/20/23 13:22:19.045
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8698.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8698.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8698.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8698.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8698.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8698.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8698.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8698.svc.cluster.local;sleep 1; done
     09/20/23 13:22:19.051
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8698.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8698.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8698.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8698.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8698.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8698.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8698.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8698.svc.cluster.local;sleep 1; done
     09/20/23 13:22:19.051
    STEP: creating a pod to probe DNS 09/20/23 13:22:19.051
    STEP: submitting the pod to kubernetes 09/20/23 13:22:19.052
    Sep 20 13:22:19.064: INFO: Waiting up to 15m0s for pod "dns-test-ac02cc16-b78a-4f46-a324-c310ed860474" in namespace "dns-8698" to be "running"
    Sep 20 13:22:19.070: INFO: Pod "dns-test-ac02cc16-b78a-4f46-a324-c310ed860474": Phase="Pending", Reason="", readiness=false. Elapsed: 6.032856ms
    Sep 20 13:22:21.074: INFO: Pod "dns-test-ac02cc16-b78a-4f46-a324-c310ed860474": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010069472s
    Sep 20 13:22:23.481: INFO: Pod "dns-test-ac02cc16-b78a-4f46-a324-c310ed860474": Phase="Pending", Reason="", readiness=false. Elapsed: 4.416378625s
    Sep 20 13:22:25.075: INFO: Pod "dns-test-ac02cc16-b78a-4f46-a324-c310ed860474": Phase="Running", Reason="", readiness=true. Elapsed: 6.010786584s
    Sep 20 13:22:25.075: INFO: Pod "dns-test-ac02cc16-b78a-4f46-a324-c310ed860474" satisfied condition "running"
    STEP: retrieving the pod 09/20/23 13:22:25.075
    STEP: looking for the results for each expected name from probers 09/20/23 13:22:25.078
    Sep 20 13:22:25.083: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8698.svc.cluster.local from pod dns-8698/dns-test-ac02cc16-b78a-4f46-a324-c310ed860474: the server could not find the requested resource (get pods dns-test-ac02cc16-b78a-4f46-a324-c310ed860474)
    Sep 20 13:22:25.086: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8698.svc.cluster.local from pod dns-8698/dns-test-ac02cc16-b78a-4f46-a324-c310ed860474: the server could not find the requested resource (get pods dns-test-ac02cc16-b78a-4f46-a324-c310ed860474)
    Sep 20 13:22:25.089: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8698.svc.cluster.local from pod dns-8698/dns-test-ac02cc16-b78a-4f46-a324-c310ed860474: the server could not find the requested resource (get pods dns-test-ac02cc16-b78a-4f46-a324-c310ed860474)
    Sep 20 13:22:25.091: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8698.svc.cluster.local from pod dns-8698/dns-test-ac02cc16-b78a-4f46-a324-c310ed860474: the server could not find the requested resource (get pods dns-test-ac02cc16-b78a-4f46-a324-c310ed860474)
    Sep 20 13:22:25.094: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8698.svc.cluster.local from pod dns-8698/dns-test-ac02cc16-b78a-4f46-a324-c310ed860474: the server could not find the requested resource (get pods dns-test-ac02cc16-b78a-4f46-a324-c310ed860474)
    Sep 20 13:22:25.097: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8698.svc.cluster.local from pod dns-8698/dns-test-ac02cc16-b78a-4f46-a324-c310ed860474: the server could not find the requested resource (get pods dns-test-ac02cc16-b78a-4f46-a324-c310ed860474)
    Sep 20 13:22:25.100: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8698.svc.cluster.local from pod dns-8698/dns-test-ac02cc16-b78a-4f46-a324-c310ed860474: the server could not find the requested resource (get pods dns-test-ac02cc16-b78a-4f46-a324-c310ed860474)
    Sep 20 13:22:25.103: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8698.svc.cluster.local from pod dns-8698/dns-test-ac02cc16-b78a-4f46-a324-c310ed860474: the server could not find the requested resource (get pods dns-test-ac02cc16-b78a-4f46-a324-c310ed860474)
    Sep 20 13:22:25.103: INFO: Lookups using dns-8698/dns-test-ac02cc16-b78a-4f46-a324-c310ed860474 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8698.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8698.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8698.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8698.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8698.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8698.svc.cluster.local jessie_udp@dns-test-service-2.dns-8698.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8698.svc.cluster.local]

    Sep 20 13:22:30.160: INFO: DNS probes using dns-8698/dns-test-ac02cc16-b78a-4f46-a324-c310ed860474 succeeded

    STEP: deleting the pod 09/20/23 13:22:30.16
    STEP: deleting the test headless service 09/20/23 13:22:30.186
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:22:30.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8698" for this suite. 09/20/23 13:22:30.267
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:22:30.281
Sep 20 13:22:30.281: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename resourcequota 09/20/23 13:22:30.282
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:22:30.636
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:22:30.639
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 09/20/23 13:22:30.645
STEP: Counting existing ResourceQuota 09/20/23 13:22:35.651
STEP: Creating a ResourceQuota 09/20/23 13:22:40.657
STEP: Ensuring resource quota status is calculated 09/20/23 13:22:40.669
STEP: Creating a Secret 09/20/23 13:22:42.677
STEP: Ensuring resource quota status captures secret creation 09/20/23 13:22:42.728
STEP: Deleting a secret 09/20/23 13:22:44.735
STEP: Ensuring resource quota status released usage 09/20/23 13:22:44.743
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep 20 13:22:46.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6659" for this suite. 09/20/23 13:22:46.757
------------------------------
â€¢ [SLOW TEST] [16.487 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:22:30.281
    Sep 20 13:22:30.281: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename resourcequota 09/20/23 13:22:30.282
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:22:30.636
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:22:30.639
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 09/20/23 13:22:30.645
    STEP: Counting existing ResourceQuota 09/20/23 13:22:35.651
    STEP: Creating a ResourceQuota 09/20/23 13:22:40.657
    STEP: Ensuring resource quota status is calculated 09/20/23 13:22:40.669
    STEP: Creating a Secret 09/20/23 13:22:42.677
    STEP: Ensuring resource quota status captures secret creation 09/20/23 13:22:42.728
    STEP: Deleting a secret 09/20/23 13:22:44.735
    STEP: Ensuring resource quota status released usage 09/20/23 13:22:44.743
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:22:46.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6659" for this suite. 09/20/23 13:22:46.757
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:22:46.772
Sep 20 13:22:46.772: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename containers 09/20/23 13:22:46.772
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:22:47.279
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:22:47.283
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 09/20/23 13:22:47.289
Sep 20 13:22:47.708: INFO: Waiting up to 5m0s for pod "client-containers-49730100-38ec-4197-a82c-f62baee41f51" in namespace "containers-5183" to be "Succeeded or Failed"
Sep 20 13:22:47.955: INFO: Pod "client-containers-49730100-38ec-4197-a82c-f62baee41f51": Phase="Pending", Reason="", readiness=false. Elapsed: 247.085113ms
Sep 20 13:22:49.974: INFO: Pod "client-containers-49730100-38ec-4197-a82c-f62baee41f51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.2655283s
Sep 20 13:22:51.963: INFO: Pod "client-containers-49730100-38ec-4197-a82c-f62baee41f51": Phase="Pending", Reason="", readiness=false. Elapsed: 4.254987784s
Sep 20 13:22:53.960: INFO: Pod "client-containers-49730100-38ec-4197-a82c-f62baee41f51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.252225098s
STEP: Saw pod success 09/20/23 13:22:53.961
Sep 20 13:22:53.961: INFO: Pod "client-containers-49730100-38ec-4197-a82c-f62baee41f51" satisfied condition "Succeeded or Failed"
Sep 20 13:22:53.964: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod client-containers-49730100-38ec-4197-a82c-f62baee41f51 container agnhost-container: <nil>
STEP: delete the pod 09/20/23 13:22:54.074
Sep 20 13:22:54.091: INFO: Waiting for pod client-containers-49730100-38ec-4197-a82c-f62baee41f51 to disappear
Sep 20 13:22:54.096: INFO: Pod client-containers-49730100-38ec-4197-a82c-f62baee41f51 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Sep 20 13:22:54.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-5183" for this suite. 09/20/23 13:22:54.1
------------------------------
â€¢ [SLOW TEST] [7.338 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:22:46.772
    Sep 20 13:22:46.772: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename containers 09/20/23 13:22:46.772
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:22:47.279
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:22:47.283
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 09/20/23 13:22:47.289
    Sep 20 13:22:47.708: INFO: Waiting up to 5m0s for pod "client-containers-49730100-38ec-4197-a82c-f62baee41f51" in namespace "containers-5183" to be "Succeeded or Failed"
    Sep 20 13:22:47.955: INFO: Pod "client-containers-49730100-38ec-4197-a82c-f62baee41f51": Phase="Pending", Reason="", readiness=false. Elapsed: 247.085113ms
    Sep 20 13:22:49.974: INFO: Pod "client-containers-49730100-38ec-4197-a82c-f62baee41f51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.2655283s
    Sep 20 13:22:51.963: INFO: Pod "client-containers-49730100-38ec-4197-a82c-f62baee41f51": Phase="Pending", Reason="", readiness=false. Elapsed: 4.254987784s
    Sep 20 13:22:53.960: INFO: Pod "client-containers-49730100-38ec-4197-a82c-f62baee41f51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.252225098s
    STEP: Saw pod success 09/20/23 13:22:53.961
    Sep 20 13:22:53.961: INFO: Pod "client-containers-49730100-38ec-4197-a82c-f62baee41f51" satisfied condition "Succeeded or Failed"
    Sep 20 13:22:53.964: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod client-containers-49730100-38ec-4197-a82c-f62baee41f51 container agnhost-container: <nil>
    STEP: delete the pod 09/20/23 13:22:54.074
    Sep 20 13:22:54.091: INFO: Waiting for pod client-containers-49730100-38ec-4197-a82c-f62baee41f51 to disappear
    Sep 20 13:22:54.096: INFO: Pod client-containers-49730100-38ec-4197-a82c-f62baee41f51 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:22:54.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-5183" for this suite. 09/20/23 13:22:54.1
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:22:54.11
Sep 20 13:22:54.110: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename secrets 09/20/23 13:22:54.111
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:22:54.132
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:22:54.137
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-fd324d49-aeb1-4fc5-8f56-d748c39625cc 09/20/23 13:22:54.141
STEP: Creating a pod to test consume secrets 09/20/23 13:22:54.148
Sep 20 13:22:54.156: INFO: Waiting up to 5m0s for pod "pod-secrets-14ebe910-53e4-40f6-95f4-347f943e49a8" in namespace "secrets-393" to be "Succeeded or Failed"
Sep 20 13:22:54.171: INFO: Pod "pod-secrets-14ebe910-53e4-40f6-95f4-347f943e49a8": Phase="Pending", Reason="", readiness=false. Elapsed: 15.051897ms
Sep 20 13:22:56.175: INFO: Pod "pod-secrets-14ebe910-53e4-40f6-95f4-347f943e49a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019297107s
Sep 20 13:22:58.346: INFO: Pod "pod-secrets-14ebe910-53e4-40f6-95f4-347f943e49a8": Phase="Running", Reason="", readiness=false. Elapsed: 4.189683743s
Sep 20 13:23:00.398: INFO: Pod "pod-secrets-14ebe910-53e4-40f6-95f4-347f943e49a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.242008913s
STEP: Saw pod success 09/20/23 13:23:00.398
Sep 20 13:23:00.398: INFO: Pod "pod-secrets-14ebe910-53e4-40f6-95f4-347f943e49a8" satisfied condition "Succeeded or Failed"
Sep 20 13:23:00.406: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-secrets-14ebe910-53e4-40f6-95f4-347f943e49a8 container secret-volume-test: <nil>
STEP: delete the pod 09/20/23 13:23:00.418
Sep 20 13:23:00.652: INFO: Waiting for pod pod-secrets-14ebe910-53e4-40f6-95f4-347f943e49a8 to disappear
Sep 20 13:23:00.659: INFO: Pod pod-secrets-14ebe910-53e4-40f6-95f4-347f943e49a8 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep 20 13:23:00.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-393" for this suite. 09/20/23 13:23:00.666
------------------------------
â€¢ [SLOW TEST] [6.637 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:22:54.11
    Sep 20 13:22:54.110: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename secrets 09/20/23 13:22:54.111
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:22:54.132
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:22:54.137
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-fd324d49-aeb1-4fc5-8f56-d748c39625cc 09/20/23 13:22:54.141
    STEP: Creating a pod to test consume secrets 09/20/23 13:22:54.148
    Sep 20 13:22:54.156: INFO: Waiting up to 5m0s for pod "pod-secrets-14ebe910-53e4-40f6-95f4-347f943e49a8" in namespace "secrets-393" to be "Succeeded or Failed"
    Sep 20 13:22:54.171: INFO: Pod "pod-secrets-14ebe910-53e4-40f6-95f4-347f943e49a8": Phase="Pending", Reason="", readiness=false. Elapsed: 15.051897ms
    Sep 20 13:22:56.175: INFO: Pod "pod-secrets-14ebe910-53e4-40f6-95f4-347f943e49a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019297107s
    Sep 20 13:22:58.346: INFO: Pod "pod-secrets-14ebe910-53e4-40f6-95f4-347f943e49a8": Phase="Running", Reason="", readiness=false. Elapsed: 4.189683743s
    Sep 20 13:23:00.398: INFO: Pod "pod-secrets-14ebe910-53e4-40f6-95f4-347f943e49a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.242008913s
    STEP: Saw pod success 09/20/23 13:23:00.398
    Sep 20 13:23:00.398: INFO: Pod "pod-secrets-14ebe910-53e4-40f6-95f4-347f943e49a8" satisfied condition "Succeeded or Failed"
    Sep 20 13:23:00.406: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-secrets-14ebe910-53e4-40f6-95f4-347f943e49a8 container secret-volume-test: <nil>
    STEP: delete the pod 09/20/23 13:23:00.418
    Sep 20 13:23:00.652: INFO: Waiting for pod pod-secrets-14ebe910-53e4-40f6-95f4-347f943e49a8 to disappear
    Sep 20 13:23:00.659: INFO: Pod pod-secrets-14ebe910-53e4-40f6-95f4-347f943e49a8 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:23:00.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-393" for this suite. 09/20/23 13:23:00.666
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:23:00.75
Sep 20 13:23:00.750: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename kubectl 09/20/23 13:23:00.751
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:23:01.731
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:23:01.736
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 09/20/23 13:23:01.741
Sep 20 13:23:01.741: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Sep 20 13:23:01.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-2270 create -f -'
Sep 20 13:23:02.837: INFO: stderr: ""
Sep 20 13:23:02.838: INFO: stdout: "service/agnhost-replica created\n"
Sep 20 13:23:02.838: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Sep 20 13:23:02.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-2270 create -f -'
Sep 20 13:23:03.835: INFO: stderr: ""
Sep 20 13:23:03.835: INFO: stdout: "service/agnhost-primary created\n"
Sep 20 13:23:03.836: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Sep 20 13:23:03.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-2270 create -f -'
Sep 20 13:23:04.871: INFO: stderr: ""
Sep 20 13:23:04.871: INFO: stdout: "service/frontend created\n"
Sep 20 13:23:04.871: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Sep 20 13:23:04.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-2270 create -f -'
Sep 20 13:23:05.055: INFO: stderr: ""
Sep 20 13:23:05.055: INFO: stdout: "deployment.apps/frontend created\n"
Sep 20 13:23:05.055: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep 20 13:23:05.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-2270 create -f -'
Sep 20 13:23:05.950: INFO: stderr: ""
Sep 20 13:23:05.950: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Sep 20 13:23:05.950: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep 20 13:23:05.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-2270 create -f -'
Sep 20 13:23:06.176: INFO: stderr: ""
Sep 20 13:23:06.176: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 09/20/23 13:23:06.176
Sep 20 13:23:06.176: INFO: Waiting for all frontend pods to be Running.
Sep 20 13:23:11.228: INFO: Waiting for frontend to serve content.
Sep 20 13:23:11.238: INFO: Trying to add a new entry to the guestbook.
Sep 20 13:23:11.247: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 09/20/23 13:23:11.254
Sep 20 13:23:11.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-2270 delete --grace-period=0 --force -f -'
Sep 20 13:23:11.606: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 20 13:23:11.606: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 09/20/23 13:23:11.606
Sep 20 13:23:11.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-2270 delete --grace-period=0 --force -f -'
Sep 20 13:23:12.193: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 20 13:23:12.193: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 09/20/23 13:23:12.193
Sep 20 13:23:12.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-2270 delete --grace-period=0 --force -f -'
Sep 20 13:23:12.384: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 20 13:23:12.384: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 09/20/23 13:23:12.384
Sep 20 13:23:12.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-2270 delete --grace-period=0 --force -f -'
Sep 20 13:23:12.469: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 20 13:23:12.469: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 09/20/23 13:23:12.469
Sep 20 13:23:12.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-2270 delete --grace-period=0 --force -f -'
Sep 20 13:23:12.867: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 20 13:23:12.867: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 09/20/23 13:23:12.867
Sep 20 13:23:12.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-2270 delete --grace-period=0 --force -f -'
Sep 20 13:23:12.940: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 20 13:23:12.940: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep 20 13:23:12.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2270" for this suite. 09/20/23 13:23:12.95
------------------------------
â€¢ [SLOW TEST] [12.302 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:23:00.75
    Sep 20 13:23:00.750: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename kubectl 09/20/23 13:23:00.751
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:23:01.731
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:23:01.736
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 09/20/23 13:23:01.741
    Sep 20 13:23:01.741: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Sep 20 13:23:01.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-2270 create -f -'
    Sep 20 13:23:02.837: INFO: stderr: ""
    Sep 20 13:23:02.838: INFO: stdout: "service/agnhost-replica created\n"
    Sep 20 13:23:02.838: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Sep 20 13:23:02.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-2270 create -f -'
    Sep 20 13:23:03.835: INFO: stderr: ""
    Sep 20 13:23:03.835: INFO: stdout: "service/agnhost-primary created\n"
    Sep 20 13:23:03.836: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Sep 20 13:23:03.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-2270 create -f -'
    Sep 20 13:23:04.871: INFO: stderr: ""
    Sep 20 13:23:04.871: INFO: stdout: "service/frontend created\n"
    Sep 20 13:23:04.871: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Sep 20 13:23:04.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-2270 create -f -'
    Sep 20 13:23:05.055: INFO: stderr: ""
    Sep 20 13:23:05.055: INFO: stdout: "deployment.apps/frontend created\n"
    Sep 20 13:23:05.055: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Sep 20 13:23:05.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-2270 create -f -'
    Sep 20 13:23:05.950: INFO: stderr: ""
    Sep 20 13:23:05.950: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Sep 20 13:23:05.950: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Sep 20 13:23:05.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-2270 create -f -'
    Sep 20 13:23:06.176: INFO: stderr: ""
    Sep 20 13:23:06.176: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 09/20/23 13:23:06.176
    Sep 20 13:23:06.176: INFO: Waiting for all frontend pods to be Running.
    Sep 20 13:23:11.228: INFO: Waiting for frontend to serve content.
    Sep 20 13:23:11.238: INFO: Trying to add a new entry to the guestbook.
    Sep 20 13:23:11.247: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 09/20/23 13:23:11.254
    Sep 20 13:23:11.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-2270 delete --grace-period=0 --force -f -'
    Sep 20 13:23:11.606: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep 20 13:23:11.606: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 09/20/23 13:23:11.606
    Sep 20 13:23:11.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-2270 delete --grace-period=0 --force -f -'
    Sep 20 13:23:12.193: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep 20 13:23:12.193: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 09/20/23 13:23:12.193
    Sep 20 13:23:12.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-2270 delete --grace-period=0 --force -f -'
    Sep 20 13:23:12.384: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep 20 13:23:12.384: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 09/20/23 13:23:12.384
    Sep 20 13:23:12.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-2270 delete --grace-period=0 --force -f -'
    Sep 20 13:23:12.469: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep 20 13:23:12.469: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 09/20/23 13:23:12.469
    Sep 20 13:23:12.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-2270 delete --grace-period=0 --force -f -'
    Sep 20 13:23:12.867: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep 20 13:23:12.867: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 09/20/23 13:23:12.867
    Sep 20 13:23:12.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-2270 delete --grace-period=0 --force -f -'
    Sep 20 13:23:12.940: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep 20 13:23:12.940: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:23:12.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2270" for this suite. 09/20/23 13:23:12.95
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:23:13.052
Sep 20 13:23:13.052: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename watch 09/20/23 13:23:13.053
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:23:13.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:23:13.08
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 09/20/23 13:23:13.085
STEP: starting a background goroutine to produce watch events 09/20/23 13:23:13.091
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 09/20/23 13:23:13.091
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Sep 20 13:23:15.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-7139" for this suite. 09/20/23 13:23:15.929
------------------------------
â€¢ [2.911 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:23:13.052
    Sep 20 13:23:13.052: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename watch 09/20/23 13:23:13.053
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:23:13.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:23:13.08
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 09/20/23 13:23:13.085
    STEP: starting a background goroutine to produce watch events 09/20/23 13:23:13.091
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 09/20/23 13:23:13.091
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:23:15.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-7139" for this suite. 09/20/23 13:23:15.929
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:23:15.964
Sep 20 13:23:15.964: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename endpointslice 09/20/23 13:23:15.965
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:23:16.322
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:23:16.325
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 09/20/23 13:23:21.972
STEP: referencing matching pods with named port 09/20/23 13:23:26.983
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 09/20/23 13:23:31.996
STEP: recreating EndpointSlices after they've been deleted 09/20/23 13:23:37.016
Sep 20 13:23:37.056: INFO: EndpointSlice for Service endpointslice-184/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Sep 20 13:23:47.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-184" for this suite. 09/20/23 13:23:47.085
------------------------------
â€¢ [SLOW TEST] [31.136 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:23:15.964
    Sep 20 13:23:15.964: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename endpointslice 09/20/23 13:23:15.965
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:23:16.322
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:23:16.325
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 09/20/23 13:23:21.972
    STEP: referencing matching pods with named port 09/20/23 13:23:26.983
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 09/20/23 13:23:31.996
    STEP: recreating EndpointSlices after they've been deleted 09/20/23 13:23:37.016
    Sep 20 13:23:37.056: INFO: EndpointSlice for Service endpointslice-184/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:23:47.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-184" for this suite. 09/20/23 13:23:47.085
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:23:47.101
Sep 20 13:23:47.101: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename sched-preemption 09/20/23 13:23:47.102
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:23:47.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:23:47.418
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Sep 20 13:23:47.549: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 20 13:24:47.589: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:24:47.594
Sep 20 13:24:47.594: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename sched-preemption-path 09/20/23 13:24:47.595
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:24:48.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:24:48.164
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 09/20/23 13:24:48.262
STEP: Trying to launch a pod without a label to get a node which can launch it. 09/20/23 13:24:48.262
Sep 20 13:24:48.280: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-4382" to be "running"
Sep 20 13:24:48.292: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 12.403107ms
Sep 20 13:24:50.303: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023123224s
Sep 20 13:24:52.299: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01899602s
Sep 20 13:24:54.297: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 6.017399984s
Sep 20 13:24:54.297: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 09/20/23 13:24:54.3
Sep 20 13:24:54.536: INFO: found a healthy node: mycluster-ww3cg64etuwi-node-1
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
Sep 20 13:25:07.638: INFO: pods created so far: [1 1 1]
Sep 20 13:25:07.638: INFO: length of pods created so far: 3
Sep 20 13:25:13.673: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Sep 20 13:25:20.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Sep 20 13:25:21.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-4382" for this suite. 09/20/23 13:25:22.004
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-1220" for this suite. 09/20/23 13:25:22.011
------------------------------
â€¢ [SLOW TEST] [94.917 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:23:47.101
    Sep 20 13:23:47.101: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename sched-preemption 09/20/23 13:23:47.102
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:23:47.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:23:47.418
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Sep 20 13:23:47.549: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep 20 13:24:47.589: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:24:47.594
    Sep 20 13:24:47.594: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename sched-preemption-path 09/20/23 13:24:47.595
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:24:48.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:24:48.164
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 09/20/23 13:24:48.262
    STEP: Trying to launch a pod without a label to get a node which can launch it. 09/20/23 13:24:48.262
    Sep 20 13:24:48.280: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-4382" to be "running"
    Sep 20 13:24:48.292: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 12.403107ms
    Sep 20 13:24:50.303: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023123224s
    Sep 20 13:24:52.299: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01899602s
    Sep 20 13:24:54.297: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 6.017399984s
    Sep 20 13:24:54.297: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 09/20/23 13:24:54.3
    Sep 20 13:24:54.536: INFO: found a healthy node: mycluster-ww3cg64etuwi-node-1
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    Sep 20 13:25:07.638: INFO: pods created so far: [1 1 1]
    Sep 20 13:25:07.638: INFO: length of pods created so far: 3
    Sep 20 13:25:13.673: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:25:20.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:25:21.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-4382" for this suite. 09/20/23 13:25:22.004
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-1220" for this suite. 09/20/23 13:25:22.011
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:25:22.019
Sep 20 13:25:22.019: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename discovery 09/20/23 13:25:22.02
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:25:22.424
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:25:22.433
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 09/20/23 13:25:22.442
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Sep 20 13:25:22.563: INFO: Checking APIGroup: apiregistration.k8s.io
Sep 20 13:25:22.565: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Sep 20 13:25:22.565: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Sep 20 13:25:22.566: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Sep 20 13:25:22.566: INFO: Checking APIGroup: apps
Sep 20 13:25:22.568: INFO: PreferredVersion.GroupVersion: apps/v1
Sep 20 13:25:22.568: INFO: Versions found [{apps/v1 v1}]
Sep 20 13:25:22.568: INFO: apps/v1 matches apps/v1
Sep 20 13:25:22.568: INFO: Checking APIGroup: events.k8s.io
Sep 20 13:25:22.571: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Sep 20 13:25:22.571: INFO: Versions found [{events.k8s.io/v1 v1}]
Sep 20 13:25:22.571: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Sep 20 13:25:22.571: INFO: Checking APIGroup: authentication.k8s.io
Sep 20 13:25:22.574: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Sep 20 13:25:22.574: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Sep 20 13:25:22.574: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Sep 20 13:25:22.574: INFO: Checking APIGroup: authorization.k8s.io
Sep 20 13:25:22.577: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Sep 20 13:25:22.577: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Sep 20 13:25:22.577: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Sep 20 13:25:22.577: INFO: Checking APIGroup: autoscaling
Sep 20 13:25:22.579: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Sep 20 13:25:22.579: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Sep 20 13:25:22.579: INFO: autoscaling/v2 matches autoscaling/v2
Sep 20 13:25:22.579: INFO: Checking APIGroup: batch
Sep 20 13:25:22.581: INFO: PreferredVersion.GroupVersion: batch/v1
Sep 20 13:25:22.581: INFO: Versions found [{batch/v1 v1}]
Sep 20 13:25:22.581: INFO: batch/v1 matches batch/v1
Sep 20 13:25:22.581: INFO: Checking APIGroup: certificates.k8s.io
Sep 20 13:25:22.583: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Sep 20 13:25:22.583: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Sep 20 13:25:22.583: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Sep 20 13:25:22.583: INFO: Checking APIGroup: networking.k8s.io
Sep 20 13:25:22.586: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Sep 20 13:25:22.586: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1alpha1 v1alpha1}]
Sep 20 13:25:22.586: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Sep 20 13:25:22.586: INFO: Checking APIGroup: policy
Sep 20 13:25:22.587: INFO: PreferredVersion.GroupVersion: policy/v1
Sep 20 13:25:22.587: INFO: Versions found [{policy/v1 v1}]
Sep 20 13:25:22.587: INFO: policy/v1 matches policy/v1
Sep 20 13:25:22.587: INFO: Checking APIGroup: rbac.authorization.k8s.io
Sep 20 13:25:22.589: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Sep 20 13:25:22.589: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Sep 20 13:25:22.589: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Sep 20 13:25:22.589: INFO: Checking APIGroup: storage.k8s.io
Sep 20 13:25:22.591: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Sep 20 13:25:22.591: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Sep 20 13:25:22.591: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Sep 20 13:25:22.591: INFO: Checking APIGroup: admissionregistration.k8s.io
Sep 20 13:25:22.592: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Sep 20 13:25:22.592: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1alpha1 v1alpha1}]
Sep 20 13:25:22.592: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Sep 20 13:25:22.592: INFO: Checking APIGroup: apiextensions.k8s.io
Sep 20 13:25:22.594: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Sep 20 13:25:22.594: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Sep 20 13:25:22.594: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Sep 20 13:25:22.594: INFO: Checking APIGroup: scheduling.k8s.io
Sep 20 13:25:22.596: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Sep 20 13:25:22.596: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Sep 20 13:25:22.596: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Sep 20 13:25:22.596: INFO: Checking APIGroup: coordination.k8s.io
Sep 20 13:25:22.599: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Sep 20 13:25:22.600: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Sep 20 13:25:22.600: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Sep 20 13:25:22.600: INFO: Checking APIGroup: node.k8s.io
Sep 20 13:25:22.602: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Sep 20 13:25:22.602: INFO: Versions found [{node.k8s.io/v1 v1}]
Sep 20 13:25:22.602: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Sep 20 13:25:22.602: INFO: Checking APIGroup: discovery.k8s.io
Sep 20 13:25:22.605: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Sep 20 13:25:22.605: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Sep 20 13:25:22.605: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Sep 20 13:25:22.605: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Sep 20 13:25:22.607: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Sep 20 13:25:22.607: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Sep 20 13:25:22.607: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Sep 20 13:25:22.607: INFO: Checking APIGroup: internal.apiserver.k8s.io
Sep 20 13:25:22.609: INFO: PreferredVersion.GroupVersion: internal.apiserver.k8s.io/v1alpha1
Sep 20 13:25:22.609: INFO: Versions found [{internal.apiserver.k8s.io/v1alpha1 v1alpha1}]
Sep 20 13:25:22.609: INFO: internal.apiserver.k8s.io/v1alpha1 matches internal.apiserver.k8s.io/v1alpha1
Sep 20 13:25:22.609: INFO: Checking APIGroup: resource.k8s.io
Sep 20 13:25:22.611: INFO: PreferredVersion.GroupVersion: resource.k8s.io/v1alpha1
Sep 20 13:25:22.611: INFO: Versions found [{resource.k8s.io/v1alpha1 v1alpha1}]
Sep 20 13:25:22.611: INFO: resource.k8s.io/v1alpha1 matches resource.k8s.io/v1alpha1
Sep 20 13:25:22.611: INFO: Checking APIGroup: metrics.k8s.io
Sep 20 13:25:22.613: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Sep 20 13:25:22.613: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Sep 20 13:25:22.613: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Sep 20 13:25:22.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-2162" for this suite. 09/20/23 13:25:22.62
------------------------------
â€¢ [1.239 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:25:22.019
    Sep 20 13:25:22.019: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename discovery 09/20/23 13:25:22.02
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:25:22.424
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:25:22.433
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 09/20/23 13:25:22.442
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Sep 20 13:25:22.563: INFO: Checking APIGroup: apiregistration.k8s.io
    Sep 20 13:25:22.565: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Sep 20 13:25:22.565: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Sep 20 13:25:22.566: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Sep 20 13:25:22.566: INFO: Checking APIGroup: apps
    Sep 20 13:25:22.568: INFO: PreferredVersion.GroupVersion: apps/v1
    Sep 20 13:25:22.568: INFO: Versions found [{apps/v1 v1}]
    Sep 20 13:25:22.568: INFO: apps/v1 matches apps/v1
    Sep 20 13:25:22.568: INFO: Checking APIGroup: events.k8s.io
    Sep 20 13:25:22.571: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Sep 20 13:25:22.571: INFO: Versions found [{events.k8s.io/v1 v1}]
    Sep 20 13:25:22.571: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Sep 20 13:25:22.571: INFO: Checking APIGroup: authentication.k8s.io
    Sep 20 13:25:22.574: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Sep 20 13:25:22.574: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Sep 20 13:25:22.574: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Sep 20 13:25:22.574: INFO: Checking APIGroup: authorization.k8s.io
    Sep 20 13:25:22.577: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Sep 20 13:25:22.577: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Sep 20 13:25:22.577: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Sep 20 13:25:22.577: INFO: Checking APIGroup: autoscaling
    Sep 20 13:25:22.579: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Sep 20 13:25:22.579: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Sep 20 13:25:22.579: INFO: autoscaling/v2 matches autoscaling/v2
    Sep 20 13:25:22.579: INFO: Checking APIGroup: batch
    Sep 20 13:25:22.581: INFO: PreferredVersion.GroupVersion: batch/v1
    Sep 20 13:25:22.581: INFO: Versions found [{batch/v1 v1}]
    Sep 20 13:25:22.581: INFO: batch/v1 matches batch/v1
    Sep 20 13:25:22.581: INFO: Checking APIGroup: certificates.k8s.io
    Sep 20 13:25:22.583: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Sep 20 13:25:22.583: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Sep 20 13:25:22.583: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Sep 20 13:25:22.583: INFO: Checking APIGroup: networking.k8s.io
    Sep 20 13:25:22.586: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Sep 20 13:25:22.586: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1alpha1 v1alpha1}]
    Sep 20 13:25:22.586: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Sep 20 13:25:22.586: INFO: Checking APIGroup: policy
    Sep 20 13:25:22.587: INFO: PreferredVersion.GroupVersion: policy/v1
    Sep 20 13:25:22.587: INFO: Versions found [{policy/v1 v1}]
    Sep 20 13:25:22.587: INFO: policy/v1 matches policy/v1
    Sep 20 13:25:22.587: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Sep 20 13:25:22.589: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Sep 20 13:25:22.589: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Sep 20 13:25:22.589: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Sep 20 13:25:22.589: INFO: Checking APIGroup: storage.k8s.io
    Sep 20 13:25:22.591: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Sep 20 13:25:22.591: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Sep 20 13:25:22.591: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Sep 20 13:25:22.591: INFO: Checking APIGroup: admissionregistration.k8s.io
    Sep 20 13:25:22.592: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Sep 20 13:25:22.592: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1alpha1 v1alpha1}]
    Sep 20 13:25:22.592: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Sep 20 13:25:22.592: INFO: Checking APIGroup: apiextensions.k8s.io
    Sep 20 13:25:22.594: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Sep 20 13:25:22.594: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Sep 20 13:25:22.594: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Sep 20 13:25:22.594: INFO: Checking APIGroup: scheduling.k8s.io
    Sep 20 13:25:22.596: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Sep 20 13:25:22.596: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Sep 20 13:25:22.596: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Sep 20 13:25:22.596: INFO: Checking APIGroup: coordination.k8s.io
    Sep 20 13:25:22.599: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Sep 20 13:25:22.600: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Sep 20 13:25:22.600: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Sep 20 13:25:22.600: INFO: Checking APIGroup: node.k8s.io
    Sep 20 13:25:22.602: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Sep 20 13:25:22.602: INFO: Versions found [{node.k8s.io/v1 v1}]
    Sep 20 13:25:22.602: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Sep 20 13:25:22.602: INFO: Checking APIGroup: discovery.k8s.io
    Sep 20 13:25:22.605: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Sep 20 13:25:22.605: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Sep 20 13:25:22.605: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Sep 20 13:25:22.605: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Sep 20 13:25:22.607: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Sep 20 13:25:22.607: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Sep 20 13:25:22.607: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Sep 20 13:25:22.607: INFO: Checking APIGroup: internal.apiserver.k8s.io
    Sep 20 13:25:22.609: INFO: PreferredVersion.GroupVersion: internal.apiserver.k8s.io/v1alpha1
    Sep 20 13:25:22.609: INFO: Versions found [{internal.apiserver.k8s.io/v1alpha1 v1alpha1}]
    Sep 20 13:25:22.609: INFO: internal.apiserver.k8s.io/v1alpha1 matches internal.apiserver.k8s.io/v1alpha1
    Sep 20 13:25:22.609: INFO: Checking APIGroup: resource.k8s.io
    Sep 20 13:25:22.611: INFO: PreferredVersion.GroupVersion: resource.k8s.io/v1alpha1
    Sep 20 13:25:22.611: INFO: Versions found [{resource.k8s.io/v1alpha1 v1alpha1}]
    Sep 20 13:25:22.611: INFO: resource.k8s.io/v1alpha1 matches resource.k8s.io/v1alpha1
    Sep 20 13:25:22.611: INFO: Checking APIGroup: metrics.k8s.io
    Sep 20 13:25:22.613: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Sep 20 13:25:22.613: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Sep 20 13:25:22.613: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:25:22.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-2162" for this suite. 09/20/23 13:25:22.62
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:25:23.264
Sep 20 13:25:23.264: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename container-probe 09/20/23 13:25:23.265
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:25:23.648
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:25:23.653
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-178e6c14-ee18-48da-b389-278935cab26b in namespace container-probe-1990 09/20/23 13:25:23.658
Sep 20 13:25:23.670: INFO: Waiting up to 5m0s for pod "test-webserver-178e6c14-ee18-48da-b389-278935cab26b" in namespace "container-probe-1990" to be "not pending"
Sep 20 13:25:23.674: INFO: Pod "test-webserver-178e6c14-ee18-48da-b389-278935cab26b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.355486ms
Sep 20 13:25:26.407: INFO: Pod "test-webserver-178e6c14-ee18-48da-b389-278935cab26b": Phase="Running", Reason="", readiness=true. Elapsed: 2.736810899s
Sep 20 13:25:26.407: INFO: Pod "test-webserver-178e6c14-ee18-48da-b389-278935cab26b" satisfied condition "not pending"
Sep 20 13:25:26.407: INFO: Started pod test-webserver-178e6c14-ee18-48da-b389-278935cab26b in namespace container-probe-1990
STEP: checking the pod's current state and verifying that restartCount is present 09/20/23 13:25:26.407
Sep 20 13:25:26.413: INFO: Initial restart count of pod test-webserver-178e6c14-ee18-48da-b389-278935cab26b is 0
STEP: deleting the pod 09/20/23 13:29:28.177
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep 20 13:29:28.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1990" for this suite. 09/20/23 13:29:28.463
------------------------------
â€¢ [SLOW TEST] [245.209 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:25:23.264
    Sep 20 13:25:23.264: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename container-probe 09/20/23 13:25:23.265
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:25:23.648
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:25:23.653
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-178e6c14-ee18-48da-b389-278935cab26b in namespace container-probe-1990 09/20/23 13:25:23.658
    Sep 20 13:25:23.670: INFO: Waiting up to 5m0s for pod "test-webserver-178e6c14-ee18-48da-b389-278935cab26b" in namespace "container-probe-1990" to be "not pending"
    Sep 20 13:25:23.674: INFO: Pod "test-webserver-178e6c14-ee18-48da-b389-278935cab26b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.355486ms
    Sep 20 13:25:26.407: INFO: Pod "test-webserver-178e6c14-ee18-48da-b389-278935cab26b": Phase="Running", Reason="", readiness=true. Elapsed: 2.736810899s
    Sep 20 13:25:26.407: INFO: Pod "test-webserver-178e6c14-ee18-48da-b389-278935cab26b" satisfied condition "not pending"
    Sep 20 13:25:26.407: INFO: Started pod test-webserver-178e6c14-ee18-48da-b389-278935cab26b in namespace container-probe-1990
    STEP: checking the pod's current state and verifying that restartCount is present 09/20/23 13:25:26.407
    Sep 20 13:25:26.413: INFO: Initial restart count of pod test-webserver-178e6c14-ee18-48da-b389-278935cab26b is 0
    STEP: deleting the pod 09/20/23 13:29:28.177
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:29:28.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1990" for this suite. 09/20/23 13:29:28.463
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:29:28.481
Sep 20 13:29:28.481: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename projected 09/20/23 13:29:28.482
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:29:28.505
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:29:28.509
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 09/20/23 13:29:28.513
Sep 20 13:29:28.529: INFO: Waiting up to 5m0s for pod "downwardapi-volume-984fd157-8053-4580-b3e1-37e18bfb3c8d" in namespace "projected-8470" to be "Succeeded or Failed"
Sep 20 13:29:28.537: INFO: Pod "downwardapi-volume-984fd157-8053-4580-b3e1-37e18bfb3c8d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.776581ms
Sep 20 13:29:30.547: INFO: Pod "downwardapi-volume-984fd157-8053-4580-b3e1-37e18bfb3c8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018406489s
Sep 20 13:29:32.542: INFO: Pod "downwardapi-volume-984fd157-8053-4580-b3e1-37e18bfb3c8d": Phase="Running", Reason="", readiness=false. Elapsed: 4.013508121s
Sep 20 13:29:34.682: INFO: Pod "downwardapi-volume-984fd157-8053-4580-b3e1-37e18bfb3c8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.153312326s
STEP: Saw pod success 09/20/23 13:29:34.682
Sep 20 13:29:34.682: INFO: Pod "downwardapi-volume-984fd157-8053-4580-b3e1-37e18bfb3c8d" satisfied condition "Succeeded or Failed"
Sep 20 13:29:34.690: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downwardapi-volume-984fd157-8053-4580-b3e1-37e18bfb3c8d container client-container: <nil>
STEP: delete the pod 09/20/23 13:29:34.76
Sep 20 13:29:34.779: INFO: Waiting for pod downwardapi-volume-984fd157-8053-4580-b3e1-37e18bfb3c8d to disappear
Sep 20 13:29:34.786: INFO: Pod downwardapi-volume-984fd157-8053-4580-b3e1-37e18bfb3c8d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep 20 13:29:34.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8470" for this suite. 09/20/23 13:29:34.792
------------------------------
â€¢ [SLOW TEST] [6.321 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:29:28.481
    Sep 20 13:29:28.481: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename projected 09/20/23 13:29:28.482
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:29:28.505
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:29:28.509
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 09/20/23 13:29:28.513
    Sep 20 13:29:28.529: INFO: Waiting up to 5m0s for pod "downwardapi-volume-984fd157-8053-4580-b3e1-37e18bfb3c8d" in namespace "projected-8470" to be "Succeeded or Failed"
    Sep 20 13:29:28.537: INFO: Pod "downwardapi-volume-984fd157-8053-4580-b3e1-37e18bfb3c8d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.776581ms
    Sep 20 13:29:30.547: INFO: Pod "downwardapi-volume-984fd157-8053-4580-b3e1-37e18bfb3c8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018406489s
    Sep 20 13:29:32.542: INFO: Pod "downwardapi-volume-984fd157-8053-4580-b3e1-37e18bfb3c8d": Phase="Running", Reason="", readiness=false. Elapsed: 4.013508121s
    Sep 20 13:29:34.682: INFO: Pod "downwardapi-volume-984fd157-8053-4580-b3e1-37e18bfb3c8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.153312326s
    STEP: Saw pod success 09/20/23 13:29:34.682
    Sep 20 13:29:34.682: INFO: Pod "downwardapi-volume-984fd157-8053-4580-b3e1-37e18bfb3c8d" satisfied condition "Succeeded or Failed"
    Sep 20 13:29:34.690: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downwardapi-volume-984fd157-8053-4580-b3e1-37e18bfb3c8d container client-container: <nil>
    STEP: delete the pod 09/20/23 13:29:34.76
    Sep 20 13:29:34.779: INFO: Waiting for pod downwardapi-volume-984fd157-8053-4580-b3e1-37e18bfb3c8d to disappear
    Sep 20 13:29:34.786: INFO: Pod downwardapi-volume-984fd157-8053-4580-b3e1-37e18bfb3c8d no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:29:34.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8470" for this suite. 09/20/23 13:29:34.792
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:29:34.804
Sep 20 13:29:34.804: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename container-lifecycle-hook 09/20/23 13:29:34.805
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:29:34.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:29:34.905
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 09/20/23 13:29:34.915
Sep 20 13:29:34.936: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3939" to be "running and ready"
Sep 20 13:29:34.948: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 11.608419ms
Sep 20 13:29:34.948: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:29:36.985: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04883902s
Sep 20 13:29:36.986: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:29:38.954: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.017491396s
Sep 20 13:29:38.954: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Sep 20 13:29:38.954: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 09/20/23 13:29:38.957
Sep 20 13:29:39.136: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-3939" to be "running and ready"
Sep 20 13:29:39.166: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 30.410221ms
Sep 20 13:29:39.166: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:29:41.171: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035513327s
Sep 20 13:29:41.171: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:29:43.190: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.053756407s
Sep 20 13:29:43.190: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Sep 20 13:29:43.190: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 09/20/23 13:29:43.2
Sep 20 13:29:44.548: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 20 13:29:44.558: INFO: Pod pod-with-prestop-http-hook still exists
Sep 20 13:29:46.558: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 20 13:29:46.582: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 09/20/23 13:29:46.583
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Sep 20 13:29:46.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-3939" for this suite. 09/20/23 13:29:46.804
------------------------------
â€¢ [SLOW TEST] [12.445 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:29:34.804
    Sep 20 13:29:34.804: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename container-lifecycle-hook 09/20/23 13:29:34.805
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:29:34.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:29:34.905
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 09/20/23 13:29:34.915
    Sep 20 13:29:34.936: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3939" to be "running and ready"
    Sep 20 13:29:34.948: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 11.608419ms
    Sep 20 13:29:34.948: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:29:36.985: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04883902s
    Sep 20 13:29:36.986: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:29:38.954: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.017491396s
    Sep 20 13:29:38.954: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Sep 20 13:29:38.954: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 09/20/23 13:29:38.957
    Sep 20 13:29:39.136: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-3939" to be "running and ready"
    Sep 20 13:29:39.166: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 30.410221ms
    Sep 20 13:29:39.166: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:29:41.171: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035513327s
    Sep 20 13:29:41.171: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:29:43.190: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.053756407s
    Sep 20 13:29:43.190: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Sep 20 13:29:43.190: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 09/20/23 13:29:43.2
    Sep 20 13:29:44.548: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Sep 20 13:29:44.558: INFO: Pod pod-with-prestop-http-hook still exists
    Sep 20 13:29:46.558: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Sep 20 13:29:46.582: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 09/20/23 13:29:46.583
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:29:46.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-3939" for this suite. 09/20/23 13:29:46.804
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:29:47.25
Sep 20 13:29:47.250: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename emptydir 09/20/23 13:29:47.251
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:29:47.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:29:47.798
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 09/20/23 13:29:48.095
Sep 20 13:29:48.113: INFO: Waiting up to 5m0s for pod "pod-adea712e-76b9-4102-9487-ea20617a54c9" in namespace "emptydir-927" to be "Succeeded or Failed"
Sep 20 13:29:48.119: INFO: Pod "pod-adea712e-76b9-4102-9487-ea20617a54c9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.696301ms
Sep 20 13:29:50.122: INFO: Pod "pod-adea712e-76b9-4102-9487-ea20617a54c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009029552s
Sep 20 13:29:52.154: INFO: Pod "pod-adea712e-76b9-4102-9487-ea20617a54c9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040506915s
Sep 20 13:29:54.133: INFO: Pod "pod-adea712e-76b9-4102-9487-ea20617a54c9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019578889s
Sep 20 13:29:56.126: INFO: Pod "pod-adea712e-76b9-4102-9487-ea20617a54c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.012612326s
STEP: Saw pod success 09/20/23 13:29:56.126
Sep 20 13:29:56.126: INFO: Pod "pod-adea712e-76b9-4102-9487-ea20617a54c9" satisfied condition "Succeeded or Failed"
Sep 20 13:29:56.131: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-adea712e-76b9-4102-9487-ea20617a54c9 container test-container: <nil>
STEP: delete the pod 09/20/23 13:29:56.142
Sep 20 13:29:56.361: INFO: Waiting for pod pod-adea712e-76b9-4102-9487-ea20617a54c9 to disappear
Sep 20 13:29:56.367: INFO: Pod pod-adea712e-76b9-4102-9487-ea20617a54c9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep 20 13:29:56.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-927" for this suite. 09/20/23 13:29:56.374
------------------------------
â€¢ [SLOW TEST] [9.137 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:29:47.25
    Sep 20 13:29:47.250: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename emptydir 09/20/23 13:29:47.251
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:29:47.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:29:47.798
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 09/20/23 13:29:48.095
    Sep 20 13:29:48.113: INFO: Waiting up to 5m0s for pod "pod-adea712e-76b9-4102-9487-ea20617a54c9" in namespace "emptydir-927" to be "Succeeded or Failed"
    Sep 20 13:29:48.119: INFO: Pod "pod-adea712e-76b9-4102-9487-ea20617a54c9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.696301ms
    Sep 20 13:29:50.122: INFO: Pod "pod-adea712e-76b9-4102-9487-ea20617a54c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009029552s
    Sep 20 13:29:52.154: INFO: Pod "pod-adea712e-76b9-4102-9487-ea20617a54c9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040506915s
    Sep 20 13:29:54.133: INFO: Pod "pod-adea712e-76b9-4102-9487-ea20617a54c9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019578889s
    Sep 20 13:29:56.126: INFO: Pod "pod-adea712e-76b9-4102-9487-ea20617a54c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.012612326s
    STEP: Saw pod success 09/20/23 13:29:56.126
    Sep 20 13:29:56.126: INFO: Pod "pod-adea712e-76b9-4102-9487-ea20617a54c9" satisfied condition "Succeeded or Failed"
    Sep 20 13:29:56.131: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-adea712e-76b9-4102-9487-ea20617a54c9 container test-container: <nil>
    STEP: delete the pod 09/20/23 13:29:56.142
    Sep 20 13:29:56.361: INFO: Waiting for pod pod-adea712e-76b9-4102-9487-ea20617a54c9 to disappear
    Sep 20 13:29:56.367: INFO: Pod pod-adea712e-76b9-4102-9487-ea20617a54c9 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:29:56.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-927" for this suite. 09/20/23 13:29:56.374
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:29:56.387
Sep 20 13:29:56.387: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename container-runtime 09/20/23 13:29:56.388
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:29:56.411
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:29:56.416
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 09/20/23 13:29:56.441
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 09/20/23 13:30:19.097
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 09/20/23 13:30:19.106
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 09/20/23 13:30:19.113
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 09/20/23 13:30:19.113
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 09/20/23 13:30:19.886
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 09/20/23 13:30:24.942
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 09/20/23 13:30:28.405
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 09/20/23 13:30:28.591
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 09/20/23 13:30:28.591
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 09/20/23 13:30:28.741
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 09/20/23 13:30:29.753
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 09/20/23 13:30:35.408
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 09/20/23 13:30:35.56
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 09/20/23 13:30:35.56
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Sep 20 13:30:38.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-34" for this suite. 09/20/23 13:30:38.084
------------------------------
â€¢ [SLOW TEST] [41.999 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:29:56.387
    Sep 20 13:29:56.387: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename container-runtime 09/20/23 13:29:56.388
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:29:56.411
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:29:56.416
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 09/20/23 13:29:56.441
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 09/20/23 13:30:19.097
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 09/20/23 13:30:19.106
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 09/20/23 13:30:19.113
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 09/20/23 13:30:19.113
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 09/20/23 13:30:19.886
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 09/20/23 13:30:24.942
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 09/20/23 13:30:28.405
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 09/20/23 13:30:28.591
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 09/20/23 13:30:28.591
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 09/20/23 13:30:28.741
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 09/20/23 13:30:29.753
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 09/20/23 13:30:35.408
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 09/20/23 13:30:35.56
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 09/20/23 13:30:35.56
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:30:38.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-34" for this suite. 09/20/23 13:30:38.084
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:30:38.387
Sep 20 13:30:38.387: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename kubectl 09/20/23 13:30:38.387
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:30:38.421
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:30:38.424
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 09/20/23 13:30:38.428
Sep 20 13:30:38.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-787 api-versions'
Sep 20 13:30:38.526: INFO: stderr: ""
Sep 20 13:30:38.526: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1alpha1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\ninternal.apiserver.k8s.io/v1alpha1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1alpha1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nresource.k8s.io/v1alpha1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep 20 13:30:38.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-787" for this suite. 09/20/23 13:30:38.532
------------------------------
â€¢ [0.154 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:30:38.387
    Sep 20 13:30:38.387: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename kubectl 09/20/23 13:30:38.387
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:30:38.421
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:30:38.424
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 09/20/23 13:30:38.428
    Sep 20 13:30:38.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-787 api-versions'
    Sep 20 13:30:38.526: INFO: stderr: ""
    Sep 20 13:30:38.526: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1alpha1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\ninternal.apiserver.k8s.io/v1alpha1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1alpha1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nresource.k8s.io/v1alpha1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:30:38.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-787" for this suite. 09/20/23 13:30:38.532
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:30:38.541
Sep 20 13:30:38.541: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename runtimeclass 09/20/23 13:30:38.542
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:30:38.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:30:38.673
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Sep 20 13:30:39.178: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1797 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Sep 20 13:30:39.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-1797" for this suite. 09/20/23 13:30:40.557
------------------------------
â€¢ [2.183 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:30:38.541
    Sep 20 13:30:38.541: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename runtimeclass 09/20/23 13:30:38.542
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:30:38.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:30:38.673
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Sep 20 13:30:39.178: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1797 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:30:39.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-1797" for this suite. 09/20/23 13:30:40.557
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:30:40.726
Sep 20 13:30:40.726: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename projected 09/20/23 13:30:40.727
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:30:40.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:30:40.75
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 09/20/23 13:30:40.756
Sep 20 13:30:40.839: INFO: Waiting up to 5m0s for pod "downwardapi-volume-35928e87-b112-4ba7-bbed-ddbb9401a8c1" in namespace "projected-1423" to be "Succeeded or Failed"
Sep 20 13:30:40.859: INFO: Pod "downwardapi-volume-35928e87-b112-4ba7-bbed-ddbb9401a8c1": Phase="Pending", Reason="", readiness=false. Elapsed: 20.570753ms
Sep 20 13:30:42.868: INFO: Pod "downwardapi-volume-35928e87-b112-4ba7-bbed-ddbb9401a8c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028809066s
Sep 20 13:30:44.878: INFO: Pod "downwardapi-volume-35928e87-b112-4ba7-bbed-ddbb9401a8c1": Phase="Running", Reason="", readiness=false. Elapsed: 4.03949676s
Sep 20 13:30:48.180: INFO: Pod "downwardapi-volume-35928e87-b112-4ba7-bbed-ddbb9401a8c1": Phase="Running", Reason="", readiness=false. Elapsed: 7.34080453s
Sep 20 13:30:48.865: INFO: Pod "downwardapi-volume-35928e87-b112-4ba7-bbed-ddbb9401a8c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.026515558s
STEP: Saw pod success 09/20/23 13:30:48.865
Sep 20 13:30:48.866: INFO: Pod "downwardapi-volume-35928e87-b112-4ba7-bbed-ddbb9401a8c1" satisfied condition "Succeeded or Failed"
Sep 20 13:30:48.870: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downwardapi-volume-35928e87-b112-4ba7-bbed-ddbb9401a8c1 container client-container: <nil>
STEP: delete the pod 09/20/23 13:30:48.878
Sep 20 13:30:48.915: INFO: Waiting for pod downwardapi-volume-35928e87-b112-4ba7-bbed-ddbb9401a8c1 to disappear
Sep 20 13:30:48.921: INFO: Pod downwardapi-volume-35928e87-b112-4ba7-bbed-ddbb9401a8c1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep 20 13:30:48.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1423" for this suite. 09/20/23 13:30:48.929
------------------------------
â€¢ [SLOW TEST] [8.214 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:30:40.726
    Sep 20 13:30:40.726: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename projected 09/20/23 13:30:40.727
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:30:40.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:30:40.75
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 09/20/23 13:30:40.756
    Sep 20 13:30:40.839: INFO: Waiting up to 5m0s for pod "downwardapi-volume-35928e87-b112-4ba7-bbed-ddbb9401a8c1" in namespace "projected-1423" to be "Succeeded or Failed"
    Sep 20 13:30:40.859: INFO: Pod "downwardapi-volume-35928e87-b112-4ba7-bbed-ddbb9401a8c1": Phase="Pending", Reason="", readiness=false. Elapsed: 20.570753ms
    Sep 20 13:30:42.868: INFO: Pod "downwardapi-volume-35928e87-b112-4ba7-bbed-ddbb9401a8c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028809066s
    Sep 20 13:30:44.878: INFO: Pod "downwardapi-volume-35928e87-b112-4ba7-bbed-ddbb9401a8c1": Phase="Running", Reason="", readiness=false. Elapsed: 4.03949676s
    Sep 20 13:30:48.180: INFO: Pod "downwardapi-volume-35928e87-b112-4ba7-bbed-ddbb9401a8c1": Phase="Running", Reason="", readiness=false. Elapsed: 7.34080453s
    Sep 20 13:30:48.865: INFO: Pod "downwardapi-volume-35928e87-b112-4ba7-bbed-ddbb9401a8c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.026515558s
    STEP: Saw pod success 09/20/23 13:30:48.865
    Sep 20 13:30:48.866: INFO: Pod "downwardapi-volume-35928e87-b112-4ba7-bbed-ddbb9401a8c1" satisfied condition "Succeeded or Failed"
    Sep 20 13:30:48.870: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downwardapi-volume-35928e87-b112-4ba7-bbed-ddbb9401a8c1 container client-container: <nil>
    STEP: delete the pod 09/20/23 13:30:48.878
    Sep 20 13:30:48.915: INFO: Waiting for pod downwardapi-volume-35928e87-b112-4ba7-bbed-ddbb9401a8c1 to disappear
    Sep 20 13:30:48.921: INFO: Pod downwardapi-volume-35928e87-b112-4ba7-bbed-ddbb9401a8c1 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:30:48.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1423" for this suite. 09/20/23 13:30:48.929
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:30:48.941
Sep 20 13:30:48.941: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename svcaccounts 09/20/23 13:30:48.941
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:30:49.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:30:49.552
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Sep 20 13:30:49.862: INFO: created pod
Sep 20 13:30:49.862: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-4739" to be "Succeeded or Failed"
Sep 20 13:30:49.870: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 7.035424ms
Sep 20 13:30:51.875: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012578107s
Sep 20 13:30:53.875: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012288453s
Sep 20 13:30:56.025: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=false. Elapsed: 6.16256685s
Sep 20 13:30:57.873: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.0108626s
STEP: Saw pod success 09/20/23 13:30:57.873
Sep 20 13:30:57.874: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Sep 20 13:31:27.874: INFO: polling logs
Sep 20 13:31:27.892: INFO: Pod logs: 
I0920 13:30:54.039359       1 log.go:198] OK: Got token
I0920 13:30:54.039424       1 log.go:198] validating with in-cluster discovery
I0920 13:30:54.039782       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
I0920 13:30:54.039805       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4739:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1695217250, NotBefore:1695216650, IssuedAt:1695216650, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4739", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"ed8777c1-5163-428e-8088-e8ef5679dea9"}}}
I0920 13:30:54.075683       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0920 13:30:54.112185       1 log.go:198] OK: Validated signature on JWT
I0920 13:30:54.112414       1 log.go:198] OK: Got valid claims from token!
I0920 13:30:54.112462       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4739:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1695217250, NotBefore:1695216650, IssuedAt:1695216650, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4739", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"ed8777c1-5163-428e-8088-e8ef5679dea9"}}}

Sep 20 13:31:27.893: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep 20 13:31:27.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4739" for this suite. 09/20/23 13:31:27.914
------------------------------
â€¢ [SLOW TEST] [38.986 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:30:48.941
    Sep 20 13:30:48.941: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename svcaccounts 09/20/23 13:30:48.941
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:30:49.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:30:49.552
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Sep 20 13:30:49.862: INFO: created pod
    Sep 20 13:30:49.862: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-4739" to be "Succeeded or Failed"
    Sep 20 13:30:49.870: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 7.035424ms
    Sep 20 13:30:51.875: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012578107s
    Sep 20 13:30:53.875: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012288453s
    Sep 20 13:30:56.025: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=false. Elapsed: 6.16256685s
    Sep 20 13:30:57.873: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.0108626s
    STEP: Saw pod success 09/20/23 13:30:57.873
    Sep 20 13:30:57.874: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Sep 20 13:31:27.874: INFO: polling logs
    Sep 20 13:31:27.892: INFO: Pod logs: 
    I0920 13:30:54.039359       1 log.go:198] OK: Got token
    I0920 13:30:54.039424       1 log.go:198] validating with in-cluster discovery
    I0920 13:30:54.039782       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0920 13:30:54.039805       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4739:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1695217250, NotBefore:1695216650, IssuedAt:1695216650, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4739", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"ed8777c1-5163-428e-8088-e8ef5679dea9"}}}
    I0920 13:30:54.075683       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0920 13:30:54.112185       1 log.go:198] OK: Validated signature on JWT
    I0920 13:30:54.112414       1 log.go:198] OK: Got valid claims from token!
    I0920 13:30:54.112462       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4739:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1695217250, NotBefore:1695216650, IssuedAt:1695216650, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4739", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"ed8777c1-5163-428e-8088-e8ef5679dea9"}}}

    Sep 20 13:31:27.893: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:31:27.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4739" for this suite. 09/20/23 13:31:27.914
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:31:27.927
Sep 20 13:31:27.927: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename projected 09/20/23 13:31:27.928
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:31:29.048
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:31:29.059
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-e0c5405c-fac7-47b7-bf15-32e3c7157633 09/20/23 13:31:29.066
STEP: Creating a pod to test consume configMaps 09/20/23 13:31:29.09
Sep 20 13:31:29.101: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cfe9bc84-1ba0-4d58-9121-d6d8c277936c" in namespace "projected-2238" to be "Succeeded or Failed"
Sep 20 13:31:29.106: INFO: Pod "pod-projected-configmaps-cfe9bc84-1ba0-4d58-9121-d6d8c277936c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.600149ms
Sep 20 13:31:31.131: INFO: Pod "pod-projected-configmaps-cfe9bc84-1ba0-4d58-9121-d6d8c277936c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030079532s
Sep 20 13:31:33.122: INFO: Pod "pod-projected-configmaps-cfe9bc84-1ba0-4d58-9121-d6d8c277936c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021543065s
Sep 20 13:31:35.111: INFO: Pod "pod-projected-configmaps-cfe9bc84-1ba0-4d58-9121-d6d8c277936c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01050228s
STEP: Saw pod success 09/20/23 13:31:35.111
Sep 20 13:31:35.111: INFO: Pod "pod-projected-configmaps-cfe9bc84-1ba0-4d58-9121-d6d8c277936c" satisfied condition "Succeeded or Failed"
Sep 20 13:31:35.131: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-projected-configmaps-cfe9bc84-1ba0-4d58-9121-d6d8c277936c container agnhost-container: <nil>
STEP: delete the pod 09/20/23 13:31:35.151
Sep 20 13:31:35.311: INFO: Waiting for pod pod-projected-configmaps-cfe9bc84-1ba0-4d58-9121-d6d8c277936c to disappear
Sep 20 13:31:35.318: INFO: Pod pod-projected-configmaps-cfe9bc84-1ba0-4d58-9121-d6d8c277936c no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep 20 13:31:35.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2238" for this suite. 09/20/23 13:31:35.323
------------------------------
â€¢ [SLOW TEST] [7.403 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:31:27.927
    Sep 20 13:31:27.927: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename projected 09/20/23 13:31:27.928
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:31:29.048
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:31:29.059
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-e0c5405c-fac7-47b7-bf15-32e3c7157633 09/20/23 13:31:29.066
    STEP: Creating a pod to test consume configMaps 09/20/23 13:31:29.09
    Sep 20 13:31:29.101: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cfe9bc84-1ba0-4d58-9121-d6d8c277936c" in namespace "projected-2238" to be "Succeeded or Failed"
    Sep 20 13:31:29.106: INFO: Pod "pod-projected-configmaps-cfe9bc84-1ba0-4d58-9121-d6d8c277936c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.600149ms
    Sep 20 13:31:31.131: INFO: Pod "pod-projected-configmaps-cfe9bc84-1ba0-4d58-9121-d6d8c277936c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030079532s
    Sep 20 13:31:33.122: INFO: Pod "pod-projected-configmaps-cfe9bc84-1ba0-4d58-9121-d6d8c277936c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021543065s
    Sep 20 13:31:35.111: INFO: Pod "pod-projected-configmaps-cfe9bc84-1ba0-4d58-9121-d6d8c277936c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01050228s
    STEP: Saw pod success 09/20/23 13:31:35.111
    Sep 20 13:31:35.111: INFO: Pod "pod-projected-configmaps-cfe9bc84-1ba0-4d58-9121-d6d8c277936c" satisfied condition "Succeeded or Failed"
    Sep 20 13:31:35.131: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-projected-configmaps-cfe9bc84-1ba0-4d58-9121-d6d8c277936c container agnhost-container: <nil>
    STEP: delete the pod 09/20/23 13:31:35.151
    Sep 20 13:31:35.311: INFO: Waiting for pod pod-projected-configmaps-cfe9bc84-1ba0-4d58-9121-d6d8c277936c to disappear
    Sep 20 13:31:35.318: INFO: Pod pod-projected-configmaps-cfe9bc84-1ba0-4d58-9121-d6d8c277936c no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:31:35.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2238" for this suite. 09/20/23 13:31:35.323
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:31:35.334
Sep 20 13:31:35.334: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename pod-network-test 09/20/23 13:31:35.334
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:31:35.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:31:35.396
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-9908 09/20/23 13:31:35.402
STEP: creating a selector 09/20/23 13:31:35.402
STEP: Creating the service pods in kubernetes 09/20/23 13:31:35.402
Sep 20 13:31:35.402: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep 20 13:31:36.540: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9908" to be "running and ready"
Sep 20 13:31:36.778: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 238.429104ms
Sep 20 13:31:36.778: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:31:38.864: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.324769831s
Sep 20 13:31:38.864: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:31:40.807: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.267061542s
Sep 20 13:31:40.807: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep 20 13:31:42.920: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.379968831s
Sep 20 13:31:42.920: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep 20 13:31:44.783: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.242869938s
Sep 20 13:31:44.783: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep 20 13:31:46.785: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.244941672s
Sep 20 13:31:46.785: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep 20 13:31:48.785: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.245127254s
Sep 20 13:31:48.785: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep 20 13:31:50.785: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.24578178s
Sep 20 13:31:50.786: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep 20 13:31:53.005: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.465740715s
Sep 20 13:31:53.006: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep 20 13:31:54.785: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.245017996s
Sep 20 13:31:54.785: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep 20 13:31:56.784: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 20.243890434s
Sep 20 13:31:56.784: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Sep 20 13:31:56.784: INFO: Pod "netserver-0" satisfied condition "running and ready"
Sep 20 13:31:56.787: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9908" to be "running and ready"
Sep 20 13:31:56.790: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 3.361943ms
Sep 20 13:31:56.790: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Sep 20 13:31:59.004: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.217394077s
Sep 20 13:31:59.005: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Sep 20 13:31:59.005: INFO: Pod "netserver-1" satisfied condition "running and ready"
Sep 20 13:31:59.012: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9908" to be "running and ready"
Sep 20 13:31:59.016: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.260706ms
Sep 20 13:31:59.016: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Sep 20 13:31:59.016: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 09/20/23 13:31:59.019
Sep 20 13:31:59.034: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9908" to be "running"
Sep 20 13:31:59.044: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.929073ms
Sep 20 13:32:01.050: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016023214s
Sep 20 13:32:03.049: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.014760959s
Sep 20 13:32:03.049: INFO: Pod "test-container-pod" satisfied condition "running"
Sep 20 13:32:03.052: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Sep 20 13:32:03.052: INFO: Breadth first check of 10.100.5.146 on host 192.168.10.173...
Sep 20 13:32:03.056: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.4.46:9080/dial?request=hostname&protocol=udp&host=10.100.5.146&port=8081&tries=1'] Namespace:pod-network-test-9908 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 20 13:32:03.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
Sep 20 13:32:03.056: INFO: ExecWithOptions: Clientset creation
Sep 20 13:32:03.056: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-9908/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.4.46%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.100.5.146%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Sep 20 13:32:03.177: INFO: Waiting for responses: map[]
Sep 20 13:32:03.177: INFO: reached 10.100.5.146 after 0/1 tries
Sep 20 13:32:03.177: INFO: Breadth first check of 10.100.4.45 on host 192.168.10.64...
Sep 20 13:32:03.181: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.4.46:9080/dial?request=hostname&protocol=udp&host=10.100.4.45&port=8081&tries=1'] Namespace:pod-network-test-9908 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 20 13:32:03.181: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
Sep 20 13:32:03.181: INFO: ExecWithOptions: Clientset creation
Sep 20 13:32:03.182: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-9908/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.4.46%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.100.4.45%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Sep 20 13:32:03.277: INFO: Waiting for responses: map[]
Sep 20 13:32:03.277: INFO: reached 10.100.4.45 after 0/1 tries
Sep 20 13:32:03.277: INFO: Breadth first check of 10.100.3.189 on host 192.168.10.172...
Sep 20 13:32:03.282: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.4.46:9080/dial?request=hostname&protocol=udp&host=10.100.3.189&port=8081&tries=1'] Namespace:pod-network-test-9908 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 20 13:32:03.282: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
Sep 20 13:32:03.282: INFO: ExecWithOptions: Clientset creation
Sep 20 13:32:03.282: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-9908/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.4.46%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.100.3.189%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Sep 20 13:32:03.965: INFO: Waiting for responses: map[]
Sep 20 13:32:03.965: INFO: reached 10.100.3.189 after 0/1 tries
Sep 20 13:32:03.965: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Sep 20 13:32:03.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-9908" for this suite. 09/20/23 13:32:03.973
------------------------------
â€¢ [SLOW TEST] [28.720 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:31:35.334
    Sep 20 13:31:35.334: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename pod-network-test 09/20/23 13:31:35.334
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:31:35.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:31:35.396
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-9908 09/20/23 13:31:35.402
    STEP: creating a selector 09/20/23 13:31:35.402
    STEP: Creating the service pods in kubernetes 09/20/23 13:31:35.402
    Sep 20 13:31:35.402: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Sep 20 13:31:36.540: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9908" to be "running and ready"
    Sep 20 13:31:36.778: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 238.429104ms
    Sep 20 13:31:36.778: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:31:38.864: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.324769831s
    Sep 20 13:31:38.864: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:31:40.807: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.267061542s
    Sep 20 13:31:40.807: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep 20 13:31:42.920: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.379968831s
    Sep 20 13:31:42.920: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep 20 13:31:44.783: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.242869938s
    Sep 20 13:31:44.783: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep 20 13:31:46.785: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.244941672s
    Sep 20 13:31:46.785: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep 20 13:31:48.785: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.245127254s
    Sep 20 13:31:48.785: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep 20 13:31:50.785: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.24578178s
    Sep 20 13:31:50.786: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep 20 13:31:53.005: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.465740715s
    Sep 20 13:31:53.006: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep 20 13:31:54.785: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.245017996s
    Sep 20 13:31:54.785: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep 20 13:31:56.784: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 20.243890434s
    Sep 20 13:31:56.784: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Sep 20 13:31:56.784: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Sep 20 13:31:56.787: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9908" to be "running and ready"
    Sep 20 13:31:56.790: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 3.361943ms
    Sep 20 13:31:56.790: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Sep 20 13:31:59.004: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.217394077s
    Sep 20 13:31:59.005: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Sep 20 13:31:59.005: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Sep 20 13:31:59.012: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9908" to be "running and ready"
    Sep 20 13:31:59.016: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.260706ms
    Sep 20 13:31:59.016: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Sep 20 13:31:59.016: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 09/20/23 13:31:59.019
    Sep 20 13:31:59.034: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9908" to be "running"
    Sep 20 13:31:59.044: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.929073ms
    Sep 20 13:32:01.050: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016023214s
    Sep 20 13:32:03.049: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.014760959s
    Sep 20 13:32:03.049: INFO: Pod "test-container-pod" satisfied condition "running"
    Sep 20 13:32:03.052: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Sep 20 13:32:03.052: INFO: Breadth first check of 10.100.5.146 on host 192.168.10.173...
    Sep 20 13:32:03.056: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.4.46:9080/dial?request=hostname&protocol=udp&host=10.100.5.146&port=8081&tries=1'] Namespace:pod-network-test-9908 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep 20 13:32:03.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    Sep 20 13:32:03.056: INFO: ExecWithOptions: Clientset creation
    Sep 20 13:32:03.056: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-9908/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.4.46%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.100.5.146%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Sep 20 13:32:03.177: INFO: Waiting for responses: map[]
    Sep 20 13:32:03.177: INFO: reached 10.100.5.146 after 0/1 tries
    Sep 20 13:32:03.177: INFO: Breadth first check of 10.100.4.45 on host 192.168.10.64...
    Sep 20 13:32:03.181: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.4.46:9080/dial?request=hostname&protocol=udp&host=10.100.4.45&port=8081&tries=1'] Namespace:pod-network-test-9908 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep 20 13:32:03.181: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    Sep 20 13:32:03.181: INFO: ExecWithOptions: Clientset creation
    Sep 20 13:32:03.182: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-9908/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.4.46%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.100.4.45%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Sep 20 13:32:03.277: INFO: Waiting for responses: map[]
    Sep 20 13:32:03.277: INFO: reached 10.100.4.45 after 0/1 tries
    Sep 20 13:32:03.277: INFO: Breadth first check of 10.100.3.189 on host 192.168.10.172...
    Sep 20 13:32:03.282: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.4.46:9080/dial?request=hostname&protocol=udp&host=10.100.3.189&port=8081&tries=1'] Namespace:pod-network-test-9908 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep 20 13:32:03.282: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    Sep 20 13:32:03.282: INFO: ExecWithOptions: Clientset creation
    Sep 20 13:32:03.282: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-9908/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.4.46%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.100.3.189%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Sep 20 13:32:03.965: INFO: Waiting for responses: map[]
    Sep 20 13:32:03.965: INFO: reached 10.100.3.189 after 0/1 tries
    Sep 20 13:32:03.965: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:32:03.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-9908" for this suite. 09/20/23 13:32:03.973
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:32:04.063
Sep 20 13:32:04.063: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename watch 09/20/23 13:32:04.063
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:32:04.649
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:32:04.656
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 09/20/23 13:32:04.67
STEP: creating a watch on configmaps with label B 09/20/23 13:32:04.673
STEP: creating a watch on configmaps with label A or B 09/20/23 13:32:04.674
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 09/20/23 13:32:04.675
Sep 20 13:32:04.703: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9705  63fce0d6-22ba-4b7b-a841-0ff0502eb17f 37595 0 2023-09-20 13:32:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-20 13:32:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 20 13:32:04.703: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9705  63fce0d6-22ba-4b7b-a841-0ff0502eb17f 37595 0 2023-09-20 13:32:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-20 13:32:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 09/20/23 13:32:04.703
Sep 20 13:32:04.721: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9705  63fce0d6-22ba-4b7b-a841-0ff0502eb17f 37596 0 2023-09-20 13:32:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-20 13:32:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 20 13:32:04.721: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9705  63fce0d6-22ba-4b7b-a841-0ff0502eb17f 37596 0 2023-09-20 13:32:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-20 13:32:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 09/20/23 13:32:04.721
Sep 20 13:32:04.733: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9705  63fce0d6-22ba-4b7b-a841-0ff0502eb17f 37597 0 2023-09-20 13:32:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-20 13:32:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 20 13:32:04.733: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9705  63fce0d6-22ba-4b7b-a841-0ff0502eb17f 37597 0 2023-09-20 13:32:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-20 13:32:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 09/20/23 13:32:04.733
Sep 20 13:32:05.018: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9705  63fce0d6-22ba-4b7b-a841-0ff0502eb17f 37598 0 2023-09-20 13:32:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-20 13:32:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 20 13:32:05.018: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9705  63fce0d6-22ba-4b7b-a841-0ff0502eb17f 37598 0 2023-09-20 13:32:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-20 13:32:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 09/20/23 13:32:05.018
Sep 20 13:32:05.026: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9705  4bc84da5-b85d-4792-b479-f5c55111e0d4 37600 0 2023-09-20 13:32:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-20 13:32:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 20 13:32:05.026: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9705  4bc84da5-b85d-4792-b479-f5c55111e0d4 37600 0 2023-09-20 13:32:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-20 13:32:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 09/20/23 13:32:15.027
Sep 20 13:32:15.036: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9705  4bc84da5-b85d-4792-b479-f5c55111e0d4 37670 0 2023-09-20 13:32:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-20 13:32:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 20 13:32:15.036: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9705  4bc84da5-b85d-4792-b479-f5c55111e0d4 37670 0 2023-09-20 13:32:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-20 13:32:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Sep 20 13:32:25.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-9705" for this suite. 09/20/23 13:32:25.044
------------------------------
â€¢ [SLOW TEST] [21.412 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:32:04.063
    Sep 20 13:32:04.063: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename watch 09/20/23 13:32:04.063
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:32:04.649
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:32:04.656
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 09/20/23 13:32:04.67
    STEP: creating a watch on configmaps with label B 09/20/23 13:32:04.673
    STEP: creating a watch on configmaps with label A or B 09/20/23 13:32:04.674
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 09/20/23 13:32:04.675
    Sep 20 13:32:04.703: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9705  63fce0d6-22ba-4b7b-a841-0ff0502eb17f 37595 0 2023-09-20 13:32:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-20 13:32:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep 20 13:32:04.703: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9705  63fce0d6-22ba-4b7b-a841-0ff0502eb17f 37595 0 2023-09-20 13:32:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-20 13:32:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 09/20/23 13:32:04.703
    Sep 20 13:32:04.721: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9705  63fce0d6-22ba-4b7b-a841-0ff0502eb17f 37596 0 2023-09-20 13:32:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-20 13:32:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep 20 13:32:04.721: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9705  63fce0d6-22ba-4b7b-a841-0ff0502eb17f 37596 0 2023-09-20 13:32:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-20 13:32:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 09/20/23 13:32:04.721
    Sep 20 13:32:04.733: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9705  63fce0d6-22ba-4b7b-a841-0ff0502eb17f 37597 0 2023-09-20 13:32:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-20 13:32:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep 20 13:32:04.733: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9705  63fce0d6-22ba-4b7b-a841-0ff0502eb17f 37597 0 2023-09-20 13:32:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-20 13:32:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 09/20/23 13:32:04.733
    Sep 20 13:32:05.018: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9705  63fce0d6-22ba-4b7b-a841-0ff0502eb17f 37598 0 2023-09-20 13:32:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-20 13:32:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep 20 13:32:05.018: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9705  63fce0d6-22ba-4b7b-a841-0ff0502eb17f 37598 0 2023-09-20 13:32:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-20 13:32:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 09/20/23 13:32:05.018
    Sep 20 13:32:05.026: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9705  4bc84da5-b85d-4792-b479-f5c55111e0d4 37600 0 2023-09-20 13:32:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-20 13:32:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep 20 13:32:05.026: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9705  4bc84da5-b85d-4792-b479-f5c55111e0d4 37600 0 2023-09-20 13:32:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-20 13:32:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 09/20/23 13:32:15.027
    Sep 20 13:32:15.036: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9705  4bc84da5-b85d-4792-b479-f5c55111e0d4 37670 0 2023-09-20 13:32:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-20 13:32:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep 20 13:32:15.036: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9705  4bc84da5-b85d-4792-b479-f5c55111e0d4 37670 0 2023-09-20 13:32:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-20 13:32:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:32:25.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-9705" for this suite. 09/20/23 13:32:25.044
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:32:25.477
Sep 20 13:32:25.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename kubectl 09/20/23 13:32:25.478
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:32:25.735
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:32:25.739
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 09/20/23 13:32:25.746
Sep 20 13:32:25.746: INFO: namespace kubectl-6842
Sep 20 13:32:25.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-6842 create -f -'
Sep 20 13:32:26.198: INFO: stderr: ""
Sep 20 13:32:26.198: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 09/20/23 13:32:26.198
Sep 20 13:32:27.204: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 20 13:32:27.204: INFO: Found 0 / 1
Sep 20 13:32:28.206: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 20 13:32:28.206: INFO: Found 0 / 1
Sep 20 13:32:29.204: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 20 13:32:29.204: INFO: Found 0 / 1
Sep 20 13:32:30.688: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 20 13:32:30.689: INFO: Found 1 / 1
Sep 20 13:32:30.689: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 20 13:32:30.692: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 20 13:32:30.692: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 20 13:32:30.692: INFO: wait on agnhost-primary startup in kubectl-6842 
Sep 20 13:32:30.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-6842 logs agnhost-primary-rztnp agnhost-primary'
Sep 20 13:32:30.851: INFO: stderr: ""
Sep 20 13:32:30.851: INFO: stdout: "Paused\n"
STEP: exposing RC 09/20/23 13:32:30.851
Sep 20 13:32:30.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-6842 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Sep 20 13:32:31.015: INFO: stderr: ""
Sep 20 13:32:31.015: INFO: stdout: "service/rm2 exposed\n"
Sep 20 13:32:31.463: INFO: Service rm2 in namespace kubectl-6842 found.
STEP: exposing service 09/20/23 13:32:33.473
Sep 20 13:32:33.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-6842 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Sep 20 13:32:33.706: INFO: stderr: ""
Sep 20 13:32:33.706: INFO: stdout: "service/rm3 exposed\n"
Sep 20 13:32:33.982: INFO: Service rm3 in namespace kubectl-6842 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep 20 13:32:36.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6842" for this suite. 09/20/23 13:32:36.602
------------------------------
â€¢ [SLOW TEST] [11.140 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:32:25.477
    Sep 20 13:32:25.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename kubectl 09/20/23 13:32:25.478
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:32:25.735
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:32:25.739
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 09/20/23 13:32:25.746
    Sep 20 13:32:25.746: INFO: namespace kubectl-6842
    Sep 20 13:32:25.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-6842 create -f -'
    Sep 20 13:32:26.198: INFO: stderr: ""
    Sep 20 13:32:26.198: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 09/20/23 13:32:26.198
    Sep 20 13:32:27.204: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep 20 13:32:27.204: INFO: Found 0 / 1
    Sep 20 13:32:28.206: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep 20 13:32:28.206: INFO: Found 0 / 1
    Sep 20 13:32:29.204: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep 20 13:32:29.204: INFO: Found 0 / 1
    Sep 20 13:32:30.688: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep 20 13:32:30.689: INFO: Found 1 / 1
    Sep 20 13:32:30.689: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Sep 20 13:32:30.692: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep 20 13:32:30.692: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Sep 20 13:32:30.692: INFO: wait on agnhost-primary startup in kubectl-6842 
    Sep 20 13:32:30.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-6842 logs agnhost-primary-rztnp agnhost-primary'
    Sep 20 13:32:30.851: INFO: stderr: ""
    Sep 20 13:32:30.851: INFO: stdout: "Paused\n"
    STEP: exposing RC 09/20/23 13:32:30.851
    Sep 20 13:32:30.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-6842 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Sep 20 13:32:31.015: INFO: stderr: ""
    Sep 20 13:32:31.015: INFO: stdout: "service/rm2 exposed\n"
    Sep 20 13:32:31.463: INFO: Service rm2 in namespace kubectl-6842 found.
    STEP: exposing service 09/20/23 13:32:33.473
    Sep 20 13:32:33.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-6842 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Sep 20 13:32:33.706: INFO: stderr: ""
    Sep 20 13:32:33.706: INFO: stdout: "service/rm3 exposed\n"
    Sep 20 13:32:33.982: INFO: Service rm3 in namespace kubectl-6842 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:32:36.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6842" for this suite. 09/20/23 13:32:36.602
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:32:36.617
Sep 20 13:32:36.617: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename container-probe 09/20/23 13:32:36.618
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:32:37.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:32:37.082
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-b3d7b5f5-a2b7-46db-a735-452c572ddb71 in namespace container-probe-6677 09/20/23 13:32:37.086
Sep 20 13:32:37.371: INFO: Waiting up to 5m0s for pod "busybox-b3d7b5f5-a2b7-46db-a735-452c572ddb71" in namespace "container-probe-6677" to be "not pending"
Sep 20 13:32:37.377: INFO: Pod "busybox-b3d7b5f5-a2b7-46db-a735-452c572ddb71": Phase="Pending", Reason="", readiness=false. Elapsed: 6.07849ms
Sep 20 13:32:39.384: INFO: Pod "busybox-b3d7b5f5-a2b7-46db-a735-452c572ddb71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01287826s
Sep 20 13:32:41.381: INFO: Pod "busybox-b3d7b5f5-a2b7-46db-a735-452c572ddb71": Phase="Running", Reason="", readiness=true. Elapsed: 4.010860694s
Sep 20 13:32:41.382: INFO: Pod "busybox-b3d7b5f5-a2b7-46db-a735-452c572ddb71" satisfied condition "not pending"
Sep 20 13:32:41.382: INFO: Started pod busybox-b3d7b5f5-a2b7-46db-a735-452c572ddb71 in namespace container-probe-6677
STEP: checking the pod's current state and verifying that restartCount is present 09/20/23 13:32:41.382
Sep 20 13:32:41.385: INFO: Initial restart count of pod busybox-b3d7b5f5-a2b7-46db-a735-452c572ddb71 is 0
Sep 20 13:33:30.168: INFO: Restart count of pod container-probe-6677/busybox-b3d7b5f5-a2b7-46db-a735-452c572ddb71 is now 1 (48.783642753s elapsed)
STEP: deleting the pod 09/20/23 13:33:30.168
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep 20 13:33:30.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6677" for this suite. 09/20/23 13:33:30.617
------------------------------
â€¢ [SLOW TEST] [54.011 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:32:36.617
    Sep 20 13:32:36.617: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename container-probe 09/20/23 13:32:36.618
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:32:37.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:32:37.082
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-b3d7b5f5-a2b7-46db-a735-452c572ddb71 in namespace container-probe-6677 09/20/23 13:32:37.086
    Sep 20 13:32:37.371: INFO: Waiting up to 5m0s for pod "busybox-b3d7b5f5-a2b7-46db-a735-452c572ddb71" in namespace "container-probe-6677" to be "not pending"
    Sep 20 13:32:37.377: INFO: Pod "busybox-b3d7b5f5-a2b7-46db-a735-452c572ddb71": Phase="Pending", Reason="", readiness=false. Elapsed: 6.07849ms
    Sep 20 13:32:39.384: INFO: Pod "busybox-b3d7b5f5-a2b7-46db-a735-452c572ddb71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01287826s
    Sep 20 13:32:41.381: INFO: Pod "busybox-b3d7b5f5-a2b7-46db-a735-452c572ddb71": Phase="Running", Reason="", readiness=true. Elapsed: 4.010860694s
    Sep 20 13:32:41.382: INFO: Pod "busybox-b3d7b5f5-a2b7-46db-a735-452c572ddb71" satisfied condition "not pending"
    Sep 20 13:32:41.382: INFO: Started pod busybox-b3d7b5f5-a2b7-46db-a735-452c572ddb71 in namespace container-probe-6677
    STEP: checking the pod's current state and verifying that restartCount is present 09/20/23 13:32:41.382
    Sep 20 13:32:41.385: INFO: Initial restart count of pod busybox-b3d7b5f5-a2b7-46db-a735-452c572ddb71 is 0
    Sep 20 13:33:30.168: INFO: Restart count of pod container-probe-6677/busybox-b3d7b5f5-a2b7-46db-a735-452c572ddb71 is now 1 (48.783642753s elapsed)
    STEP: deleting the pod 09/20/23 13:33:30.168
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:33:30.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6677" for this suite. 09/20/23 13:33:30.617
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:33:30.629
Sep 20 13:33:30.629: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename webhook 09/20/23 13:33:30.63
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:33:30.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:33:30.661
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/20/23 13:33:30.682
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/20/23 13:33:30.853
STEP: Deploying the webhook pod 09/20/23 13:33:31.399
STEP: Wait for the deployment to be ready 09/20/23 13:33:31.645
Sep 20 13:33:31.656: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 13:33:33.779: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 33, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 33, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 33, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 33, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 13:33:37.059: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 33, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 33, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 33, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 33, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 13:33:38.049: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 33, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 33, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 33, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 33, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/20/23 13:33:39.785
STEP: Verifying the service has paired with the endpoint 09/20/23 13:33:40.201
Sep 20 13:33:41.201: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 09/20/23 13:33:46.806
STEP: Creating a configMap that should be mutated 09/20/23 13:33:46.826
STEP: Deleting the collection of validation webhooks 09/20/23 13:33:46.861
STEP: Creating a configMap that should not be mutated 09/20/23 13:33:47.005
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep 20 13:33:47.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8069" for this suite. 09/20/23 13:33:47.971
STEP: Destroying namespace "webhook-8069-markers" for this suite. 09/20/23 13:33:48.181
------------------------------
â€¢ [SLOW TEST] [17.562 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:33:30.629
    Sep 20 13:33:30.629: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename webhook 09/20/23 13:33:30.63
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:33:30.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:33:30.661
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/20/23 13:33:30.682
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/20/23 13:33:30.853
    STEP: Deploying the webhook pod 09/20/23 13:33:31.399
    STEP: Wait for the deployment to be ready 09/20/23 13:33:31.645
    Sep 20 13:33:31.656: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Sep 20 13:33:33.779: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 33, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 33, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 33, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 33, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 13:33:37.059: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 33, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 33, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 33, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 33, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 13:33:38.049: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 33, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 33, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 33, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 33, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/20/23 13:33:39.785
    STEP: Verifying the service has paired with the endpoint 09/20/23 13:33:40.201
    Sep 20 13:33:41.201: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 09/20/23 13:33:46.806
    STEP: Creating a configMap that should be mutated 09/20/23 13:33:46.826
    STEP: Deleting the collection of validation webhooks 09/20/23 13:33:46.861
    STEP: Creating a configMap that should not be mutated 09/20/23 13:33:47.005
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:33:47.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8069" for this suite. 09/20/23 13:33:47.971
    STEP: Destroying namespace "webhook-8069-markers" for this suite. 09/20/23 13:33:48.181
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:33:48.193
Sep 20 13:33:48.193: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename svcaccounts 09/20/23 13:33:48.194
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:33:48.629
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:33:48.634
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 09/20/23 13:33:48.644
STEP: watching for the ServiceAccount to be added 09/20/23 13:33:48.714
STEP: patching the ServiceAccount 09/20/23 13:33:48.719
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 09/20/23 13:33:48.798
STEP: deleting the ServiceAccount 09/20/23 13:33:48.803
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep 20 13:33:48.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-6178" for this suite. 09/20/23 13:33:48.975
------------------------------
â€¢ [1.013 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:33:48.193
    Sep 20 13:33:48.193: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename svcaccounts 09/20/23 13:33:48.194
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:33:48.629
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:33:48.634
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 09/20/23 13:33:48.644
    STEP: watching for the ServiceAccount to be added 09/20/23 13:33:48.714
    STEP: patching the ServiceAccount 09/20/23 13:33:48.719
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 09/20/23 13:33:48.798
    STEP: deleting the ServiceAccount 09/20/23 13:33:48.803
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:33:48.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-6178" for this suite. 09/20/23 13:33:48.975
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:33:49.207
Sep 20 13:33:49.207: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename security-context-test 09/20/23 13:33:49.208
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:33:49.304
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:33:49.309
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Sep 20 13:33:49.621: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-99186090-8bcd-4966-b635-54aa5d5eaa5f" in namespace "security-context-test-9139" to be "Succeeded or Failed"
Sep 20 13:33:49.628: INFO: Pod "alpine-nnp-false-99186090-8bcd-4966-b635-54aa5d5eaa5f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.899045ms
Sep 20 13:33:52.163: INFO: Pod "alpine-nnp-false-99186090-8bcd-4966-b635-54aa5d5eaa5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.542640972s
Sep 20 13:33:54.088: INFO: Pod "alpine-nnp-false-99186090-8bcd-4966-b635-54aa5d5eaa5f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.466884997s
Sep 20 13:33:55.685: INFO: Pod "alpine-nnp-false-99186090-8bcd-4966-b635-54aa5d5eaa5f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.063935869s
Sep 20 13:33:57.633: INFO: Pod "alpine-nnp-false-99186090-8bcd-4966-b635-54aa5d5eaa5f": Phase="Running", Reason="", readiness=true. Elapsed: 8.012432165s
Sep 20 13:33:59.634: INFO: Pod "alpine-nnp-false-99186090-8bcd-4966-b635-54aa5d5eaa5f": Phase="Running", Reason="", readiness=false. Elapsed: 10.01320202s
Sep 20 13:34:01.782: INFO: Pod "alpine-nnp-false-99186090-8bcd-4966-b635-54aa5d5eaa5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.161184277s
Sep 20 13:34:01.782: INFO: Pod "alpine-nnp-false-99186090-8bcd-4966-b635-54aa5d5eaa5f" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep 20 13:34:01.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-9139" for this suite. 09/20/23 13:34:01.851
------------------------------
â€¢ [SLOW TEST] [12.651 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:33:49.207
    Sep 20 13:33:49.207: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename security-context-test 09/20/23 13:33:49.208
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:33:49.304
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:33:49.309
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Sep 20 13:33:49.621: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-99186090-8bcd-4966-b635-54aa5d5eaa5f" in namespace "security-context-test-9139" to be "Succeeded or Failed"
    Sep 20 13:33:49.628: INFO: Pod "alpine-nnp-false-99186090-8bcd-4966-b635-54aa5d5eaa5f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.899045ms
    Sep 20 13:33:52.163: INFO: Pod "alpine-nnp-false-99186090-8bcd-4966-b635-54aa5d5eaa5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.542640972s
    Sep 20 13:33:54.088: INFO: Pod "alpine-nnp-false-99186090-8bcd-4966-b635-54aa5d5eaa5f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.466884997s
    Sep 20 13:33:55.685: INFO: Pod "alpine-nnp-false-99186090-8bcd-4966-b635-54aa5d5eaa5f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.063935869s
    Sep 20 13:33:57.633: INFO: Pod "alpine-nnp-false-99186090-8bcd-4966-b635-54aa5d5eaa5f": Phase="Running", Reason="", readiness=true. Elapsed: 8.012432165s
    Sep 20 13:33:59.634: INFO: Pod "alpine-nnp-false-99186090-8bcd-4966-b635-54aa5d5eaa5f": Phase="Running", Reason="", readiness=false. Elapsed: 10.01320202s
    Sep 20 13:34:01.782: INFO: Pod "alpine-nnp-false-99186090-8bcd-4966-b635-54aa5d5eaa5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.161184277s
    Sep 20 13:34:01.782: INFO: Pod "alpine-nnp-false-99186090-8bcd-4966-b635-54aa5d5eaa5f" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:34:01.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-9139" for this suite. 09/20/23 13:34:01.851
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:34:01.861
Sep 20 13:34:01.861: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename dns 09/20/23 13:34:01.861
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:34:02.102
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:34:02.106
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 09/20/23 13:34:02.113
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 09/20/23 13:34:02.114
STEP: creating a pod to probe DNS 09/20/23 13:34:02.114
STEP: submitting the pod to kubernetes 09/20/23 13:34:02.114
Sep 20 13:34:02.128: INFO: Waiting up to 15m0s for pod "dns-test-a73ad268-2007-4c97-8561-19f36f18b786" in namespace "dns-8401" to be "running"
Sep 20 13:34:02.132: INFO: Pod "dns-test-a73ad268-2007-4c97-8561-19f36f18b786": Phase="Pending", Reason="", readiness=false. Elapsed: 3.731699ms
Sep 20 13:34:04.137: INFO: Pod "dns-test-a73ad268-2007-4c97-8561-19f36f18b786": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008891922s
Sep 20 13:34:06.257: INFO: Pod "dns-test-a73ad268-2007-4c97-8561-19f36f18b786": Phase="Pending", Reason="", readiness=false. Elapsed: 4.129601629s
Sep 20 13:34:08.137: INFO: Pod "dns-test-a73ad268-2007-4c97-8561-19f36f18b786": Phase="Running", Reason="", readiness=true. Elapsed: 6.00878156s
Sep 20 13:34:08.137: INFO: Pod "dns-test-a73ad268-2007-4c97-8561-19f36f18b786" satisfied condition "running"
STEP: retrieving the pod 09/20/23 13:34:08.137
STEP: looking for the results for each expected name from probers 09/20/23 13:34:08.141
Sep 20 13:34:08.167: INFO: DNS probes using dns-8401/dns-test-a73ad268-2007-4c97-8561-19f36f18b786 succeeded

STEP: deleting the pod 09/20/23 13:34:08.167
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep 20 13:34:08.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8401" for this suite. 09/20/23 13:34:08.435
------------------------------
â€¢ [SLOW TEST] [6.588 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:34:01.861
    Sep 20 13:34:01.861: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename dns 09/20/23 13:34:01.861
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:34:02.102
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:34:02.106
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     09/20/23 13:34:02.113
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     09/20/23 13:34:02.114
    STEP: creating a pod to probe DNS 09/20/23 13:34:02.114
    STEP: submitting the pod to kubernetes 09/20/23 13:34:02.114
    Sep 20 13:34:02.128: INFO: Waiting up to 15m0s for pod "dns-test-a73ad268-2007-4c97-8561-19f36f18b786" in namespace "dns-8401" to be "running"
    Sep 20 13:34:02.132: INFO: Pod "dns-test-a73ad268-2007-4c97-8561-19f36f18b786": Phase="Pending", Reason="", readiness=false. Elapsed: 3.731699ms
    Sep 20 13:34:04.137: INFO: Pod "dns-test-a73ad268-2007-4c97-8561-19f36f18b786": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008891922s
    Sep 20 13:34:06.257: INFO: Pod "dns-test-a73ad268-2007-4c97-8561-19f36f18b786": Phase="Pending", Reason="", readiness=false. Elapsed: 4.129601629s
    Sep 20 13:34:08.137: INFO: Pod "dns-test-a73ad268-2007-4c97-8561-19f36f18b786": Phase="Running", Reason="", readiness=true. Elapsed: 6.00878156s
    Sep 20 13:34:08.137: INFO: Pod "dns-test-a73ad268-2007-4c97-8561-19f36f18b786" satisfied condition "running"
    STEP: retrieving the pod 09/20/23 13:34:08.137
    STEP: looking for the results for each expected name from probers 09/20/23 13:34:08.141
    Sep 20 13:34:08.167: INFO: DNS probes using dns-8401/dns-test-a73ad268-2007-4c97-8561-19f36f18b786 succeeded

    STEP: deleting the pod 09/20/23 13:34:08.167
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:34:08.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8401" for this suite. 09/20/23 13:34:08.435
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:34:08.449
Sep 20 13:34:08.449: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename services 09/20/23 13:34:08.45
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:34:08.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:34:08.479
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-168 09/20/23 13:34:08.485
STEP: creating service affinity-clusterip-transition in namespace services-168 09/20/23 13:34:08.485
STEP: creating replication controller affinity-clusterip-transition in namespace services-168 09/20/23 13:34:08.498
I0920 13:34:08.520356      20 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-168, replica count: 3
I0920 13:34:11.570921      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0920 13:34:14.571705      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0920 13:34:17.573646      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 20 13:34:17.581: INFO: Creating new exec pod
Sep 20 13:34:17.976: INFO: Waiting up to 5m0s for pod "execpod-affinitytxxnt" in namespace "services-168" to be "running"
Sep 20 13:34:18.655: INFO: Pod "execpod-affinitytxxnt": Phase="Pending", Reason="", readiness=false. Elapsed: 678.742579ms
Sep 20 13:34:20.668: INFO: Pod "execpod-affinitytxxnt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.692297123s
Sep 20 13:34:22.660: INFO: Pod "execpod-affinitytxxnt": Phase="Running", Reason="", readiness=true. Elapsed: 4.684171508s
Sep 20 13:34:22.660: INFO: Pod "execpod-affinitytxxnt" satisfied condition "running"
Sep 20 13:34:23.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-168 exec execpod-affinitytxxnt -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Sep 20 13:34:23.894: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Sep 20 13:34:23.894: INFO: stdout: ""
Sep 20 13:34:23.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-168 exec execpod-affinitytxxnt -- /bin/sh -x -c nc -v -z -w 2 10.254.232.50 80'
Sep 20 13:34:24.097: INFO: stderr: "+ nc -v -z -w 2 10.254.232.50 80\nConnection to 10.254.232.50 80 port [tcp/http] succeeded!\n"
Sep 20 13:34:24.097: INFO: stdout: ""
Sep 20 13:34:24.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-168 exec execpod-affinitytxxnt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.232.50:80/ ; done'
Sep 20 13:34:24.604: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n"
Sep 20 13:34:24.604: INFO: stdout: "\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-l7xhz\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xhdp6\naffinity-clusterip-transition-xhdp6\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-l7xhz\naffinity-clusterip-transition-xhdp6\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xhdp6\naffinity-clusterip-transition-l7xhz\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-l7xhz\naffinity-clusterip-transition-xbchc"
Sep 20 13:34:24.604: INFO: Received response from host: affinity-clusterip-transition-xbchc
Sep 20 13:34:24.604: INFO: Received response from host: affinity-clusterip-transition-l7xhz
Sep 20 13:34:24.604: INFO: Received response from host: affinity-clusterip-transition-xbchc
Sep 20 13:34:24.604: INFO: Received response from host: affinity-clusterip-transition-xbchc
Sep 20 13:34:24.604: INFO: Received response from host: affinity-clusterip-transition-xbchc
Sep 20 13:34:24.604: INFO: Received response from host: affinity-clusterip-transition-xhdp6
Sep 20 13:34:24.604: INFO: Received response from host: affinity-clusterip-transition-xhdp6
Sep 20 13:34:24.604: INFO: Received response from host: affinity-clusterip-transition-xbchc
Sep 20 13:34:24.604: INFO: Received response from host: affinity-clusterip-transition-l7xhz
Sep 20 13:34:24.604: INFO: Received response from host: affinity-clusterip-transition-xhdp6
Sep 20 13:34:24.604: INFO: Received response from host: affinity-clusterip-transition-xbchc
Sep 20 13:34:24.604: INFO: Received response from host: affinity-clusterip-transition-xhdp6
Sep 20 13:34:24.604: INFO: Received response from host: affinity-clusterip-transition-l7xhz
Sep 20 13:34:24.604: INFO: Received response from host: affinity-clusterip-transition-xbchc
Sep 20 13:34:24.604: INFO: Received response from host: affinity-clusterip-transition-l7xhz
Sep 20 13:34:24.604: INFO: Received response from host: affinity-clusterip-transition-xbchc
Sep 20 13:34:24.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-168 exec execpod-affinitytxxnt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.232.50:80/ ; done'
Sep 20 13:34:24.887: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n"
Sep 20 13:34:24.887: INFO: stdout: "\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xbchc"
Sep 20 13:34:24.887: INFO: Received response from host: affinity-clusterip-transition-xbchc
Sep 20 13:34:24.887: INFO: Received response from host: affinity-clusterip-transition-xbchc
Sep 20 13:34:24.887: INFO: Received response from host: affinity-clusterip-transition-xbchc
Sep 20 13:34:24.887: INFO: Received response from host: affinity-clusterip-transition-xbchc
Sep 20 13:34:24.887: INFO: Received response from host: affinity-clusterip-transition-xbchc
Sep 20 13:34:24.887: INFO: Received response from host: affinity-clusterip-transition-xbchc
Sep 20 13:34:24.887: INFO: Received response from host: affinity-clusterip-transition-xbchc
Sep 20 13:34:24.887: INFO: Received response from host: affinity-clusterip-transition-xbchc
Sep 20 13:34:24.887: INFO: Received response from host: affinity-clusterip-transition-xbchc
Sep 20 13:34:24.887: INFO: Received response from host: affinity-clusterip-transition-xbchc
Sep 20 13:34:24.887: INFO: Received response from host: affinity-clusterip-transition-xbchc
Sep 20 13:34:24.887: INFO: Received response from host: affinity-clusterip-transition-xbchc
Sep 20 13:34:24.887: INFO: Received response from host: affinity-clusterip-transition-xbchc
Sep 20 13:34:24.887: INFO: Received response from host: affinity-clusterip-transition-xbchc
Sep 20 13:34:24.887: INFO: Received response from host: affinity-clusterip-transition-xbchc
Sep 20 13:34:24.887: INFO: Received response from host: affinity-clusterip-transition-xbchc
Sep 20 13:34:24.887: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-168, will wait for the garbage collector to delete the pods 09/20/23 13:34:25.029
Sep 20 13:34:25.133: INFO: Deleting ReplicationController affinity-clusterip-transition took: 45.420969ms
Sep 20 13:34:26.233: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 1.100377946s
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep 20 13:34:32.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-168" for this suite. 09/20/23 13:34:32.461
------------------------------
â€¢ [SLOW TEST] [24.019 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:34:08.449
    Sep 20 13:34:08.449: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename services 09/20/23 13:34:08.45
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:34:08.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:34:08.479
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-168 09/20/23 13:34:08.485
    STEP: creating service affinity-clusterip-transition in namespace services-168 09/20/23 13:34:08.485
    STEP: creating replication controller affinity-clusterip-transition in namespace services-168 09/20/23 13:34:08.498
    I0920 13:34:08.520356      20 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-168, replica count: 3
    I0920 13:34:11.570921      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0920 13:34:14.571705      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0920 13:34:17.573646      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep 20 13:34:17.581: INFO: Creating new exec pod
    Sep 20 13:34:17.976: INFO: Waiting up to 5m0s for pod "execpod-affinitytxxnt" in namespace "services-168" to be "running"
    Sep 20 13:34:18.655: INFO: Pod "execpod-affinitytxxnt": Phase="Pending", Reason="", readiness=false. Elapsed: 678.742579ms
    Sep 20 13:34:20.668: INFO: Pod "execpod-affinitytxxnt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.692297123s
    Sep 20 13:34:22.660: INFO: Pod "execpod-affinitytxxnt": Phase="Running", Reason="", readiness=true. Elapsed: 4.684171508s
    Sep 20 13:34:22.660: INFO: Pod "execpod-affinitytxxnt" satisfied condition "running"
    Sep 20 13:34:23.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-168 exec execpod-affinitytxxnt -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Sep 20 13:34:23.894: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Sep 20 13:34:23.894: INFO: stdout: ""
    Sep 20 13:34:23.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-168 exec execpod-affinitytxxnt -- /bin/sh -x -c nc -v -z -w 2 10.254.232.50 80'
    Sep 20 13:34:24.097: INFO: stderr: "+ nc -v -z -w 2 10.254.232.50 80\nConnection to 10.254.232.50 80 port [tcp/http] succeeded!\n"
    Sep 20 13:34:24.097: INFO: stdout: ""
    Sep 20 13:34:24.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-168 exec execpod-affinitytxxnt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.232.50:80/ ; done'
    Sep 20 13:34:24.604: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n"
    Sep 20 13:34:24.604: INFO: stdout: "\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-l7xhz\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xhdp6\naffinity-clusterip-transition-xhdp6\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-l7xhz\naffinity-clusterip-transition-xhdp6\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xhdp6\naffinity-clusterip-transition-l7xhz\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-l7xhz\naffinity-clusterip-transition-xbchc"
    Sep 20 13:34:24.604: INFO: Received response from host: affinity-clusterip-transition-xbchc
    Sep 20 13:34:24.604: INFO: Received response from host: affinity-clusterip-transition-l7xhz
    Sep 20 13:34:24.604: INFO: Received response from host: affinity-clusterip-transition-xbchc
    Sep 20 13:34:24.604: INFO: Received response from host: affinity-clusterip-transition-xbchc
    Sep 20 13:34:24.604: INFO: Received response from host: affinity-clusterip-transition-xbchc
    Sep 20 13:34:24.604: INFO: Received response from host: affinity-clusterip-transition-xhdp6
    Sep 20 13:34:24.604: INFO: Received response from host: affinity-clusterip-transition-xhdp6
    Sep 20 13:34:24.604: INFO: Received response from host: affinity-clusterip-transition-xbchc
    Sep 20 13:34:24.604: INFO: Received response from host: affinity-clusterip-transition-l7xhz
    Sep 20 13:34:24.604: INFO: Received response from host: affinity-clusterip-transition-xhdp6
    Sep 20 13:34:24.604: INFO: Received response from host: affinity-clusterip-transition-xbchc
    Sep 20 13:34:24.604: INFO: Received response from host: affinity-clusterip-transition-xhdp6
    Sep 20 13:34:24.604: INFO: Received response from host: affinity-clusterip-transition-l7xhz
    Sep 20 13:34:24.604: INFO: Received response from host: affinity-clusterip-transition-xbchc
    Sep 20 13:34:24.604: INFO: Received response from host: affinity-clusterip-transition-l7xhz
    Sep 20 13:34:24.604: INFO: Received response from host: affinity-clusterip-transition-xbchc
    Sep 20 13:34:24.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-168 exec execpod-affinitytxxnt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.232.50:80/ ; done'
    Sep 20 13:34:24.887: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.232.50:80/\n"
    Sep 20 13:34:24.887: INFO: stdout: "\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xbchc\naffinity-clusterip-transition-xbchc"
    Sep 20 13:34:24.887: INFO: Received response from host: affinity-clusterip-transition-xbchc
    Sep 20 13:34:24.887: INFO: Received response from host: affinity-clusterip-transition-xbchc
    Sep 20 13:34:24.887: INFO: Received response from host: affinity-clusterip-transition-xbchc
    Sep 20 13:34:24.887: INFO: Received response from host: affinity-clusterip-transition-xbchc
    Sep 20 13:34:24.887: INFO: Received response from host: affinity-clusterip-transition-xbchc
    Sep 20 13:34:24.887: INFO: Received response from host: affinity-clusterip-transition-xbchc
    Sep 20 13:34:24.887: INFO: Received response from host: affinity-clusterip-transition-xbchc
    Sep 20 13:34:24.887: INFO: Received response from host: affinity-clusterip-transition-xbchc
    Sep 20 13:34:24.887: INFO: Received response from host: affinity-clusterip-transition-xbchc
    Sep 20 13:34:24.887: INFO: Received response from host: affinity-clusterip-transition-xbchc
    Sep 20 13:34:24.887: INFO: Received response from host: affinity-clusterip-transition-xbchc
    Sep 20 13:34:24.887: INFO: Received response from host: affinity-clusterip-transition-xbchc
    Sep 20 13:34:24.887: INFO: Received response from host: affinity-clusterip-transition-xbchc
    Sep 20 13:34:24.887: INFO: Received response from host: affinity-clusterip-transition-xbchc
    Sep 20 13:34:24.887: INFO: Received response from host: affinity-clusterip-transition-xbchc
    Sep 20 13:34:24.887: INFO: Received response from host: affinity-clusterip-transition-xbchc
    Sep 20 13:34:24.887: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-168, will wait for the garbage collector to delete the pods 09/20/23 13:34:25.029
    Sep 20 13:34:25.133: INFO: Deleting ReplicationController affinity-clusterip-transition took: 45.420969ms
    Sep 20 13:34:26.233: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 1.100377946s
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:34:32.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-168" for this suite. 09/20/23 13:34:32.461
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:34:32.468
Sep 20 13:34:32.468: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename resourcequota 09/20/23 13:34:32.469
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:34:32.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:34:32.583
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-t26g9" 09/20/23 13:34:32.594
Sep 20 13:34:32.604: INFO: Resource quota "e2e-rq-status-t26g9" reports spec: hard cpu limit of 500m
Sep 20 13:34:32.604: INFO: Resource quota "e2e-rq-status-t26g9" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-t26g9" /status 09/20/23 13:34:32.604
STEP: Confirm /status for "e2e-rq-status-t26g9" resourceQuota via watch 09/20/23 13:34:32.615
Sep 20 13:34:32.618: INFO: observed resourceQuota "e2e-rq-status-t26g9" in namespace "resourcequota-9076" with hard status: v1.ResourceList(nil)
Sep 20 13:34:32.618: INFO: Found resourceQuota "e2e-rq-status-t26g9" in namespace "resourcequota-9076" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Sep 20 13:34:32.618: INFO: ResourceQuota "e2e-rq-status-t26g9" /status was updated
STEP: Patching hard spec values for cpu & memory 09/20/23 13:34:32.621
Sep 20 13:34:32.630: INFO: Resource quota "e2e-rq-status-t26g9" reports spec: hard cpu limit of 1
Sep 20 13:34:32.630: INFO: Resource quota "e2e-rq-status-t26g9" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-t26g9" /status 09/20/23 13:34:32.63
STEP: Confirm /status for "e2e-rq-status-t26g9" resourceQuota via watch 09/20/23 13:34:32.637
Sep 20 13:34:32.639: INFO: observed resourceQuota "e2e-rq-status-t26g9" in namespace "resourcequota-9076" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Sep 20 13:34:32.639: INFO: Found resourceQuota "e2e-rq-status-t26g9" in namespace "resourcequota-9076" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Sep 20 13:34:32.640: INFO: ResourceQuota "e2e-rq-status-t26g9" /status was patched
STEP: Get "e2e-rq-status-t26g9" /status 09/20/23 13:34:32.64
Sep 20 13:34:32.646: INFO: Resourcequota "e2e-rq-status-t26g9" reports status: hard cpu of 1
Sep 20 13:34:32.646: INFO: Resourcequota "e2e-rq-status-t26g9" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-t26g9" /status before checking Spec is unchanged 09/20/23 13:34:32.65
Sep 20 13:34:32.657: INFO: Resourcequota "e2e-rq-status-t26g9" reports status: hard cpu of 2
Sep 20 13:34:32.657: INFO: Resourcequota "e2e-rq-status-t26g9" reports status: hard memory of 2Gi
Sep 20 13:34:32.659: INFO: Found resourceQuota "e2e-rq-status-t26g9" in namespace "resourcequota-9076" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Sep 20 13:34:37.757: INFO: ResourceQuota "e2e-rq-status-t26g9" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep 20 13:34:37.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9076" for this suite. 09/20/23 13:34:37.767
------------------------------
â€¢ [SLOW TEST] [5.309 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:34:32.468
    Sep 20 13:34:32.468: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename resourcequota 09/20/23 13:34:32.469
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:34:32.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:34:32.583
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-t26g9" 09/20/23 13:34:32.594
    Sep 20 13:34:32.604: INFO: Resource quota "e2e-rq-status-t26g9" reports spec: hard cpu limit of 500m
    Sep 20 13:34:32.604: INFO: Resource quota "e2e-rq-status-t26g9" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-t26g9" /status 09/20/23 13:34:32.604
    STEP: Confirm /status for "e2e-rq-status-t26g9" resourceQuota via watch 09/20/23 13:34:32.615
    Sep 20 13:34:32.618: INFO: observed resourceQuota "e2e-rq-status-t26g9" in namespace "resourcequota-9076" with hard status: v1.ResourceList(nil)
    Sep 20 13:34:32.618: INFO: Found resourceQuota "e2e-rq-status-t26g9" in namespace "resourcequota-9076" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Sep 20 13:34:32.618: INFO: ResourceQuota "e2e-rq-status-t26g9" /status was updated
    STEP: Patching hard spec values for cpu & memory 09/20/23 13:34:32.621
    Sep 20 13:34:32.630: INFO: Resource quota "e2e-rq-status-t26g9" reports spec: hard cpu limit of 1
    Sep 20 13:34:32.630: INFO: Resource quota "e2e-rq-status-t26g9" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-t26g9" /status 09/20/23 13:34:32.63
    STEP: Confirm /status for "e2e-rq-status-t26g9" resourceQuota via watch 09/20/23 13:34:32.637
    Sep 20 13:34:32.639: INFO: observed resourceQuota "e2e-rq-status-t26g9" in namespace "resourcequota-9076" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Sep 20 13:34:32.639: INFO: Found resourceQuota "e2e-rq-status-t26g9" in namespace "resourcequota-9076" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Sep 20 13:34:32.640: INFO: ResourceQuota "e2e-rq-status-t26g9" /status was patched
    STEP: Get "e2e-rq-status-t26g9" /status 09/20/23 13:34:32.64
    Sep 20 13:34:32.646: INFO: Resourcequota "e2e-rq-status-t26g9" reports status: hard cpu of 1
    Sep 20 13:34:32.646: INFO: Resourcequota "e2e-rq-status-t26g9" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-t26g9" /status before checking Spec is unchanged 09/20/23 13:34:32.65
    Sep 20 13:34:32.657: INFO: Resourcequota "e2e-rq-status-t26g9" reports status: hard cpu of 2
    Sep 20 13:34:32.657: INFO: Resourcequota "e2e-rq-status-t26g9" reports status: hard memory of 2Gi
    Sep 20 13:34:32.659: INFO: Found resourceQuota "e2e-rq-status-t26g9" in namespace "resourcequota-9076" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Sep 20 13:34:37.757: INFO: ResourceQuota "e2e-rq-status-t26g9" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:34:37.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9076" for this suite. 09/20/23 13:34:37.767
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:34:37.778
Sep 20 13:34:37.778: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename svc-latency 09/20/23 13:34:37.779
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:34:37.81
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:34:37.812
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Sep 20 13:34:37.873: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: creating replication controller svc-latency-rc in namespace svc-latency-2940 09/20/23 13:34:37.874
I0920 13:34:37.890716      20 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2940, replica count: 1
I0920 13:34:38.941590      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0920 13:34:39.942879      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0920 13:34:40.943685      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 20 13:34:41.164: INFO: Created: latency-svc-kwdgd
Sep 20 13:34:41.179: INFO: Got endpoints: latency-svc-kwdgd [135.062049ms]
Sep 20 13:34:41.200: INFO: Created: latency-svc-hcl59
Sep 20 13:34:41.211: INFO: Got endpoints: latency-svc-hcl59 [32.383929ms]
Sep 20 13:34:41.217: INFO: Created: latency-svc-mjgpx
Sep 20 13:34:41.241: INFO: Got endpoints: latency-svc-mjgpx [61.11518ms]
Sep 20 13:34:41.247: INFO: Created: latency-svc-bcqdp
Sep 20 13:34:41.262: INFO: Got endpoints: latency-svc-bcqdp [81.965593ms]
Sep 20 13:34:41.297: INFO: Created: latency-svc-9bx7k
Sep 20 13:34:41.304: INFO: Created: latency-svc-vlrdg
Sep 20 13:34:41.325: INFO: Got endpoints: latency-svc-9bx7k [145.150922ms]
Sep 20 13:34:41.332: INFO: Got endpoints: latency-svc-vlrdg [151.986529ms]
Sep 20 13:34:41.337: INFO: Created: latency-svc-4lglm
Sep 20 13:34:41.450: INFO: Got endpoints: latency-svc-4lglm [270.690644ms]
Sep 20 13:34:41.451: INFO: Created: latency-svc-g5kcb
Sep 20 13:34:41.459: INFO: Got endpoints: latency-svc-g5kcb [279.536286ms]
Sep 20 13:34:41.463: INFO: Created: latency-svc-2vdpz
Sep 20 13:34:41.767: INFO: Got endpoints: latency-svc-2vdpz [587.238326ms]
Sep 20 13:34:41.772: INFO: Created: latency-svc-krkk7
Sep 20 13:34:41.956: INFO: Got endpoints: latency-svc-krkk7 [776.40561ms]
Sep 20 13:34:41.960: INFO: Created: latency-svc-72ljn
Sep 20 13:34:42.080: INFO: Got endpoints: latency-svc-72ljn [900.669309ms]
Sep 20 13:34:42.084: INFO: Created: latency-svc-8s2r7
Sep 20 13:34:42.095: INFO: Got endpoints: latency-svc-8s2r7 [915.722042ms]
Sep 20 13:34:42.097: INFO: Created: latency-svc-t9bdv
Sep 20 13:34:42.130: INFO: Got endpoints: latency-svc-t9bdv [950.453064ms]
Sep 20 13:34:42.136: INFO: Created: latency-svc-vxfr9
Sep 20 13:34:42.162: INFO: Got endpoints: latency-svc-vxfr9 [982.076109ms]
Sep 20 13:34:42.165: INFO: Created: latency-svc-nfz4x
Sep 20 13:34:42.179: INFO: Got endpoints: latency-svc-nfz4x [999.423495ms]
Sep 20 13:34:42.194: INFO: Created: latency-svc-8tkwh
Sep 20 13:34:42.214: INFO: Got endpoints: latency-svc-8tkwh [1.033752448s]
Sep 20 13:34:42.240: INFO: Created: latency-svc-bhctj
Sep 20 13:34:42.255: INFO: Created: latency-svc-fcrtg
Sep 20 13:34:42.262: INFO: Got endpoints: latency-svc-bhctj [1.050302893s]
Sep 20 13:34:42.297: INFO: Got endpoints: latency-svc-fcrtg [1.056272789s]
Sep 20 13:34:42.302: INFO: Created: latency-svc-l2gp6
Sep 20 13:34:42.315: INFO: Created: latency-svc-vss6w
Sep 20 13:34:42.319: INFO: Got endpoints: latency-svc-l2gp6 [1.057380326s]
Sep 20 13:34:42.336: INFO: Got endpoints: latency-svc-vss6w [1.011910596s]
Sep 20 13:34:42.346: INFO: Created: latency-svc-jrk7w
Sep 20 13:34:42.371: INFO: Got endpoints: latency-svc-jrk7w [1.039298967s]
Sep 20 13:34:42.372: INFO: Created: latency-svc-ptttn
Sep 20 13:34:42.397: INFO: Got endpoints: latency-svc-ptttn [947.051465ms]
Sep 20 13:34:42.770: INFO: Created: latency-svc-kf27w
Sep 20 13:34:42.773: INFO: Created: latency-svc-pswz6
Sep 20 13:34:42.808: INFO: Got endpoints: latency-svc-kf27w [629.081664ms]
Sep 20 13:34:42.818: INFO: Created: latency-svc-bxjbt
Sep 20 13:34:42.818: INFO: Created: latency-svc-6s6c4
Sep 20 13:34:42.818: INFO: Created: latency-svc-426zm
Sep 20 13:34:42.819: INFO: Created: latency-svc-7rj9p
Sep 20 13:34:42.819: INFO: Created: latency-svc-twq6p
Sep 20 13:34:42.820: INFO: Created: latency-svc-cq4rj
Sep 20 13:34:42.820: INFO: Created: latency-svc-7p8xs
Sep 20 13:34:42.820: INFO: Created: latency-svc-hq62m
Sep 20 13:34:42.821: INFO: Created: latency-svc-qhrjd
Sep 20 13:34:42.821: INFO: Created: latency-svc-lj56d
Sep 20 13:34:42.821: INFO: Created: latency-svc-hzsjk
Sep 20 13:34:42.821: INFO: Created: latency-svc-28ndv
Sep 20 13:34:42.821: INFO: Created: latency-svc-wsh64
Sep 20 13:34:42.885: INFO: Got endpoints: latency-svc-426zm [622.913977ms]
Sep 20 13:34:42.885: INFO: Got endpoints: latency-svc-6s6c4 [755.316936ms]
Sep 20 13:34:42.890: INFO: Got endpoints: latency-svc-qhrjd [592.419397ms]
Sep 20 13:34:42.907: INFO: Got endpoints: latency-svc-hq62m [587.777312ms]
Sep 20 13:34:42.907: INFO: Got endpoints: latency-svc-cq4rj [1.447898301s]
Sep 20 13:34:42.953: INFO: Created: latency-svc-bgqrx
Sep 20 13:34:42.954: INFO: Got endpoints: latency-svc-28ndv [792.423963ms]
Sep 20 13:34:42.955: INFO: Got endpoints: latency-svc-7p8xs [557.030496ms]
Sep 20 13:34:42.955: INFO: Got endpoints: latency-svc-twq6p [583.59791ms]
Sep 20 13:34:42.955: INFO: Got endpoints: latency-svc-7rj9p [874.830367ms]
Sep 20 13:34:42.955: INFO: Got endpoints: latency-svc-wsh64 [999.177302ms]
Sep 20 13:34:43.012: INFO: Got endpoints: latency-svc-lj56d [798.677342ms]
Sep 20 13:34:43.012: INFO: Got endpoints: latency-svc-hzsjk [916.961806ms]
Sep 20 13:34:43.014: INFO: Got endpoints: latency-svc-pswz6 [677.670869ms]
Sep 20 13:34:43.015: INFO: Got endpoints: latency-svc-bgqrx [206.207331ms]
Sep 20 13:34:43.015: INFO: Got endpoints: latency-svc-bxjbt [1.248148043s]
Sep 20 13:34:43.038: INFO: Created: latency-svc-648p5
Sep 20 13:34:43.050: INFO: Created: latency-svc-jcv9x
Sep 20 13:34:43.054: INFO: Got endpoints: latency-svc-648p5 [168.962364ms]
Sep 20 13:34:43.069: INFO: Got endpoints: latency-svc-jcv9x [114.732926ms]
Sep 20 13:34:43.069: INFO: Created: latency-svc-8bjgz
Sep 20 13:34:43.279: INFO: Got endpoints: latency-svc-8bjgz [323.88861ms]
Sep 20 13:34:43.287: INFO: Created: latency-svc-6r258
Sep 20 13:34:43.304: INFO: Got endpoints: latency-svc-6r258 [413.944113ms]
Sep 20 13:34:43.316: INFO: Created: latency-svc-6jqft
Sep 20 13:34:43.330: INFO: Got endpoints: latency-svc-6jqft [423.336212ms]
Sep 20 13:34:43.336: INFO: Created: latency-svc-sx9kx
Sep 20 13:34:43.348: INFO: Got endpoints: latency-svc-sx9kx [440.921478ms]
Sep 20 13:34:43.369: INFO: Created: latency-svc-f669l
Sep 20 13:34:43.371: INFO: Created: latency-svc-5wvd2
Sep 20 13:34:43.508: INFO: Got endpoints: latency-svc-5wvd2 [553.160144ms]
Sep 20 13:34:43.508: INFO: Got endpoints: latency-svc-f669l [553.563935ms]
Sep 20 13:34:43.514: INFO: Created: latency-svc-qxzfj
Sep 20 13:34:43.531: INFO: Got endpoints: latency-svc-qxzfj [575.553186ms]
Sep 20 13:34:43.542: INFO: Created: latency-svc-px7p2
Sep 20 13:34:43.572: INFO: Created: latency-svc-ktv27
Sep 20 13:34:43.577: INFO: Got endpoints: latency-svc-px7p2 [692.121829ms]
Sep 20 13:34:43.669: INFO: Created: latency-svc-gdl27
Sep 20 13:34:43.674: INFO: Got endpoints: latency-svc-ktv27 [661.045363ms]
Sep 20 13:34:43.687: INFO: Got endpoints: latency-svc-gdl27 [671.638046ms]
Sep 20 13:34:44.989: INFO: Created: latency-svc-97r8w
Sep 20 13:34:44.990: INFO: Created: latency-svc-xp65k
Sep 20 13:34:44.991: INFO: Created: latency-svc-fb8f7
Sep 20 13:34:45.010: INFO: Created: latency-svc-knqb7
Sep 20 13:34:45.011: INFO: Created: latency-svc-mlh2v
Sep 20 13:34:45.011: INFO: Created: latency-svc-cgzhl
Sep 20 13:34:45.011: INFO: Created: latency-svc-mtmcg
Sep 20 13:34:45.011: INFO: Created: latency-svc-q6s8h
Sep 20 13:34:45.011: INFO: Created: latency-svc-t5zl7
Sep 20 13:34:45.011: INFO: Created: latency-svc-pck2f
Sep 20 13:34:45.011: INFO: Created: latency-svc-kcpvm
Sep 20 13:34:45.012: INFO: Created: latency-svc-xf8lc
Sep 20 13:34:45.012: INFO: Created: latency-svc-sz6zl
Sep 20 13:34:45.012: INFO: Created: latency-svc-gbdtq
Sep 20 13:34:45.012: INFO: Created: latency-svc-zglld
Sep 20 13:34:45.068: INFO: Got endpoints: latency-svc-97r8w [1.788803264s]
Sep 20 13:34:45.083: INFO: Got endpoints: latency-svc-xf8lc [2.068025098s]
Sep 20 13:34:45.083: INFO: Got endpoints: latency-svc-fb8f7 [1.396305129s]
Sep 20 13:34:45.083: INFO: Got endpoints: latency-svc-zglld [1.552331806s]
Sep 20 13:34:45.083: INFO: Got endpoints: latency-svc-xp65k [1.77940415s]
Sep 20 13:34:45.313: INFO: Got endpoints: latency-svc-kcpvm [2.259372798s]
Sep 20 13:34:45.803: INFO: Got endpoints: latency-svc-q6s8h [2.790470616s]
Sep 20 13:34:45.803: INFO: Got endpoints: latency-svc-t5zl7 [2.455212751s]
Sep 20 13:34:45.803: INFO: Got endpoints: latency-svc-cgzhl [2.472942609s]
Sep 20 13:34:45.804: INFO: Got endpoints: latency-svc-mtmcg [2.295831043s]
Sep 20 13:34:45.905: INFO: Got endpoints: latency-svc-mlh2v [2.397060293s]
Sep 20 13:34:45.922: INFO: Got endpoints: latency-svc-sz6zl [2.344368068s]
Sep 20 13:34:45.922: INFO: Got endpoints: latency-svc-pck2f [2.248117528s]
Sep 20 13:34:45.922: INFO: Got endpoints: latency-svc-knqb7 [2.907616958s]
Sep 20 13:34:45.922: INFO: Got endpoints: latency-svc-gbdtq [2.852962218s]
Sep 20 13:34:45.937: INFO: Created: latency-svc-c2lbl
Sep 20 13:34:45.948: INFO: Created: latency-svc-rv7d5
Sep 20 13:34:45.956: INFO: Got endpoints: latency-svc-c2lbl [887.932987ms]
Sep 20 13:34:46.143: INFO: Got endpoints: latency-svc-rv7d5 [1.059792771s]
Sep 20 13:34:46.145: INFO: Created: latency-svc-4t8fn
Sep 20 13:34:46.159: INFO: Got endpoints: latency-svc-4t8fn [1.076108754s]
Sep 20 13:34:46.170: INFO: Created: latency-svc-6w2mj
Sep 20 13:34:46.188: INFO: Got endpoints: latency-svc-6w2mj [1.105165075s]
Sep 20 13:34:46.205: INFO: Created: latency-svc-dvtjf
Sep 20 13:34:46.212: INFO: Got endpoints: latency-svc-dvtjf [1.128754559s]
Sep 20 13:34:46.224: INFO: Created: latency-svc-7g8kp
Sep 20 13:34:46.232: INFO: Got endpoints: latency-svc-7g8kp [919.108511ms]
Sep 20 13:34:46.243: INFO: Created: latency-svc-49k5v
Sep 20 13:34:46.261: INFO: Created: latency-svc-tbmx7
Sep 20 13:34:46.262: INFO: Got endpoints: latency-svc-49k5v [458.694175ms]
Sep 20 13:34:46.275: INFO: Got endpoints: latency-svc-tbmx7 [471.63986ms]
Sep 20 13:34:46.282: INFO: Created: latency-svc-rhb52
Sep 20 13:34:46.294: INFO: Got endpoints: latency-svc-rhb52 [491.102221ms]
Sep 20 13:34:46.320: INFO: Created: latency-svc-wx8f6
Sep 20 13:34:46.340: INFO: Got endpoints: latency-svc-wx8f6 [536.542673ms]
Sep 20 13:34:46.354: INFO: Created: latency-svc-tbb7v
Sep 20 13:34:46.364: INFO: Created: latency-svc-m69kk
Sep 20 13:34:46.543: INFO: Got endpoints: latency-svc-m69kk [620.893703ms]
Sep 20 13:34:46.543: INFO: Got endpoints: latency-svc-tbb7v [637.056518ms]
Sep 20 13:34:46.547: INFO: Created: latency-svc-87rms
Sep 20 13:34:46.569: INFO: Got endpoints: latency-svc-87rms [647.276957ms]
Sep 20 13:34:46.574: INFO: Created: latency-svc-swfv8
Sep 20 13:34:46.586: INFO: Created: latency-svc-hx2wp
Sep 20 13:34:46.590: INFO: Got endpoints: latency-svc-swfv8 [667.609868ms]
Sep 20 13:34:46.594: INFO: Got endpoints: latency-svc-hx2wp [671.995618ms]
Sep 20 13:34:46.618: INFO: Created: latency-svc-fxfjz
Sep 20 13:34:46.618: INFO: Got endpoints: latency-svc-fxfjz [662.446843ms]
Sep 20 13:34:46.619: INFO: Created: latency-svc-86jln
Sep 20 13:34:46.627: INFO: Got endpoints: latency-svc-86jln [483.866128ms]
Sep 20 13:34:46.636: INFO: Created: latency-svc-2452f
Sep 20 13:34:46.659: INFO: Got endpoints: latency-svc-2452f [499.406522ms]
Sep 20 13:34:46.693: INFO: Created: latency-svc-t599h
Sep 20 13:34:46.712: INFO: Got endpoints: latency-svc-t599h [523.437489ms]
Sep 20 13:34:46.717: INFO: Created: latency-svc-9vrh9
Sep 20 13:34:46.729: INFO: Created: latency-svc-wxtm7
Sep 20 13:34:46.733: INFO: Got endpoints: latency-svc-9vrh9 [521.508605ms]
Sep 20 13:34:46.743: INFO: Created: latency-svc-9x7k5
Sep 20 13:34:47.196: INFO: Got endpoints: latency-svc-wxtm7 [963.724381ms]
Sep 20 13:34:47.199: INFO: Got endpoints: latency-svc-9x7k5 [937.202103ms]
Sep 20 13:34:47.287: INFO: Created: latency-svc-8gc9g
Sep 20 13:34:47.297: INFO: Got endpoints: latency-svc-8gc9g [1.022345823s]
Sep 20 13:34:47.305: INFO: Created: latency-svc-2bphl
Sep 20 13:34:47.317: INFO: Created: latency-svc-npmtw
Sep 20 13:34:47.317: INFO: Got endpoints: latency-svc-2bphl [1.022382251s]
Sep 20 13:34:47.334: INFO: Got endpoints: latency-svc-npmtw [994.140284ms]
Sep 20 13:34:47.335: INFO: Created: latency-svc-qdfq6
Sep 20 13:34:47.348: INFO: Got endpoints: latency-svc-qdfq6 [805.369566ms]
Sep 20 13:34:47.353: INFO: Created: latency-svc-9t62j
Sep 20 13:34:47.363: INFO: Created: latency-svc-fcpts
Sep 20 13:34:47.367: INFO: Got endpoints: latency-svc-9t62j [823.991152ms]
Sep 20 13:34:47.389: INFO: Got endpoints: latency-svc-fcpts [819.574985ms]
Sep 20 13:34:47.797: INFO: Created: latency-svc-lpczn
Sep 20 13:34:47.804: INFO: Created: latency-svc-ql7zx
Sep 20 13:34:47.804: INFO: Created: latency-svc-gnw8s
Sep 20 13:34:47.807: INFO: Created: latency-svc-rrkbx
Sep 20 13:34:47.832: INFO: Created: latency-svc-nc6bk
Sep 20 13:34:47.833: INFO: Created: latency-svc-4lz95
Sep 20 13:34:47.837: INFO: Created: latency-svc-26fwl
Sep 20 13:34:47.837: INFO: Created: latency-svc-hf54q
Sep 20 13:34:47.838: INFO: Created: latency-svc-zgqtd
Sep 20 13:34:47.838: INFO: Created: latency-svc-qz8l5
Sep 20 13:34:47.838: INFO: Created: latency-svc-ck8k7
Sep 20 13:34:47.838: INFO: Created: latency-svc-kf65b
Sep 20 13:34:47.838: INFO: Created: latency-svc-4jkpn
Sep 20 13:34:47.838: INFO: Created: latency-svc-qdh2m
Sep 20 13:34:47.838: INFO: Created: latency-svc-r5rjx
Sep 20 13:34:47.845: INFO: Got endpoints: latency-svc-lpczn [1.186413418s]
Sep 20 13:34:47.845: INFO: Got endpoints: latency-svc-4jkpn [646.296721ms]
Sep 20 13:34:47.896: INFO: Got endpoints: latency-svc-ck8k7 [1.183946992s]
Sep 20 13:34:47.896: INFO: Got endpoints: latency-svc-gnw8s [1.162321778s]
Sep 20 13:34:47.896: INFO: Got endpoints: latency-svc-qz8l5 [699.628148ms]
Sep 20 13:34:47.917: INFO: Got endpoints: latency-svc-26fwl [550.339773ms]
Sep 20 13:34:47.917: INFO: Got endpoints: latency-svc-4lz95 [620.00136ms]
Sep 20 13:34:47.931: INFO: Got endpoints: latency-svc-qdh2m [583.069974ms]
Sep 20 13:34:47.948: INFO: Created: latency-svc-d94gm
Sep 20 13:34:47.950: INFO: Got endpoints: latency-svc-kf65b [615.044193ms]
Sep 20 13:34:47.950: INFO: Got endpoints: latency-svc-ql7zx [632.658361ms]
Sep 20 13:34:47.951: INFO: Got endpoints: latency-svc-r5rjx [562.759356ms]
Sep 20 13:34:47.954: INFO: Got endpoints: latency-svc-zgqtd [1.335463144s]
Sep 20 13:34:47.967: INFO: Got endpoints: latency-svc-rrkbx [1.339776858s]
Sep 20 13:34:47.981: INFO: Created: latency-svc-9cl42
Sep 20 13:34:48.105: INFO: Got endpoints: latency-svc-d94gm [260.151562ms]
Sep 20 13:34:48.106: INFO: Got endpoints: latency-svc-hf54q [1.516200097s]
Sep 20 13:34:48.106: INFO: Got endpoints: latency-svc-nc6bk [1.51176929s]
Sep 20 13:34:48.107: INFO: Created: latency-svc-fk8jj
Sep 20 13:34:48.110: INFO: Got endpoints: latency-svc-9cl42 [264.473523ms]
Sep 20 13:34:48.122: INFO: Created: latency-svc-ggwcq
Sep 20 13:34:48.125: INFO: Got endpoints: latency-svc-fk8jj [229.522587ms]
Sep 20 13:34:48.138: INFO: Created: latency-svc-q9nxk
Sep 20 13:34:48.141: INFO: Got endpoints: latency-svc-ggwcq [245.494152ms]
Sep 20 13:34:48.147: INFO: Got endpoints: latency-svc-q9nxk [250.669531ms]
Sep 20 13:34:48.170: INFO: Created: latency-svc-9c8n2
Sep 20 13:34:48.185: INFO: Created: latency-svc-gbr8j
Sep 20 13:34:48.185: INFO: Got endpoints: latency-svc-9c8n2 [268.079857ms]
Sep 20 13:34:48.294: INFO: Got endpoints: latency-svc-gbr8j [376.353135ms]
Sep 20 13:34:48.294: INFO: Created: latency-svc-wkvrm
Sep 20 13:34:48.315: INFO: Created: latency-svc-84ksb
Sep 20 13:34:48.316: INFO: Got endpoints: latency-svc-wkvrm [384.329328ms]
Sep 20 13:34:48.333: INFO: Created: latency-svc-w5jj9
Sep 20 13:34:48.434: INFO: Got endpoints: latency-svc-w5jj9 [482.016597ms]
Sep 20 13:34:48.437: INFO: Got endpoints: latency-svc-84ksb [487.374038ms]
Sep 20 13:34:48.442: INFO: Created: latency-svc-bv8k2
Sep 20 13:34:48.460: INFO: Created: latency-svc-6tkvg
Sep 20 13:34:48.460: INFO: Got endpoints: latency-svc-bv8k2 [510.760871ms]
Sep 20 13:34:48.468: INFO: Created: latency-svc-mkjwh
Sep 20 13:34:48.480: INFO: Got endpoints: latency-svc-6tkvg [525.685834ms]
Sep 20 13:34:48.483: INFO: Created: latency-svc-5w6rp
Sep 20 13:34:48.488: INFO: Got endpoints: latency-svc-mkjwh [521.649682ms]
Sep 20 13:34:48.496: INFO: Got endpoints: latency-svc-5w6rp [390.967813ms]
Sep 20 13:34:48.499: INFO: Created: latency-svc-76rhx
Sep 20 13:34:48.832: INFO: Got endpoints: latency-svc-76rhx [725.935622ms]
Sep 20 13:34:48.924: INFO: Created: latency-svc-zknfp
Sep 20 13:34:49.331: INFO: Got endpoints: latency-svc-zknfp [1.224788714s]
Sep 20 13:34:49.692: INFO: Created: latency-svc-6428p
Sep 20 13:34:50.131: INFO: Got endpoints: latency-svc-6428p [2.021042489s]
Sep 20 13:34:50.327: INFO: Created: latency-svc-c5hj4
Sep 20 13:34:50.392: INFO: Got endpoints: latency-svc-c5hj4 [2.267071851s]
Sep 20 13:34:50.625: INFO: Created: latency-svc-bb5b4
Sep 20 13:34:51.401: INFO: Got endpoints: latency-svc-bb5b4 [3.259874465s]
Sep 20 13:34:51.424: INFO: Created: latency-svc-z8h4k
Sep 20 13:34:51.494: INFO: Got endpoints: latency-svc-z8h4k [3.347581673s]
Sep 20 13:34:51.623: INFO: Created: latency-svc-d9k5d
Sep 20 13:34:52.207: INFO: Got endpoints: latency-svc-d9k5d [4.021482501s]
Sep 20 13:34:52.249: INFO: Created: latency-svc-sdt4x
Sep 20 13:34:52.506: INFO: Got endpoints: latency-svc-sdt4x [4.211802056s]
Sep 20 13:34:52.700: INFO: Created: latency-svc-f98bq
Sep 20 13:34:52.837: INFO: Got endpoints: latency-svc-f98bq [4.521280811s]
Sep 20 13:34:52.960: INFO: Created: latency-svc-bvcff
Sep 20 13:34:53.037: INFO: Got endpoints: latency-svc-bvcff [4.603792704s]
Sep 20 13:34:53.045: INFO: Created: latency-svc-f5bzr
Sep 20 13:34:53.055: INFO: Got endpoints: latency-svc-f5bzr [4.617848951s]
Sep 20 13:34:53.069: INFO: Created: latency-svc-m8g5m
Sep 20 13:34:53.097: INFO: Created: latency-svc-9bdj5
Sep 20 13:34:53.099: INFO: Got endpoints: latency-svc-m8g5m [4.638296998s]
Sep 20 13:34:53.114: INFO: Created: latency-svc-7pp2b
Sep 20 13:34:53.117: INFO: Got endpoints: latency-svc-9bdj5 [4.637226281s]
Sep 20 13:34:53.131: INFO: Created: latency-svc-hd2rw
Sep 20 13:34:53.162: INFO: Created: latency-svc-jcdsl
Sep 20 13:34:53.164: INFO: Got endpoints: latency-svc-hd2rw [4.666954728s]
Sep 20 13:34:53.164: INFO: Got endpoints: latency-svc-7pp2b [4.675400286s]
Sep 20 13:34:53.205: INFO: Created: latency-svc-dtzvw
Sep 20 13:34:53.206: INFO: Got endpoints: latency-svc-jcdsl [4.374216504s]
Sep 20 13:34:53.222: INFO: Got endpoints: latency-svc-dtzvw [3.891252333s]
Sep 20 13:34:53.319: INFO: Created: latency-svc-btgnp
Sep 20 13:34:54.298: INFO: Got endpoints: latency-svc-btgnp [4.16651543s]
Sep 20 13:34:54.805: INFO: Created: latency-svc-vj4hn
Sep 20 13:34:55.287: INFO: Got endpoints: latency-svc-vj4hn [3.885821483s]
Sep 20 13:34:55.446: INFO: Created: latency-svc-bfdrc
Sep 20 13:34:56.670: INFO: Got endpoints: latency-svc-bfdrc [5.176206606s]
Sep 20 13:34:56.675: INFO: Created: latency-svc-rmfft
Sep 20 13:34:57.033: INFO: Got endpoints: latency-svc-rmfft [4.826215729s]
Sep 20 13:34:57.569: INFO: Created: latency-svc-c6v8s
Sep 20 13:34:58.191: INFO: Got endpoints: latency-svc-c6v8s [5.684934948s]
Sep 20 13:34:59.029: INFO: Created: latency-svc-8tj4q
Sep 20 13:34:59.623: INFO: Got endpoints: latency-svc-8tj4q [6.785685178s]
Sep 20 13:35:00.451: INFO: Created: latency-svc-t4mr9
Sep 20 13:35:00.661: INFO: Got endpoints: latency-svc-t4mr9 [7.623285967s]
Sep 20 13:35:00.694: INFO: Created: latency-svc-l25dm
Sep 20 13:35:00.976: INFO: Got endpoints: latency-svc-l25dm [7.921096986s]
Sep 20 13:35:01.782: INFO: Created: latency-svc-6jfbw
Sep 20 13:35:01.870: INFO: Got endpoints: latency-svc-6jfbw [8.771004005s]
Sep 20 13:35:01.877: INFO: Created: latency-svc-rqvhk
Sep 20 13:35:01.959: INFO: Got endpoints: latency-svc-rqvhk [8.842409598s]
Sep 20 13:35:01.969: INFO: Created: latency-svc-6tvq6
Sep 20 13:35:01.993: INFO: Got endpoints: latency-svc-6tvq6 [8.829013344s]
Sep 20 13:35:01.995: INFO: Created: latency-svc-dp8ln
Sep 20 13:35:02.008: INFO: Got endpoints: latency-svc-dp8ln [8.844765335s]
Sep 20 13:35:02.011: INFO: Created: latency-svc-slvqs
Sep 20 13:35:02.273: INFO: Got endpoints: latency-svc-slvqs [9.066405245s]
Sep 20 13:35:02.395: INFO: Created: latency-svc-r5mxq
Sep 20 13:35:02.395: INFO: Created: latency-svc-hn6hr
Sep 20 13:35:03.494: INFO: Got endpoints: latency-svc-hn6hr [10.271563812s]
Sep 20 13:35:03.494: INFO: Got endpoints: latency-svc-r5mxq [13.1016616s]
Sep 20 13:35:04.174: INFO: Created: latency-svc-r6l45
Sep 20 13:35:04.378: INFO: Got endpoints: latency-svc-r6l45 [10.080357498s]
Sep 20 13:35:04.546: INFO: Created: latency-svc-vbgbd
Sep 20 13:35:04.863: INFO: Got endpoints: latency-svc-vbgbd [9.576036228s]
Sep 20 13:35:05.082: INFO: Created: latency-svc-8g2r9
Sep 20 13:35:05.437: INFO: Got endpoints: latency-svc-8g2r9 [8.766416115s]
Sep 20 13:35:05.656: INFO: Created: latency-svc-mg6gt
Sep 20 13:35:06.375: INFO: Got endpoints: latency-svc-mg6gt [9.342232936s]
Sep 20 13:35:10.323: INFO: Created: latency-svc-wcpgb
Sep 20 13:35:10.323: INFO: Created: latency-svc-jx86p
Sep 20 13:35:10.866: INFO: Created: latency-svc-jt8lv
Sep 20 13:35:10.873: INFO: Created: latency-svc-8886h
Sep 20 13:35:10.878: INFO: Created: latency-svc-65k49
Sep 20 13:35:10.878: INFO: Created: latency-svc-g6p6p
Sep 20 13:35:10.878: INFO: Created: latency-svc-ldrwb
Sep 20 13:35:10.878: INFO: Created: latency-svc-7hn9z
Sep 20 13:35:10.879: INFO: Created: latency-svc-plmn4
Sep 20 13:35:10.879: INFO: Created: latency-svc-64vkv
Sep 20 13:35:10.893: INFO: Created: latency-svc-dk2rm
Sep 20 13:35:10.893: INFO: Created: latency-svc-dr4tv
Sep 20 13:35:10.893: INFO: Created: latency-svc-9stct
Sep 20 13:35:10.895: INFO: Created: latency-svc-dm86l
Sep 20 13:35:10.896: INFO: Created: latency-svc-qlggt
Sep 20 13:35:10.953: INFO: Got endpoints: latency-svc-jx86p [6.089966251s]
Sep 20 13:35:11.012: INFO: Got endpoints: latency-svc-jt8lv [4.636190078s]
Sep 20 13:35:11.012: INFO: Got endpoints: latency-svc-wcpgb [9.052269799s]
Sep 20 13:35:11.012: INFO: Got endpoints: latency-svc-g6p6p [12.821350454s]
Sep 20 13:35:11.013: INFO: Got endpoints: latency-svc-7hn9z [10.351858286s]
Sep 20 13:35:11.013: INFO: Got endpoints: latency-svc-ldrwb [11.390131922s]
Sep 20 13:35:11.364: INFO: Got endpoints: latency-svc-dr4tv [9.493901754s]
Sep 20 13:35:11.398: INFO: Created: latency-svc-nq7s8
Sep 20 13:35:11.399: INFO: Got endpoints: latency-svc-dm86l [10.423438569s]
Sep 20 13:35:11.400: INFO: Got endpoints: latency-svc-65k49 [7.905585687s]
Sep 20 13:35:11.403: INFO: Got endpoints: latency-svc-plmn4 [7.90869843s]
Sep 20 13:35:11.403: INFO: Got endpoints: latency-svc-64vkv [7.02508541s]
Sep 20 13:35:11.403: INFO: Got endpoints: latency-svc-dk2rm [5.96626366s]
Sep 20 13:35:11.511: INFO: Created: latency-svc-b5kp9
Sep 20 13:35:11.518: INFO: Got endpoints: latency-svc-qlggt [9.245601606s]
Sep 20 13:35:11.521: INFO: Got endpoints: latency-svc-9stct [9.527825735s]
Sep 20 13:35:11.537: INFO: Got endpoints: latency-svc-nq7s8 [584.378359ms]
Sep 20 13:35:11.538: INFO: Got endpoints: latency-svc-8886h [9.529159267s]
Sep 20 13:35:11.570: INFO: Got endpoints: latency-svc-b5kp9 [557.962811ms]
Sep 20 13:35:16.075: INFO: Created: latency-svc-l8z2z
Sep 20 13:35:16.077: INFO: Created: latency-svc-whr4z
Sep 20 13:35:16.077: INFO: Created: latency-svc-8rssc
Sep 20 13:35:16.082: INFO: Created: latency-svc-wbmwz
Sep 20 13:35:16.083: INFO: Created: latency-svc-9sstl
Sep 20 13:35:16.083: INFO: Created: latency-svc-r5lnp
Sep 20 13:35:16.083: INFO: Created: latency-svc-jp7pf
Sep 20 13:35:16.084: INFO: Created: latency-svc-k6qj8
Sep 20 13:35:16.084: INFO: Created: latency-svc-c84sd
Sep 20 13:35:16.084: INFO: Created: latency-svc-4hznf
Sep 20 13:35:16.086: INFO: Created: latency-svc-g7tps
Sep 20 13:35:16.086: INFO: Created: latency-svc-ld9zq
Sep 20 13:35:16.087: INFO: Created: latency-svc-srb74
Sep 20 13:35:16.087: INFO: Created: latency-svc-djgs6
Sep 20 13:35:16.149: INFO: Created: latency-svc-vwt5j
Sep 20 13:35:16.277: INFO: Got endpoints: latency-svc-8rssc [4.739934136s]
Sep 20 13:35:16.282: INFO: Got endpoints: latency-svc-l8z2z [4.882867812s]
Sep 20 13:35:16.283: INFO: Got endpoints: latency-svc-whr4z [4.918821085s]
Sep 20 13:35:16.283: INFO: Got endpoints: latency-svc-srb74 [4.745034483s]
Sep 20 13:35:16.283: INFO: Got endpoints: latency-svc-ld9zq [4.880176914s]
Sep 20 13:35:16.569: INFO: Got endpoints: latency-svc-g7tps [5.165583457s]
Sep 20 13:35:16.569: INFO: Got endpoints: latency-svc-djgs6 [5.169311349s]
Sep 20 13:35:16.580: INFO: Got endpoints: latency-svc-jp7pf [5.176703402s]
Sep 20 13:35:16.580: INFO: Got endpoints: latency-svc-9sstl [5.010435373s]
Sep 20 13:35:16.580: INFO: Got endpoints: latency-svc-r5lnp [5.56820364s]
Sep 20 13:35:17.105: INFO: Got endpoints: latency-svc-k6qj8 [5.584593682s]
Sep 20 13:35:17.106: INFO: Got endpoints: latency-svc-vwt5j [6.09303907s]
Sep 20 13:35:17.106: INFO: Got endpoints: latency-svc-c84sd [6.093699875s]
Sep 20 13:35:17.106: INFO: Got endpoints: latency-svc-4hznf [5.587495328s]
Sep 20 13:35:17.106: INFO: Got endpoints: latency-svc-wbmwz [6.093072313s]
Sep 20 13:35:17.122: INFO: Created: latency-svc-gx8kf
Sep 20 13:35:17.748: INFO: Got endpoints: latency-svc-gx8kf [1.465037144s]
Sep 20 13:35:17.753: INFO: Created: latency-svc-wfj6w
Sep 20 13:35:18.060: INFO: Got endpoints: latency-svc-wfj6w [1.776746644s]
Sep 20 13:35:18.378: INFO: Created: latency-svc-8vgkw
Sep 20 13:35:19.034: INFO: Got endpoints: latency-svc-8vgkw [2.750808146s]
Sep 20 13:35:19.346: INFO: Created: latency-svc-h4j96
Sep 20 13:35:19.475: INFO: Got endpoints: latency-svc-h4j96 [3.197837019s]
Sep 20 13:35:19.484: INFO: Created: latency-svc-gz5q9
Sep 20 13:35:19.502: INFO: Got endpoints: latency-svc-gz5q9 [3.21961507s]
Sep 20 13:35:19.509: INFO: Created: latency-svc-mxdh5
Sep 20 13:35:19.515: INFO: Got endpoints: latency-svc-mxdh5 [2.94586865s]
Sep 20 13:35:19.519: INFO: Created: latency-svc-cd9t4
Sep 20 13:35:20.108: INFO: Got endpoints: latency-svc-cd9t4 [3.538505004s]
Sep 20 13:35:20.548: INFO: Created: latency-svc-wmx4x
Sep 20 13:35:20.898: INFO: Got endpoints: latency-svc-wmx4x [4.317779088s]
Sep 20 13:35:21.058: INFO: Created: latency-svc-ptng2
Sep 20 13:35:21.641: INFO: Got endpoints: latency-svc-ptng2 [5.060212686s]
Sep 20 13:35:22.045: INFO: Created: latency-svc-wmslh
Sep 20 13:35:22.559: INFO: Got endpoints: latency-svc-wmslh [5.978395463s]
Sep 20 13:35:23.215: INFO: Created: latency-svc-gj59q
Sep 20 13:35:23.568: INFO: Got endpoints: latency-svc-gj59q [6.46266868s]
Sep 20 13:35:23.568: INFO: Latencies: [32.383929ms 61.11518ms 81.965593ms 114.732926ms 145.150922ms 151.986529ms 168.962364ms 206.207331ms 229.522587ms 245.494152ms 250.669531ms 260.151562ms 264.473523ms 268.079857ms 270.690644ms 279.536286ms 323.88861ms 376.353135ms 384.329328ms 390.967813ms 413.944113ms 423.336212ms 440.921478ms 458.694175ms 471.63986ms 482.016597ms 483.866128ms 487.374038ms 491.102221ms 499.406522ms 510.760871ms 521.508605ms 521.649682ms 523.437489ms 525.685834ms 536.542673ms 550.339773ms 553.160144ms 553.563935ms 557.030496ms 557.962811ms 562.759356ms 575.553186ms 583.069974ms 583.59791ms 584.378359ms 587.238326ms 587.777312ms 592.419397ms 615.044193ms 620.00136ms 620.893703ms 622.913977ms 629.081664ms 632.658361ms 637.056518ms 646.296721ms 647.276957ms 661.045363ms 662.446843ms 667.609868ms 671.638046ms 671.995618ms 677.670869ms 692.121829ms 699.628148ms 725.935622ms 755.316936ms 776.40561ms 792.423963ms 798.677342ms 805.369566ms 819.574985ms 823.991152ms 874.830367ms 887.932987ms 900.669309ms 915.722042ms 916.961806ms 919.108511ms 937.202103ms 947.051465ms 950.453064ms 963.724381ms 982.076109ms 994.140284ms 999.177302ms 999.423495ms 1.011910596s 1.022345823s 1.022382251s 1.033752448s 1.039298967s 1.050302893s 1.056272789s 1.057380326s 1.059792771s 1.076108754s 1.105165075s 1.128754559s 1.162321778s 1.183946992s 1.186413418s 1.224788714s 1.248148043s 1.335463144s 1.339776858s 1.396305129s 1.447898301s 1.465037144s 1.51176929s 1.516200097s 1.552331806s 1.776746644s 1.77940415s 1.788803264s 2.021042489s 2.068025098s 2.248117528s 2.259372798s 2.267071851s 2.295831043s 2.344368068s 2.397060293s 2.455212751s 2.472942609s 2.750808146s 2.790470616s 2.852962218s 2.907616958s 2.94586865s 3.197837019s 3.21961507s 3.259874465s 3.347581673s 3.538505004s 3.885821483s 3.891252333s 4.021482501s 4.16651543s 4.211802056s 4.317779088s 4.374216504s 4.521280811s 4.603792704s 4.617848951s 4.636190078s 4.637226281s 4.638296998s 4.666954728s 4.675400286s 4.739934136s 4.745034483s 4.826215729s 4.880176914s 4.882867812s 4.918821085s 5.010435373s 5.060212686s 5.165583457s 5.169311349s 5.176206606s 5.176703402s 5.56820364s 5.584593682s 5.587495328s 5.684934948s 5.96626366s 5.978395463s 6.089966251s 6.09303907s 6.093072313s 6.093699875s 6.46266868s 6.785685178s 7.02508541s 7.623285967s 7.905585687s 7.90869843s 7.921096986s 8.766416115s 8.771004005s 8.829013344s 8.842409598s 8.844765335s 9.052269799s 9.066405245s 9.245601606s 9.342232936s 9.493901754s 9.527825735s 9.529159267s 9.576036228s 10.080357498s 10.271563812s 10.351858286s 10.423438569s 11.390131922s 12.821350454s 13.1016616s]
Sep 20 13:35:23.569: INFO: 50 %ile: 1.162321778s
Sep 20 13:35:23.569: INFO: 90 %ile: 8.766416115s
Sep 20 13:35:23.569: INFO: 99 %ile: 12.821350454s
Sep 20 13:35:23.569: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Sep 20 13:35:23.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-2940" for this suite. 09/20/23 13:35:23.581
------------------------------
â€¢ [SLOW TEST] [45.868 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:34:37.778
    Sep 20 13:34:37.778: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename svc-latency 09/20/23 13:34:37.779
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:34:37.81
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:34:37.812
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Sep 20 13:34:37.873: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-2940 09/20/23 13:34:37.874
    I0920 13:34:37.890716      20 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2940, replica count: 1
    I0920 13:34:38.941590      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0920 13:34:39.942879      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0920 13:34:40.943685      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep 20 13:34:41.164: INFO: Created: latency-svc-kwdgd
    Sep 20 13:34:41.179: INFO: Got endpoints: latency-svc-kwdgd [135.062049ms]
    Sep 20 13:34:41.200: INFO: Created: latency-svc-hcl59
    Sep 20 13:34:41.211: INFO: Got endpoints: latency-svc-hcl59 [32.383929ms]
    Sep 20 13:34:41.217: INFO: Created: latency-svc-mjgpx
    Sep 20 13:34:41.241: INFO: Got endpoints: latency-svc-mjgpx [61.11518ms]
    Sep 20 13:34:41.247: INFO: Created: latency-svc-bcqdp
    Sep 20 13:34:41.262: INFO: Got endpoints: latency-svc-bcqdp [81.965593ms]
    Sep 20 13:34:41.297: INFO: Created: latency-svc-9bx7k
    Sep 20 13:34:41.304: INFO: Created: latency-svc-vlrdg
    Sep 20 13:34:41.325: INFO: Got endpoints: latency-svc-9bx7k [145.150922ms]
    Sep 20 13:34:41.332: INFO: Got endpoints: latency-svc-vlrdg [151.986529ms]
    Sep 20 13:34:41.337: INFO: Created: latency-svc-4lglm
    Sep 20 13:34:41.450: INFO: Got endpoints: latency-svc-4lglm [270.690644ms]
    Sep 20 13:34:41.451: INFO: Created: latency-svc-g5kcb
    Sep 20 13:34:41.459: INFO: Got endpoints: latency-svc-g5kcb [279.536286ms]
    Sep 20 13:34:41.463: INFO: Created: latency-svc-2vdpz
    Sep 20 13:34:41.767: INFO: Got endpoints: latency-svc-2vdpz [587.238326ms]
    Sep 20 13:34:41.772: INFO: Created: latency-svc-krkk7
    Sep 20 13:34:41.956: INFO: Got endpoints: latency-svc-krkk7 [776.40561ms]
    Sep 20 13:34:41.960: INFO: Created: latency-svc-72ljn
    Sep 20 13:34:42.080: INFO: Got endpoints: latency-svc-72ljn [900.669309ms]
    Sep 20 13:34:42.084: INFO: Created: latency-svc-8s2r7
    Sep 20 13:34:42.095: INFO: Got endpoints: latency-svc-8s2r7 [915.722042ms]
    Sep 20 13:34:42.097: INFO: Created: latency-svc-t9bdv
    Sep 20 13:34:42.130: INFO: Got endpoints: latency-svc-t9bdv [950.453064ms]
    Sep 20 13:34:42.136: INFO: Created: latency-svc-vxfr9
    Sep 20 13:34:42.162: INFO: Got endpoints: latency-svc-vxfr9 [982.076109ms]
    Sep 20 13:34:42.165: INFO: Created: latency-svc-nfz4x
    Sep 20 13:34:42.179: INFO: Got endpoints: latency-svc-nfz4x [999.423495ms]
    Sep 20 13:34:42.194: INFO: Created: latency-svc-8tkwh
    Sep 20 13:34:42.214: INFO: Got endpoints: latency-svc-8tkwh [1.033752448s]
    Sep 20 13:34:42.240: INFO: Created: latency-svc-bhctj
    Sep 20 13:34:42.255: INFO: Created: latency-svc-fcrtg
    Sep 20 13:34:42.262: INFO: Got endpoints: latency-svc-bhctj [1.050302893s]
    Sep 20 13:34:42.297: INFO: Got endpoints: latency-svc-fcrtg [1.056272789s]
    Sep 20 13:34:42.302: INFO: Created: latency-svc-l2gp6
    Sep 20 13:34:42.315: INFO: Created: latency-svc-vss6w
    Sep 20 13:34:42.319: INFO: Got endpoints: latency-svc-l2gp6 [1.057380326s]
    Sep 20 13:34:42.336: INFO: Got endpoints: latency-svc-vss6w [1.011910596s]
    Sep 20 13:34:42.346: INFO: Created: latency-svc-jrk7w
    Sep 20 13:34:42.371: INFO: Got endpoints: latency-svc-jrk7w [1.039298967s]
    Sep 20 13:34:42.372: INFO: Created: latency-svc-ptttn
    Sep 20 13:34:42.397: INFO: Got endpoints: latency-svc-ptttn [947.051465ms]
    Sep 20 13:34:42.770: INFO: Created: latency-svc-kf27w
    Sep 20 13:34:42.773: INFO: Created: latency-svc-pswz6
    Sep 20 13:34:42.808: INFO: Got endpoints: latency-svc-kf27w [629.081664ms]
    Sep 20 13:34:42.818: INFO: Created: latency-svc-bxjbt
    Sep 20 13:34:42.818: INFO: Created: latency-svc-6s6c4
    Sep 20 13:34:42.818: INFO: Created: latency-svc-426zm
    Sep 20 13:34:42.819: INFO: Created: latency-svc-7rj9p
    Sep 20 13:34:42.819: INFO: Created: latency-svc-twq6p
    Sep 20 13:34:42.820: INFO: Created: latency-svc-cq4rj
    Sep 20 13:34:42.820: INFO: Created: latency-svc-7p8xs
    Sep 20 13:34:42.820: INFO: Created: latency-svc-hq62m
    Sep 20 13:34:42.821: INFO: Created: latency-svc-qhrjd
    Sep 20 13:34:42.821: INFO: Created: latency-svc-lj56d
    Sep 20 13:34:42.821: INFO: Created: latency-svc-hzsjk
    Sep 20 13:34:42.821: INFO: Created: latency-svc-28ndv
    Sep 20 13:34:42.821: INFO: Created: latency-svc-wsh64
    Sep 20 13:34:42.885: INFO: Got endpoints: latency-svc-426zm [622.913977ms]
    Sep 20 13:34:42.885: INFO: Got endpoints: latency-svc-6s6c4 [755.316936ms]
    Sep 20 13:34:42.890: INFO: Got endpoints: latency-svc-qhrjd [592.419397ms]
    Sep 20 13:34:42.907: INFO: Got endpoints: latency-svc-hq62m [587.777312ms]
    Sep 20 13:34:42.907: INFO: Got endpoints: latency-svc-cq4rj [1.447898301s]
    Sep 20 13:34:42.953: INFO: Created: latency-svc-bgqrx
    Sep 20 13:34:42.954: INFO: Got endpoints: latency-svc-28ndv [792.423963ms]
    Sep 20 13:34:42.955: INFO: Got endpoints: latency-svc-7p8xs [557.030496ms]
    Sep 20 13:34:42.955: INFO: Got endpoints: latency-svc-twq6p [583.59791ms]
    Sep 20 13:34:42.955: INFO: Got endpoints: latency-svc-7rj9p [874.830367ms]
    Sep 20 13:34:42.955: INFO: Got endpoints: latency-svc-wsh64 [999.177302ms]
    Sep 20 13:34:43.012: INFO: Got endpoints: latency-svc-lj56d [798.677342ms]
    Sep 20 13:34:43.012: INFO: Got endpoints: latency-svc-hzsjk [916.961806ms]
    Sep 20 13:34:43.014: INFO: Got endpoints: latency-svc-pswz6 [677.670869ms]
    Sep 20 13:34:43.015: INFO: Got endpoints: latency-svc-bgqrx [206.207331ms]
    Sep 20 13:34:43.015: INFO: Got endpoints: latency-svc-bxjbt [1.248148043s]
    Sep 20 13:34:43.038: INFO: Created: latency-svc-648p5
    Sep 20 13:34:43.050: INFO: Created: latency-svc-jcv9x
    Sep 20 13:34:43.054: INFO: Got endpoints: latency-svc-648p5 [168.962364ms]
    Sep 20 13:34:43.069: INFO: Got endpoints: latency-svc-jcv9x [114.732926ms]
    Sep 20 13:34:43.069: INFO: Created: latency-svc-8bjgz
    Sep 20 13:34:43.279: INFO: Got endpoints: latency-svc-8bjgz [323.88861ms]
    Sep 20 13:34:43.287: INFO: Created: latency-svc-6r258
    Sep 20 13:34:43.304: INFO: Got endpoints: latency-svc-6r258 [413.944113ms]
    Sep 20 13:34:43.316: INFO: Created: latency-svc-6jqft
    Sep 20 13:34:43.330: INFO: Got endpoints: latency-svc-6jqft [423.336212ms]
    Sep 20 13:34:43.336: INFO: Created: latency-svc-sx9kx
    Sep 20 13:34:43.348: INFO: Got endpoints: latency-svc-sx9kx [440.921478ms]
    Sep 20 13:34:43.369: INFO: Created: latency-svc-f669l
    Sep 20 13:34:43.371: INFO: Created: latency-svc-5wvd2
    Sep 20 13:34:43.508: INFO: Got endpoints: latency-svc-5wvd2 [553.160144ms]
    Sep 20 13:34:43.508: INFO: Got endpoints: latency-svc-f669l [553.563935ms]
    Sep 20 13:34:43.514: INFO: Created: latency-svc-qxzfj
    Sep 20 13:34:43.531: INFO: Got endpoints: latency-svc-qxzfj [575.553186ms]
    Sep 20 13:34:43.542: INFO: Created: latency-svc-px7p2
    Sep 20 13:34:43.572: INFO: Created: latency-svc-ktv27
    Sep 20 13:34:43.577: INFO: Got endpoints: latency-svc-px7p2 [692.121829ms]
    Sep 20 13:34:43.669: INFO: Created: latency-svc-gdl27
    Sep 20 13:34:43.674: INFO: Got endpoints: latency-svc-ktv27 [661.045363ms]
    Sep 20 13:34:43.687: INFO: Got endpoints: latency-svc-gdl27 [671.638046ms]
    Sep 20 13:34:44.989: INFO: Created: latency-svc-97r8w
    Sep 20 13:34:44.990: INFO: Created: latency-svc-xp65k
    Sep 20 13:34:44.991: INFO: Created: latency-svc-fb8f7
    Sep 20 13:34:45.010: INFO: Created: latency-svc-knqb7
    Sep 20 13:34:45.011: INFO: Created: latency-svc-mlh2v
    Sep 20 13:34:45.011: INFO: Created: latency-svc-cgzhl
    Sep 20 13:34:45.011: INFO: Created: latency-svc-mtmcg
    Sep 20 13:34:45.011: INFO: Created: latency-svc-q6s8h
    Sep 20 13:34:45.011: INFO: Created: latency-svc-t5zl7
    Sep 20 13:34:45.011: INFO: Created: latency-svc-pck2f
    Sep 20 13:34:45.011: INFO: Created: latency-svc-kcpvm
    Sep 20 13:34:45.012: INFO: Created: latency-svc-xf8lc
    Sep 20 13:34:45.012: INFO: Created: latency-svc-sz6zl
    Sep 20 13:34:45.012: INFO: Created: latency-svc-gbdtq
    Sep 20 13:34:45.012: INFO: Created: latency-svc-zglld
    Sep 20 13:34:45.068: INFO: Got endpoints: latency-svc-97r8w [1.788803264s]
    Sep 20 13:34:45.083: INFO: Got endpoints: latency-svc-xf8lc [2.068025098s]
    Sep 20 13:34:45.083: INFO: Got endpoints: latency-svc-fb8f7 [1.396305129s]
    Sep 20 13:34:45.083: INFO: Got endpoints: latency-svc-zglld [1.552331806s]
    Sep 20 13:34:45.083: INFO: Got endpoints: latency-svc-xp65k [1.77940415s]
    Sep 20 13:34:45.313: INFO: Got endpoints: latency-svc-kcpvm [2.259372798s]
    Sep 20 13:34:45.803: INFO: Got endpoints: latency-svc-q6s8h [2.790470616s]
    Sep 20 13:34:45.803: INFO: Got endpoints: latency-svc-t5zl7 [2.455212751s]
    Sep 20 13:34:45.803: INFO: Got endpoints: latency-svc-cgzhl [2.472942609s]
    Sep 20 13:34:45.804: INFO: Got endpoints: latency-svc-mtmcg [2.295831043s]
    Sep 20 13:34:45.905: INFO: Got endpoints: latency-svc-mlh2v [2.397060293s]
    Sep 20 13:34:45.922: INFO: Got endpoints: latency-svc-sz6zl [2.344368068s]
    Sep 20 13:34:45.922: INFO: Got endpoints: latency-svc-pck2f [2.248117528s]
    Sep 20 13:34:45.922: INFO: Got endpoints: latency-svc-knqb7 [2.907616958s]
    Sep 20 13:34:45.922: INFO: Got endpoints: latency-svc-gbdtq [2.852962218s]
    Sep 20 13:34:45.937: INFO: Created: latency-svc-c2lbl
    Sep 20 13:34:45.948: INFO: Created: latency-svc-rv7d5
    Sep 20 13:34:45.956: INFO: Got endpoints: latency-svc-c2lbl [887.932987ms]
    Sep 20 13:34:46.143: INFO: Got endpoints: latency-svc-rv7d5 [1.059792771s]
    Sep 20 13:34:46.145: INFO: Created: latency-svc-4t8fn
    Sep 20 13:34:46.159: INFO: Got endpoints: latency-svc-4t8fn [1.076108754s]
    Sep 20 13:34:46.170: INFO: Created: latency-svc-6w2mj
    Sep 20 13:34:46.188: INFO: Got endpoints: latency-svc-6w2mj [1.105165075s]
    Sep 20 13:34:46.205: INFO: Created: latency-svc-dvtjf
    Sep 20 13:34:46.212: INFO: Got endpoints: latency-svc-dvtjf [1.128754559s]
    Sep 20 13:34:46.224: INFO: Created: latency-svc-7g8kp
    Sep 20 13:34:46.232: INFO: Got endpoints: latency-svc-7g8kp [919.108511ms]
    Sep 20 13:34:46.243: INFO: Created: latency-svc-49k5v
    Sep 20 13:34:46.261: INFO: Created: latency-svc-tbmx7
    Sep 20 13:34:46.262: INFO: Got endpoints: latency-svc-49k5v [458.694175ms]
    Sep 20 13:34:46.275: INFO: Got endpoints: latency-svc-tbmx7 [471.63986ms]
    Sep 20 13:34:46.282: INFO: Created: latency-svc-rhb52
    Sep 20 13:34:46.294: INFO: Got endpoints: latency-svc-rhb52 [491.102221ms]
    Sep 20 13:34:46.320: INFO: Created: latency-svc-wx8f6
    Sep 20 13:34:46.340: INFO: Got endpoints: latency-svc-wx8f6 [536.542673ms]
    Sep 20 13:34:46.354: INFO: Created: latency-svc-tbb7v
    Sep 20 13:34:46.364: INFO: Created: latency-svc-m69kk
    Sep 20 13:34:46.543: INFO: Got endpoints: latency-svc-m69kk [620.893703ms]
    Sep 20 13:34:46.543: INFO: Got endpoints: latency-svc-tbb7v [637.056518ms]
    Sep 20 13:34:46.547: INFO: Created: latency-svc-87rms
    Sep 20 13:34:46.569: INFO: Got endpoints: latency-svc-87rms [647.276957ms]
    Sep 20 13:34:46.574: INFO: Created: latency-svc-swfv8
    Sep 20 13:34:46.586: INFO: Created: latency-svc-hx2wp
    Sep 20 13:34:46.590: INFO: Got endpoints: latency-svc-swfv8 [667.609868ms]
    Sep 20 13:34:46.594: INFO: Got endpoints: latency-svc-hx2wp [671.995618ms]
    Sep 20 13:34:46.618: INFO: Created: latency-svc-fxfjz
    Sep 20 13:34:46.618: INFO: Got endpoints: latency-svc-fxfjz [662.446843ms]
    Sep 20 13:34:46.619: INFO: Created: latency-svc-86jln
    Sep 20 13:34:46.627: INFO: Got endpoints: latency-svc-86jln [483.866128ms]
    Sep 20 13:34:46.636: INFO: Created: latency-svc-2452f
    Sep 20 13:34:46.659: INFO: Got endpoints: latency-svc-2452f [499.406522ms]
    Sep 20 13:34:46.693: INFO: Created: latency-svc-t599h
    Sep 20 13:34:46.712: INFO: Got endpoints: latency-svc-t599h [523.437489ms]
    Sep 20 13:34:46.717: INFO: Created: latency-svc-9vrh9
    Sep 20 13:34:46.729: INFO: Created: latency-svc-wxtm7
    Sep 20 13:34:46.733: INFO: Got endpoints: latency-svc-9vrh9 [521.508605ms]
    Sep 20 13:34:46.743: INFO: Created: latency-svc-9x7k5
    Sep 20 13:34:47.196: INFO: Got endpoints: latency-svc-wxtm7 [963.724381ms]
    Sep 20 13:34:47.199: INFO: Got endpoints: latency-svc-9x7k5 [937.202103ms]
    Sep 20 13:34:47.287: INFO: Created: latency-svc-8gc9g
    Sep 20 13:34:47.297: INFO: Got endpoints: latency-svc-8gc9g [1.022345823s]
    Sep 20 13:34:47.305: INFO: Created: latency-svc-2bphl
    Sep 20 13:34:47.317: INFO: Created: latency-svc-npmtw
    Sep 20 13:34:47.317: INFO: Got endpoints: latency-svc-2bphl [1.022382251s]
    Sep 20 13:34:47.334: INFO: Got endpoints: latency-svc-npmtw [994.140284ms]
    Sep 20 13:34:47.335: INFO: Created: latency-svc-qdfq6
    Sep 20 13:34:47.348: INFO: Got endpoints: latency-svc-qdfq6 [805.369566ms]
    Sep 20 13:34:47.353: INFO: Created: latency-svc-9t62j
    Sep 20 13:34:47.363: INFO: Created: latency-svc-fcpts
    Sep 20 13:34:47.367: INFO: Got endpoints: latency-svc-9t62j [823.991152ms]
    Sep 20 13:34:47.389: INFO: Got endpoints: latency-svc-fcpts [819.574985ms]
    Sep 20 13:34:47.797: INFO: Created: latency-svc-lpczn
    Sep 20 13:34:47.804: INFO: Created: latency-svc-ql7zx
    Sep 20 13:34:47.804: INFO: Created: latency-svc-gnw8s
    Sep 20 13:34:47.807: INFO: Created: latency-svc-rrkbx
    Sep 20 13:34:47.832: INFO: Created: latency-svc-nc6bk
    Sep 20 13:34:47.833: INFO: Created: latency-svc-4lz95
    Sep 20 13:34:47.837: INFO: Created: latency-svc-26fwl
    Sep 20 13:34:47.837: INFO: Created: latency-svc-hf54q
    Sep 20 13:34:47.838: INFO: Created: latency-svc-zgqtd
    Sep 20 13:34:47.838: INFO: Created: latency-svc-qz8l5
    Sep 20 13:34:47.838: INFO: Created: latency-svc-ck8k7
    Sep 20 13:34:47.838: INFO: Created: latency-svc-kf65b
    Sep 20 13:34:47.838: INFO: Created: latency-svc-4jkpn
    Sep 20 13:34:47.838: INFO: Created: latency-svc-qdh2m
    Sep 20 13:34:47.838: INFO: Created: latency-svc-r5rjx
    Sep 20 13:34:47.845: INFO: Got endpoints: latency-svc-lpczn [1.186413418s]
    Sep 20 13:34:47.845: INFO: Got endpoints: latency-svc-4jkpn [646.296721ms]
    Sep 20 13:34:47.896: INFO: Got endpoints: latency-svc-ck8k7 [1.183946992s]
    Sep 20 13:34:47.896: INFO: Got endpoints: latency-svc-gnw8s [1.162321778s]
    Sep 20 13:34:47.896: INFO: Got endpoints: latency-svc-qz8l5 [699.628148ms]
    Sep 20 13:34:47.917: INFO: Got endpoints: latency-svc-26fwl [550.339773ms]
    Sep 20 13:34:47.917: INFO: Got endpoints: latency-svc-4lz95 [620.00136ms]
    Sep 20 13:34:47.931: INFO: Got endpoints: latency-svc-qdh2m [583.069974ms]
    Sep 20 13:34:47.948: INFO: Created: latency-svc-d94gm
    Sep 20 13:34:47.950: INFO: Got endpoints: latency-svc-kf65b [615.044193ms]
    Sep 20 13:34:47.950: INFO: Got endpoints: latency-svc-ql7zx [632.658361ms]
    Sep 20 13:34:47.951: INFO: Got endpoints: latency-svc-r5rjx [562.759356ms]
    Sep 20 13:34:47.954: INFO: Got endpoints: latency-svc-zgqtd [1.335463144s]
    Sep 20 13:34:47.967: INFO: Got endpoints: latency-svc-rrkbx [1.339776858s]
    Sep 20 13:34:47.981: INFO: Created: latency-svc-9cl42
    Sep 20 13:34:48.105: INFO: Got endpoints: latency-svc-d94gm [260.151562ms]
    Sep 20 13:34:48.106: INFO: Got endpoints: latency-svc-hf54q [1.516200097s]
    Sep 20 13:34:48.106: INFO: Got endpoints: latency-svc-nc6bk [1.51176929s]
    Sep 20 13:34:48.107: INFO: Created: latency-svc-fk8jj
    Sep 20 13:34:48.110: INFO: Got endpoints: latency-svc-9cl42 [264.473523ms]
    Sep 20 13:34:48.122: INFO: Created: latency-svc-ggwcq
    Sep 20 13:34:48.125: INFO: Got endpoints: latency-svc-fk8jj [229.522587ms]
    Sep 20 13:34:48.138: INFO: Created: latency-svc-q9nxk
    Sep 20 13:34:48.141: INFO: Got endpoints: latency-svc-ggwcq [245.494152ms]
    Sep 20 13:34:48.147: INFO: Got endpoints: latency-svc-q9nxk [250.669531ms]
    Sep 20 13:34:48.170: INFO: Created: latency-svc-9c8n2
    Sep 20 13:34:48.185: INFO: Created: latency-svc-gbr8j
    Sep 20 13:34:48.185: INFO: Got endpoints: latency-svc-9c8n2 [268.079857ms]
    Sep 20 13:34:48.294: INFO: Got endpoints: latency-svc-gbr8j [376.353135ms]
    Sep 20 13:34:48.294: INFO: Created: latency-svc-wkvrm
    Sep 20 13:34:48.315: INFO: Created: latency-svc-84ksb
    Sep 20 13:34:48.316: INFO: Got endpoints: latency-svc-wkvrm [384.329328ms]
    Sep 20 13:34:48.333: INFO: Created: latency-svc-w5jj9
    Sep 20 13:34:48.434: INFO: Got endpoints: latency-svc-w5jj9 [482.016597ms]
    Sep 20 13:34:48.437: INFO: Got endpoints: latency-svc-84ksb [487.374038ms]
    Sep 20 13:34:48.442: INFO: Created: latency-svc-bv8k2
    Sep 20 13:34:48.460: INFO: Created: latency-svc-6tkvg
    Sep 20 13:34:48.460: INFO: Got endpoints: latency-svc-bv8k2 [510.760871ms]
    Sep 20 13:34:48.468: INFO: Created: latency-svc-mkjwh
    Sep 20 13:34:48.480: INFO: Got endpoints: latency-svc-6tkvg [525.685834ms]
    Sep 20 13:34:48.483: INFO: Created: latency-svc-5w6rp
    Sep 20 13:34:48.488: INFO: Got endpoints: latency-svc-mkjwh [521.649682ms]
    Sep 20 13:34:48.496: INFO: Got endpoints: latency-svc-5w6rp [390.967813ms]
    Sep 20 13:34:48.499: INFO: Created: latency-svc-76rhx
    Sep 20 13:34:48.832: INFO: Got endpoints: latency-svc-76rhx [725.935622ms]
    Sep 20 13:34:48.924: INFO: Created: latency-svc-zknfp
    Sep 20 13:34:49.331: INFO: Got endpoints: latency-svc-zknfp [1.224788714s]
    Sep 20 13:34:49.692: INFO: Created: latency-svc-6428p
    Sep 20 13:34:50.131: INFO: Got endpoints: latency-svc-6428p [2.021042489s]
    Sep 20 13:34:50.327: INFO: Created: latency-svc-c5hj4
    Sep 20 13:34:50.392: INFO: Got endpoints: latency-svc-c5hj4 [2.267071851s]
    Sep 20 13:34:50.625: INFO: Created: latency-svc-bb5b4
    Sep 20 13:34:51.401: INFO: Got endpoints: latency-svc-bb5b4 [3.259874465s]
    Sep 20 13:34:51.424: INFO: Created: latency-svc-z8h4k
    Sep 20 13:34:51.494: INFO: Got endpoints: latency-svc-z8h4k [3.347581673s]
    Sep 20 13:34:51.623: INFO: Created: latency-svc-d9k5d
    Sep 20 13:34:52.207: INFO: Got endpoints: latency-svc-d9k5d [4.021482501s]
    Sep 20 13:34:52.249: INFO: Created: latency-svc-sdt4x
    Sep 20 13:34:52.506: INFO: Got endpoints: latency-svc-sdt4x [4.211802056s]
    Sep 20 13:34:52.700: INFO: Created: latency-svc-f98bq
    Sep 20 13:34:52.837: INFO: Got endpoints: latency-svc-f98bq [4.521280811s]
    Sep 20 13:34:52.960: INFO: Created: latency-svc-bvcff
    Sep 20 13:34:53.037: INFO: Got endpoints: latency-svc-bvcff [4.603792704s]
    Sep 20 13:34:53.045: INFO: Created: latency-svc-f5bzr
    Sep 20 13:34:53.055: INFO: Got endpoints: latency-svc-f5bzr [4.617848951s]
    Sep 20 13:34:53.069: INFO: Created: latency-svc-m8g5m
    Sep 20 13:34:53.097: INFO: Created: latency-svc-9bdj5
    Sep 20 13:34:53.099: INFO: Got endpoints: latency-svc-m8g5m [4.638296998s]
    Sep 20 13:34:53.114: INFO: Created: latency-svc-7pp2b
    Sep 20 13:34:53.117: INFO: Got endpoints: latency-svc-9bdj5 [4.637226281s]
    Sep 20 13:34:53.131: INFO: Created: latency-svc-hd2rw
    Sep 20 13:34:53.162: INFO: Created: latency-svc-jcdsl
    Sep 20 13:34:53.164: INFO: Got endpoints: latency-svc-hd2rw [4.666954728s]
    Sep 20 13:34:53.164: INFO: Got endpoints: latency-svc-7pp2b [4.675400286s]
    Sep 20 13:34:53.205: INFO: Created: latency-svc-dtzvw
    Sep 20 13:34:53.206: INFO: Got endpoints: latency-svc-jcdsl [4.374216504s]
    Sep 20 13:34:53.222: INFO: Got endpoints: latency-svc-dtzvw [3.891252333s]
    Sep 20 13:34:53.319: INFO: Created: latency-svc-btgnp
    Sep 20 13:34:54.298: INFO: Got endpoints: latency-svc-btgnp [4.16651543s]
    Sep 20 13:34:54.805: INFO: Created: latency-svc-vj4hn
    Sep 20 13:34:55.287: INFO: Got endpoints: latency-svc-vj4hn [3.885821483s]
    Sep 20 13:34:55.446: INFO: Created: latency-svc-bfdrc
    Sep 20 13:34:56.670: INFO: Got endpoints: latency-svc-bfdrc [5.176206606s]
    Sep 20 13:34:56.675: INFO: Created: latency-svc-rmfft
    Sep 20 13:34:57.033: INFO: Got endpoints: latency-svc-rmfft [4.826215729s]
    Sep 20 13:34:57.569: INFO: Created: latency-svc-c6v8s
    Sep 20 13:34:58.191: INFO: Got endpoints: latency-svc-c6v8s [5.684934948s]
    Sep 20 13:34:59.029: INFO: Created: latency-svc-8tj4q
    Sep 20 13:34:59.623: INFO: Got endpoints: latency-svc-8tj4q [6.785685178s]
    Sep 20 13:35:00.451: INFO: Created: latency-svc-t4mr9
    Sep 20 13:35:00.661: INFO: Got endpoints: latency-svc-t4mr9 [7.623285967s]
    Sep 20 13:35:00.694: INFO: Created: latency-svc-l25dm
    Sep 20 13:35:00.976: INFO: Got endpoints: latency-svc-l25dm [7.921096986s]
    Sep 20 13:35:01.782: INFO: Created: latency-svc-6jfbw
    Sep 20 13:35:01.870: INFO: Got endpoints: latency-svc-6jfbw [8.771004005s]
    Sep 20 13:35:01.877: INFO: Created: latency-svc-rqvhk
    Sep 20 13:35:01.959: INFO: Got endpoints: latency-svc-rqvhk [8.842409598s]
    Sep 20 13:35:01.969: INFO: Created: latency-svc-6tvq6
    Sep 20 13:35:01.993: INFO: Got endpoints: latency-svc-6tvq6 [8.829013344s]
    Sep 20 13:35:01.995: INFO: Created: latency-svc-dp8ln
    Sep 20 13:35:02.008: INFO: Got endpoints: latency-svc-dp8ln [8.844765335s]
    Sep 20 13:35:02.011: INFO: Created: latency-svc-slvqs
    Sep 20 13:35:02.273: INFO: Got endpoints: latency-svc-slvqs [9.066405245s]
    Sep 20 13:35:02.395: INFO: Created: latency-svc-r5mxq
    Sep 20 13:35:02.395: INFO: Created: latency-svc-hn6hr
    Sep 20 13:35:03.494: INFO: Got endpoints: latency-svc-hn6hr [10.271563812s]
    Sep 20 13:35:03.494: INFO: Got endpoints: latency-svc-r5mxq [13.1016616s]
    Sep 20 13:35:04.174: INFO: Created: latency-svc-r6l45
    Sep 20 13:35:04.378: INFO: Got endpoints: latency-svc-r6l45 [10.080357498s]
    Sep 20 13:35:04.546: INFO: Created: latency-svc-vbgbd
    Sep 20 13:35:04.863: INFO: Got endpoints: latency-svc-vbgbd [9.576036228s]
    Sep 20 13:35:05.082: INFO: Created: latency-svc-8g2r9
    Sep 20 13:35:05.437: INFO: Got endpoints: latency-svc-8g2r9 [8.766416115s]
    Sep 20 13:35:05.656: INFO: Created: latency-svc-mg6gt
    Sep 20 13:35:06.375: INFO: Got endpoints: latency-svc-mg6gt [9.342232936s]
    Sep 20 13:35:10.323: INFO: Created: latency-svc-wcpgb
    Sep 20 13:35:10.323: INFO: Created: latency-svc-jx86p
    Sep 20 13:35:10.866: INFO: Created: latency-svc-jt8lv
    Sep 20 13:35:10.873: INFO: Created: latency-svc-8886h
    Sep 20 13:35:10.878: INFO: Created: latency-svc-65k49
    Sep 20 13:35:10.878: INFO: Created: latency-svc-g6p6p
    Sep 20 13:35:10.878: INFO: Created: latency-svc-ldrwb
    Sep 20 13:35:10.878: INFO: Created: latency-svc-7hn9z
    Sep 20 13:35:10.879: INFO: Created: latency-svc-plmn4
    Sep 20 13:35:10.879: INFO: Created: latency-svc-64vkv
    Sep 20 13:35:10.893: INFO: Created: latency-svc-dk2rm
    Sep 20 13:35:10.893: INFO: Created: latency-svc-dr4tv
    Sep 20 13:35:10.893: INFO: Created: latency-svc-9stct
    Sep 20 13:35:10.895: INFO: Created: latency-svc-dm86l
    Sep 20 13:35:10.896: INFO: Created: latency-svc-qlggt
    Sep 20 13:35:10.953: INFO: Got endpoints: latency-svc-jx86p [6.089966251s]
    Sep 20 13:35:11.012: INFO: Got endpoints: latency-svc-jt8lv [4.636190078s]
    Sep 20 13:35:11.012: INFO: Got endpoints: latency-svc-wcpgb [9.052269799s]
    Sep 20 13:35:11.012: INFO: Got endpoints: latency-svc-g6p6p [12.821350454s]
    Sep 20 13:35:11.013: INFO: Got endpoints: latency-svc-7hn9z [10.351858286s]
    Sep 20 13:35:11.013: INFO: Got endpoints: latency-svc-ldrwb [11.390131922s]
    Sep 20 13:35:11.364: INFO: Got endpoints: latency-svc-dr4tv [9.493901754s]
    Sep 20 13:35:11.398: INFO: Created: latency-svc-nq7s8
    Sep 20 13:35:11.399: INFO: Got endpoints: latency-svc-dm86l [10.423438569s]
    Sep 20 13:35:11.400: INFO: Got endpoints: latency-svc-65k49 [7.905585687s]
    Sep 20 13:35:11.403: INFO: Got endpoints: latency-svc-plmn4 [7.90869843s]
    Sep 20 13:35:11.403: INFO: Got endpoints: latency-svc-64vkv [7.02508541s]
    Sep 20 13:35:11.403: INFO: Got endpoints: latency-svc-dk2rm [5.96626366s]
    Sep 20 13:35:11.511: INFO: Created: latency-svc-b5kp9
    Sep 20 13:35:11.518: INFO: Got endpoints: latency-svc-qlggt [9.245601606s]
    Sep 20 13:35:11.521: INFO: Got endpoints: latency-svc-9stct [9.527825735s]
    Sep 20 13:35:11.537: INFO: Got endpoints: latency-svc-nq7s8 [584.378359ms]
    Sep 20 13:35:11.538: INFO: Got endpoints: latency-svc-8886h [9.529159267s]
    Sep 20 13:35:11.570: INFO: Got endpoints: latency-svc-b5kp9 [557.962811ms]
    Sep 20 13:35:16.075: INFO: Created: latency-svc-l8z2z
    Sep 20 13:35:16.077: INFO: Created: latency-svc-whr4z
    Sep 20 13:35:16.077: INFO: Created: latency-svc-8rssc
    Sep 20 13:35:16.082: INFO: Created: latency-svc-wbmwz
    Sep 20 13:35:16.083: INFO: Created: latency-svc-9sstl
    Sep 20 13:35:16.083: INFO: Created: latency-svc-r5lnp
    Sep 20 13:35:16.083: INFO: Created: latency-svc-jp7pf
    Sep 20 13:35:16.084: INFO: Created: latency-svc-k6qj8
    Sep 20 13:35:16.084: INFO: Created: latency-svc-c84sd
    Sep 20 13:35:16.084: INFO: Created: latency-svc-4hznf
    Sep 20 13:35:16.086: INFO: Created: latency-svc-g7tps
    Sep 20 13:35:16.086: INFO: Created: latency-svc-ld9zq
    Sep 20 13:35:16.087: INFO: Created: latency-svc-srb74
    Sep 20 13:35:16.087: INFO: Created: latency-svc-djgs6
    Sep 20 13:35:16.149: INFO: Created: latency-svc-vwt5j
    Sep 20 13:35:16.277: INFO: Got endpoints: latency-svc-8rssc [4.739934136s]
    Sep 20 13:35:16.282: INFO: Got endpoints: latency-svc-l8z2z [4.882867812s]
    Sep 20 13:35:16.283: INFO: Got endpoints: latency-svc-whr4z [4.918821085s]
    Sep 20 13:35:16.283: INFO: Got endpoints: latency-svc-srb74 [4.745034483s]
    Sep 20 13:35:16.283: INFO: Got endpoints: latency-svc-ld9zq [4.880176914s]
    Sep 20 13:35:16.569: INFO: Got endpoints: latency-svc-g7tps [5.165583457s]
    Sep 20 13:35:16.569: INFO: Got endpoints: latency-svc-djgs6 [5.169311349s]
    Sep 20 13:35:16.580: INFO: Got endpoints: latency-svc-jp7pf [5.176703402s]
    Sep 20 13:35:16.580: INFO: Got endpoints: latency-svc-9sstl [5.010435373s]
    Sep 20 13:35:16.580: INFO: Got endpoints: latency-svc-r5lnp [5.56820364s]
    Sep 20 13:35:17.105: INFO: Got endpoints: latency-svc-k6qj8 [5.584593682s]
    Sep 20 13:35:17.106: INFO: Got endpoints: latency-svc-vwt5j [6.09303907s]
    Sep 20 13:35:17.106: INFO: Got endpoints: latency-svc-c84sd [6.093699875s]
    Sep 20 13:35:17.106: INFO: Got endpoints: latency-svc-4hznf [5.587495328s]
    Sep 20 13:35:17.106: INFO: Got endpoints: latency-svc-wbmwz [6.093072313s]
    Sep 20 13:35:17.122: INFO: Created: latency-svc-gx8kf
    Sep 20 13:35:17.748: INFO: Got endpoints: latency-svc-gx8kf [1.465037144s]
    Sep 20 13:35:17.753: INFO: Created: latency-svc-wfj6w
    Sep 20 13:35:18.060: INFO: Got endpoints: latency-svc-wfj6w [1.776746644s]
    Sep 20 13:35:18.378: INFO: Created: latency-svc-8vgkw
    Sep 20 13:35:19.034: INFO: Got endpoints: latency-svc-8vgkw [2.750808146s]
    Sep 20 13:35:19.346: INFO: Created: latency-svc-h4j96
    Sep 20 13:35:19.475: INFO: Got endpoints: latency-svc-h4j96 [3.197837019s]
    Sep 20 13:35:19.484: INFO: Created: latency-svc-gz5q9
    Sep 20 13:35:19.502: INFO: Got endpoints: latency-svc-gz5q9 [3.21961507s]
    Sep 20 13:35:19.509: INFO: Created: latency-svc-mxdh5
    Sep 20 13:35:19.515: INFO: Got endpoints: latency-svc-mxdh5 [2.94586865s]
    Sep 20 13:35:19.519: INFO: Created: latency-svc-cd9t4
    Sep 20 13:35:20.108: INFO: Got endpoints: latency-svc-cd9t4 [3.538505004s]
    Sep 20 13:35:20.548: INFO: Created: latency-svc-wmx4x
    Sep 20 13:35:20.898: INFO: Got endpoints: latency-svc-wmx4x [4.317779088s]
    Sep 20 13:35:21.058: INFO: Created: latency-svc-ptng2
    Sep 20 13:35:21.641: INFO: Got endpoints: latency-svc-ptng2 [5.060212686s]
    Sep 20 13:35:22.045: INFO: Created: latency-svc-wmslh
    Sep 20 13:35:22.559: INFO: Got endpoints: latency-svc-wmslh [5.978395463s]
    Sep 20 13:35:23.215: INFO: Created: latency-svc-gj59q
    Sep 20 13:35:23.568: INFO: Got endpoints: latency-svc-gj59q [6.46266868s]
    Sep 20 13:35:23.568: INFO: Latencies: [32.383929ms 61.11518ms 81.965593ms 114.732926ms 145.150922ms 151.986529ms 168.962364ms 206.207331ms 229.522587ms 245.494152ms 250.669531ms 260.151562ms 264.473523ms 268.079857ms 270.690644ms 279.536286ms 323.88861ms 376.353135ms 384.329328ms 390.967813ms 413.944113ms 423.336212ms 440.921478ms 458.694175ms 471.63986ms 482.016597ms 483.866128ms 487.374038ms 491.102221ms 499.406522ms 510.760871ms 521.508605ms 521.649682ms 523.437489ms 525.685834ms 536.542673ms 550.339773ms 553.160144ms 553.563935ms 557.030496ms 557.962811ms 562.759356ms 575.553186ms 583.069974ms 583.59791ms 584.378359ms 587.238326ms 587.777312ms 592.419397ms 615.044193ms 620.00136ms 620.893703ms 622.913977ms 629.081664ms 632.658361ms 637.056518ms 646.296721ms 647.276957ms 661.045363ms 662.446843ms 667.609868ms 671.638046ms 671.995618ms 677.670869ms 692.121829ms 699.628148ms 725.935622ms 755.316936ms 776.40561ms 792.423963ms 798.677342ms 805.369566ms 819.574985ms 823.991152ms 874.830367ms 887.932987ms 900.669309ms 915.722042ms 916.961806ms 919.108511ms 937.202103ms 947.051465ms 950.453064ms 963.724381ms 982.076109ms 994.140284ms 999.177302ms 999.423495ms 1.011910596s 1.022345823s 1.022382251s 1.033752448s 1.039298967s 1.050302893s 1.056272789s 1.057380326s 1.059792771s 1.076108754s 1.105165075s 1.128754559s 1.162321778s 1.183946992s 1.186413418s 1.224788714s 1.248148043s 1.335463144s 1.339776858s 1.396305129s 1.447898301s 1.465037144s 1.51176929s 1.516200097s 1.552331806s 1.776746644s 1.77940415s 1.788803264s 2.021042489s 2.068025098s 2.248117528s 2.259372798s 2.267071851s 2.295831043s 2.344368068s 2.397060293s 2.455212751s 2.472942609s 2.750808146s 2.790470616s 2.852962218s 2.907616958s 2.94586865s 3.197837019s 3.21961507s 3.259874465s 3.347581673s 3.538505004s 3.885821483s 3.891252333s 4.021482501s 4.16651543s 4.211802056s 4.317779088s 4.374216504s 4.521280811s 4.603792704s 4.617848951s 4.636190078s 4.637226281s 4.638296998s 4.666954728s 4.675400286s 4.739934136s 4.745034483s 4.826215729s 4.880176914s 4.882867812s 4.918821085s 5.010435373s 5.060212686s 5.165583457s 5.169311349s 5.176206606s 5.176703402s 5.56820364s 5.584593682s 5.587495328s 5.684934948s 5.96626366s 5.978395463s 6.089966251s 6.09303907s 6.093072313s 6.093699875s 6.46266868s 6.785685178s 7.02508541s 7.623285967s 7.905585687s 7.90869843s 7.921096986s 8.766416115s 8.771004005s 8.829013344s 8.842409598s 8.844765335s 9.052269799s 9.066405245s 9.245601606s 9.342232936s 9.493901754s 9.527825735s 9.529159267s 9.576036228s 10.080357498s 10.271563812s 10.351858286s 10.423438569s 11.390131922s 12.821350454s 13.1016616s]
    Sep 20 13:35:23.569: INFO: 50 %ile: 1.162321778s
    Sep 20 13:35:23.569: INFO: 90 %ile: 8.766416115s
    Sep 20 13:35:23.569: INFO: 99 %ile: 12.821350454s
    Sep 20 13:35:23.569: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:35:23.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-2940" for this suite. 09/20/23 13:35:23.581
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:35:23.655
Sep 20 13:35:23.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename var-expansion 09/20/23 13:35:23.655
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:35:24.115
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:35:24.118
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 09/20/23 13:35:24.406
Sep 20 13:35:24.454: INFO: Waiting up to 5m0s for pod "var-expansion-ba22e413-19e1-4a0e-939c-a941fab409ee" in namespace "var-expansion-3975" to be "Succeeded or Failed"
Sep 20 13:35:24.463: INFO: Pod "var-expansion-ba22e413-19e1-4a0e-939c-a941fab409ee": Phase="Pending", Reason="", readiness=false. Elapsed: 8.524487ms
Sep 20 13:35:26.623: INFO: Pod "var-expansion-ba22e413-19e1-4a0e-939c-a941fab409ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.168906875s
Sep 20 13:35:30.457: INFO: Pod "var-expansion-ba22e413-19e1-4a0e-939c-a941fab409ee": Phase="Running", Reason="", readiness=true. Elapsed: 6.002723639s
Sep 20 13:35:30.629: INFO: Pod "var-expansion-ba22e413-19e1-4a0e-939c-a941fab409ee": Phase="Running", Reason="", readiness=false. Elapsed: 6.174545601s
Sep 20 13:35:32.938: INFO: Pod "var-expansion-ba22e413-19e1-4a0e-939c-a941fab409ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.483114916s
STEP: Saw pod success 09/20/23 13:35:32.938
Sep 20 13:35:32.938: INFO: Pod "var-expansion-ba22e413-19e1-4a0e-939c-a941fab409ee" satisfied condition "Succeeded or Failed"
Sep 20 13:35:33.477: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod var-expansion-ba22e413-19e1-4a0e-939c-a941fab409ee container dapi-container: <nil>
STEP: delete the pod 09/20/23 13:35:33.916
Sep 20 13:35:36.671: INFO: Waiting for pod var-expansion-ba22e413-19e1-4a0e-939c-a941fab409ee to disappear
Sep 20 13:35:36.934: INFO: Pod var-expansion-ba22e413-19e1-4a0e-939c-a941fab409ee no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep 20 13:35:36.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-3975" for this suite. 09/20/23 13:35:37.66
------------------------------
â€¢ [SLOW TEST] [15.025 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:35:23.655
    Sep 20 13:35:23.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename var-expansion 09/20/23 13:35:23.655
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:35:24.115
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:35:24.118
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 09/20/23 13:35:24.406
    Sep 20 13:35:24.454: INFO: Waiting up to 5m0s for pod "var-expansion-ba22e413-19e1-4a0e-939c-a941fab409ee" in namespace "var-expansion-3975" to be "Succeeded or Failed"
    Sep 20 13:35:24.463: INFO: Pod "var-expansion-ba22e413-19e1-4a0e-939c-a941fab409ee": Phase="Pending", Reason="", readiness=false. Elapsed: 8.524487ms
    Sep 20 13:35:26.623: INFO: Pod "var-expansion-ba22e413-19e1-4a0e-939c-a941fab409ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.168906875s
    Sep 20 13:35:30.457: INFO: Pod "var-expansion-ba22e413-19e1-4a0e-939c-a941fab409ee": Phase="Running", Reason="", readiness=true. Elapsed: 6.002723639s
    Sep 20 13:35:30.629: INFO: Pod "var-expansion-ba22e413-19e1-4a0e-939c-a941fab409ee": Phase="Running", Reason="", readiness=false. Elapsed: 6.174545601s
    Sep 20 13:35:32.938: INFO: Pod "var-expansion-ba22e413-19e1-4a0e-939c-a941fab409ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.483114916s
    STEP: Saw pod success 09/20/23 13:35:32.938
    Sep 20 13:35:32.938: INFO: Pod "var-expansion-ba22e413-19e1-4a0e-939c-a941fab409ee" satisfied condition "Succeeded or Failed"
    Sep 20 13:35:33.477: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod var-expansion-ba22e413-19e1-4a0e-939c-a941fab409ee container dapi-container: <nil>
    STEP: delete the pod 09/20/23 13:35:33.916
    Sep 20 13:35:36.671: INFO: Waiting for pod var-expansion-ba22e413-19e1-4a0e-939c-a941fab409ee to disappear
    Sep 20 13:35:36.934: INFO: Pod var-expansion-ba22e413-19e1-4a0e-939c-a941fab409ee no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:35:36.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-3975" for this suite. 09/20/23 13:35:37.66
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:35:38.682
Sep 20 13:35:38.682: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename podtemplate 09/20/23 13:35:38.683
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:35:40.963
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:35:40.983
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Sep 20 13:35:45.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-4793" for this suite. 09/20/23 13:35:46.102
------------------------------
â€¢ [SLOW TEST] [9.542 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:35:38.682
    Sep 20 13:35:38.682: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename podtemplate 09/20/23 13:35:38.683
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:35:40.963
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:35:40.983
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:35:45.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-4793" for this suite. 09/20/23 13:35:46.102
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:35:48.225
Sep 20 13:35:48.225: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename endpointslicemirroring 09/20/23 13:35:48.226
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:35:49.478
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:35:49.511
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 09/20/23 13:35:52.059
STEP: mirroring an update to a custom Endpoint 09/20/23 13:35:52.886
Sep 20 13:35:55.531: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 09/20/23 13:35:58.245
Sep 20 13:35:59.032: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Sep 20 13:36:01.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-1260" for this suite. 09/20/23 13:36:01.975
------------------------------
â€¢ [SLOW TEST] [14.664 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:35:48.225
    Sep 20 13:35:48.225: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename endpointslicemirroring 09/20/23 13:35:48.226
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:35:49.478
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:35:49.511
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 09/20/23 13:35:52.059
    STEP: mirroring an update to a custom Endpoint 09/20/23 13:35:52.886
    Sep 20 13:35:55.531: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 09/20/23 13:35:58.245
    Sep 20 13:35:59.032: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:36:01.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-1260" for this suite. 09/20/23 13:36:01.975
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:36:02.894
Sep 20 13:36:02.894: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename replication-controller 09/20/23 13:36:02.895
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:36:03.421
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:36:03.424
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-06fa5b08-e670-4874-8050-f254b030366f 09/20/23 13:36:03.705
Sep 20 13:36:04.127: INFO: Pod name my-hostname-basic-06fa5b08-e670-4874-8050-f254b030366f: Found 0 pods out of 1
Sep 20 13:36:09.403: INFO: Pod name my-hostname-basic-06fa5b08-e670-4874-8050-f254b030366f: Found 1 pods out of 1
Sep 20 13:36:09.404: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-06fa5b08-e670-4874-8050-f254b030366f" are running
Sep 20 13:36:09.404: INFO: Waiting up to 5m0s for pod "my-hostname-basic-06fa5b08-e670-4874-8050-f254b030366f-bbbqz" in namespace "replication-controller-9759" to be "running"
Sep 20 13:36:09.775: INFO: Pod "my-hostname-basic-06fa5b08-e670-4874-8050-f254b030366f-bbbqz": Phase="Running", Reason="", readiness=true. Elapsed: 370.946881ms
Sep 20 13:36:09.775: INFO: Pod "my-hostname-basic-06fa5b08-e670-4874-8050-f254b030366f-bbbqz" satisfied condition "running"
Sep 20 13:36:09.775: INFO: Pod "my-hostname-basic-06fa5b08-e670-4874-8050-f254b030366f-bbbqz" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-20 13:36:04 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-20 13:36:07 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-20 13:36:07 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-20 13:36:04 +0000 UTC Reason: Message:}])
Sep 20 13:36:09.775: INFO: Trying to dial the pod
Sep 20 13:36:15.495: INFO: Controller my-hostname-basic-06fa5b08-e670-4874-8050-f254b030366f: Got expected result from replica 1 [my-hostname-basic-06fa5b08-e670-4874-8050-f254b030366f-bbbqz]: "my-hostname-basic-06fa5b08-e670-4874-8050-f254b030366f-bbbqz", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep 20 13:36:15.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-9759" for this suite. 09/20/23 13:36:15.501
------------------------------
â€¢ [SLOW TEST] [12.753 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:36:02.894
    Sep 20 13:36:02.894: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename replication-controller 09/20/23 13:36:02.895
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:36:03.421
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:36:03.424
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-06fa5b08-e670-4874-8050-f254b030366f 09/20/23 13:36:03.705
    Sep 20 13:36:04.127: INFO: Pod name my-hostname-basic-06fa5b08-e670-4874-8050-f254b030366f: Found 0 pods out of 1
    Sep 20 13:36:09.403: INFO: Pod name my-hostname-basic-06fa5b08-e670-4874-8050-f254b030366f: Found 1 pods out of 1
    Sep 20 13:36:09.404: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-06fa5b08-e670-4874-8050-f254b030366f" are running
    Sep 20 13:36:09.404: INFO: Waiting up to 5m0s for pod "my-hostname-basic-06fa5b08-e670-4874-8050-f254b030366f-bbbqz" in namespace "replication-controller-9759" to be "running"
    Sep 20 13:36:09.775: INFO: Pod "my-hostname-basic-06fa5b08-e670-4874-8050-f254b030366f-bbbqz": Phase="Running", Reason="", readiness=true. Elapsed: 370.946881ms
    Sep 20 13:36:09.775: INFO: Pod "my-hostname-basic-06fa5b08-e670-4874-8050-f254b030366f-bbbqz" satisfied condition "running"
    Sep 20 13:36:09.775: INFO: Pod "my-hostname-basic-06fa5b08-e670-4874-8050-f254b030366f-bbbqz" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-20 13:36:04 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-20 13:36:07 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-20 13:36:07 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-20 13:36:04 +0000 UTC Reason: Message:}])
    Sep 20 13:36:09.775: INFO: Trying to dial the pod
    Sep 20 13:36:15.495: INFO: Controller my-hostname-basic-06fa5b08-e670-4874-8050-f254b030366f: Got expected result from replica 1 [my-hostname-basic-06fa5b08-e670-4874-8050-f254b030366f-bbbqz]: "my-hostname-basic-06fa5b08-e670-4874-8050-f254b030366f-bbbqz", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:36:15.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-9759" for this suite. 09/20/23 13:36:15.501
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:36:15.647
Sep 20 13:36:15.647: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename kubectl 09/20/23 13:36:15.648
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:36:16.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:36:16.978
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/20/23 13:36:16.986
Sep 20 13:36:16.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-3919 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Sep 20 13:36:17.338: INFO: stderr: ""
Sep 20 13:36:17.338: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 09/20/23 13:36:17.338
Sep 20 13:36:17.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-3919 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Sep 20 13:36:17.683: INFO: stderr: ""
Sep 20 13:36:17.683: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/20/23 13:36:17.683
Sep 20 13:36:18.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-3919 delete pods e2e-test-httpd-pod'
Sep 20 13:36:24.237: INFO: stderr: ""
Sep 20 13:36:24.237: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep 20 13:36:24.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3919" for this suite. 09/20/23 13:36:25.878
------------------------------
â€¢ [SLOW TEST] [11.758 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:36:15.647
    Sep 20 13:36:15.647: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename kubectl 09/20/23 13:36:15.648
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:36:16.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:36:16.978
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/20/23 13:36:16.986
    Sep 20 13:36:16.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-3919 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Sep 20 13:36:17.338: INFO: stderr: ""
    Sep 20 13:36:17.338: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 09/20/23 13:36:17.338
    Sep 20 13:36:17.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-3919 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Sep 20 13:36:17.683: INFO: stderr: ""
    Sep 20 13:36:17.683: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/20/23 13:36:17.683
    Sep 20 13:36:18.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-3919 delete pods e2e-test-httpd-pod'
    Sep 20 13:36:24.237: INFO: stderr: ""
    Sep 20 13:36:24.237: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:36:24.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3919" for this suite. 09/20/23 13:36:25.878
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:36:27.406
Sep 20 13:36:27.406: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename daemonsets 09/20/23 13:36:27.407
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:36:28.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:36:28.92
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205
Sep 20 13:36:30.152: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 09/20/23 13:36:30.163
Sep 20 13:36:30.347: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 13:36:30.347: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 09/20/23 13:36:30.347
Sep 20 13:36:32.511: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 13:36:32.511: INFO: Node mycluster-ww3cg64etuwi-node-2 is running 0 daemon pod, expected 1
Sep 20 13:36:34.741: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 13:36:34.741: INFO: Node mycluster-ww3cg64etuwi-node-2 is running 0 daemon pod, expected 1
Sep 20 13:36:35.537: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 13:36:35.537: INFO: Node mycluster-ww3cg64etuwi-node-2 is running 0 daemon pod, expected 1
Sep 20 13:36:37.103: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep 20 13:36:37.103: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 09/20/23 13:36:37.215
Sep 20 13:36:38.177: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep 20 13:36:38.177: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Sep 20 13:36:39.505: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 13:36:39.505: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 09/20/23 13:36:39.505
Sep 20 13:36:40.181: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 13:36:40.182: INFO: Node mycluster-ww3cg64etuwi-node-2 is running 0 daemon pod, expected 1
Sep 20 13:36:41.726: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 13:36:41.726: INFO: Node mycluster-ww3cg64etuwi-node-2 is running 0 daemon pod, expected 1
Sep 20 13:36:42.238: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 13:36:42.238: INFO: Node mycluster-ww3cg64etuwi-node-2 is running 0 daemon pod, expected 1
Sep 20 13:36:43.598: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 13:36:43.598: INFO: Node mycluster-ww3cg64etuwi-node-2 is running 0 daemon pod, expected 1
Sep 20 13:36:44.727: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 13:36:44.727: INFO: Node mycluster-ww3cg64etuwi-node-2 is running 0 daemon pod, expected 1
Sep 20 13:36:46.447: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep 20 13:36:46.447: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 09/20/23 13:36:46.945
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6467, will wait for the garbage collector to delete the pods 09/20/23 13:36:46.946
Sep 20 13:36:47.874: INFO: Deleting DaemonSet.extensions daemon-set took: 872.122988ms
Sep 20 13:36:48.174: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.45019ms
Sep 20 13:36:50.528: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 13:36:50.528: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep 20 13:36:50.974: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"40590"},"items":null}

Sep 20 13:36:51.139: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"40591"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep 20 13:36:52.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-6467" for this suite. 09/20/23 13:36:53.046
------------------------------
â€¢ [SLOW TEST] [25.666 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:36:27.406
    Sep 20 13:36:27.406: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename daemonsets 09/20/23 13:36:27.407
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:36:28.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:36:28.92
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:205
    Sep 20 13:36:30.152: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 09/20/23 13:36:30.163
    Sep 20 13:36:30.347: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 13:36:30.347: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 09/20/23 13:36:30.347
    Sep 20 13:36:32.511: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 13:36:32.511: INFO: Node mycluster-ww3cg64etuwi-node-2 is running 0 daemon pod, expected 1
    Sep 20 13:36:34.741: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 13:36:34.741: INFO: Node mycluster-ww3cg64etuwi-node-2 is running 0 daemon pod, expected 1
    Sep 20 13:36:35.537: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 13:36:35.537: INFO: Node mycluster-ww3cg64etuwi-node-2 is running 0 daemon pod, expected 1
    Sep 20 13:36:37.103: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep 20 13:36:37.103: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 09/20/23 13:36:37.215
    Sep 20 13:36:38.177: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep 20 13:36:38.177: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Sep 20 13:36:39.505: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 13:36:39.505: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 09/20/23 13:36:39.505
    Sep 20 13:36:40.181: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 13:36:40.182: INFO: Node mycluster-ww3cg64etuwi-node-2 is running 0 daemon pod, expected 1
    Sep 20 13:36:41.726: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 13:36:41.726: INFO: Node mycluster-ww3cg64etuwi-node-2 is running 0 daemon pod, expected 1
    Sep 20 13:36:42.238: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 13:36:42.238: INFO: Node mycluster-ww3cg64etuwi-node-2 is running 0 daemon pod, expected 1
    Sep 20 13:36:43.598: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 13:36:43.598: INFO: Node mycluster-ww3cg64etuwi-node-2 is running 0 daemon pod, expected 1
    Sep 20 13:36:44.727: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 13:36:44.727: INFO: Node mycluster-ww3cg64etuwi-node-2 is running 0 daemon pod, expected 1
    Sep 20 13:36:46.447: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep 20 13:36:46.447: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 09/20/23 13:36:46.945
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6467, will wait for the garbage collector to delete the pods 09/20/23 13:36:46.946
    Sep 20 13:36:47.874: INFO: Deleting DaemonSet.extensions daemon-set took: 872.122988ms
    Sep 20 13:36:48.174: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.45019ms
    Sep 20 13:36:50.528: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 13:36:50.528: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep 20 13:36:50.974: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"40590"},"items":null}

    Sep 20 13:36:51.139: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"40591"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:36:52.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-6467" for this suite. 09/20/23 13:36:53.046
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:36:53.072
Sep 20 13:36:53.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename var-expansion 09/20/23 13:36:53.073
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:36:54.382
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:36:54.386
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 09/20/23 13:36:54.39
Sep 20 13:36:54.640: INFO: Waiting up to 5m0s for pod "var-expansion-9b206ea3-1b60-493e-81d8-83a1b4240484" in namespace "var-expansion-1281" to be "Succeeded or Failed"
Sep 20 13:36:55.147: INFO: Pod "var-expansion-9b206ea3-1b60-493e-81d8-83a1b4240484": Phase="Pending", Reason="", readiness=false. Elapsed: 507.389961ms
Sep 20 13:36:57.270: INFO: Pod "var-expansion-9b206ea3-1b60-493e-81d8-83a1b4240484": Phase="Pending", Reason="", readiness=false. Elapsed: 2.629758631s
Sep 20 13:36:59.361: INFO: Pod "var-expansion-9b206ea3-1b60-493e-81d8-83a1b4240484": Phase="Pending", Reason="", readiness=false. Elapsed: 4.720846932s
Sep 20 13:37:01.454: INFO: Pod "var-expansion-9b206ea3-1b60-493e-81d8-83a1b4240484": Phase="Pending", Reason="", readiness=false. Elapsed: 6.813995883s
Sep 20 13:37:03.387: INFO: Pod "var-expansion-9b206ea3-1b60-493e-81d8-83a1b4240484": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.747325222s
STEP: Saw pod success 09/20/23 13:37:03.387
Sep 20 13:37:03.388: INFO: Pod "var-expansion-9b206ea3-1b60-493e-81d8-83a1b4240484" satisfied condition "Succeeded or Failed"
Sep 20 13:37:03.390: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod var-expansion-9b206ea3-1b60-493e-81d8-83a1b4240484 container dapi-container: <nil>
STEP: delete the pod 09/20/23 13:37:03.604
Sep 20 13:37:04.120: INFO: Waiting for pod var-expansion-9b206ea3-1b60-493e-81d8-83a1b4240484 to disappear
Sep 20 13:37:04.309: INFO: Pod var-expansion-9b206ea3-1b60-493e-81d8-83a1b4240484 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep 20 13:37:04.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-1281" for this suite. 09/20/23 13:37:04.419
------------------------------
â€¢ [SLOW TEST] [11.356 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:36:53.072
    Sep 20 13:36:53.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename var-expansion 09/20/23 13:36:53.073
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:36:54.382
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:36:54.386
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 09/20/23 13:36:54.39
    Sep 20 13:36:54.640: INFO: Waiting up to 5m0s for pod "var-expansion-9b206ea3-1b60-493e-81d8-83a1b4240484" in namespace "var-expansion-1281" to be "Succeeded or Failed"
    Sep 20 13:36:55.147: INFO: Pod "var-expansion-9b206ea3-1b60-493e-81d8-83a1b4240484": Phase="Pending", Reason="", readiness=false. Elapsed: 507.389961ms
    Sep 20 13:36:57.270: INFO: Pod "var-expansion-9b206ea3-1b60-493e-81d8-83a1b4240484": Phase="Pending", Reason="", readiness=false. Elapsed: 2.629758631s
    Sep 20 13:36:59.361: INFO: Pod "var-expansion-9b206ea3-1b60-493e-81d8-83a1b4240484": Phase="Pending", Reason="", readiness=false. Elapsed: 4.720846932s
    Sep 20 13:37:01.454: INFO: Pod "var-expansion-9b206ea3-1b60-493e-81d8-83a1b4240484": Phase="Pending", Reason="", readiness=false. Elapsed: 6.813995883s
    Sep 20 13:37:03.387: INFO: Pod "var-expansion-9b206ea3-1b60-493e-81d8-83a1b4240484": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.747325222s
    STEP: Saw pod success 09/20/23 13:37:03.387
    Sep 20 13:37:03.388: INFO: Pod "var-expansion-9b206ea3-1b60-493e-81d8-83a1b4240484" satisfied condition "Succeeded or Failed"
    Sep 20 13:37:03.390: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod var-expansion-9b206ea3-1b60-493e-81d8-83a1b4240484 container dapi-container: <nil>
    STEP: delete the pod 09/20/23 13:37:03.604
    Sep 20 13:37:04.120: INFO: Waiting for pod var-expansion-9b206ea3-1b60-493e-81d8-83a1b4240484 to disappear
    Sep 20 13:37:04.309: INFO: Pod var-expansion-9b206ea3-1b60-493e-81d8-83a1b4240484 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:37:04.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-1281" for this suite. 09/20/23 13:37:04.419
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:37:04.429
Sep 20 13:37:04.429: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename pod-network-test 09/20/23 13:37:04.43
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:37:06.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:37:06.238
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-1708 09/20/23 13:37:06.243
STEP: creating a selector 09/20/23 13:37:06.243
STEP: Creating the service pods in kubernetes 09/20/23 13:37:06.243
Sep 20 13:37:06.243: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep 20 13:37:07.733: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1708" to be "running and ready"
Sep 20 13:37:07.757: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 23.043895ms
Sep 20 13:37:07.757: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:37:10.042: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.308063382s
Sep 20 13:37:10.042: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:37:12.026: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.292312686s
Sep 20 13:37:12.026: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep 20 13:37:14.857: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 7.123385993s
Sep 20 13:37:14.857: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep 20 13:37:15.933: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.199148935s
Sep 20 13:37:15.933: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep 20 13:37:17.794: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.060262556s
Sep 20 13:37:17.794: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep 20 13:37:19.818: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.084619606s
Sep 20 13:37:19.818: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep 20 13:37:21.812: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.078012443s
Sep 20 13:37:21.812: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep 20 13:37:24.311: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.577282832s
Sep 20 13:37:24.311: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep 20 13:37:26.381: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.647516951s
Sep 20 13:37:26.381: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep 20 13:37:27.778: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.044556041s
Sep 20 13:37:27.778: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep 20 13:37:30.255: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.521143463s
Sep 20 13:37:30.255: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Sep 20 13:37:30.255: INFO: Pod "netserver-0" satisfied condition "running and ready"
Sep 20 13:37:30.395: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1708" to be "running and ready"
Sep 20 13:37:30.454: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 58.867406ms
Sep 20 13:37:30.454: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Sep 20 13:37:30.454: INFO: Pod "netserver-1" satisfied condition "running and ready"
Sep 20 13:37:31.089: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1708" to be "running and ready"
Sep 20 13:37:31.531: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 442.27614ms
Sep 20 13:37:31.531: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Sep 20 13:37:31.531: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 09/20/23 13:37:31.709
Sep 20 13:37:32.588: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1708" to be "running"
Sep 20 13:37:33.814: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.226378156s
Sep 20 13:37:36.009: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.421321913s
Sep 20 13:37:39.003: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.41570065s
Sep 20 13:37:39.003: INFO: Pod "test-container-pod" satisfied condition "running"
Sep 20 13:37:39.349: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-1708" to be "running"
Sep 20 13:37:39.436: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 86.511105ms
Sep 20 13:37:39.436: INFO: Pod "host-test-container-pod" satisfied condition "running"
Sep 20 13:37:39.487: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Sep 20 13:37:39.487: INFO: Going to poll 10.100.5.148 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Sep 20 13:37:39.563: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.5.148 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1708 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 20 13:37:39.563: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
Sep 20 13:37:39.564: INFO: ExecWithOptions: Clientset creation
Sep 20 13:37:39.564: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-1708/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.100.5.148+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep 20 13:37:40.845: INFO: Found all 1 expected endpoints: [netserver-0]
Sep 20 13:37:40.845: INFO: Going to poll 10.100.4.59 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Sep 20 13:37:40.862: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.4.59 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1708 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 20 13:37:40.862: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
Sep 20 13:37:40.863: INFO: ExecWithOptions: Clientset creation
Sep 20 13:37:40.863: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-1708/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.100.4.59+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep 20 13:37:42.007: INFO: Found all 1 expected endpoints: [netserver-1]
Sep 20 13:37:42.007: INFO: Going to poll 10.100.3.193 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Sep 20 13:37:42.195: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.3.193 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1708 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 20 13:37:42.195: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
Sep 20 13:37:42.196: INFO: ExecWithOptions: Clientset creation
Sep 20 13:37:42.196: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-1708/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.100.3.193+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep 20 13:37:43.794: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Sep 20 13:37:43.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-1708" for this suite. 09/20/23 13:37:43.889
------------------------------
â€¢ [SLOW TEST] [40.017 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:37:04.429
    Sep 20 13:37:04.429: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename pod-network-test 09/20/23 13:37:04.43
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:37:06.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:37:06.238
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-1708 09/20/23 13:37:06.243
    STEP: creating a selector 09/20/23 13:37:06.243
    STEP: Creating the service pods in kubernetes 09/20/23 13:37:06.243
    Sep 20 13:37:06.243: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Sep 20 13:37:07.733: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1708" to be "running and ready"
    Sep 20 13:37:07.757: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 23.043895ms
    Sep 20 13:37:07.757: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:37:10.042: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.308063382s
    Sep 20 13:37:10.042: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:37:12.026: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.292312686s
    Sep 20 13:37:12.026: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep 20 13:37:14.857: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 7.123385993s
    Sep 20 13:37:14.857: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep 20 13:37:15.933: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.199148935s
    Sep 20 13:37:15.933: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep 20 13:37:17.794: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.060262556s
    Sep 20 13:37:17.794: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep 20 13:37:19.818: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.084619606s
    Sep 20 13:37:19.818: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep 20 13:37:21.812: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.078012443s
    Sep 20 13:37:21.812: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep 20 13:37:24.311: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.577282832s
    Sep 20 13:37:24.311: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep 20 13:37:26.381: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.647516951s
    Sep 20 13:37:26.381: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep 20 13:37:27.778: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.044556041s
    Sep 20 13:37:27.778: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep 20 13:37:30.255: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.521143463s
    Sep 20 13:37:30.255: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Sep 20 13:37:30.255: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Sep 20 13:37:30.395: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1708" to be "running and ready"
    Sep 20 13:37:30.454: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 58.867406ms
    Sep 20 13:37:30.454: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Sep 20 13:37:30.454: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Sep 20 13:37:31.089: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1708" to be "running and ready"
    Sep 20 13:37:31.531: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 442.27614ms
    Sep 20 13:37:31.531: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Sep 20 13:37:31.531: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 09/20/23 13:37:31.709
    Sep 20 13:37:32.588: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1708" to be "running"
    Sep 20 13:37:33.814: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.226378156s
    Sep 20 13:37:36.009: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.421321913s
    Sep 20 13:37:39.003: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.41570065s
    Sep 20 13:37:39.003: INFO: Pod "test-container-pod" satisfied condition "running"
    Sep 20 13:37:39.349: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-1708" to be "running"
    Sep 20 13:37:39.436: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 86.511105ms
    Sep 20 13:37:39.436: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Sep 20 13:37:39.487: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Sep 20 13:37:39.487: INFO: Going to poll 10.100.5.148 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Sep 20 13:37:39.563: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.5.148 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1708 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep 20 13:37:39.563: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    Sep 20 13:37:39.564: INFO: ExecWithOptions: Clientset creation
    Sep 20 13:37:39.564: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-1708/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.100.5.148+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep 20 13:37:40.845: INFO: Found all 1 expected endpoints: [netserver-0]
    Sep 20 13:37:40.845: INFO: Going to poll 10.100.4.59 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Sep 20 13:37:40.862: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.4.59 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1708 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep 20 13:37:40.862: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    Sep 20 13:37:40.863: INFO: ExecWithOptions: Clientset creation
    Sep 20 13:37:40.863: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-1708/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.100.4.59+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep 20 13:37:42.007: INFO: Found all 1 expected endpoints: [netserver-1]
    Sep 20 13:37:42.007: INFO: Going to poll 10.100.3.193 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Sep 20 13:37:42.195: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.3.193 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1708 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep 20 13:37:42.195: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    Sep 20 13:37:42.196: INFO: ExecWithOptions: Clientset creation
    Sep 20 13:37:42.196: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-1708/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.100.3.193+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep 20 13:37:43.794: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:37:43.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-1708" for this suite. 09/20/23 13:37:43.889
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:37:44.447
Sep 20 13:37:44.447: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename projected 09/20/23 13:37:44.448
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:37:46.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:37:46.098
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-a9c29ead-0280-4d75-8735-fc054abb2422 09/20/23 13:37:46.147
STEP: Creating configMap with name cm-test-opt-upd-eac9fbee-f487-4d96-a27f-0057aaeebec3 09/20/23 13:37:46.16
STEP: Creating the pod 09/20/23 13:37:46.177
Sep 20 13:37:46.201: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4b8d1985-bfd9-4cb4-94e6-074e798cda02" in namespace "projected-410" to be "running and ready"
Sep 20 13:37:46.225: INFO: Pod "pod-projected-configmaps-4b8d1985-bfd9-4cb4-94e6-074e798cda02": Phase="Pending", Reason="", readiness=false. Elapsed: 23.944833ms
Sep 20 13:37:46.225: INFO: The phase of Pod pod-projected-configmaps-4b8d1985-bfd9-4cb4-94e6-074e798cda02 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:37:48.259: INFO: Pod "pod-projected-configmaps-4b8d1985-bfd9-4cb4-94e6-074e798cda02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058096434s
Sep 20 13:37:48.260: INFO: The phase of Pod pod-projected-configmaps-4b8d1985-bfd9-4cb4-94e6-074e798cda02 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:37:50.732: INFO: Pod "pod-projected-configmaps-4b8d1985-bfd9-4cb4-94e6-074e798cda02": Phase="Running", Reason="", readiness=true. Elapsed: 4.531094035s
Sep 20 13:37:50.732: INFO: The phase of Pod pod-projected-configmaps-4b8d1985-bfd9-4cb4-94e6-074e798cda02 is Running (Ready = true)
Sep 20 13:37:50.733: INFO: Pod "pod-projected-configmaps-4b8d1985-bfd9-4cb4-94e6-074e798cda02" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-a9c29ead-0280-4d75-8735-fc054abb2422 09/20/23 13:37:53.832
STEP: Updating configmap cm-test-opt-upd-eac9fbee-f487-4d96-a27f-0057aaeebec3 09/20/23 13:37:54.172
STEP: Creating configMap with name cm-test-opt-create-59253d9b-2e2b-498a-9166-9c5b8e541d9f 09/20/23 13:37:54.705
STEP: waiting to observe update in volume 09/20/23 13:37:55.277
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep 20 13:39:16.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-410" for this suite. 09/20/23 13:39:16.9
------------------------------
â€¢ [SLOW TEST] [92.615 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:37:44.447
    Sep 20 13:37:44.447: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename projected 09/20/23 13:37:44.448
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:37:46.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:37:46.098
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-a9c29ead-0280-4d75-8735-fc054abb2422 09/20/23 13:37:46.147
    STEP: Creating configMap with name cm-test-opt-upd-eac9fbee-f487-4d96-a27f-0057aaeebec3 09/20/23 13:37:46.16
    STEP: Creating the pod 09/20/23 13:37:46.177
    Sep 20 13:37:46.201: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4b8d1985-bfd9-4cb4-94e6-074e798cda02" in namespace "projected-410" to be "running and ready"
    Sep 20 13:37:46.225: INFO: Pod "pod-projected-configmaps-4b8d1985-bfd9-4cb4-94e6-074e798cda02": Phase="Pending", Reason="", readiness=false. Elapsed: 23.944833ms
    Sep 20 13:37:46.225: INFO: The phase of Pod pod-projected-configmaps-4b8d1985-bfd9-4cb4-94e6-074e798cda02 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:37:48.259: INFO: Pod "pod-projected-configmaps-4b8d1985-bfd9-4cb4-94e6-074e798cda02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058096434s
    Sep 20 13:37:48.260: INFO: The phase of Pod pod-projected-configmaps-4b8d1985-bfd9-4cb4-94e6-074e798cda02 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:37:50.732: INFO: Pod "pod-projected-configmaps-4b8d1985-bfd9-4cb4-94e6-074e798cda02": Phase="Running", Reason="", readiness=true. Elapsed: 4.531094035s
    Sep 20 13:37:50.732: INFO: The phase of Pod pod-projected-configmaps-4b8d1985-bfd9-4cb4-94e6-074e798cda02 is Running (Ready = true)
    Sep 20 13:37:50.733: INFO: Pod "pod-projected-configmaps-4b8d1985-bfd9-4cb4-94e6-074e798cda02" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-a9c29ead-0280-4d75-8735-fc054abb2422 09/20/23 13:37:53.832
    STEP: Updating configmap cm-test-opt-upd-eac9fbee-f487-4d96-a27f-0057aaeebec3 09/20/23 13:37:54.172
    STEP: Creating configMap with name cm-test-opt-create-59253d9b-2e2b-498a-9166-9c5b8e541d9f 09/20/23 13:37:54.705
    STEP: waiting to observe update in volume 09/20/23 13:37:55.277
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:39:16.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-410" for this suite. 09/20/23 13:39:16.9
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:39:17.063
Sep 20 13:39:17.063: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename watch 09/20/23 13:39:17.064
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:39:17.096
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:39:17.099
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 09/20/23 13:39:17.102
STEP: creating a new configmap 09/20/23 13:39:17.103
STEP: modifying the configmap once 09/20/23 13:39:17.108
STEP: closing the watch once it receives two notifications 09/20/23 13:39:17.116
Sep 20 13:39:17.116: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7096  0e3b1c01-dfbf-424b-ac64-238ea1c53fab 41819 0 2023-09-20 13:39:17 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-20 13:39:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 20 13:39:17.116: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7096  0e3b1c01-dfbf-424b-ac64-238ea1c53fab 41820 0 2023-09-20 13:39:17 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-20 13:39:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 09/20/23 13:39:17.116
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 09/20/23 13:39:17.122
STEP: deleting the configmap 09/20/23 13:39:17.123
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 09/20/23 13:39:17.129
Sep 20 13:39:17.129: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7096  0e3b1c01-dfbf-424b-ac64-238ea1c53fab 41821 0 2023-09-20 13:39:17 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-20 13:39:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 20 13:39:17.130: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7096  0e3b1c01-dfbf-424b-ac64-238ea1c53fab 41822 0 2023-09-20 13:39:17 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-20 13:39:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Sep 20 13:39:17.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-7096" for this suite. 09/20/23 13:39:17.134
------------------------------
â€¢ [0.077 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:39:17.063
    Sep 20 13:39:17.063: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename watch 09/20/23 13:39:17.064
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:39:17.096
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:39:17.099
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 09/20/23 13:39:17.102
    STEP: creating a new configmap 09/20/23 13:39:17.103
    STEP: modifying the configmap once 09/20/23 13:39:17.108
    STEP: closing the watch once it receives two notifications 09/20/23 13:39:17.116
    Sep 20 13:39:17.116: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7096  0e3b1c01-dfbf-424b-ac64-238ea1c53fab 41819 0 2023-09-20 13:39:17 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-20 13:39:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep 20 13:39:17.116: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7096  0e3b1c01-dfbf-424b-ac64-238ea1c53fab 41820 0 2023-09-20 13:39:17 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-20 13:39:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 09/20/23 13:39:17.116
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 09/20/23 13:39:17.122
    STEP: deleting the configmap 09/20/23 13:39:17.123
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 09/20/23 13:39:17.129
    Sep 20 13:39:17.129: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7096  0e3b1c01-dfbf-424b-ac64-238ea1c53fab 41821 0 2023-09-20 13:39:17 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-20 13:39:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep 20 13:39:17.130: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7096  0e3b1c01-dfbf-424b-ac64-238ea1c53fab 41822 0 2023-09-20 13:39:17 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-20 13:39:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:39:17.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-7096" for this suite. 09/20/23 13:39:17.134
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:39:17.142
Sep 20 13:39:17.142: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename services 09/20/23 13:39:17.143
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:39:17.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:39:17.162
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-9627 09/20/23 13:39:17.166
STEP: creating service affinity-nodeport-transition in namespace services-9627 09/20/23 13:39:17.166
STEP: creating replication controller affinity-nodeport-transition in namespace services-9627 09/20/23 13:39:17.254
I0920 13:39:17.544002      20 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-9627, replica count: 3
I0920 13:39:20.595753      20 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0920 13:39:23.597463      20 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 20 13:39:23.608: INFO: Creating new exec pod
Sep 20 13:39:23.624: INFO: Waiting up to 5m0s for pod "execpod-affinitys9hwj" in namespace "services-9627" to be "running"
Sep 20 13:39:23.641: INFO: Pod "execpod-affinitys9hwj": Phase="Pending", Reason="", readiness=false. Elapsed: 16.775368ms
Sep 20 13:39:25.647: INFO: Pod "execpod-affinitys9hwj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022405356s
Sep 20 13:39:28.270: INFO: Pod "execpod-affinitys9hwj": Phase="Running", Reason="", readiness=true. Elapsed: 4.645766326s
Sep 20 13:39:28.270: INFO: Pod "execpod-affinitys9hwj" satisfied condition "running"
Sep 20 13:39:29.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-9627 exec execpod-affinitys9hwj -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Sep 20 13:39:29.476: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Sep 20 13:39:29.476: INFO: stdout: ""
Sep 20 13:39:29.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-9627 exec execpod-affinitys9hwj -- /bin/sh -x -c nc -v -z -w 2 10.254.173.117 80'
Sep 20 13:39:29.658: INFO: stderr: "+ nc -v -z -w 2 10.254.173.117 80\nConnection to 10.254.173.117 80 port [tcp/http] succeeded!\n"
Sep 20 13:39:29.659: INFO: stdout: ""
Sep 20 13:39:29.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-9627 exec execpod-affinitys9hwj -- /bin/sh -x -c nc -v -z -w 2 192.168.10.172 31662'
Sep 20 13:39:29.850: INFO: stderr: "+ nc -v -z -w 2 192.168.10.172 31662\nConnection to 192.168.10.172 31662 port [tcp/*] succeeded!\n"
Sep 20 13:39:29.850: INFO: stdout: ""
Sep 20 13:39:29.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-9627 exec execpod-affinitys9hwj -- /bin/sh -x -c nc -v -z -w 2 192.168.10.64 31662'
Sep 20 13:39:30.076: INFO: stderr: "+ nc -v -z -w 2 192.168.10.64 31662\nConnection to 192.168.10.64 31662 port [tcp/*] succeeded!\n"
Sep 20 13:39:30.076: INFO: stdout: ""
Sep 20 13:39:30.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-9627 exec execpod-affinitys9hwj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.10.173:31662/ ; done'
Sep 20 13:39:31.305: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n"
Sep 20 13:39:31.305: INFO: stdout: "\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-4wg5l\naffinity-nodeport-transition-kq62j\naffinity-nodeport-transition-kq62j\naffinity-nodeport-transition-4wg5l\naffinity-nodeport-transition-kq62j\naffinity-nodeport-transition-kq62j\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-kq62j\naffinity-nodeport-transition-kq62j\naffinity-nodeport-transition-4wg5l\naffinity-nodeport-transition-kq62j\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-9bmmj"
Sep 20 13:39:31.305: INFO: Received response from host: affinity-nodeport-transition-9bmmj
Sep 20 13:39:31.305: INFO: Received response from host: affinity-nodeport-transition-9bmmj
Sep 20 13:39:31.305: INFO: Received response from host: affinity-nodeport-transition-4wg5l
Sep 20 13:39:31.305: INFO: Received response from host: affinity-nodeport-transition-kq62j
Sep 20 13:39:31.305: INFO: Received response from host: affinity-nodeport-transition-kq62j
Sep 20 13:39:31.305: INFO: Received response from host: affinity-nodeport-transition-4wg5l
Sep 20 13:39:31.305: INFO: Received response from host: affinity-nodeport-transition-kq62j
Sep 20 13:39:31.305: INFO: Received response from host: affinity-nodeport-transition-kq62j
Sep 20 13:39:31.305: INFO: Received response from host: affinity-nodeport-transition-9bmmj
Sep 20 13:39:31.305: INFO: Received response from host: affinity-nodeport-transition-9bmmj
Sep 20 13:39:31.305: INFO: Received response from host: affinity-nodeport-transition-kq62j
Sep 20 13:39:31.305: INFO: Received response from host: affinity-nodeport-transition-kq62j
Sep 20 13:39:31.305: INFO: Received response from host: affinity-nodeport-transition-4wg5l
Sep 20 13:39:31.305: INFO: Received response from host: affinity-nodeport-transition-kq62j
Sep 20 13:39:31.305: INFO: Received response from host: affinity-nodeport-transition-9bmmj
Sep 20 13:39:31.305: INFO: Received response from host: affinity-nodeport-transition-9bmmj
Sep 20 13:39:31.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-9627 exec execpod-affinitys9hwj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.10.173:31662/ ; done'
Sep 20 13:39:31.631: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n"
Sep 20 13:39:31.631: INFO: stdout: "\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-9bmmj"
Sep 20 13:39:31.631: INFO: Received response from host: affinity-nodeport-transition-9bmmj
Sep 20 13:39:31.631: INFO: Received response from host: affinity-nodeport-transition-9bmmj
Sep 20 13:39:31.631: INFO: Received response from host: affinity-nodeport-transition-9bmmj
Sep 20 13:39:31.631: INFO: Received response from host: affinity-nodeport-transition-9bmmj
Sep 20 13:39:31.631: INFO: Received response from host: affinity-nodeport-transition-9bmmj
Sep 20 13:39:31.631: INFO: Received response from host: affinity-nodeport-transition-9bmmj
Sep 20 13:39:31.631: INFO: Received response from host: affinity-nodeport-transition-9bmmj
Sep 20 13:39:31.631: INFO: Received response from host: affinity-nodeport-transition-9bmmj
Sep 20 13:39:31.631: INFO: Received response from host: affinity-nodeport-transition-9bmmj
Sep 20 13:39:31.631: INFO: Received response from host: affinity-nodeport-transition-9bmmj
Sep 20 13:39:31.631: INFO: Received response from host: affinity-nodeport-transition-9bmmj
Sep 20 13:39:31.631: INFO: Received response from host: affinity-nodeport-transition-9bmmj
Sep 20 13:39:31.631: INFO: Received response from host: affinity-nodeport-transition-9bmmj
Sep 20 13:39:31.631: INFO: Received response from host: affinity-nodeport-transition-9bmmj
Sep 20 13:39:31.631: INFO: Received response from host: affinity-nodeport-transition-9bmmj
Sep 20 13:39:31.631: INFO: Received response from host: affinity-nodeport-transition-9bmmj
Sep 20 13:39:31.631: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9627, will wait for the garbage collector to delete the pods 09/20/23 13:39:31.896
Sep 20 13:39:32.106: INFO: Deleting ReplicationController affinity-nodeport-transition took: 10.910482ms
Sep 20 13:39:32.606: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 500.42984ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep 20 13:39:35.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9627" for this suite. 09/20/23 13:39:35.938
------------------------------
â€¢ [SLOW TEST] [18.817 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:39:17.142
    Sep 20 13:39:17.142: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename services 09/20/23 13:39:17.143
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:39:17.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:39:17.162
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-9627 09/20/23 13:39:17.166
    STEP: creating service affinity-nodeport-transition in namespace services-9627 09/20/23 13:39:17.166
    STEP: creating replication controller affinity-nodeport-transition in namespace services-9627 09/20/23 13:39:17.254
    I0920 13:39:17.544002      20 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-9627, replica count: 3
    I0920 13:39:20.595753      20 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0920 13:39:23.597463      20 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep 20 13:39:23.608: INFO: Creating new exec pod
    Sep 20 13:39:23.624: INFO: Waiting up to 5m0s for pod "execpod-affinitys9hwj" in namespace "services-9627" to be "running"
    Sep 20 13:39:23.641: INFO: Pod "execpod-affinitys9hwj": Phase="Pending", Reason="", readiness=false. Elapsed: 16.775368ms
    Sep 20 13:39:25.647: INFO: Pod "execpod-affinitys9hwj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022405356s
    Sep 20 13:39:28.270: INFO: Pod "execpod-affinitys9hwj": Phase="Running", Reason="", readiness=true. Elapsed: 4.645766326s
    Sep 20 13:39:28.270: INFO: Pod "execpod-affinitys9hwj" satisfied condition "running"
    Sep 20 13:39:29.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-9627 exec execpod-affinitys9hwj -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Sep 20 13:39:29.476: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Sep 20 13:39:29.476: INFO: stdout: ""
    Sep 20 13:39:29.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-9627 exec execpod-affinitys9hwj -- /bin/sh -x -c nc -v -z -w 2 10.254.173.117 80'
    Sep 20 13:39:29.658: INFO: stderr: "+ nc -v -z -w 2 10.254.173.117 80\nConnection to 10.254.173.117 80 port [tcp/http] succeeded!\n"
    Sep 20 13:39:29.659: INFO: stdout: ""
    Sep 20 13:39:29.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-9627 exec execpod-affinitys9hwj -- /bin/sh -x -c nc -v -z -w 2 192.168.10.172 31662'
    Sep 20 13:39:29.850: INFO: stderr: "+ nc -v -z -w 2 192.168.10.172 31662\nConnection to 192.168.10.172 31662 port [tcp/*] succeeded!\n"
    Sep 20 13:39:29.850: INFO: stdout: ""
    Sep 20 13:39:29.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-9627 exec execpod-affinitys9hwj -- /bin/sh -x -c nc -v -z -w 2 192.168.10.64 31662'
    Sep 20 13:39:30.076: INFO: stderr: "+ nc -v -z -w 2 192.168.10.64 31662\nConnection to 192.168.10.64 31662 port [tcp/*] succeeded!\n"
    Sep 20 13:39:30.076: INFO: stdout: ""
    Sep 20 13:39:30.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-9627 exec execpod-affinitys9hwj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.10.173:31662/ ; done'
    Sep 20 13:39:31.305: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n"
    Sep 20 13:39:31.305: INFO: stdout: "\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-4wg5l\naffinity-nodeport-transition-kq62j\naffinity-nodeport-transition-kq62j\naffinity-nodeport-transition-4wg5l\naffinity-nodeport-transition-kq62j\naffinity-nodeport-transition-kq62j\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-kq62j\naffinity-nodeport-transition-kq62j\naffinity-nodeport-transition-4wg5l\naffinity-nodeport-transition-kq62j\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-9bmmj"
    Sep 20 13:39:31.305: INFO: Received response from host: affinity-nodeport-transition-9bmmj
    Sep 20 13:39:31.305: INFO: Received response from host: affinity-nodeport-transition-9bmmj
    Sep 20 13:39:31.305: INFO: Received response from host: affinity-nodeport-transition-4wg5l
    Sep 20 13:39:31.305: INFO: Received response from host: affinity-nodeport-transition-kq62j
    Sep 20 13:39:31.305: INFO: Received response from host: affinity-nodeport-transition-kq62j
    Sep 20 13:39:31.305: INFO: Received response from host: affinity-nodeport-transition-4wg5l
    Sep 20 13:39:31.305: INFO: Received response from host: affinity-nodeport-transition-kq62j
    Sep 20 13:39:31.305: INFO: Received response from host: affinity-nodeport-transition-kq62j
    Sep 20 13:39:31.305: INFO: Received response from host: affinity-nodeport-transition-9bmmj
    Sep 20 13:39:31.305: INFO: Received response from host: affinity-nodeport-transition-9bmmj
    Sep 20 13:39:31.305: INFO: Received response from host: affinity-nodeport-transition-kq62j
    Sep 20 13:39:31.305: INFO: Received response from host: affinity-nodeport-transition-kq62j
    Sep 20 13:39:31.305: INFO: Received response from host: affinity-nodeport-transition-4wg5l
    Sep 20 13:39:31.305: INFO: Received response from host: affinity-nodeport-transition-kq62j
    Sep 20 13:39:31.305: INFO: Received response from host: affinity-nodeport-transition-9bmmj
    Sep 20 13:39:31.305: INFO: Received response from host: affinity-nodeport-transition-9bmmj
    Sep 20 13:39:31.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-9627 exec execpod-affinitys9hwj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.10.173:31662/ ; done'
    Sep 20 13:39:31.631: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31662/\n"
    Sep 20 13:39:31.631: INFO: stdout: "\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-9bmmj\naffinity-nodeport-transition-9bmmj"
    Sep 20 13:39:31.631: INFO: Received response from host: affinity-nodeport-transition-9bmmj
    Sep 20 13:39:31.631: INFO: Received response from host: affinity-nodeport-transition-9bmmj
    Sep 20 13:39:31.631: INFO: Received response from host: affinity-nodeport-transition-9bmmj
    Sep 20 13:39:31.631: INFO: Received response from host: affinity-nodeport-transition-9bmmj
    Sep 20 13:39:31.631: INFO: Received response from host: affinity-nodeport-transition-9bmmj
    Sep 20 13:39:31.631: INFO: Received response from host: affinity-nodeport-transition-9bmmj
    Sep 20 13:39:31.631: INFO: Received response from host: affinity-nodeport-transition-9bmmj
    Sep 20 13:39:31.631: INFO: Received response from host: affinity-nodeport-transition-9bmmj
    Sep 20 13:39:31.631: INFO: Received response from host: affinity-nodeport-transition-9bmmj
    Sep 20 13:39:31.631: INFO: Received response from host: affinity-nodeport-transition-9bmmj
    Sep 20 13:39:31.631: INFO: Received response from host: affinity-nodeport-transition-9bmmj
    Sep 20 13:39:31.631: INFO: Received response from host: affinity-nodeport-transition-9bmmj
    Sep 20 13:39:31.631: INFO: Received response from host: affinity-nodeport-transition-9bmmj
    Sep 20 13:39:31.631: INFO: Received response from host: affinity-nodeport-transition-9bmmj
    Sep 20 13:39:31.631: INFO: Received response from host: affinity-nodeport-transition-9bmmj
    Sep 20 13:39:31.631: INFO: Received response from host: affinity-nodeport-transition-9bmmj
    Sep 20 13:39:31.631: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9627, will wait for the garbage collector to delete the pods 09/20/23 13:39:31.896
    Sep 20 13:39:32.106: INFO: Deleting ReplicationController affinity-nodeport-transition took: 10.910482ms
    Sep 20 13:39:32.606: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 500.42984ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:39:35.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9627" for this suite. 09/20/23 13:39:35.938
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:39:35.961
Sep 20 13:39:35.961: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename downward-api 09/20/23 13:39:35.962
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:39:36.013
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:39:36.017
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 09/20/23 13:39:36.024
Sep 20 13:39:36.037: INFO: Waiting up to 5m0s for pod "downwardapi-volume-89f0211d-ccda-45c8-90bb-e417c3fcd5e5" in namespace "downward-api-345" to be "Succeeded or Failed"
Sep 20 13:39:36.043: INFO: Pod "downwardapi-volume-89f0211d-ccda-45c8-90bb-e417c3fcd5e5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.399645ms
Sep 20 13:39:38.048: INFO: Pod "downwardapi-volume-89f0211d-ccda-45c8-90bb-e417c3fcd5e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011361976s
Sep 20 13:39:40.250: INFO: Pod "downwardapi-volume-89f0211d-ccda-45c8-90bb-e417c3fcd5e5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.213360932s
Sep 20 13:39:42.103: INFO: Pod "downwardapi-volume-89f0211d-ccda-45c8-90bb-e417c3fcd5e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.065976165s
STEP: Saw pod success 09/20/23 13:39:42.103
Sep 20 13:39:42.103: INFO: Pod "downwardapi-volume-89f0211d-ccda-45c8-90bb-e417c3fcd5e5" satisfied condition "Succeeded or Failed"
Sep 20 13:39:42.106: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downwardapi-volume-89f0211d-ccda-45c8-90bb-e417c3fcd5e5 container client-container: <nil>
STEP: delete the pod 09/20/23 13:39:42.191
Sep 20 13:39:42.351: INFO: Waiting for pod downwardapi-volume-89f0211d-ccda-45c8-90bb-e417c3fcd5e5 to disappear
Sep 20 13:39:42.357: INFO: Pod downwardapi-volume-89f0211d-ccda-45c8-90bb-e417c3fcd5e5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep 20 13:39:42.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-345" for this suite. 09/20/23 13:39:42.364
------------------------------
â€¢ [SLOW TEST] [6.643 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:39:35.961
    Sep 20 13:39:35.961: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename downward-api 09/20/23 13:39:35.962
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:39:36.013
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:39:36.017
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 09/20/23 13:39:36.024
    Sep 20 13:39:36.037: INFO: Waiting up to 5m0s for pod "downwardapi-volume-89f0211d-ccda-45c8-90bb-e417c3fcd5e5" in namespace "downward-api-345" to be "Succeeded or Failed"
    Sep 20 13:39:36.043: INFO: Pod "downwardapi-volume-89f0211d-ccda-45c8-90bb-e417c3fcd5e5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.399645ms
    Sep 20 13:39:38.048: INFO: Pod "downwardapi-volume-89f0211d-ccda-45c8-90bb-e417c3fcd5e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011361976s
    Sep 20 13:39:40.250: INFO: Pod "downwardapi-volume-89f0211d-ccda-45c8-90bb-e417c3fcd5e5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.213360932s
    Sep 20 13:39:42.103: INFO: Pod "downwardapi-volume-89f0211d-ccda-45c8-90bb-e417c3fcd5e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.065976165s
    STEP: Saw pod success 09/20/23 13:39:42.103
    Sep 20 13:39:42.103: INFO: Pod "downwardapi-volume-89f0211d-ccda-45c8-90bb-e417c3fcd5e5" satisfied condition "Succeeded or Failed"
    Sep 20 13:39:42.106: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downwardapi-volume-89f0211d-ccda-45c8-90bb-e417c3fcd5e5 container client-container: <nil>
    STEP: delete the pod 09/20/23 13:39:42.191
    Sep 20 13:39:42.351: INFO: Waiting for pod downwardapi-volume-89f0211d-ccda-45c8-90bb-e417c3fcd5e5 to disappear
    Sep 20 13:39:42.357: INFO: Pod downwardapi-volume-89f0211d-ccda-45c8-90bb-e417c3fcd5e5 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:39:42.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-345" for this suite. 09/20/23 13:39:42.364
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:39:42.605
Sep 20 13:39:42.606: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename gc 09/20/23 13:39:42.606
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:39:43.195
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:39:43.199
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 09/20/23 13:39:43.203
STEP: Wait for the Deployment to create new ReplicaSet 09/20/23 13:39:43.212
STEP: delete the deployment 09/20/23 13:39:44.323
STEP: wait for all rs to be garbage collected 09/20/23 13:39:44.332
STEP: expected 0 rs, got 1 rs 09/20/23 13:39:44.356
STEP: expected 0 pods, got 2 pods 09/20/23 13:39:44.365
STEP: Gathering metrics 09/20/23 13:39:44.874
W0920 13:39:44.883257      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Sep 20 13:39:44.883: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep 20 13:39:44.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9195" for this suite. 09/20/23 13:39:44.887
------------------------------
â€¢ [2.290 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:39:42.605
    Sep 20 13:39:42.606: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename gc 09/20/23 13:39:42.606
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:39:43.195
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:39:43.199
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 09/20/23 13:39:43.203
    STEP: Wait for the Deployment to create new ReplicaSet 09/20/23 13:39:43.212
    STEP: delete the deployment 09/20/23 13:39:44.323
    STEP: wait for all rs to be garbage collected 09/20/23 13:39:44.332
    STEP: expected 0 rs, got 1 rs 09/20/23 13:39:44.356
    STEP: expected 0 pods, got 2 pods 09/20/23 13:39:44.365
    STEP: Gathering metrics 09/20/23 13:39:44.874
    W0920 13:39:44.883257      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Sep 20 13:39:44.883: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:39:44.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9195" for this suite. 09/20/23 13:39:44.887
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:39:44.899
Sep 20 13:39:44.899: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename namespaces 09/20/23 13:39:44.9
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:39:44.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:39:44.96
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-5231" 09/20/23 13:39:44.968
Sep 20 13:39:44.978: INFO: Namespace "namespaces-5231" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"e878e702-44f2-4f20-b5dd-9ccb6fb27e1a", "kubernetes.io/metadata.name":"namespaces-5231", "namespaces-5231":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep 20 13:39:44.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-5231" for this suite. 09/20/23 13:39:44.983
------------------------------
â€¢ [0.091 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:39:44.899
    Sep 20 13:39:44.899: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename namespaces 09/20/23 13:39:44.9
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:39:44.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:39:44.96
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-5231" 09/20/23 13:39:44.968
    Sep 20 13:39:44.978: INFO: Namespace "namespaces-5231" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"e878e702-44f2-4f20-b5dd-9ccb6fb27e1a", "kubernetes.io/metadata.name":"namespaces-5231", "namespaces-5231":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:39:44.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-5231" for this suite. 09/20/23 13:39:44.983
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:39:44.991
Sep 20 13:39:44.991: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename kubectl 09/20/23 13:39:44.992
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:39:45.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:39:45.01
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 09/20/23 13:39:45.018
Sep 20 13:39:45.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 create -f -'
Sep 20 13:39:45.320: INFO: stderr: ""
Sep 20 13:39:45.320: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 09/20/23 13:39:45.32
Sep 20 13:39:45.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 20 13:39:45.647: INFO: stderr: ""
Sep 20 13:39:45.647: INFO: stdout: "update-demo-nautilus-qmrrp update-demo-nautilus-z846n "
Sep 20 13:39:45.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods update-demo-nautilus-qmrrp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 20 13:39:45.724: INFO: stderr: ""
Sep 20 13:39:45.724: INFO: stdout: ""
Sep 20 13:39:45.724: INFO: update-demo-nautilus-qmrrp is created but not running
Sep 20 13:39:50.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 20 13:39:50.954: INFO: stderr: ""
Sep 20 13:39:50.954: INFO: stdout: "update-demo-nautilus-qmrrp update-demo-nautilus-z846n "
Sep 20 13:39:50.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods update-demo-nautilus-qmrrp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 20 13:39:51.038: INFO: stderr: ""
Sep 20 13:39:51.038: INFO: stdout: "true"
Sep 20 13:39:51.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods update-demo-nautilus-qmrrp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 20 13:39:51.110: INFO: stderr: ""
Sep 20 13:39:51.110: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep 20 13:39:51.110: INFO: validating pod update-demo-nautilus-qmrrp
Sep 20 13:39:51.130: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 20 13:39:51.130: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 20 13:39:51.130: INFO: update-demo-nautilus-qmrrp is verified up and running
Sep 20 13:39:51.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods update-demo-nautilus-z846n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 20 13:39:51.203: INFO: stderr: ""
Sep 20 13:39:51.203: INFO: stdout: ""
Sep 20 13:39:51.203: INFO: update-demo-nautilus-z846n is created but not running
Sep 20 13:39:56.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 20 13:39:56.352: INFO: stderr: ""
Sep 20 13:39:56.352: INFO: stdout: "update-demo-nautilus-qmrrp update-demo-nautilus-z846n "
Sep 20 13:39:56.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods update-demo-nautilus-qmrrp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 20 13:39:56.447: INFO: stderr: ""
Sep 20 13:39:56.447: INFO: stdout: "true"
Sep 20 13:39:56.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods update-demo-nautilus-qmrrp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 20 13:39:56.939: INFO: stderr: ""
Sep 20 13:39:56.939: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep 20 13:39:56.939: INFO: validating pod update-demo-nautilus-qmrrp
Sep 20 13:39:56.943: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 20 13:39:56.943: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 20 13:39:56.943: INFO: update-demo-nautilus-qmrrp is verified up and running
Sep 20 13:39:56.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods update-demo-nautilus-z846n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 20 13:39:57.023: INFO: stderr: ""
Sep 20 13:39:57.023: INFO: stdout: ""
Sep 20 13:39:57.023: INFO: update-demo-nautilus-z846n is created but not running
Sep 20 13:40:02.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 20 13:40:02.108: INFO: stderr: ""
Sep 20 13:40:02.108: INFO: stdout: "update-demo-nautilus-qmrrp update-demo-nautilus-z846n "
Sep 20 13:40:02.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods update-demo-nautilus-qmrrp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 20 13:40:02.210: INFO: stderr: ""
Sep 20 13:40:02.210: INFO: stdout: "true"
Sep 20 13:40:02.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods update-demo-nautilus-qmrrp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 20 13:40:02.281: INFO: stderr: ""
Sep 20 13:40:02.281: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep 20 13:40:02.281: INFO: validating pod update-demo-nautilus-qmrrp
Sep 20 13:40:02.287: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 20 13:40:02.287: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 20 13:40:02.287: INFO: update-demo-nautilus-qmrrp is verified up and running
Sep 20 13:40:02.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods update-demo-nautilus-z846n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 20 13:40:02.370: INFO: stderr: ""
Sep 20 13:40:02.370: INFO: stdout: "true"
Sep 20 13:40:02.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods update-demo-nautilus-z846n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 20 13:40:02.440: INFO: stderr: ""
Sep 20 13:40:02.440: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep 20 13:40:02.440: INFO: validating pod update-demo-nautilus-z846n
Sep 20 13:40:02.448: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 20 13:40:02.448: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 20 13:40:02.448: INFO: update-demo-nautilus-z846n is verified up and running
STEP: scaling down the replication controller 09/20/23 13:40:02.448
Sep 20 13:40:02.449: INFO: scanned /root for discovery docs: <nil>
Sep 20 13:40:02.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Sep 20 13:40:04.508: INFO: stderr: ""
Sep 20 13:40:04.508: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 09/20/23 13:40:04.508
Sep 20 13:40:04.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 20 13:40:04.735: INFO: stderr: ""
Sep 20 13:40:04.735: INFO: stdout: "update-demo-nautilus-qmrrp update-demo-nautilus-z846n "
STEP: Replicas for name=update-demo: expected=1 actual=2 09/20/23 13:40:04.735
Sep 20 13:40:09.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 20 13:40:09.929: INFO: stderr: ""
Sep 20 13:40:09.929: INFO: stdout: "update-demo-nautilus-qmrrp "
Sep 20 13:40:09.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods update-demo-nautilus-qmrrp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 20 13:40:09.998: INFO: stderr: ""
Sep 20 13:40:09.998: INFO: stdout: "true"
Sep 20 13:40:09.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods update-demo-nautilus-qmrrp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 20 13:40:10.296: INFO: stderr: ""
Sep 20 13:40:10.296: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep 20 13:40:10.296: INFO: validating pod update-demo-nautilus-qmrrp
Sep 20 13:40:10.301: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 20 13:40:10.301: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 20 13:40:10.301: INFO: update-demo-nautilus-qmrrp is verified up and running
STEP: scaling up the replication controller 09/20/23 13:40:10.301
Sep 20 13:40:10.302: INFO: scanned /root for discovery docs: <nil>
Sep 20 13:40:10.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Sep 20 13:40:11.742: INFO: stderr: ""
Sep 20 13:40:11.742: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 09/20/23 13:40:11.742
Sep 20 13:40:11.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 20 13:40:11.884: INFO: stderr: ""
Sep 20 13:40:11.884: INFO: stdout: "update-demo-nautilus-7r48j update-demo-nautilus-qmrrp "
Sep 20 13:40:11.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods update-demo-nautilus-7r48j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 20 13:40:11.962: INFO: stderr: ""
Sep 20 13:40:11.962: INFO: stdout: ""
Sep 20 13:40:11.962: INFO: update-demo-nautilus-7r48j is created but not running
Sep 20 13:40:16.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 20 13:40:17.295: INFO: stderr: ""
Sep 20 13:40:17.295: INFO: stdout: "update-demo-nautilus-7r48j update-demo-nautilus-qmrrp "
Sep 20 13:40:17.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods update-demo-nautilus-7r48j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 20 13:40:17.370: INFO: stderr: ""
Sep 20 13:40:17.370: INFO: stdout: "true"
Sep 20 13:40:17.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods update-demo-nautilus-7r48j -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 20 13:40:17.451: INFO: stderr: ""
Sep 20 13:40:17.451: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep 20 13:40:17.451: INFO: validating pod update-demo-nautilus-7r48j
Sep 20 13:40:17.615: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 20 13:40:17.615: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 20 13:40:17.615: INFO: update-demo-nautilus-7r48j is verified up and running
Sep 20 13:40:17.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods update-demo-nautilus-qmrrp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 20 13:40:17.689: INFO: stderr: ""
Sep 20 13:40:17.689: INFO: stdout: "true"
Sep 20 13:40:17.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods update-demo-nautilus-qmrrp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 20 13:40:17.758: INFO: stderr: ""
Sep 20 13:40:17.758: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep 20 13:40:17.758: INFO: validating pod update-demo-nautilus-qmrrp
Sep 20 13:40:17.763: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 20 13:40:17.763: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 20 13:40:17.764: INFO: update-demo-nautilus-qmrrp is verified up and running
STEP: using delete to clean up resources 09/20/23 13:40:17.764
Sep 20 13:40:17.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 delete --grace-period=0 --force -f -'
Sep 20 13:40:17.856: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 20 13:40:17.856: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep 20 13:40:17.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get rc,svc -l name=update-demo --no-headers'
Sep 20 13:40:18.171: INFO: stderr: "No resources found in kubectl-8183 namespace.\n"
Sep 20 13:40:18.171: INFO: stdout: ""
Sep 20 13:40:18.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 20 13:40:18.450: INFO: stderr: ""
Sep 20 13:40:18.450: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep 20 13:40:18.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8183" for this suite. 09/20/23 13:40:18.458
------------------------------
â€¢ [SLOW TEST] [34.042 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:39:44.991
    Sep 20 13:39:44.991: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename kubectl 09/20/23 13:39:44.992
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:39:45.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:39:45.01
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 09/20/23 13:39:45.018
    Sep 20 13:39:45.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 create -f -'
    Sep 20 13:39:45.320: INFO: stderr: ""
    Sep 20 13:39:45.320: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 09/20/23 13:39:45.32
    Sep 20 13:39:45.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep 20 13:39:45.647: INFO: stderr: ""
    Sep 20 13:39:45.647: INFO: stdout: "update-demo-nautilus-qmrrp update-demo-nautilus-z846n "
    Sep 20 13:39:45.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods update-demo-nautilus-qmrrp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep 20 13:39:45.724: INFO: stderr: ""
    Sep 20 13:39:45.724: INFO: stdout: ""
    Sep 20 13:39:45.724: INFO: update-demo-nautilus-qmrrp is created but not running
    Sep 20 13:39:50.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep 20 13:39:50.954: INFO: stderr: ""
    Sep 20 13:39:50.954: INFO: stdout: "update-demo-nautilus-qmrrp update-demo-nautilus-z846n "
    Sep 20 13:39:50.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods update-demo-nautilus-qmrrp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep 20 13:39:51.038: INFO: stderr: ""
    Sep 20 13:39:51.038: INFO: stdout: "true"
    Sep 20 13:39:51.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods update-demo-nautilus-qmrrp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep 20 13:39:51.110: INFO: stderr: ""
    Sep 20 13:39:51.110: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep 20 13:39:51.110: INFO: validating pod update-demo-nautilus-qmrrp
    Sep 20 13:39:51.130: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep 20 13:39:51.130: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep 20 13:39:51.130: INFO: update-demo-nautilus-qmrrp is verified up and running
    Sep 20 13:39:51.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods update-demo-nautilus-z846n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep 20 13:39:51.203: INFO: stderr: ""
    Sep 20 13:39:51.203: INFO: stdout: ""
    Sep 20 13:39:51.203: INFO: update-demo-nautilus-z846n is created but not running
    Sep 20 13:39:56.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep 20 13:39:56.352: INFO: stderr: ""
    Sep 20 13:39:56.352: INFO: stdout: "update-demo-nautilus-qmrrp update-demo-nautilus-z846n "
    Sep 20 13:39:56.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods update-demo-nautilus-qmrrp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep 20 13:39:56.447: INFO: stderr: ""
    Sep 20 13:39:56.447: INFO: stdout: "true"
    Sep 20 13:39:56.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods update-demo-nautilus-qmrrp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep 20 13:39:56.939: INFO: stderr: ""
    Sep 20 13:39:56.939: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep 20 13:39:56.939: INFO: validating pod update-demo-nautilus-qmrrp
    Sep 20 13:39:56.943: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep 20 13:39:56.943: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep 20 13:39:56.943: INFO: update-demo-nautilus-qmrrp is verified up and running
    Sep 20 13:39:56.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods update-demo-nautilus-z846n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep 20 13:39:57.023: INFO: stderr: ""
    Sep 20 13:39:57.023: INFO: stdout: ""
    Sep 20 13:39:57.023: INFO: update-demo-nautilus-z846n is created but not running
    Sep 20 13:40:02.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep 20 13:40:02.108: INFO: stderr: ""
    Sep 20 13:40:02.108: INFO: stdout: "update-demo-nautilus-qmrrp update-demo-nautilus-z846n "
    Sep 20 13:40:02.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods update-demo-nautilus-qmrrp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep 20 13:40:02.210: INFO: stderr: ""
    Sep 20 13:40:02.210: INFO: stdout: "true"
    Sep 20 13:40:02.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods update-demo-nautilus-qmrrp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep 20 13:40:02.281: INFO: stderr: ""
    Sep 20 13:40:02.281: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep 20 13:40:02.281: INFO: validating pod update-demo-nautilus-qmrrp
    Sep 20 13:40:02.287: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep 20 13:40:02.287: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep 20 13:40:02.287: INFO: update-demo-nautilus-qmrrp is verified up and running
    Sep 20 13:40:02.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods update-demo-nautilus-z846n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep 20 13:40:02.370: INFO: stderr: ""
    Sep 20 13:40:02.370: INFO: stdout: "true"
    Sep 20 13:40:02.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods update-demo-nautilus-z846n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep 20 13:40:02.440: INFO: stderr: ""
    Sep 20 13:40:02.440: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep 20 13:40:02.440: INFO: validating pod update-demo-nautilus-z846n
    Sep 20 13:40:02.448: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep 20 13:40:02.448: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep 20 13:40:02.448: INFO: update-demo-nautilus-z846n is verified up and running
    STEP: scaling down the replication controller 09/20/23 13:40:02.448
    Sep 20 13:40:02.449: INFO: scanned /root for discovery docs: <nil>
    Sep 20 13:40:02.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Sep 20 13:40:04.508: INFO: stderr: ""
    Sep 20 13:40:04.508: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 09/20/23 13:40:04.508
    Sep 20 13:40:04.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep 20 13:40:04.735: INFO: stderr: ""
    Sep 20 13:40:04.735: INFO: stdout: "update-demo-nautilus-qmrrp update-demo-nautilus-z846n "
    STEP: Replicas for name=update-demo: expected=1 actual=2 09/20/23 13:40:04.735
    Sep 20 13:40:09.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep 20 13:40:09.929: INFO: stderr: ""
    Sep 20 13:40:09.929: INFO: stdout: "update-demo-nautilus-qmrrp "
    Sep 20 13:40:09.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods update-demo-nautilus-qmrrp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep 20 13:40:09.998: INFO: stderr: ""
    Sep 20 13:40:09.998: INFO: stdout: "true"
    Sep 20 13:40:09.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods update-demo-nautilus-qmrrp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep 20 13:40:10.296: INFO: stderr: ""
    Sep 20 13:40:10.296: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep 20 13:40:10.296: INFO: validating pod update-demo-nautilus-qmrrp
    Sep 20 13:40:10.301: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep 20 13:40:10.301: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep 20 13:40:10.301: INFO: update-demo-nautilus-qmrrp is verified up and running
    STEP: scaling up the replication controller 09/20/23 13:40:10.301
    Sep 20 13:40:10.302: INFO: scanned /root for discovery docs: <nil>
    Sep 20 13:40:10.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Sep 20 13:40:11.742: INFO: stderr: ""
    Sep 20 13:40:11.742: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 09/20/23 13:40:11.742
    Sep 20 13:40:11.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep 20 13:40:11.884: INFO: stderr: ""
    Sep 20 13:40:11.884: INFO: stdout: "update-demo-nautilus-7r48j update-demo-nautilus-qmrrp "
    Sep 20 13:40:11.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods update-demo-nautilus-7r48j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep 20 13:40:11.962: INFO: stderr: ""
    Sep 20 13:40:11.962: INFO: stdout: ""
    Sep 20 13:40:11.962: INFO: update-demo-nautilus-7r48j is created but not running
    Sep 20 13:40:16.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep 20 13:40:17.295: INFO: stderr: ""
    Sep 20 13:40:17.295: INFO: stdout: "update-demo-nautilus-7r48j update-demo-nautilus-qmrrp "
    Sep 20 13:40:17.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods update-demo-nautilus-7r48j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep 20 13:40:17.370: INFO: stderr: ""
    Sep 20 13:40:17.370: INFO: stdout: "true"
    Sep 20 13:40:17.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods update-demo-nautilus-7r48j -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep 20 13:40:17.451: INFO: stderr: ""
    Sep 20 13:40:17.451: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep 20 13:40:17.451: INFO: validating pod update-demo-nautilus-7r48j
    Sep 20 13:40:17.615: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep 20 13:40:17.615: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep 20 13:40:17.615: INFO: update-demo-nautilus-7r48j is verified up and running
    Sep 20 13:40:17.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods update-demo-nautilus-qmrrp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep 20 13:40:17.689: INFO: stderr: ""
    Sep 20 13:40:17.689: INFO: stdout: "true"
    Sep 20 13:40:17.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods update-demo-nautilus-qmrrp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep 20 13:40:17.758: INFO: stderr: ""
    Sep 20 13:40:17.758: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep 20 13:40:17.758: INFO: validating pod update-demo-nautilus-qmrrp
    Sep 20 13:40:17.763: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep 20 13:40:17.763: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep 20 13:40:17.764: INFO: update-demo-nautilus-qmrrp is verified up and running
    STEP: using delete to clean up resources 09/20/23 13:40:17.764
    Sep 20 13:40:17.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 delete --grace-period=0 --force -f -'
    Sep 20 13:40:17.856: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep 20 13:40:17.856: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Sep 20 13:40:17.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get rc,svc -l name=update-demo --no-headers'
    Sep 20 13:40:18.171: INFO: stderr: "No resources found in kubectl-8183 namespace.\n"
    Sep 20 13:40:18.171: INFO: stdout: ""
    Sep 20 13:40:18.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8183 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Sep 20 13:40:18.450: INFO: stderr: ""
    Sep 20 13:40:18.450: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:40:18.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8183" for this suite. 09/20/23 13:40:18.458
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:40:19.033
Sep 20 13:40:19.033: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename secrets 09/20/23 13:40:19.034
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:40:19.496
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:40:19.502
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-d802cd3e-02f2-429f-aa94-1225cea751b6 09/20/23 13:40:20.63
STEP: Creating a pod to test consume secrets 09/20/23 13:40:20.647
Sep 20 13:40:20.667: INFO: Waiting up to 5m0s for pod "pod-secrets-a5196072-18e2-4570-9f53-af01f4e75bdd" in namespace "secrets-4908" to be "Succeeded or Failed"
Sep 20 13:40:20.675: INFO: Pod "pod-secrets-a5196072-18e2-4570-9f53-af01f4e75bdd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.954753ms
Sep 20 13:40:22.680: INFO: Pod "pod-secrets-a5196072-18e2-4570-9f53-af01f4e75bdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013598166s
Sep 20 13:40:24.679: INFO: Pod "pod-secrets-a5196072-18e2-4570-9f53-af01f4e75bdd": Phase="Running", Reason="", readiness=true. Elapsed: 4.012337314s
Sep 20 13:40:27.462: INFO: Pod "pod-secrets-a5196072-18e2-4570-9f53-af01f4e75bdd": Phase="Running", Reason="", readiness=false. Elapsed: 6.795409634s
Sep 20 13:40:28.679: INFO: Pod "pod-secrets-a5196072-18e2-4570-9f53-af01f4e75bdd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.012556512s
STEP: Saw pod success 09/20/23 13:40:28.679
Sep 20 13:40:28.680: INFO: Pod "pod-secrets-a5196072-18e2-4570-9f53-af01f4e75bdd" satisfied condition "Succeeded or Failed"
Sep 20 13:40:28.686: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-secrets-a5196072-18e2-4570-9f53-af01f4e75bdd container secret-volume-test: <nil>
STEP: delete the pod 09/20/23 13:40:28.696
Sep 20 13:40:28.770: INFO: Waiting for pod pod-secrets-a5196072-18e2-4570-9f53-af01f4e75bdd to disappear
Sep 20 13:40:28.778: INFO: Pod pod-secrets-a5196072-18e2-4570-9f53-af01f4e75bdd no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep 20 13:40:28.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4908" for this suite. 09/20/23 13:40:28.785
STEP: Destroying namespace "secret-namespace-3208" for this suite. 09/20/23 13:40:28.811
------------------------------
â€¢ [SLOW TEST] [9.792 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:40:19.033
    Sep 20 13:40:19.033: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename secrets 09/20/23 13:40:19.034
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:40:19.496
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:40:19.502
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-d802cd3e-02f2-429f-aa94-1225cea751b6 09/20/23 13:40:20.63
    STEP: Creating a pod to test consume secrets 09/20/23 13:40:20.647
    Sep 20 13:40:20.667: INFO: Waiting up to 5m0s for pod "pod-secrets-a5196072-18e2-4570-9f53-af01f4e75bdd" in namespace "secrets-4908" to be "Succeeded or Failed"
    Sep 20 13:40:20.675: INFO: Pod "pod-secrets-a5196072-18e2-4570-9f53-af01f4e75bdd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.954753ms
    Sep 20 13:40:22.680: INFO: Pod "pod-secrets-a5196072-18e2-4570-9f53-af01f4e75bdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013598166s
    Sep 20 13:40:24.679: INFO: Pod "pod-secrets-a5196072-18e2-4570-9f53-af01f4e75bdd": Phase="Running", Reason="", readiness=true. Elapsed: 4.012337314s
    Sep 20 13:40:27.462: INFO: Pod "pod-secrets-a5196072-18e2-4570-9f53-af01f4e75bdd": Phase="Running", Reason="", readiness=false. Elapsed: 6.795409634s
    Sep 20 13:40:28.679: INFO: Pod "pod-secrets-a5196072-18e2-4570-9f53-af01f4e75bdd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.012556512s
    STEP: Saw pod success 09/20/23 13:40:28.679
    Sep 20 13:40:28.680: INFO: Pod "pod-secrets-a5196072-18e2-4570-9f53-af01f4e75bdd" satisfied condition "Succeeded or Failed"
    Sep 20 13:40:28.686: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-secrets-a5196072-18e2-4570-9f53-af01f4e75bdd container secret-volume-test: <nil>
    STEP: delete the pod 09/20/23 13:40:28.696
    Sep 20 13:40:28.770: INFO: Waiting for pod pod-secrets-a5196072-18e2-4570-9f53-af01f4e75bdd to disappear
    Sep 20 13:40:28.778: INFO: Pod pod-secrets-a5196072-18e2-4570-9f53-af01f4e75bdd no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:40:28.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4908" for this suite. 09/20/23 13:40:28.785
    STEP: Destroying namespace "secret-namespace-3208" for this suite. 09/20/23 13:40:28.811
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:40:28.826
Sep 20 13:40:28.826: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename webhook 09/20/23 13:40:28.827
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:40:28.984
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:40:28.987
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/20/23 13:40:29.325
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/20/23 13:40:29.784
STEP: Deploying the webhook pod 09/20/23 13:40:29.946
STEP: Wait for the deployment to be ready 09/20/23 13:40:31.923
Sep 20 13:40:32.895: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 13:40:35.195: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 40, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 40, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 40, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 40, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/20/23 13:40:37.213
STEP: Verifying the service has paired with the endpoint 09/20/23 13:40:37.258
Sep 20 13:40:38.259: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Sep 20 13:40:38.382: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Registering the custom resource webhook via the AdmissionRegistration API 09/20/23 13:40:38.903
STEP: Creating a custom resource that should be denied by the webhook 09/20/23 13:40:39.099
STEP: Creating a custom resource whose deletion would be denied by the webhook 09/20/23 13:40:41.133
STEP: Updating the custom resource with disallowed data should be denied 09/20/23 13:40:41.146
STEP: Deleting the custom resource should be denied 09/20/23 13:40:41.158
STEP: Remove the offending key and value from the custom resource data 09/20/23 13:40:41.165
STEP: Deleting the updated custom resource should be successful 09/20/23 13:40:41.181
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep 20 13:40:41.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2191" for this suite. 09/20/23 13:40:42.926
STEP: Destroying namespace "webhook-2191-markers" for this suite. 09/20/23 13:40:42.94
------------------------------
â€¢ [SLOW TEST] [14.122 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:40:28.826
    Sep 20 13:40:28.826: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename webhook 09/20/23 13:40:28.827
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:40:28.984
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:40:28.987
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/20/23 13:40:29.325
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/20/23 13:40:29.784
    STEP: Deploying the webhook pod 09/20/23 13:40:29.946
    STEP: Wait for the deployment to be ready 09/20/23 13:40:31.923
    Sep 20 13:40:32.895: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Sep 20 13:40:35.195: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 40, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 40, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 40, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 40, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/20/23 13:40:37.213
    STEP: Verifying the service has paired with the endpoint 09/20/23 13:40:37.258
    Sep 20 13:40:38.259: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Sep 20 13:40:38.382: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 09/20/23 13:40:38.903
    STEP: Creating a custom resource that should be denied by the webhook 09/20/23 13:40:39.099
    STEP: Creating a custom resource whose deletion would be denied by the webhook 09/20/23 13:40:41.133
    STEP: Updating the custom resource with disallowed data should be denied 09/20/23 13:40:41.146
    STEP: Deleting the custom resource should be denied 09/20/23 13:40:41.158
    STEP: Remove the offending key and value from the custom resource data 09/20/23 13:40:41.165
    STEP: Deleting the updated custom resource should be successful 09/20/23 13:40:41.181
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:40:41.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2191" for this suite. 09/20/23 13:40:42.926
    STEP: Destroying namespace "webhook-2191-markers" for this suite. 09/20/23 13:40:42.94
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:40:42.949
Sep 20 13:40:42.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename replication-controller 09/20/23 13:40:42.951
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:40:42.981
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:40:42.984
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 09/20/23 13:40:42.988
STEP: When the matched label of one of its pods change 09/20/23 13:40:42.995
Sep 20 13:40:43.000: INFO: Pod name pod-release: Found 0 pods out of 1
Sep 20 13:40:48.014: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 09/20/23 13:40:48.655
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep 20 13:40:49.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-3005" for this suite. 09/20/23 13:40:49.495
------------------------------
â€¢ [SLOW TEST] [7.868 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:40:42.949
    Sep 20 13:40:42.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename replication-controller 09/20/23 13:40:42.951
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:40:42.981
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:40:42.984
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 09/20/23 13:40:42.988
    STEP: When the matched label of one of its pods change 09/20/23 13:40:42.995
    Sep 20 13:40:43.000: INFO: Pod name pod-release: Found 0 pods out of 1
    Sep 20 13:40:48.014: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 09/20/23 13:40:48.655
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:40:49.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-3005" for this suite. 09/20/23 13:40:49.495
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:40:50.818
Sep 20 13:40:50.818: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename deployment 09/20/23 13:40:50.819
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:40:51.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:40:51.179
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Sep 20 13:40:51.186: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Sep 20 13:40:51.314: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep 20 13:40:56.320: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/20/23 13:40:56.32
Sep 20 13:40:56.320: INFO: Creating deployment "test-rolling-update-deployment"
Sep 20 13:40:56.480: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Sep 20 13:40:56.596: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Sep 20 13:40:58.606: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Sep 20 13:40:58.609: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 40, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 40, 56, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 40, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 40, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-7549d9f46d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 13:41:00.615: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep 20 13:41:00.629: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4815  345465ed-4f8a-4571-b3d3-1a5ff8408b04 42665 1 2023-09-20 13:40:56 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-09-20 13:40:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 13:40:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c19178 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-20 13:40:56 +0000 UTC,LastTransitionTime:2023-09-20 13:40:56 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-09-20 13:40:59 +0000 UTC,LastTransitionTime:2023-09-20 13:40:56 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep 20 13:41:00.633: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-4815  ab527454-ed16-4d43-ada0-917348ac10a9 42656 1 2023-09-20 13:40:56 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 345465ed-4f8a-4571-b3d3-1a5ff8408b04 0xc004e3f447 0xc004e3f448}] [] [{kube-controller-manager Update apps/v1 2023-09-20 13:40:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"345465ed-4f8a-4571-b3d3-1a5ff8408b04\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 13:40:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e3f4f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 20 13:41:00.633: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Sep 20 13:41:00.634: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4815  cacc17ea-4900-4dd7-a5c9-f442b6ec10fd 42664 2 2023-09-20 13:40:51 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 345465ed-4f8a-4571-b3d3-1a5ff8408b04 0xc004e3f317 0xc004e3f318}] [] [{e2e.test Update apps/v1 2023-09-20 13:40:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 13:40:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"345465ed-4f8a-4571-b3d3-1a5ff8408b04\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-09-20 13:40:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004e3f3d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 20 13:41:00.638: INFO: Pod "test-rolling-update-deployment-7549d9f46d-txk8f" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-txk8f test-rolling-update-deployment-7549d9f46d- deployment-4815  92009744-fb6c-4ddd-bcaa-b878dff8ab9e 42655 0 2023-09-20 13:40:56 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d ab527454-ed16-4d43-ada0-917348ac10a9 0xc00007fbc7 0xc00007fbc8}] [] [{kube-controller-manager Update v1 2023-09-20 13:40:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ab527454-ed16-4d43-ada0-917348ac10a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:40:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.4.70\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-88s72,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-88s72,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:40:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:40:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:40:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:40:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.64,PodIP:10.100.4.70,StartTime:2023-09-20 13:40:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-20 13:40:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://06ea369fda0af2737d141c26e4a393f1f4e591f82c88a16f33e88d7f8b726310,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.4.70,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep 20 13:41:00.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4815" for this suite. 09/20/23 13:41:00.65
------------------------------
â€¢ [SLOW TEST] [9.865 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:40:50.818
    Sep 20 13:40:50.818: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename deployment 09/20/23 13:40:50.819
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:40:51.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:40:51.179
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Sep 20 13:40:51.186: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Sep 20 13:40:51.314: INFO: Pod name sample-pod: Found 0 pods out of 1
    Sep 20 13:40:56.320: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/20/23 13:40:56.32
    Sep 20 13:40:56.320: INFO: Creating deployment "test-rolling-update-deployment"
    Sep 20 13:40:56.480: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Sep 20 13:40:56.596: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Sep 20 13:40:58.606: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Sep 20 13:40:58.609: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 40, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 40, 56, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 40, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 40, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-7549d9f46d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 13:41:00.615: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep 20 13:41:00.629: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4815  345465ed-4f8a-4571-b3d3-1a5ff8408b04 42665 1 2023-09-20 13:40:56 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-09-20 13:40:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 13:40:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c19178 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-20 13:40:56 +0000 UTC,LastTransitionTime:2023-09-20 13:40:56 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-09-20 13:40:59 +0000 UTC,LastTransitionTime:2023-09-20 13:40:56 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Sep 20 13:41:00.633: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-4815  ab527454-ed16-4d43-ada0-917348ac10a9 42656 1 2023-09-20 13:40:56 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 345465ed-4f8a-4571-b3d3-1a5ff8408b04 0xc004e3f447 0xc004e3f448}] [] [{kube-controller-manager Update apps/v1 2023-09-20 13:40:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"345465ed-4f8a-4571-b3d3-1a5ff8408b04\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 13:40:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e3f4f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep 20 13:41:00.633: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Sep 20 13:41:00.634: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4815  cacc17ea-4900-4dd7-a5c9-f442b6ec10fd 42664 2 2023-09-20 13:40:51 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 345465ed-4f8a-4571-b3d3-1a5ff8408b04 0xc004e3f317 0xc004e3f318}] [] [{e2e.test Update apps/v1 2023-09-20 13:40:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-20 13:40:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"345465ed-4f8a-4571-b3d3-1a5ff8408b04\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-09-20 13:40:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004e3f3d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep 20 13:41:00.638: INFO: Pod "test-rolling-update-deployment-7549d9f46d-txk8f" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-txk8f test-rolling-update-deployment-7549d9f46d- deployment-4815  92009744-fb6c-4ddd-bcaa-b878dff8ab9e 42655 0 2023-09-20 13:40:56 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d ab527454-ed16-4d43-ada0-917348ac10a9 0xc00007fbc7 0xc00007fbc8}] [] [{kube-controller-manager Update v1 2023-09-20 13:40:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ab527454-ed16-4d43-ada0-917348ac10a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-20 13:40:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.4.70\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-88s72,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-88s72,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mycluster-ww3cg64etuwi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:40:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:40:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:40:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-20 13:40:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.10.64,PodIP:10.100.4.70,StartTime:2023-09-20 13:40:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-20 13:40:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://06ea369fda0af2737d141c26e4a393f1f4e591f82c88a16f33e88d7f8b726310,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.4.70,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:41:00.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4815" for this suite. 09/20/23 13:41:00.65
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:41:00.683
Sep 20 13:41:00.683: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename services 09/20/23 13:41:00.684
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:41:00.707
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:41:00.71
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6110 09/20/23 13:41:00.716
STEP: changing the ExternalName service to type=ClusterIP 09/20/23 13:41:00.729
STEP: creating replication controller externalname-service in namespace services-6110 09/20/23 13:41:00.772
I0920 13:41:01.002048      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6110, replica count: 2
I0920 13:41:04.052345      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0920 13:41:07.052563      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 20 13:41:07.052: INFO: Creating new exec pod
Sep 20 13:41:07.367: INFO: Waiting up to 5m0s for pod "execpodbdkz6" in namespace "services-6110" to be "running"
Sep 20 13:41:07.431: INFO: Pod "execpodbdkz6": Phase="Pending", Reason="", readiness=false. Elapsed: 63.389182ms
Sep 20 13:41:09.435: INFO: Pod "execpodbdkz6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.067742665s
Sep 20 13:41:11.437: INFO: Pod "execpodbdkz6": Phase="Running", Reason="", readiness=true. Elapsed: 4.069698612s
Sep 20 13:41:11.437: INFO: Pod "execpodbdkz6" satisfied condition "running"
Sep 20 13:41:12.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-6110 exec execpodbdkz6 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Sep 20 13:41:12.627: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep 20 13:41:12.627: INFO: stdout: ""
Sep 20 13:41:12.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-6110 exec execpodbdkz6 -- /bin/sh -x -c nc -v -z -w 2 10.254.207.157 80'
Sep 20 13:41:12.836: INFO: stderr: "+ nc -v -z -w 2 10.254.207.157 80\nConnection to 10.254.207.157 80 port [tcp/http] succeeded!\n"
Sep 20 13:41:12.836: INFO: stdout: ""
Sep 20 13:41:12.836: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep 20 13:41:13.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6110" for this suite. 09/20/23 13:41:13.059
------------------------------
â€¢ [SLOW TEST] [12.389 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:41:00.683
    Sep 20 13:41:00.683: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename services 09/20/23 13:41:00.684
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:41:00.707
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:41:00.71
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-6110 09/20/23 13:41:00.716
    STEP: changing the ExternalName service to type=ClusterIP 09/20/23 13:41:00.729
    STEP: creating replication controller externalname-service in namespace services-6110 09/20/23 13:41:00.772
    I0920 13:41:01.002048      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6110, replica count: 2
    I0920 13:41:04.052345      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0920 13:41:07.052563      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep 20 13:41:07.052: INFO: Creating new exec pod
    Sep 20 13:41:07.367: INFO: Waiting up to 5m0s for pod "execpodbdkz6" in namespace "services-6110" to be "running"
    Sep 20 13:41:07.431: INFO: Pod "execpodbdkz6": Phase="Pending", Reason="", readiness=false. Elapsed: 63.389182ms
    Sep 20 13:41:09.435: INFO: Pod "execpodbdkz6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.067742665s
    Sep 20 13:41:11.437: INFO: Pod "execpodbdkz6": Phase="Running", Reason="", readiness=true. Elapsed: 4.069698612s
    Sep 20 13:41:11.437: INFO: Pod "execpodbdkz6" satisfied condition "running"
    Sep 20 13:41:12.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-6110 exec execpodbdkz6 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Sep 20 13:41:12.627: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Sep 20 13:41:12.627: INFO: stdout: ""
    Sep 20 13:41:12.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-6110 exec execpodbdkz6 -- /bin/sh -x -c nc -v -z -w 2 10.254.207.157 80'
    Sep 20 13:41:12.836: INFO: stderr: "+ nc -v -z -w 2 10.254.207.157 80\nConnection to 10.254.207.157 80 port [tcp/http] succeeded!\n"
    Sep 20 13:41:12.836: INFO: stdout: ""
    Sep 20 13:41:12.836: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:41:13.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6110" for this suite. 09/20/23 13:41:13.059
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:41:13.072
Sep 20 13:41:13.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename downward-api 09/20/23 13:41:13.073
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:41:13.094
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:41:13.099
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 09/20/23 13:41:13.11
Sep 20 13:41:13.128: INFO: Waiting up to 5m0s for pod "downward-api-3e601055-cd1e-4a4b-8196-236caac13326" in namespace "downward-api-494" to be "Succeeded or Failed"
Sep 20 13:41:13.134: INFO: Pod "downward-api-3e601055-cd1e-4a4b-8196-236caac13326": Phase="Pending", Reason="", readiness=false. Elapsed: 6.098286ms
Sep 20 13:41:15.141: INFO: Pod "downward-api-3e601055-cd1e-4a4b-8196-236caac13326": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013273254s
Sep 20 13:41:17.156: INFO: Pod "downward-api-3e601055-cd1e-4a4b-8196-236caac13326": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027909816s
Sep 20 13:41:19.232: INFO: Pod "downward-api-3e601055-cd1e-4a4b-8196-236caac13326": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.103969348s
STEP: Saw pod success 09/20/23 13:41:19.232
Sep 20 13:41:19.232: INFO: Pod "downward-api-3e601055-cd1e-4a4b-8196-236caac13326" satisfied condition "Succeeded or Failed"
Sep 20 13:41:19.235: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-0 pod downward-api-3e601055-cd1e-4a4b-8196-236caac13326 container dapi-container: <nil>
STEP: delete the pod 09/20/23 13:41:19.288
Sep 20 13:41:19.505: INFO: Waiting for pod downward-api-3e601055-cd1e-4a4b-8196-236caac13326 to disappear
Sep 20 13:41:19.531: INFO: Pod downward-api-3e601055-cd1e-4a4b-8196-236caac13326 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Sep 20 13:41:19.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-494" for this suite. 09/20/23 13:41:19.541
------------------------------
â€¢ [SLOW TEST] [6.481 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:41:13.072
    Sep 20 13:41:13.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename downward-api 09/20/23 13:41:13.073
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:41:13.094
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:41:13.099
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 09/20/23 13:41:13.11
    Sep 20 13:41:13.128: INFO: Waiting up to 5m0s for pod "downward-api-3e601055-cd1e-4a4b-8196-236caac13326" in namespace "downward-api-494" to be "Succeeded or Failed"
    Sep 20 13:41:13.134: INFO: Pod "downward-api-3e601055-cd1e-4a4b-8196-236caac13326": Phase="Pending", Reason="", readiness=false. Elapsed: 6.098286ms
    Sep 20 13:41:15.141: INFO: Pod "downward-api-3e601055-cd1e-4a4b-8196-236caac13326": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013273254s
    Sep 20 13:41:17.156: INFO: Pod "downward-api-3e601055-cd1e-4a4b-8196-236caac13326": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027909816s
    Sep 20 13:41:19.232: INFO: Pod "downward-api-3e601055-cd1e-4a4b-8196-236caac13326": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.103969348s
    STEP: Saw pod success 09/20/23 13:41:19.232
    Sep 20 13:41:19.232: INFO: Pod "downward-api-3e601055-cd1e-4a4b-8196-236caac13326" satisfied condition "Succeeded or Failed"
    Sep 20 13:41:19.235: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-0 pod downward-api-3e601055-cd1e-4a4b-8196-236caac13326 container dapi-container: <nil>
    STEP: delete the pod 09/20/23 13:41:19.288
    Sep 20 13:41:19.505: INFO: Waiting for pod downward-api-3e601055-cd1e-4a4b-8196-236caac13326 to disappear
    Sep 20 13:41:19.531: INFO: Pod downward-api-3e601055-cd1e-4a4b-8196-236caac13326 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:41:19.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-494" for this suite. 09/20/23 13:41:19.541
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:41:19.555
Sep 20 13:41:19.555: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename sched-pred 09/20/23 13:41:19.556
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:41:19.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:41:19.583
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Sep 20 13:41:19.588: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 20 13:41:19.912: INFO: Waiting for terminating namespaces to be deleted...
Sep 20 13:41:19.920: INFO: 
Logging pods the apiserver thinks is on node mycluster-ww3cg64etuwi-node-0 before test
Sep 20 13:41:19.959: INFO: csi-cinder-nodeplugin-k6qp5 from kube-system started at 2023-09-20 11:51:32 +0000 UTC (3 container statuses recorded)
Sep 20 13:41:19.959: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Sep 20 13:41:19.959: INFO: 	Container liveness-probe ready: true, restart count 0
Sep 20 13:41:19.959: INFO: 	Container node-driver-registrar ready: true, restart count 0
Sep 20 13:41:19.959: INFO: kube-flannel-ds-chfqx from kube-system started at 2023-09-20 11:51:32 +0000 UTC (1 container statuses recorded)
Sep 20 13:41:19.959: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 20 13:41:19.959: INFO: npd-ntx42 from kube-system started at 2023-09-20 11:52:06 +0000 UTC (1 container statuses recorded)
Sep 20 13:41:19.959: INFO: 	Container node-problem-detector ready: true, restart count 0
Sep 20 13:41:19.959: INFO: sonobuoy-e2e-job-2c0bc69190d741e4 from sonobuoy started at 2023-09-20 12:04:03 +0000 UTC (2 container statuses recorded)
Sep 20 13:41:19.959: INFO: 	Container e2e ready: true, restart count 0
Sep 20 13:41:19.959: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 13:41:19.959: INFO: sonobuoy-systemd-logs-daemon-set-6128a9cd7eba4f1a-r9rqh from sonobuoy started at 2023-09-20 12:04:03 +0000 UTC (2 container statuses recorded)
Sep 20 13:41:19.959: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 13:41:19.959: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 20 13:41:19.959: INFO: 
Logging pods the apiserver thinks is on node mycluster-ww3cg64etuwi-node-1 before test
Sep 20 13:41:20.852: INFO: csi-cinder-nodeplugin-r6zgs from kube-system started at 2023-09-20 11:51:30 +0000 UTC (3 container statuses recorded)
Sep 20 13:41:20.852: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Sep 20 13:41:20.852: INFO: 	Container liveness-probe ready: true, restart count 0
Sep 20 13:41:20.852: INFO: 	Container node-driver-registrar ready: true, restart count 0
Sep 20 13:41:20.852: INFO: kube-flannel-ds-nc8g9 from kube-system started at 2023-09-20 12:44:25 +0000 UTC (1 container statuses recorded)
Sep 20 13:41:20.852: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 20 13:41:20.852: INFO: npd-dqxrp from kube-system started at 2023-09-20 11:51:59 +0000 UTC (1 container statuses recorded)
Sep 20 13:41:20.853: INFO: 	Container node-problem-detector ready: true, restart count 0
Sep 20 13:41:20.853: INFO: externalname-service-8qvmg from services-6110 started at 2023-09-20 13:41:01 +0000 UTC (1 container statuses recorded)
Sep 20 13:41:20.853: INFO: 	Container externalname-service ready: true, restart count 0
Sep 20 13:41:20.853: INFO: sonobuoy-systemd-logs-daemon-set-6128a9cd7eba4f1a-8k2ck from sonobuoy started at 2023-09-20 12:04:03 +0000 UTC (2 container statuses recorded)
Sep 20 13:41:20.853: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 13:41:20.853: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 20 13:41:20.853: INFO: 
Logging pods the apiserver thinks is on node mycluster-ww3cg64etuwi-node-2 before test
Sep 20 13:41:21.485: INFO: csi-cinder-nodeplugin-qcqrp from kube-system started at 2023-09-20 11:51:31 +0000 UTC (3 container statuses recorded)
Sep 20 13:41:21.485: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Sep 20 13:41:21.485: INFO: 	Container liveness-probe ready: true, restart count 0
Sep 20 13:41:21.485: INFO: 	Container node-driver-registrar ready: true, restart count 0
Sep 20 13:41:21.485: INFO: kube-dns-autoscaler-86977fd5fc-l9tnc from kube-system started at 2023-09-20 12:43:53 +0000 UTC (1 container statuses recorded)
Sep 20 13:41:21.485: INFO: 	Container autoscaler ready: true, restart count 0
Sep 20 13:41:21.485: INFO: kube-flannel-ds-ncx55 from kube-system started at 2023-09-20 11:51:30 +0000 UTC (1 container statuses recorded)
Sep 20 13:41:21.485: INFO: 	Container kube-flannel ready: true, restart count 0
Sep 20 13:41:21.485: INFO: magnum-metrics-server-6b5dcd575f-gdlm2 from kube-system started at 2023-09-20 12:43:53 +0000 UTC (1 container statuses recorded)
Sep 20 13:41:21.485: INFO: 	Container metrics-server ready: true, restart count 0
Sep 20 13:41:21.485: INFO: npd-k978m from kube-system started at 2023-09-20 11:52:36 +0000 UTC (1 container statuses recorded)
Sep 20 13:41:21.485: INFO: 	Container node-problem-detector ready: true, restart count 0
Sep 20 13:41:21.485: INFO: externalname-service-p2jpb from services-6110 started at 2023-09-20 13:41:01 +0000 UTC (1 container statuses recorded)
Sep 20 13:41:21.485: INFO: 	Container externalname-service ready: true, restart count 0
Sep 20 13:41:21.485: INFO: sonobuoy from sonobuoy started at 2023-09-20 12:03:39 +0000 UTC (1 container statuses recorded)
Sep 20 13:41:21.485: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 20 13:41:21.485: INFO: sonobuoy-systemd-logs-daemon-set-6128a9cd7eba4f1a-bw4zv from sonobuoy started at 2023-09-20 12:04:03 +0000 UTC (2 container statuses recorded)
Sep 20 13:41:21.485: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 20 13:41:21.485: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node mycluster-ww3cg64etuwi-node-0 09/20/23 13:41:21.623
STEP: verifying the node has the label node mycluster-ww3cg64etuwi-node-1 09/20/23 13:41:21.664
STEP: verifying the node has the label node mycluster-ww3cg64etuwi-node-2 09/20/23 13:41:21.736
Sep 20 13:41:21.751: INFO: Pod csi-cinder-nodeplugin-k6qp5 requesting resource cpu=20m on Node mycluster-ww3cg64etuwi-node-0
Sep 20 13:41:21.751: INFO: Pod csi-cinder-nodeplugin-qcqrp requesting resource cpu=20m on Node mycluster-ww3cg64etuwi-node-2
Sep 20 13:41:21.751: INFO: Pod csi-cinder-nodeplugin-r6zgs requesting resource cpu=20m on Node mycluster-ww3cg64etuwi-node-1
Sep 20 13:41:21.751: INFO: Pod kube-dns-autoscaler-86977fd5fc-l9tnc requesting resource cpu=20m on Node mycluster-ww3cg64etuwi-node-2
Sep 20 13:41:21.752: INFO: Pod kube-flannel-ds-chfqx requesting resource cpu=100m on Node mycluster-ww3cg64etuwi-node-0
Sep 20 13:41:21.752: INFO: Pod kube-flannel-ds-nc8g9 requesting resource cpu=100m on Node mycluster-ww3cg64etuwi-node-1
Sep 20 13:41:21.752: INFO: Pod kube-flannel-ds-ncx55 requesting resource cpu=100m on Node mycluster-ww3cg64etuwi-node-2
Sep 20 13:41:21.752: INFO: Pod magnum-metrics-server-6b5dcd575f-gdlm2 requesting resource cpu=100m on Node mycluster-ww3cg64etuwi-node-2
Sep 20 13:41:21.752: INFO: Pod npd-dqxrp requesting resource cpu=20m on Node mycluster-ww3cg64etuwi-node-1
Sep 20 13:41:21.752: INFO: Pod npd-k978m requesting resource cpu=20m on Node mycluster-ww3cg64etuwi-node-2
Sep 20 13:41:21.752: INFO: Pod npd-ntx42 requesting resource cpu=20m on Node mycluster-ww3cg64etuwi-node-0
Sep 20 13:41:21.752: INFO: Pod sonobuoy requesting resource cpu=0m on Node mycluster-ww3cg64etuwi-node-2
Sep 20 13:41:21.752: INFO: Pod sonobuoy-e2e-job-2c0bc69190d741e4 requesting resource cpu=0m on Node mycluster-ww3cg64etuwi-node-0
Sep 20 13:41:21.752: INFO: Pod sonobuoy-systemd-logs-daemon-set-6128a9cd7eba4f1a-8k2ck requesting resource cpu=0m on Node mycluster-ww3cg64etuwi-node-1
Sep 20 13:41:21.752: INFO: Pod sonobuoy-systemd-logs-daemon-set-6128a9cd7eba4f1a-bw4zv requesting resource cpu=0m on Node mycluster-ww3cg64etuwi-node-2
Sep 20 13:41:21.752: INFO: Pod sonobuoy-systemd-logs-daemon-set-6128a9cd7eba4f1a-r9rqh requesting resource cpu=0m on Node mycluster-ww3cg64etuwi-node-0
STEP: Starting Pods to consume most of the cluster CPU. 09/20/23 13:41:21.752
Sep 20 13:41:21.752: INFO: Creating a pod which consumes cpu=1302m on Node mycluster-ww3cg64etuwi-node-0
Sep 20 13:41:21.768: INFO: Creating a pod which consumes cpu=1302m on Node mycluster-ww3cg64etuwi-node-1
Sep 20 13:41:21.855: INFO: Creating a pod which consumes cpu=1218m on Node mycluster-ww3cg64etuwi-node-2
Sep 20 13:41:21.877: INFO: Waiting up to 5m0s for pod "filler-pod-d587865f-fe20-44e1-ba84-f3c7da7f1077" in namespace "sched-pred-8542" to be "running"
Sep 20 13:41:21.891: INFO: Pod "filler-pod-d587865f-fe20-44e1-ba84-f3c7da7f1077": Phase="Pending", Reason="", readiness=false. Elapsed: 13.970665ms
Sep 20 13:41:23.896: INFO: Pod "filler-pod-d587865f-fe20-44e1-ba84-f3c7da7f1077": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018860769s
Sep 20 13:41:26.118: INFO: Pod "filler-pod-d587865f-fe20-44e1-ba84-f3c7da7f1077": Phase="Running", Reason="", readiness=true. Elapsed: 4.241070554s
Sep 20 13:41:26.118: INFO: Pod "filler-pod-d587865f-fe20-44e1-ba84-f3c7da7f1077" satisfied condition "running"
Sep 20 13:41:26.118: INFO: Waiting up to 5m0s for pod "filler-pod-8c303229-0143-498f-8f94-709eae5de26b" in namespace "sched-pred-8542" to be "running"
Sep 20 13:41:26.128: INFO: Pod "filler-pod-8c303229-0143-498f-8f94-709eae5de26b": Phase="Running", Reason="", readiness=true. Elapsed: 9.101704ms
Sep 20 13:41:26.128: INFO: Pod "filler-pod-8c303229-0143-498f-8f94-709eae5de26b" satisfied condition "running"
Sep 20 13:41:26.128: INFO: Waiting up to 5m0s for pod "filler-pod-ddc4c9a2-3603-413d-99c3-daf791f6705b" in namespace "sched-pred-8542" to be "running"
Sep 20 13:41:26.135: INFO: Pod "filler-pod-ddc4c9a2-3603-413d-99c3-daf791f6705b": Phase="Running", Reason="", readiness=true. Elapsed: 7.53845ms
Sep 20 13:41:26.135: INFO: Pod "filler-pod-ddc4c9a2-3603-413d-99c3-daf791f6705b" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 09/20/23 13:41:26.135
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8c303229-0143-498f-8f94-709eae5de26b.17869f22df7006ae], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8542/filler-pod-8c303229-0143-498f-8f94-709eae5de26b to mycluster-ww3cg64etuwi-node-1] 09/20/23 13:41:26.141
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8c303229-0143-498f-8f94-709eae5de26b.17869f238796bd7c], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 09/20/23 13:41:26.141
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8c303229-0143-498f-8f94-709eae5de26b.17869f238d006acf], Reason = [Created], Message = [Created container filler-pod-8c303229-0143-498f-8f94-709eae5de26b] 09/20/23 13:41:26.141
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8c303229-0143-498f-8f94-709eae5de26b.17869f2391f5b167], Reason = [Started], Message = [Started container filler-pod-8c303229-0143-498f-8f94-709eae5de26b] 09/20/23 13:41:26.141
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d587865f-fe20-44e1-ba84-f3c7da7f1077.17869f22ddf35a61], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8542/filler-pod-d587865f-fe20-44e1-ba84-f3c7da7f1077 to mycluster-ww3cg64etuwi-node-0] 09/20/23 13:41:26.141
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d587865f-fe20-44e1-ba84-f3c7da7f1077.17869f23874d676b], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 09/20/23 13:41:26.141
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d587865f-fe20-44e1-ba84-f3c7da7f1077.17869f2391564e18], Reason = [Created], Message = [Created container filler-pod-d587865f-fe20-44e1-ba84-f3c7da7f1077] 09/20/23 13:41:26.141
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d587865f-fe20-44e1-ba84-f3c7da7f1077.17869f23974d7603], Reason = [Started], Message = [Started container filler-pod-d587865f-fe20-44e1-ba84-f3c7da7f1077] 09/20/23 13:41:26.142
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ddc4c9a2-3603-413d-99c3-daf791f6705b.17869f22e105e339], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8542/filler-pod-ddc4c9a2-3603-413d-99c3-daf791f6705b to mycluster-ww3cg64etuwi-node-2] 09/20/23 13:41:26.142
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ddc4c9a2-3603-413d-99c3-daf791f6705b.17869f235a7c57a1], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 09/20/23 13:41:26.142
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ddc4c9a2-3603-413d-99c3-daf791f6705b.17869f236fdf35e6], Reason = [Created], Message = [Created container filler-pod-ddc4c9a2-3603-413d-99c3-daf791f6705b] 09/20/23 13:41:26.142
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ddc4c9a2-3603-413d-99c3-daf791f6705b.17869f238fc03438], Reason = [Started], Message = [Started container filler-pod-ddc4c9a2-3603-413d-99c3-daf791f6705b] 09/20/23 13:41:26.142
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.17869f23de42c7c2], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }. preemption: 0/6 nodes are available: 3 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling..] 09/20/23 13:41:26.162
STEP: removing the label node off the node mycluster-ww3cg64etuwi-node-2 09/20/23 13:41:27.231
STEP: verifying the node doesn't have the label node 09/20/23 13:41:27.541
STEP: removing the label node off the node mycluster-ww3cg64etuwi-node-0 09/20/23 13:41:27.547
STEP: verifying the node doesn't have the label node 09/20/23 13:41:27.61
STEP: removing the label node off the node mycluster-ww3cg64etuwi-node-1 09/20/23 13:41:27.615
STEP: verifying the node doesn't have the label node 09/20/23 13:41:27.635
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Sep 20 13:41:27.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-8542" for this suite. 09/20/23 13:41:27.652
------------------------------
â€¢ [SLOW TEST] [8.106 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:41:19.555
    Sep 20 13:41:19.555: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename sched-pred 09/20/23 13:41:19.556
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:41:19.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:41:19.583
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Sep 20 13:41:19.588: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Sep 20 13:41:19.912: INFO: Waiting for terminating namespaces to be deleted...
    Sep 20 13:41:19.920: INFO: 
    Logging pods the apiserver thinks is on node mycluster-ww3cg64etuwi-node-0 before test
    Sep 20 13:41:19.959: INFO: csi-cinder-nodeplugin-k6qp5 from kube-system started at 2023-09-20 11:51:32 +0000 UTC (3 container statuses recorded)
    Sep 20 13:41:19.959: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Sep 20 13:41:19.959: INFO: 	Container liveness-probe ready: true, restart count 0
    Sep 20 13:41:19.959: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Sep 20 13:41:19.959: INFO: kube-flannel-ds-chfqx from kube-system started at 2023-09-20 11:51:32 +0000 UTC (1 container statuses recorded)
    Sep 20 13:41:19.959: INFO: 	Container kube-flannel ready: true, restart count 0
    Sep 20 13:41:19.959: INFO: npd-ntx42 from kube-system started at 2023-09-20 11:52:06 +0000 UTC (1 container statuses recorded)
    Sep 20 13:41:19.959: INFO: 	Container node-problem-detector ready: true, restart count 0
    Sep 20 13:41:19.959: INFO: sonobuoy-e2e-job-2c0bc69190d741e4 from sonobuoy started at 2023-09-20 12:04:03 +0000 UTC (2 container statuses recorded)
    Sep 20 13:41:19.959: INFO: 	Container e2e ready: true, restart count 0
    Sep 20 13:41:19.959: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep 20 13:41:19.959: INFO: sonobuoy-systemd-logs-daemon-set-6128a9cd7eba4f1a-r9rqh from sonobuoy started at 2023-09-20 12:04:03 +0000 UTC (2 container statuses recorded)
    Sep 20 13:41:19.959: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep 20 13:41:19.959: INFO: 	Container systemd-logs ready: true, restart count 0
    Sep 20 13:41:19.959: INFO: 
    Logging pods the apiserver thinks is on node mycluster-ww3cg64etuwi-node-1 before test
    Sep 20 13:41:20.852: INFO: csi-cinder-nodeplugin-r6zgs from kube-system started at 2023-09-20 11:51:30 +0000 UTC (3 container statuses recorded)
    Sep 20 13:41:20.852: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Sep 20 13:41:20.852: INFO: 	Container liveness-probe ready: true, restart count 0
    Sep 20 13:41:20.852: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Sep 20 13:41:20.852: INFO: kube-flannel-ds-nc8g9 from kube-system started at 2023-09-20 12:44:25 +0000 UTC (1 container statuses recorded)
    Sep 20 13:41:20.852: INFO: 	Container kube-flannel ready: true, restart count 0
    Sep 20 13:41:20.852: INFO: npd-dqxrp from kube-system started at 2023-09-20 11:51:59 +0000 UTC (1 container statuses recorded)
    Sep 20 13:41:20.853: INFO: 	Container node-problem-detector ready: true, restart count 0
    Sep 20 13:41:20.853: INFO: externalname-service-8qvmg from services-6110 started at 2023-09-20 13:41:01 +0000 UTC (1 container statuses recorded)
    Sep 20 13:41:20.853: INFO: 	Container externalname-service ready: true, restart count 0
    Sep 20 13:41:20.853: INFO: sonobuoy-systemd-logs-daemon-set-6128a9cd7eba4f1a-8k2ck from sonobuoy started at 2023-09-20 12:04:03 +0000 UTC (2 container statuses recorded)
    Sep 20 13:41:20.853: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep 20 13:41:20.853: INFO: 	Container systemd-logs ready: true, restart count 0
    Sep 20 13:41:20.853: INFO: 
    Logging pods the apiserver thinks is on node mycluster-ww3cg64etuwi-node-2 before test
    Sep 20 13:41:21.485: INFO: csi-cinder-nodeplugin-qcqrp from kube-system started at 2023-09-20 11:51:31 +0000 UTC (3 container statuses recorded)
    Sep 20 13:41:21.485: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Sep 20 13:41:21.485: INFO: 	Container liveness-probe ready: true, restart count 0
    Sep 20 13:41:21.485: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Sep 20 13:41:21.485: INFO: kube-dns-autoscaler-86977fd5fc-l9tnc from kube-system started at 2023-09-20 12:43:53 +0000 UTC (1 container statuses recorded)
    Sep 20 13:41:21.485: INFO: 	Container autoscaler ready: true, restart count 0
    Sep 20 13:41:21.485: INFO: kube-flannel-ds-ncx55 from kube-system started at 2023-09-20 11:51:30 +0000 UTC (1 container statuses recorded)
    Sep 20 13:41:21.485: INFO: 	Container kube-flannel ready: true, restart count 0
    Sep 20 13:41:21.485: INFO: magnum-metrics-server-6b5dcd575f-gdlm2 from kube-system started at 2023-09-20 12:43:53 +0000 UTC (1 container statuses recorded)
    Sep 20 13:41:21.485: INFO: 	Container metrics-server ready: true, restart count 0
    Sep 20 13:41:21.485: INFO: npd-k978m from kube-system started at 2023-09-20 11:52:36 +0000 UTC (1 container statuses recorded)
    Sep 20 13:41:21.485: INFO: 	Container node-problem-detector ready: true, restart count 0
    Sep 20 13:41:21.485: INFO: externalname-service-p2jpb from services-6110 started at 2023-09-20 13:41:01 +0000 UTC (1 container statuses recorded)
    Sep 20 13:41:21.485: INFO: 	Container externalname-service ready: true, restart count 0
    Sep 20 13:41:21.485: INFO: sonobuoy from sonobuoy started at 2023-09-20 12:03:39 +0000 UTC (1 container statuses recorded)
    Sep 20 13:41:21.485: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Sep 20 13:41:21.485: INFO: sonobuoy-systemd-logs-daemon-set-6128a9cd7eba4f1a-bw4zv from sonobuoy started at 2023-09-20 12:04:03 +0000 UTC (2 container statuses recorded)
    Sep 20 13:41:21.485: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep 20 13:41:21.485: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node mycluster-ww3cg64etuwi-node-0 09/20/23 13:41:21.623
    STEP: verifying the node has the label node mycluster-ww3cg64etuwi-node-1 09/20/23 13:41:21.664
    STEP: verifying the node has the label node mycluster-ww3cg64etuwi-node-2 09/20/23 13:41:21.736
    Sep 20 13:41:21.751: INFO: Pod csi-cinder-nodeplugin-k6qp5 requesting resource cpu=20m on Node mycluster-ww3cg64etuwi-node-0
    Sep 20 13:41:21.751: INFO: Pod csi-cinder-nodeplugin-qcqrp requesting resource cpu=20m on Node mycluster-ww3cg64etuwi-node-2
    Sep 20 13:41:21.751: INFO: Pod csi-cinder-nodeplugin-r6zgs requesting resource cpu=20m on Node mycluster-ww3cg64etuwi-node-1
    Sep 20 13:41:21.751: INFO: Pod kube-dns-autoscaler-86977fd5fc-l9tnc requesting resource cpu=20m on Node mycluster-ww3cg64etuwi-node-2
    Sep 20 13:41:21.752: INFO: Pod kube-flannel-ds-chfqx requesting resource cpu=100m on Node mycluster-ww3cg64etuwi-node-0
    Sep 20 13:41:21.752: INFO: Pod kube-flannel-ds-nc8g9 requesting resource cpu=100m on Node mycluster-ww3cg64etuwi-node-1
    Sep 20 13:41:21.752: INFO: Pod kube-flannel-ds-ncx55 requesting resource cpu=100m on Node mycluster-ww3cg64etuwi-node-2
    Sep 20 13:41:21.752: INFO: Pod magnum-metrics-server-6b5dcd575f-gdlm2 requesting resource cpu=100m on Node mycluster-ww3cg64etuwi-node-2
    Sep 20 13:41:21.752: INFO: Pod npd-dqxrp requesting resource cpu=20m on Node mycluster-ww3cg64etuwi-node-1
    Sep 20 13:41:21.752: INFO: Pod npd-k978m requesting resource cpu=20m on Node mycluster-ww3cg64etuwi-node-2
    Sep 20 13:41:21.752: INFO: Pod npd-ntx42 requesting resource cpu=20m on Node mycluster-ww3cg64etuwi-node-0
    Sep 20 13:41:21.752: INFO: Pod sonobuoy requesting resource cpu=0m on Node mycluster-ww3cg64etuwi-node-2
    Sep 20 13:41:21.752: INFO: Pod sonobuoy-e2e-job-2c0bc69190d741e4 requesting resource cpu=0m on Node mycluster-ww3cg64etuwi-node-0
    Sep 20 13:41:21.752: INFO: Pod sonobuoy-systemd-logs-daemon-set-6128a9cd7eba4f1a-8k2ck requesting resource cpu=0m on Node mycluster-ww3cg64etuwi-node-1
    Sep 20 13:41:21.752: INFO: Pod sonobuoy-systemd-logs-daemon-set-6128a9cd7eba4f1a-bw4zv requesting resource cpu=0m on Node mycluster-ww3cg64etuwi-node-2
    Sep 20 13:41:21.752: INFO: Pod sonobuoy-systemd-logs-daemon-set-6128a9cd7eba4f1a-r9rqh requesting resource cpu=0m on Node mycluster-ww3cg64etuwi-node-0
    STEP: Starting Pods to consume most of the cluster CPU. 09/20/23 13:41:21.752
    Sep 20 13:41:21.752: INFO: Creating a pod which consumes cpu=1302m on Node mycluster-ww3cg64etuwi-node-0
    Sep 20 13:41:21.768: INFO: Creating a pod which consumes cpu=1302m on Node mycluster-ww3cg64etuwi-node-1
    Sep 20 13:41:21.855: INFO: Creating a pod which consumes cpu=1218m on Node mycluster-ww3cg64etuwi-node-2
    Sep 20 13:41:21.877: INFO: Waiting up to 5m0s for pod "filler-pod-d587865f-fe20-44e1-ba84-f3c7da7f1077" in namespace "sched-pred-8542" to be "running"
    Sep 20 13:41:21.891: INFO: Pod "filler-pod-d587865f-fe20-44e1-ba84-f3c7da7f1077": Phase="Pending", Reason="", readiness=false. Elapsed: 13.970665ms
    Sep 20 13:41:23.896: INFO: Pod "filler-pod-d587865f-fe20-44e1-ba84-f3c7da7f1077": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018860769s
    Sep 20 13:41:26.118: INFO: Pod "filler-pod-d587865f-fe20-44e1-ba84-f3c7da7f1077": Phase="Running", Reason="", readiness=true. Elapsed: 4.241070554s
    Sep 20 13:41:26.118: INFO: Pod "filler-pod-d587865f-fe20-44e1-ba84-f3c7da7f1077" satisfied condition "running"
    Sep 20 13:41:26.118: INFO: Waiting up to 5m0s for pod "filler-pod-8c303229-0143-498f-8f94-709eae5de26b" in namespace "sched-pred-8542" to be "running"
    Sep 20 13:41:26.128: INFO: Pod "filler-pod-8c303229-0143-498f-8f94-709eae5de26b": Phase="Running", Reason="", readiness=true. Elapsed: 9.101704ms
    Sep 20 13:41:26.128: INFO: Pod "filler-pod-8c303229-0143-498f-8f94-709eae5de26b" satisfied condition "running"
    Sep 20 13:41:26.128: INFO: Waiting up to 5m0s for pod "filler-pod-ddc4c9a2-3603-413d-99c3-daf791f6705b" in namespace "sched-pred-8542" to be "running"
    Sep 20 13:41:26.135: INFO: Pod "filler-pod-ddc4c9a2-3603-413d-99c3-daf791f6705b": Phase="Running", Reason="", readiness=true. Elapsed: 7.53845ms
    Sep 20 13:41:26.135: INFO: Pod "filler-pod-ddc4c9a2-3603-413d-99c3-daf791f6705b" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 09/20/23 13:41:26.135
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-8c303229-0143-498f-8f94-709eae5de26b.17869f22df7006ae], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8542/filler-pod-8c303229-0143-498f-8f94-709eae5de26b to mycluster-ww3cg64etuwi-node-1] 09/20/23 13:41:26.141
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-8c303229-0143-498f-8f94-709eae5de26b.17869f238796bd7c], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 09/20/23 13:41:26.141
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-8c303229-0143-498f-8f94-709eae5de26b.17869f238d006acf], Reason = [Created], Message = [Created container filler-pod-8c303229-0143-498f-8f94-709eae5de26b] 09/20/23 13:41:26.141
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-8c303229-0143-498f-8f94-709eae5de26b.17869f2391f5b167], Reason = [Started], Message = [Started container filler-pod-8c303229-0143-498f-8f94-709eae5de26b] 09/20/23 13:41:26.141
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-d587865f-fe20-44e1-ba84-f3c7da7f1077.17869f22ddf35a61], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8542/filler-pod-d587865f-fe20-44e1-ba84-f3c7da7f1077 to mycluster-ww3cg64etuwi-node-0] 09/20/23 13:41:26.141
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-d587865f-fe20-44e1-ba84-f3c7da7f1077.17869f23874d676b], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 09/20/23 13:41:26.141
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-d587865f-fe20-44e1-ba84-f3c7da7f1077.17869f2391564e18], Reason = [Created], Message = [Created container filler-pod-d587865f-fe20-44e1-ba84-f3c7da7f1077] 09/20/23 13:41:26.141
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-d587865f-fe20-44e1-ba84-f3c7da7f1077.17869f23974d7603], Reason = [Started], Message = [Started container filler-pod-d587865f-fe20-44e1-ba84-f3c7da7f1077] 09/20/23 13:41:26.142
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ddc4c9a2-3603-413d-99c3-daf791f6705b.17869f22e105e339], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8542/filler-pod-ddc4c9a2-3603-413d-99c3-daf791f6705b to mycluster-ww3cg64etuwi-node-2] 09/20/23 13:41:26.142
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ddc4c9a2-3603-413d-99c3-daf791f6705b.17869f235a7c57a1], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 09/20/23 13:41:26.142
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ddc4c9a2-3603-413d-99c3-daf791f6705b.17869f236fdf35e6], Reason = [Created], Message = [Created container filler-pod-ddc4c9a2-3603-413d-99c3-daf791f6705b] 09/20/23 13:41:26.142
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ddc4c9a2-3603-413d-99c3-daf791f6705b.17869f238fc03438], Reason = [Started], Message = [Started container filler-pod-ddc4c9a2-3603-413d-99c3-daf791f6705b] 09/20/23 13:41:26.142
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.17869f23de42c7c2], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }. preemption: 0/6 nodes are available: 3 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling..] 09/20/23 13:41:26.162
    STEP: removing the label node off the node mycluster-ww3cg64etuwi-node-2 09/20/23 13:41:27.231
    STEP: verifying the node doesn't have the label node 09/20/23 13:41:27.541
    STEP: removing the label node off the node mycluster-ww3cg64etuwi-node-0 09/20/23 13:41:27.547
    STEP: verifying the node doesn't have the label node 09/20/23 13:41:27.61
    STEP: removing the label node off the node mycluster-ww3cg64etuwi-node-1 09/20/23 13:41:27.615
    STEP: verifying the node doesn't have the label node 09/20/23 13:41:27.635
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:41:27.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-8542" for this suite. 09/20/23 13:41:27.652
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:41:27.664
Sep 20 13:41:27.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename services 09/20/23 13:41:27.665
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:41:27.684
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:41:27.688
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep 20 13:41:27.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4157" for this suite. 09/20/23 13:41:27.698
------------------------------
â€¢ [0.041 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:41:27.664
    Sep 20 13:41:27.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename services 09/20/23 13:41:27.665
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:41:27.684
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:41:27.688
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:41:27.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4157" for this suite. 09/20/23 13:41:27.698
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:41:27.708
Sep 20 13:41:27.708: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename container-lifecycle-hook 09/20/23 13:41:27.708
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:41:27.733
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:41:27.735
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 09/20/23 13:41:27.744
Sep 20 13:41:27.758: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5910" to be "running and ready"
Sep 20 13:41:27.764: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.568369ms
Sep 20 13:41:27.764: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:41:29.769: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010884244s
Sep 20 13:41:29.769: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:41:31.932: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.173125963s
Sep 20 13:41:31.932: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Sep 20 13:41:31.932: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 09/20/23 13:41:31.934
Sep 20 13:41:31.941: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-5910" to be "running and ready"
Sep 20 13:41:31.950: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 8.705858ms
Sep 20 13:41:31.950: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:41:33.958: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016883464s
Sep 20 13:41:33.958: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:41:36.236: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.294567864s
Sep 20 13:41:36.236: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:41:37.978: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.036905373s
Sep 20 13:41:37.978: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:41:39.955: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 8.01409088s
Sep 20 13:41:39.955: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Sep 20 13:41:39.955: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 09/20/23 13:41:39.959
STEP: delete the pod with lifecycle hook 09/20/23 13:41:39.967
Sep 20 13:41:40.011: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 20 13:41:40.017: INFO: Pod pod-with-poststart-http-hook still exists
Sep 20 13:41:42.017: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 20 13:41:42.022: INFO: Pod pod-with-poststart-http-hook still exists
Sep 20 13:41:44.018: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 20 13:41:44.023: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Sep 20 13:41:44.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-5910" for this suite. 09/20/23 13:41:44.029
------------------------------
â€¢ [SLOW TEST] [16.330 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:41:27.708
    Sep 20 13:41:27.708: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename container-lifecycle-hook 09/20/23 13:41:27.708
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:41:27.733
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:41:27.735
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 09/20/23 13:41:27.744
    Sep 20 13:41:27.758: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5910" to be "running and ready"
    Sep 20 13:41:27.764: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.568369ms
    Sep 20 13:41:27.764: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:41:29.769: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010884244s
    Sep 20 13:41:29.769: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:41:31.932: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.173125963s
    Sep 20 13:41:31.932: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Sep 20 13:41:31.932: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 09/20/23 13:41:31.934
    Sep 20 13:41:31.941: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-5910" to be "running and ready"
    Sep 20 13:41:31.950: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 8.705858ms
    Sep 20 13:41:31.950: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:41:33.958: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016883464s
    Sep 20 13:41:33.958: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:41:36.236: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.294567864s
    Sep 20 13:41:36.236: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:41:37.978: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.036905373s
    Sep 20 13:41:37.978: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:41:39.955: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 8.01409088s
    Sep 20 13:41:39.955: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Sep 20 13:41:39.955: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 09/20/23 13:41:39.959
    STEP: delete the pod with lifecycle hook 09/20/23 13:41:39.967
    Sep 20 13:41:40.011: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Sep 20 13:41:40.017: INFO: Pod pod-with-poststart-http-hook still exists
    Sep 20 13:41:42.017: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Sep 20 13:41:42.022: INFO: Pod pod-with-poststart-http-hook still exists
    Sep 20 13:41:44.018: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Sep 20 13:41:44.023: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:41:44.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-5910" for this suite. 09/20/23 13:41:44.029
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:41:44.041
Sep 20 13:41:44.041: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename projected 09/20/23 13:41:44.042
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:41:44.087
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:41:44.093
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 09/20/23 13:41:44.1
Sep 20 13:41:44.113: INFO: Waiting up to 5m0s for pod "downwardapi-volume-61dee86e-fd7b-42c4-bd17-f9d5a82f8e06" in namespace "projected-3303" to be "Succeeded or Failed"
Sep 20 13:41:44.121: INFO: Pod "downwardapi-volume-61dee86e-fd7b-42c4-bd17-f9d5a82f8e06": Phase="Pending", Reason="", readiness=false. Elapsed: 7.381424ms
Sep 20 13:41:46.142: INFO: Pod "downwardapi-volume-61dee86e-fd7b-42c4-bd17-f9d5a82f8e06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028707026s
Sep 20 13:41:48.192: INFO: Pod "downwardapi-volume-61dee86e-fd7b-42c4-bd17-f9d5a82f8e06": Phase="Pending", Reason="", readiness=false. Elapsed: 4.078035937s
Sep 20 13:41:50.546: INFO: Pod "downwardapi-volume-61dee86e-fd7b-42c4-bd17-f9d5a82f8e06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.432728654s
STEP: Saw pod success 09/20/23 13:41:50.546
Sep 20 13:41:50.546: INFO: Pod "downwardapi-volume-61dee86e-fd7b-42c4-bd17-f9d5a82f8e06" satisfied condition "Succeeded or Failed"
Sep 20 13:41:50.552: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downwardapi-volume-61dee86e-fd7b-42c4-bd17-f9d5a82f8e06 container client-container: <nil>
STEP: delete the pod 09/20/23 13:41:50.918
Sep 20 13:41:50.984: INFO: Waiting for pod downwardapi-volume-61dee86e-fd7b-42c4-bd17-f9d5a82f8e06 to disappear
Sep 20 13:41:50.990: INFO: Pod downwardapi-volume-61dee86e-fd7b-42c4-bd17-f9d5a82f8e06 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep 20 13:41:50.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3303" for this suite. 09/20/23 13:41:50.998
------------------------------
â€¢ [SLOW TEST] [7.299 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:41:44.041
    Sep 20 13:41:44.041: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename projected 09/20/23 13:41:44.042
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:41:44.087
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:41:44.093
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 09/20/23 13:41:44.1
    Sep 20 13:41:44.113: INFO: Waiting up to 5m0s for pod "downwardapi-volume-61dee86e-fd7b-42c4-bd17-f9d5a82f8e06" in namespace "projected-3303" to be "Succeeded or Failed"
    Sep 20 13:41:44.121: INFO: Pod "downwardapi-volume-61dee86e-fd7b-42c4-bd17-f9d5a82f8e06": Phase="Pending", Reason="", readiness=false. Elapsed: 7.381424ms
    Sep 20 13:41:46.142: INFO: Pod "downwardapi-volume-61dee86e-fd7b-42c4-bd17-f9d5a82f8e06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028707026s
    Sep 20 13:41:48.192: INFO: Pod "downwardapi-volume-61dee86e-fd7b-42c4-bd17-f9d5a82f8e06": Phase="Pending", Reason="", readiness=false. Elapsed: 4.078035937s
    Sep 20 13:41:50.546: INFO: Pod "downwardapi-volume-61dee86e-fd7b-42c4-bd17-f9d5a82f8e06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.432728654s
    STEP: Saw pod success 09/20/23 13:41:50.546
    Sep 20 13:41:50.546: INFO: Pod "downwardapi-volume-61dee86e-fd7b-42c4-bd17-f9d5a82f8e06" satisfied condition "Succeeded or Failed"
    Sep 20 13:41:50.552: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downwardapi-volume-61dee86e-fd7b-42c4-bd17-f9d5a82f8e06 container client-container: <nil>
    STEP: delete the pod 09/20/23 13:41:50.918
    Sep 20 13:41:50.984: INFO: Waiting for pod downwardapi-volume-61dee86e-fd7b-42c4-bd17-f9d5a82f8e06 to disappear
    Sep 20 13:41:50.990: INFO: Pod downwardapi-volume-61dee86e-fd7b-42c4-bd17-f9d5a82f8e06 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:41:50.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3303" for this suite. 09/20/23 13:41:50.998
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:41:51.345
Sep 20 13:41:51.345: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename downward-api 09/20/23 13:41:51.346
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:41:51.389
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:41:51.393
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 09/20/23 13:41:51.398
Sep 20 13:41:51.415: INFO: Waiting up to 5m0s for pod "downwardapi-volume-69ad3c43-475e-4eaa-9760-aa83e04ad696" in namespace "downward-api-3100" to be "Succeeded or Failed"
Sep 20 13:41:51.425: INFO: Pod "downwardapi-volume-69ad3c43-475e-4eaa-9760-aa83e04ad696": Phase="Pending", Reason="", readiness=false. Elapsed: 9.801713ms
Sep 20 13:41:53.567: INFO: Pod "downwardapi-volume-69ad3c43-475e-4eaa-9760-aa83e04ad696": Phase="Pending", Reason="", readiness=false. Elapsed: 2.151776245s
Sep 20 13:41:55.430: INFO: Pod "downwardapi-volume-69ad3c43-475e-4eaa-9760-aa83e04ad696": Phase="Running", Reason="", readiness=false. Elapsed: 4.014503144s
Sep 20 13:41:57.823: INFO: Pod "downwardapi-volume-69ad3c43-475e-4eaa-9760-aa83e04ad696": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.408382253s
STEP: Saw pod success 09/20/23 13:41:57.824
Sep 20 13:41:57.824: INFO: Pod "downwardapi-volume-69ad3c43-475e-4eaa-9760-aa83e04ad696" satisfied condition "Succeeded or Failed"
Sep 20 13:41:57.826: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downwardapi-volume-69ad3c43-475e-4eaa-9760-aa83e04ad696 container client-container: <nil>
STEP: delete the pod 09/20/23 13:41:57.867
Sep 20 13:41:58.209: INFO: Waiting for pod downwardapi-volume-69ad3c43-475e-4eaa-9760-aa83e04ad696 to disappear
Sep 20 13:41:58.214: INFO: Pod downwardapi-volume-69ad3c43-475e-4eaa-9760-aa83e04ad696 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep 20 13:41:58.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3100" for this suite. 09/20/23 13:41:58.218
------------------------------
â€¢ [SLOW TEST] [6.880 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:41:51.345
    Sep 20 13:41:51.345: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename downward-api 09/20/23 13:41:51.346
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:41:51.389
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:41:51.393
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 09/20/23 13:41:51.398
    Sep 20 13:41:51.415: INFO: Waiting up to 5m0s for pod "downwardapi-volume-69ad3c43-475e-4eaa-9760-aa83e04ad696" in namespace "downward-api-3100" to be "Succeeded or Failed"
    Sep 20 13:41:51.425: INFO: Pod "downwardapi-volume-69ad3c43-475e-4eaa-9760-aa83e04ad696": Phase="Pending", Reason="", readiness=false. Elapsed: 9.801713ms
    Sep 20 13:41:53.567: INFO: Pod "downwardapi-volume-69ad3c43-475e-4eaa-9760-aa83e04ad696": Phase="Pending", Reason="", readiness=false. Elapsed: 2.151776245s
    Sep 20 13:41:55.430: INFO: Pod "downwardapi-volume-69ad3c43-475e-4eaa-9760-aa83e04ad696": Phase="Running", Reason="", readiness=false. Elapsed: 4.014503144s
    Sep 20 13:41:57.823: INFO: Pod "downwardapi-volume-69ad3c43-475e-4eaa-9760-aa83e04ad696": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.408382253s
    STEP: Saw pod success 09/20/23 13:41:57.824
    Sep 20 13:41:57.824: INFO: Pod "downwardapi-volume-69ad3c43-475e-4eaa-9760-aa83e04ad696" satisfied condition "Succeeded or Failed"
    Sep 20 13:41:57.826: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downwardapi-volume-69ad3c43-475e-4eaa-9760-aa83e04ad696 container client-container: <nil>
    STEP: delete the pod 09/20/23 13:41:57.867
    Sep 20 13:41:58.209: INFO: Waiting for pod downwardapi-volume-69ad3c43-475e-4eaa-9760-aa83e04ad696 to disappear
    Sep 20 13:41:58.214: INFO: Pod downwardapi-volume-69ad3c43-475e-4eaa-9760-aa83e04ad696 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:41:58.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3100" for this suite. 09/20/23 13:41:58.218
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:41:58.227
Sep 20 13:41:58.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename subpath 09/20/23 13:41:58.227
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:41:58.243
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:41:58.246
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/20/23 13:41:58.251
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-mvqd 09/20/23 13:41:58.266
STEP: Creating a pod to test atomic-volume-subpath 09/20/23 13:41:58.266
Sep 20 13:41:58.693: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-mvqd" in namespace "subpath-4779" to be "Succeeded or Failed"
Sep 20 13:41:58.697: INFO: Pod "pod-subpath-test-downwardapi-mvqd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.102448ms
Sep 20 13:42:00.782: INFO: Pod "pod-subpath-test-downwardapi-mvqd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.089337264s
Sep 20 13:42:02.703: INFO: Pod "pod-subpath-test-downwardapi-mvqd": Phase="Running", Reason="", readiness=true. Elapsed: 4.01043384s
Sep 20 13:42:05.149: INFO: Pod "pod-subpath-test-downwardapi-mvqd": Phase="Running", Reason="", readiness=true. Elapsed: 6.455788782s
Sep 20 13:42:06.702: INFO: Pod "pod-subpath-test-downwardapi-mvqd": Phase="Running", Reason="", readiness=true. Elapsed: 8.008707574s
Sep 20 13:42:08.994: INFO: Pod "pod-subpath-test-downwardapi-mvqd": Phase="Running", Reason="", readiness=true. Elapsed: 10.301352354s
Sep 20 13:42:10.947: INFO: Pod "pod-subpath-test-downwardapi-mvqd": Phase="Running", Reason="", readiness=true. Elapsed: 12.253881468s
Sep 20 13:42:12.702: INFO: Pod "pod-subpath-test-downwardapi-mvqd": Phase="Running", Reason="", readiness=true. Elapsed: 14.009176123s
Sep 20 13:42:14.702: INFO: Pod "pod-subpath-test-downwardapi-mvqd": Phase="Running", Reason="", readiness=true. Elapsed: 16.008867796s
Sep 20 13:42:16.703: INFO: Pod "pod-subpath-test-downwardapi-mvqd": Phase="Running", Reason="", readiness=true. Elapsed: 18.010325924s
Sep 20 13:42:18.702: INFO: Pod "pod-subpath-test-downwardapi-mvqd": Phase="Running", Reason="", readiness=true. Elapsed: 20.008764176s
Sep 20 13:42:20.836: INFO: Pod "pod-subpath-test-downwardapi-mvqd": Phase="Running", Reason="", readiness=true. Elapsed: 22.142891406s
Sep 20 13:42:22.821: INFO: Pod "pod-subpath-test-downwardapi-mvqd": Phase="Running", Reason="", readiness=false. Elapsed: 24.128526861s
Sep 20 13:42:24.702: INFO: Pod "pod-subpath-test-downwardapi-mvqd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.00886309s
STEP: Saw pod success 09/20/23 13:42:24.702
Sep 20 13:42:24.702: INFO: Pod "pod-subpath-test-downwardapi-mvqd" satisfied condition "Succeeded or Failed"
Sep 20 13:42:24.704: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-subpath-test-downwardapi-mvqd container test-container-subpath-downwardapi-mvqd: <nil>
STEP: delete the pod 09/20/23 13:42:24.713
Sep 20 13:42:25.010: INFO: Waiting for pod pod-subpath-test-downwardapi-mvqd to disappear
Sep 20 13:42:25.013: INFO: Pod pod-subpath-test-downwardapi-mvqd no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-mvqd 09/20/23 13:42:25.013
Sep 20 13:42:25.014: INFO: Deleting pod "pod-subpath-test-downwardapi-mvqd" in namespace "subpath-4779"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Sep 20 13:42:25.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-4779" for this suite. 09/20/23 13:42:25.021
------------------------------
â€¢ [SLOW TEST] [26.800 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:41:58.227
    Sep 20 13:41:58.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename subpath 09/20/23 13:41:58.227
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:41:58.243
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:41:58.246
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/20/23 13:41:58.251
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-mvqd 09/20/23 13:41:58.266
    STEP: Creating a pod to test atomic-volume-subpath 09/20/23 13:41:58.266
    Sep 20 13:41:58.693: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-mvqd" in namespace "subpath-4779" to be "Succeeded or Failed"
    Sep 20 13:41:58.697: INFO: Pod "pod-subpath-test-downwardapi-mvqd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.102448ms
    Sep 20 13:42:00.782: INFO: Pod "pod-subpath-test-downwardapi-mvqd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.089337264s
    Sep 20 13:42:02.703: INFO: Pod "pod-subpath-test-downwardapi-mvqd": Phase="Running", Reason="", readiness=true. Elapsed: 4.01043384s
    Sep 20 13:42:05.149: INFO: Pod "pod-subpath-test-downwardapi-mvqd": Phase="Running", Reason="", readiness=true. Elapsed: 6.455788782s
    Sep 20 13:42:06.702: INFO: Pod "pod-subpath-test-downwardapi-mvqd": Phase="Running", Reason="", readiness=true. Elapsed: 8.008707574s
    Sep 20 13:42:08.994: INFO: Pod "pod-subpath-test-downwardapi-mvqd": Phase="Running", Reason="", readiness=true. Elapsed: 10.301352354s
    Sep 20 13:42:10.947: INFO: Pod "pod-subpath-test-downwardapi-mvqd": Phase="Running", Reason="", readiness=true. Elapsed: 12.253881468s
    Sep 20 13:42:12.702: INFO: Pod "pod-subpath-test-downwardapi-mvqd": Phase="Running", Reason="", readiness=true. Elapsed: 14.009176123s
    Sep 20 13:42:14.702: INFO: Pod "pod-subpath-test-downwardapi-mvqd": Phase="Running", Reason="", readiness=true. Elapsed: 16.008867796s
    Sep 20 13:42:16.703: INFO: Pod "pod-subpath-test-downwardapi-mvqd": Phase="Running", Reason="", readiness=true. Elapsed: 18.010325924s
    Sep 20 13:42:18.702: INFO: Pod "pod-subpath-test-downwardapi-mvqd": Phase="Running", Reason="", readiness=true. Elapsed: 20.008764176s
    Sep 20 13:42:20.836: INFO: Pod "pod-subpath-test-downwardapi-mvqd": Phase="Running", Reason="", readiness=true. Elapsed: 22.142891406s
    Sep 20 13:42:22.821: INFO: Pod "pod-subpath-test-downwardapi-mvqd": Phase="Running", Reason="", readiness=false. Elapsed: 24.128526861s
    Sep 20 13:42:24.702: INFO: Pod "pod-subpath-test-downwardapi-mvqd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.00886309s
    STEP: Saw pod success 09/20/23 13:42:24.702
    Sep 20 13:42:24.702: INFO: Pod "pod-subpath-test-downwardapi-mvqd" satisfied condition "Succeeded or Failed"
    Sep 20 13:42:24.704: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-subpath-test-downwardapi-mvqd container test-container-subpath-downwardapi-mvqd: <nil>
    STEP: delete the pod 09/20/23 13:42:24.713
    Sep 20 13:42:25.010: INFO: Waiting for pod pod-subpath-test-downwardapi-mvqd to disappear
    Sep 20 13:42:25.013: INFO: Pod pod-subpath-test-downwardapi-mvqd no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-mvqd 09/20/23 13:42:25.013
    Sep 20 13:42:25.014: INFO: Deleting pod "pod-subpath-test-downwardapi-mvqd" in namespace "subpath-4779"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:42:25.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-4779" for this suite. 09/20/23 13:42:25.021
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:42:25.027
Sep 20 13:42:25.027: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename var-expansion 09/20/23 13:42:25.027
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:42:25.055
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:42:25.06
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 09/20/23 13:42:25.064
STEP: waiting for pod running 09/20/23 13:42:25.332
Sep 20 13:42:25.332: INFO: Waiting up to 2m0s for pod "var-expansion-ff5b3a6f-ce59-46c6-88c8-c234ecf9f54b" in namespace "var-expansion-8890" to be "running"
Sep 20 13:42:25.337: INFO: Pod "var-expansion-ff5b3a6f-ce59-46c6-88c8-c234ecf9f54b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.172012ms
Sep 20 13:42:27.423: INFO: Pod "var-expansion-ff5b3a6f-ce59-46c6-88c8-c234ecf9f54b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.091151308s
Sep 20 13:42:29.342: INFO: Pod "var-expansion-ff5b3a6f-ce59-46c6-88c8-c234ecf9f54b": Phase="Running", Reason="", readiness=true. Elapsed: 4.010135777s
Sep 20 13:42:29.342: INFO: Pod "var-expansion-ff5b3a6f-ce59-46c6-88c8-c234ecf9f54b" satisfied condition "running"
STEP: creating a file in subpath 09/20/23 13:42:29.342
Sep 20 13:42:29.346: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8890 PodName:var-expansion-ff5b3a6f-ce59-46c6-88c8-c234ecf9f54b ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 20 13:42:29.346: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
Sep 20 13:42:29.347: INFO: ExecWithOptions: Clientset creation
Sep 20 13:42:29.347: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/var-expansion-8890/pods/var-expansion-ff5b3a6f-ce59-46c6-88c8-c234ecf9f54b/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 09/20/23 13:42:29.451
Sep 20 13:42:29.456: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8890 PodName:var-expansion-ff5b3a6f-ce59-46c6-88c8-c234ecf9f54b ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 20 13:42:29.456: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
Sep 20 13:42:29.457: INFO: ExecWithOptions: Clientset creation
Sep 20 13:42:29.457: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/var-expansion-8890/pods/var-expansion-ff5b3a6f-ce59-46c6-88c8-c234ecf9f54b/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 09/20/23 13:42:29.581
Sep 20 13:42:30.105: INFO: Successfully updated pod "var-expansion-ff5b3a6f-ce59-46c6-88c8-c234ecf9f54b"
STEP: waiting for annotated pod running 09/20/23 13:42:30.105
Sep 20 13:42:30.105: INFO: Waiting up to 2m0s for pod "var-expansion-ff5b3a6f-ce59-46c6-88c8-c234ecf9f54b" in namespace "var-expansion-8890" to be "running"
Sep 20 13:42:30.111: INFO: Pod "var-expansion-ff5b3a6f-ce59-46c6-88c8-c234ecf9f54b": Phase="Running", Reason="", readiness=true. Elapsed: 5.870337ms
Sep 20 13:42:30.111: INFO: Pod "var-expansion-ff5b3a6f-ce59-46c6-88c8-c234ecf9f54b" satisfied condition "running"
STEP: deleting the pod gracefully 09/20/23 13:42:30.111
Sep 20 13:42:30.111: INFO: Deleting pod "var-expansion-ff5b3a6f-ce59-46c6-88c8-c234ecf9f54b" in namespace "var-expansion-8890"
Sep 20 13:42:30.126: INFO: Wait up to 5m0s for pod "var-expansion-ff5b3a6f-ce59-46c6-88c8-c234ecf9f54b" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep 20 13:43:04.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8890" for this suite. 09/20/23 13:43:04.146
------------------------------
â€¢ [SLOW TEST] [39.206 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:42:25.027
    Sep 20 13:42:25.027: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename var-expansion 09/20/23 13:42:25.027
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:42:25.055
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:42:25.06
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 09/20/23 13:42:25.064
    STEP: waiting for pod running 09/20/23 13:42:25.332
    Sep 20 13:42:25.332: INFO: Waiting up to 2m0s for pod "var-expansion-ff5b3a6f-ce59-46c6-88c8-c234ecf9f54b" in namespace "var-expansion-8890" to be "running"
    Sep 20 13:42:25.337: INFO: Pod "var-expansion-ff5b3a6f-ce59-46c6-88c8-c234ecf9f54b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.172012ms
    Sep 20 13:42:27.423: INFO: Pod "var-expansion-ff5b3a6f-ce59-46c6-88c8-c234ecf9f54b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.091151308s
    Sep 20 13:42:29.342: INFO: Pod "var-expansion-ff5b3a6f-ce59-46c6-88c8-c234ecf9f54b": Phase="Running", Reason="", readiness=true. Elapsed: 4.010135777s
    Sep 20 13:42:29.342: INFO: Pod "var-expansion-ff5b3a6f-ce59-46c6-88c8-c234ecf9f54b" satisfied condition "running"
    STEP: creating a file in subpath 09/20/23 13:42:29.342
    Sep 20 13:42:29.346: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8890 PodName:var-expansion-ff5b3a6f-ce59-46c6-88c8-c234ecf9f54b ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep 20 13:42:29.346: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    Sep 20 13:42:29.347: INFO: ExecWithOptions: Clientset creation
    Sep 20 13:42:29.347: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/var-expansion-8890/pods/var-expansion-ff5b3a6f-ce59-46c6-88c8-c234ecf9f54b/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 09/20/23 13:42:29.451
    Sep 20 13:42:29.456: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8890 PodName:var-expansion-ff5b3a6f-ce59-46c6-88c8-c234ecf9f54b ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep 20 13:42:29.456: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    Sep 20 13:42:29.457: INFO: ExecWithOptions: Clientset creation
    Sep 20 13:42:29.457: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/var-expansion-8890/pods/var-expansion-ff5b3a6f-ce59-46c6-88c8-c234ecf9f54b/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 09/20/23 13:42:29.581
    Sep 20 13:42:30.105: INFO: Successfully updated pod "var-expansion-ff5b3a6f-ce59-46c6-88c8-c234ecf9f54b"
    STEP: waiting for annotated pod running 09/20/23 13:42:30.105
    Sep 20 13:42:30.105: INFO: Waiting up to 2m0s for pod "var-expansion-ff5b3a6f-ce59-46c6-88c8-c234ecf9f54b" in namespace "var-expansion-8890" to be "running"
    Sep 20 13:42:30.111: INFO: Pod "var-expansion-ff5b3a6f-ce59-46c6-88c8-c234ecf9f54b": Phase="Running", Reason="", readiness=true. Elapsed: 5.870337ms
    Sep 20 13:42:30.111: INFO: Pod "var-expansion-ff5b3a6f-ce59-46c6-88c8-c234ecf9f54b" satisfied condition "running"
    STEP: deleting the pod gracefully 09/20/23 13:42:30.111
    Sep 20 13:42:30.111: INFO: Deleting pod "var-expansion-ff5b3a6f-ce59-46c6-88c8-c234ecf9f54b" in namespace "var-expansion-8890"
    Sep 20 13:42:30.126: INFO: Wait up to 5m0s for pod "var-expansion-ff5b3a6f-ce59-46c6-88c8-c234ecf9f54b" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:43:04.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8890" for this suite. 09/20/23 13:43:04.146
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:43:04.234
Sep 20 13:43:04.234: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename resourcequota 09/20/23 13:43:04.235
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:43:04.261
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:43:04.267
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 09/20/23 13:43:04.276
STEP: Creating a ResourceQuota 09/20/23 13:43:09.28
STEP: Ensuring resource quota status is calculated 09/20/23 13:43:09.302
STEP: Creating a Service 09/20/23 13:43:11.31
STEP: Creating a NodePort Service 09/20/23 13:43:11.414
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 09/20/23 13:43:11.459
STEP: Ensuring resource quota status captures service creation 09/20/23 13:43:11.526
STEP: Deleting Services 09/20/23 13:43:13.532
STEP: Ensuring resource quota status released usage 09/20/23 13:43:13.631
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep 20 13:43:15.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1860" for this suite. 09/20/23 13:43:15.643
------------------------------
â€¢ [SLOW TEST] [11.709 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:43:04.234
    Sep 20 13:43:04.234: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename resourcequota 09/20/23 13:43:04.235
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:43:04.261
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:43:04.267
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 09/20/23 13:43:04.276
    STEP: Creating a ResourceQuota 09/20/23 13:43:09.28
    STEP: Ensuring resource quota status is calculated 09/20/23 13:43:09.302
    STEP: Creating a Service 09/20/23 13:43:11.31
    STEP: Creating a NodePort Service 09/20/23 13:43:11.414
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 09/20/23 13:43:11.459
    STEP: Ensuring resource quota status captures service creation 09/20/23 13:43:11.526
    STEP: Deleting Services 09/20/23 13:43:13.532
    STEP: Ensuring resource quota status released usage 09/20/23 13:43:13.631
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:43:15.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1860" for this suite. 09/20/23 13:43:15.643
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:43:15.949
Sep 20 13:43:15.949: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename gc 09/20/23 13:43:15.949
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:43:15.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:43:16.002
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Sep 20 13:43:16.079: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"24f97369-6cbe-46e7-8853-e05fbfc71fc6", Controller:(*bool)(0xc0033898f6), BlockOwnerDeletion:(*bool)(0xc0033898f7)}}
Sep 20 13:43:16.091: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"60491583-6c36-4878-8d55-052c105d5e26", Controller:(*bool)(0xc0047a0ece), BlockOwnerDeletion:(*bool)(0xc0047a0ecf)}}
Sep 20 13:43:16.098: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"d4a91361-af6d-4931-bd6a-b447912fa05b", Controller:(*bool)(0xc003389b7e), BlockOwnerDeletion:(*bool)(0xc003389b7f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep 20 13:43:21.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-7211" for this suite. 09/20/23 13:43:21.172
------------------------------
â€¢ [SLOW TEST] [5.232 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:43:15.949
    Sep 20 13:43:15.949: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename gc 09/20/23 13:43:15.949
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:43:15.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:43:16.002
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Sep 20 13:43:16.079: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"24f97369-6cbe-46e7-8853-e05fbfc71fc6", Controller:(*bool)(0xc0033898f6), BlockOwnerDeletion:(*bool)(0xc0033898f7)}}
    Sep 20 13:43:16.091: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"60491583-6c36-4878-8d55-052c105d5e26", Controller:(*bool)(0xc0047a0ece), BlockOwnerDeletion:(*bool)(0xc0047a0ecf)}}
    Sep 20 13:43:16.098: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"d4a91361-af6d-4931-bd6a-b447912fa05b", Controller:(*bool)(0xc003389b7e), BlockOwnerDeletion:(*bool)(0xc003389b7f)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:43:21.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-7211" for this suite. 09/20/23 13:43:21.172
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:43:21.181
Sep 20 13:43:21.181: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename crd-publish-openapi 09/20/23 13:43:21.182
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:43:21.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:43:21.213
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Sep 20 13:43:21.218: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/20/23 13:43:23.37
Sep 20 13:43:23.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-2324 --namespace=crd-publish-openapi-2324 create -f -'
Sep 20 13:43:25.020: INFO: stderr: ""
Sep 20 13:43:25.020: INFO: stdout: "e2e-test-crd-publish-openapi-727-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep 20 13:43:25.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-2324 --namespace=crd-publish-openapi-2324 delete e2e-test-crd-publish-openapi-727-crds test-cr'
Sep 20 13:43:25.119: INFO: stderr: ""
Sep 20 13:43:25.119: INFO: stdout: "e2e-test-crd-publish-openapi-727-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Sep 20 13:43:25.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-2324 --namespace=crd-publish-openapi-2324 apply -f -'
Sep 20 13:43:25.319: INFO: stderr: ""
Sep 20 13:43:25.319: INFO: stdout: "e2e-test-crd-publish-openapi-727-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep 20 13:43:25.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-2324 --namespace=crd-publish-openapi-2324 delete e2e-test-crd-publish-openapi-727-crds test-cr'
Sep 20 13:43:25.768: INFO: stderr: ""
Sep 20 13:43:25.768: INFO: stdout: "e2e-test-crd-publish-openapi-727-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 09/20/23 13:43:25.768
Sep 20 13:43:25.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-2324 explain e2e-test-crd-publish-openapi-727-crds'
Sep 20 13:43:26.433: INFO: stderr: ""
Sep 20 13:43:26.433: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-727-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep 20 13:43:28.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2324" for this suite. 09/20/23 13:43:29.022
------------------------------
â€¢ [SLOW TEST] [7.932 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:43:21.181
    Sep 20 13:43:21.181: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename crd-publish-openapi 09/20/23 13:43:21.182
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:43:21.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:43:21.213
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Sep 20 13:43:21.218: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/20/23 13:43:23.37
    Sep 20 13:43:23.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-2324 --namespace=crd-publish-openapi-2324 create -f -'
    Sep 20 13:43:25.020: INFO: stderr: ""
    Sep 20 13:43:25.020: INFO: stdout: "e2e-test-crd-publish-openapi-727-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Sep 20 13:43:25.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-2324 --namespace=crd-publish-openapi-2324 delete e2e-test-crd-publish-openapi-727-crds test-cr'
    Sep 20 13:43:25.119: INFO: stderr: ""
    Sep 20 13:43:25.119: INFO: stdout: "e2e-test-crd-publish-openapi-727-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Sep 20 13:43:25.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-2324 --namespace=crd-publish-openapi-2324 apply -f -'
    Sep 20 13:43:25.319: INFO: stderr: ""
    Sep 20 13:43:25.319: INFO: stdout: "e2e-test-crd-publish-openapi-727-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Sep 20 13:43:25.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-2324 --namespace=crd-publish-openapi-2324 delete e2e-test-crd-publish-openapi-727-crds test-cr'
    Sep 20 13:43:25.768: INFO: stderr: ""
    Sep 20 13:43:25.768: INFO: stdout: "e2e-test-crd-publish-openapi-727-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 09/20/23 13:43:25.768
    Sep 20 13:43:25.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-2324 explain e2e-test-crd-publish-openapi-727-crds'
    Sep 20 13:43:26.433: INFO: stderr: ""
    Sep 20 13:43:26.433: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-727-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:43:28.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2324" for this suite. 09/20/23 13:43:29.022
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:43:29.114
Sep 20 13:43:29.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename emptydir 09/20/23 13:43:29.115
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:43:29.461
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:43:29.465
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 09/20/23 13:43:29.471
Sep 20 13:43:29.539: INFO: Waiting up to 5m0s for pod "pod-4c93a0c2-d539-47ac-abb8-8b2fd467fb4c" in namespace "emptydir-958" to be "Succeeded or Failed"
Sep 20 13:43:30.498: INFO: Pod "pod-4c93a0c2-d539-47ac-abb8-8b2fd467fb4c": Phase="Pending", Reason="", readiness=false. Elapsed: 958.269465ms
Sep 20 13:43:32.543: INFO: Pod "pod-4c93a0c2-d539-47ac-abb8-8b2fd467fb4c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.003542927s
Sep 20 13:43:34.930: INFO: Pod "pod-4c93a0c2-d539-47ac-abb8-8b2fd467fb4c": Phase="Running", Reason="", readiness=false. Elapsed: 5.390463339s
Sep 20 13:43:36.504: INFO: Pod "pod-4c93a0c2-d539-47ac-abb8-8b2fd467fb4c": Phase="Running", Reason="", readiness=false. Elapsed: 6.964602043s
Sep 20 13:43:38.870: INFO: Pod "pod-4c93a0c2-d539-47ac-abb8-8b2fd467fb4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.330747031s
STEP: Saw pod success 09/20/23 13:43:38.87
Sep 20 13:43:38.870: INFO: Pod "pod-4c93a0c2-d539-47ac-abb8-8b2fd467fb4c" satisfied condition "Succeeded or Failed"
Sep 20 13:43:38.875: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-4c93a0c2-d539-47ac-abb8-8b2fd467fb4c container test-container: <nil>
STEP: delete the pod 09/20/23 13:43:39.01
Sep 20 13:43:39.039: INFO: Waiting for pod pod-4c93a0c2-d539-47ac-abb8-8b2fd467fb4c to disappear
Sep 20 13:43:39.042: INFO: Pod pod-4c93a0c2-d539-47ac-abb8-8b2fd467fb4c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep 20 13:43:39.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-958" for this suite. 09/20/23 13:43:39.046
------------------------------
â€¢ [SLOW TEST] [9.948 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:43:29.114
    Sep 20 13:43:29.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename emptydir 09/20/23 13:43:29.115
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:43:29.461
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:43:29.465
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 09/20/23 13:43:29.471
    Sep 20 13:43:29.539: INFO: Waiting up to 5m0s for pod "pod-4c93a0c2-d539-47ac-abb8-8b2fd467fb4c" in namespace "emptydir-958" to be "Succeeded or Failed"
    Sep 20 13:43:30.498: INFO: Pod "pod-4c93a0c2-d539-47ac-abb8-8b2fd467fb4c": Phase="Pending", Reason="", readiness=false. Elapsed: 958.269465ms
    Sep 20 13:43:32.543: INFO: Pod "pod-4c93a0c2-d539-47ac-abb8-8b2fd467fb4c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.003542927s
    Sep 20 13:43:34.930: INFO: Pod "pod-4c93a0c2-d539-47ac-abb8-8b2fd467fb4c": Phase="Running", Reason="", readiness=false. Elapsed: 5.390463339s
    Sep 20 13:43:36.504: INFO: Pod "pod-4c93a0c2-d539-47ac-abb8-8b2fd467fb4c": Phase="Running", Reason="", readiness=false. Elapsed: 6.964602043s
    Sep 20 13:43:38.870: INFO: Pod "pod-4c93a0c2-d539-47ac-abb8-8b2fd467fb4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.330747031s
    STEP: Saw pod success 09/20/23 13:43:38.87
    Sep 20 13:43:38.870: INFO: Pod "pod-4c93a0c2-d539-47ac-abb8-8b2fd467fb4c" satisfied condition "Succeeded or Failed"
    Sep 20 13:43:38.875: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-4c93a0c2-d539-47ac-abb8-8b2fd467fb4c container test-container: <nil>
    STEP: delete the pod 09/20/23 13:43:39.01
    Sep 20 13:43:39.039: INFO: Waiting for pod pod-4c93a0c2-d539-47ac-abb8-8b2fd467fb4c to disappear
    Sep 20 13:43:39.042: INFO: Pod pod-4c93a0c2-d539-47ac-abb8-8b2fd467fb4c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:43:39.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-958" for this suite. 09/20/23 13:43:39.046
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:43:39.072
Sep 20 13:43:39.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename resourcequota 09/20/23 13:43:39.073
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:43:39.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:43:39.243
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 09/20/23 13:43:39.251
STEP: Ensuring ResourceQuota status is calculated 09/20/23 13:43:39.26
STEP: Creating a ResourceQuota with not best effort scope 09/20/23 13:43:41.266
STEP: Ensuring ResourceQuota status is calculated 09/20/23 13:43:41.33
STEP: Creating a best-effort pod 09/20/23 13:43:43.334
STEP: Ensuring resource quota with best effort scope captures the pod usage 09/20/23 13:43:43.438
STEP: Ensuring resource quota with not best effort ignored the pod usage 09/20/23 13:43:45.623
STEP: Deleting the pod 09/20/23 13:43:47.628
STEP: Ensuring resource quota status released the pod usage 09/20/23 13:43:47.64
STEP: Creating a not best-effort pod 09/20/23 13:43:49.645
STEP: Ensuring resource quota with not best effort scope captures the pod usage 09/20/23 13:43:50.158
STEP: Ensuring resource quota with best effort scope ignored the pod usage 09/20/23 13:43:52.163
STEP: Deleting the pod 09/20/23 13:43:54.168
STEP: Ensuring resource quota status released the pod usage 09/20/23 13:43:54.679
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep 20 13:43:56.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6105" for this suite. 09/20/23 13:43:56.689
------------------------------
â€¢ [SLOW TEST] [17.726 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:43:39.072
    Sep 20 13:43:39.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename resourcequota 09/20/23 13:43:39.073
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:43:39.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:43:39.243
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 09/20/23 13:43:39.251
    STEP: Ensuring ResourceQuota status is calculated 09/20/23 13:43:39.26
    STEP: Creating a ResourceQuota with not best effort scope 09/20/23 13:43:41.266
    STEP: Ensuring ResourceQuota status is calculated 09/20/23 13:43:41.33
    STEP: Creating a best-effort pod 09/20/23 13:43:43.334
    STEP: Ensuring resource quota with best effort scope captures the pod usage 09/20/23 13:43:43.438
    STEP: Ensuring resource quota with not best effort ignored the pod usage 09/20/23 13:43:45.623
    STEP: Deleting the pod 09/20/23 13:43:47.628
    STEP: Ensuring resource quota status released the pod usage 09/20/23 13:43:47.64
    STEP: Creating a not best-effort pod 09/20/23 13:43:49.645
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 09/20/23 13:43:50.158
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 09/20/23 13:43:52.163
    STEP: Deleting the pod 09/20/23 13:43:54.168
    STEP: Ensuring resource quota status released the pod usage 09/20/23 13:43:54.679
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:43:56.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6105" for this suite. 09/20/23 13:43:56.689
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:43:56.798
Sep 20 13:43:56.798: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename configmap 09/20/23 13:43:56.798
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:43:56.821
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:43:56.825
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-e5028584-0fa1-4d0f-84ce-d876d0d2f3dc 09/20/23 13:43:56.829
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep 20 13:43:56.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2706" for this suite. 09/20/23 13:43:56.836
------------------------------
â€¢ [0.045 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:43:56.798
    Sep 20 13:43:56.798: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename configmap 09/20/23 13:43:56.798
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:43:56.821
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:43:56.825
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-e5028584-0fa1-4d0f-84ce-d876d0d2f3dc 09/20/23 13:43:56.829
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:43:56.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2706" for this suite. 09/20/23 13:43:56.836
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:43:56.844
Sep 20 13:43:56.844: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename gc 09/20/23 13:43:56.845
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:43:56.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:43:56.874
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 09/20/23 13:43:56.878
STEP: Wait for the Deployment to create new ReplicaSet 09/20/23 13:43:56.884
STEP: delete the deployment 09/20/23 13:43:58.062
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 09/20/23 13:43:58.085
STEP: Gathering metrics 09/20/23 13:43:59.189
W0920 13:43:59.200634      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Sep 20 13:43:59.200: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep 20 13:43:59.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-5574" for this suite. 09/20/23 13:43:59.207
------------------------------
â€¢ [2.404 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:43:56.844
    Sep 20 13:43:56.844: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename gc 09/20/23 13:43:56.845
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:43:56.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:43:56.874
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 09/20/23 13:43:56.878
    STEP: Wait for the Deployment to create new ReplicaSet 09/20/23 13:43:56.884
    STEP: delete the deployment 09/20/23 13:43:58.062
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 09/20/23 13:43:58.085
    STEP: Gathering metrics 09/20/23 13:43:59.189
    W0920 13:43:59.200634      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Sep 20 13:43:59.200: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:43:59.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-5574" for this suite. 09/20/23 13:43:59.207
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:43:59.25
Sep 20 13:43:59.250: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename projected 09/20/23 13:43:59.251
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:43:59.278
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:43:59.29
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-8e457d11-225e-4039-942d-6dcd768dadc5 09/20/23 13:43:59.3
STEP: Creating a pod to test consume secrets 09/20/23 13:43:59.307
Sep 20 13:43:59.326: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5a45ee9d-81be-4fca-9d42-7cf9f2a246f4" in namespace "projected-5010" to be "Succeeded or Failed"
Sep 20 13:43:59.332: INFO: Pod "pod-projected-secrets-5a45ee9d-81be-4fca-9d42-7cf9f2a246f4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.632459ms
Sep 20 13:44:01.339: INFO: Pod "pod-projected-secrets-5a45ee9d-81be-4fca-9d42-7cf9f2a246f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012911874s
Sep 20 13:44:04.186: INFO: Pod "pod-projected-secrets-5a45ee9d-81be-4fca-9d42-7cf9f2a246f4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.860072963s
Sep 20 13:44:05.671: INFO: Pod "pod-projected-secrets-5a45ee9d-81be-4fca-9d42-7cf9f2a246f4": Phase="Running", Reason="", readiness=true. Elapsed: 6.345327049s
Sep 20 13:44:07.382: INFO: Pod "pod-projected-secrets-5a45ee9d-81be-4fca-9d42-7cf9f2a246f4": Phase="Running", Reason="", readiness=false. Elapsed: 8.055948526s
Sep 20 13:44:09.336: INFO: Pod "pod-projected-secrets-5a45ee9d-81be-4fca-9d42-7cf9f2a246f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.010103192s
STEP: Saw pod success 09/20/23 13:44:09.336
Sep 20 13:44:09.337: INFO: Pod "pod-projected-secrets-5a45ee9d-81be-4fca-9d42-7cf9f2a246f4" satisfied condition "Succeeded or Failed"
Sep 20 13:44:09.339: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-projected-secrets-5a45ee9d-81be-4fca-9d42-7cf9f2a246f4 container projected-secret-volume-test: <nil>
STEP: delete the pod 09/20/23 13:44:09.347
Sep 20 13:44:10.302: INFO: Waiting for pod pod-projected-secrets-5a45ee9d-81be-4fca-9d42-7cf9f2a246f4 to disappear
Sep 20 13:44:10.307: INFO: Pod pod-projected-secrets-5a45ee9d-81be-4fca-9d42-7cf9f2a246f4 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep 20 13:44:10.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5010" for this suite. 09/20/23 13:44:10.581
------------------------------
â€¢ [SLOW TEST] [11.705 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:43:59.25
    Sep 20 13:43:59.250: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename projected 09/20/23 13:43:59.251
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:43:59.278
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:43:59.29
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-8e457d11-225e-4039-942d-6dcd768dadc5 09/20/23 13:43:59.3
    STEP: Creating a pod to test consume secrets 09/20/23 13:43:59.307
    Sep 20 13:43:59.326: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5a45ee9d-81be-4fca-9d42-7cf9f2a246f4" in namespace "projected-5010" to be "Succeeded or Failed"
    Sep 20 13:43:59.332: INFO: Pod "pod-projected-secrets-5a45ee9d-81be-4fca-9d42-7cf9f2a246f4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.632459ms
    Sep 20 13:44:01.339: INFO: Pod "pod-projected-secrets-5a45ee9d-81be-4fca-9d42-7cf9f2a246f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012911874s
    Sep 20 13:44:04.186: INFO: Pod "pod-projected-secrets-5a45ee9d-81be-4fca-9d42-7cf9f2a246f4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.860072963s
    Sep 20 13:44:05.671: INFO: Pod "pod-projected-secrets-5a45ee9d-81be-4fca-9d42-7cf9f2a246f4": Phase="Running", Reason="", readiness=true. Elapsed: 6.345327049s
    Sep 20 13:44:07.382: INFO: Pod "pod-projected-secrets-5a45ee9d-81be-4fca-9d42-7cf9f2a246f4": Phase="Running", Reason="", readiness=false. Elapsed: 8.055948526s
    Sep 20 13:44:09.336: INFO: Pod "pod-projected-secrets-5a45ee9d-81be-4fca-9d42-7cf9f2a246f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.010103192s
    STEP: Saw pod success 09/20/23 13:44:09.336
    Sep 20 13:44:09.337: INFO: Pod "pod-projected-secrets-5a45ee9d-81be-4fca-9d42-7cf9f2a246f4" satisfied condition "Succeeded or Failed"
    Sep 20 13:44:09.339: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-projected-secrets-5a45ee9d-81be-4fca-9d42-7cf9f2a246f4 container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/20/23 13:44:09.347
    Sep 20 13:44:10.302: INFO: Waiting for pod pod-projected-secrets-5a45ee9d-81be-4fca-9d42-7cf9f2a246f4 to disappear
    Sep 20 13:44:10.307: INFO: Pod pod-projected-secrets-5a45ee9d-81be-4fca-9d42-7cf9f2a246f4 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:44:10.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5010" for this suite. 09/20/23 13:44:10.581
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:44:10.957
Sep 20 13:44:10.957: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename downward-api 09/20/23 13:44:10.957
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:44:11.757
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:44:11.759
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 09/20/23 13:44:11.764
Sep 20 13:44:12.003: INFO: Waiting up to 5m0s for pod "downward-api-5b7a6691-8f7e-4e9d-9d63-e2a6639ac230" in namespace "downward-api-9464" to be "Succeeded or Failed"
Sep 20 13:44:12.066: INFO: Pod "downward-api-5b7a6691-8f7e-4e9d-9d63-e2a6639ac230": Phase="Pending", Reason="", readiness=false. Elapsed: 63.253396ms
Sep 20 13:44:14.071: INFO: Pod "downward-api-5b7a6691-8f7e-4e9d-9d63-e2a6639ac230": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0675594s
Sep 20 13:44:16.321: INFO: Pod "downward-api-5b7a6691-8f7e-4e9d-9d63-e2a6639ac230": Phase="Running", Reason="", readiness=true. Elapsed: 4.317785989s
Sep 20 13:44:18.072: INFO: Pod "downward-api-5b7a6691-8f7e-4e9d-9d63-e2a6639ac230": Phase="Running", Reason="", readiness=false. Elapsed: 6.068862539s
Sep 20 13:44:20.449: INFO: Pod "downward-api-5b7a6691-8f7e-4e9d-9d63-e2a6639ac230": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.4460404s
STEP: Saw pod success 09/20/23 13:44:20.449
Sep 20 13:44:20.449: INFO: Pod "downward-api-5b7a6691-8f7e-4e9d-9d63-e2a6639ac230" satisfied condition "Succeeded or Failed"
Sep 20 13:44:20.505: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downward-api-5b7a6691-8f7e-4e9d-9d63-e2a6639ac230 container dapi-container: <nil>
STEP: delete the pod 09/20/23 13:44:20.516
Sep 20 13:44:20.963: INFO: Waiting for pod downward-api-5b7a6691-8f7e-4e9d-9d63-e2a6639ac230 to disappear
Sep 20 13:44:20.971: INFO: Pod downward-api-5b7a6691-8f7e-4e9d-9d63-e2a6639ac230 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Sep 20 13:44:20.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9464" for this suite. 09/20/23 13:44:20.975
------------------------------
â€¢ [SLOW TEST] [10.112 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:44:10.957
    Sep 20 13:44:10.957: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename downward-api 09/20/23 13:44:10.957
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:44:11.757
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:44:11.759
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 09/20/23 13:44:11.764
    Sep 20 13:44:12.003: INFO: Waiting up to 5m0s for pod "downward-api-5b7a6691-8f7e-4e9d-9d63-e2a6639ac230" in namespace "downward-api-9464" to be "Succeeded or Failed"
    Sep 20 13:44:12.066: INFO: Pod "downward-api-5b7a6691-8f7e-4e9d-9d63-e2a6639ac230": Phase="Pending", Reason="", readiness=false. Elapsed: 63.253396ms
    Sep 20 13:44:14.071: INFO: Pod "downward-api-5b7a6691-8f7e-4e9d-9d63-e2a6639ac230": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0675594s
    Sep 20 13:44:16.321: INFO: Pod "downward-api-5b7a6691-8f7e-4e9d-9d63-e2a6639ac230": Phase="Running", Reason="", readiness=true. Elapsed: 4.317785989s
    Sep 20 13:44:18.072: INFO: Pod "downward-api-5b7a6691-8f7e-4e9d-9d63-e2a6639ac230": Phase="Running", Reason="", readiness=false. Elapsed: 6.068862539s
    Sep 20 13:44:20.449: INFO: Pod "downward-api-5b7a6691-8f7e-4e9d-9d63-e2a6639ac230": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.4460404s
    STEP: Saw pod success 09/20/23 13:44:20.449
    Sep 20 13:44:20.449: INFO: Pod "downward-api-5b7a6691-8f7e-4e9d-9d63-e2a6639ac230" satisfied condition "Succeeded or Failed"
    Sep 20 13:44:20.505: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downward-api-5b7a6691-8f7e-4e9d-9d63-e2a6639ac230 container dapi-container: <nil>
    STEP: delete the pod 09/20/23 13:44:20.516
    Sep 20 13:44:20.963: INFO: Waiting for pod downward-api-5b7a6691-8f7e-4e9d-9d63-e2a6639ac230 to disappear
    Sep 20 13:44:20.971: INFO: Pod downward-api-5b7a6691-8f7e-4e9d-9d63-e2a6639ac230 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:44:20.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9464" for this suite. 09/20/23 13:44:20.975
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:44:21.074
Sep 20 13:44:21.074: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename downward-api 09/20/23 13:44:21.074
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:44:21.497
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:44:21.501
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 09/20/23 13:44:21.505
Sep 20 13:44:22.745: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5f6f9865-afc1-47cd-b14f-3084c2727530" in namespace "downward-api-5298" to be "Succeeded or Failed"
Sep 20 13:44:22.754: INFO: Pod "downwardapi-volume-5f6f9865-afc1-47cd-b14f-3084c2727530": Phase="Pending", Reason="", readiness=false. Elapsed: 8.445739ms
Sep 20 13:44:25.051: INFO: Pod "downwardapi-volume-5f6f9865-afc1-47cd-b14f-3084c2727530": Phase="Pending", Reason="", readiness=false. Elapsed: 2.305425024s
Sep 20 13:44:27.470: INFO: Pod "downwardapi-volume-5f6f9865-afc1-47cd-b14f-3084c2727530": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.724714668s
STEP: Saw pod success 09/20/23 13:44:27.47
Sep 20 13:44:27.470: INFO: Pod "downwardapi-volume-5f6f9865-afc1-47cd-b14f-3084c2727530" satisfied condition "Succeeded or Failed"
Sep 20 13:44:27.476: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downwardapi-volume-5f6f9865-afc1-47cd-b14f-3084c2727530 container client-container: <nil>
STEP: delete the pod 09/20/23 13:44:27.486
Sep 20 13:44:28.367: INFO: Waiting for pod downwardapi-volume-5f6f9865-afc1-47cd-b14f-3084c2727530 to disappear
Sep 20 13:44:28.462: INFO: Pod downwardapi-volume-5f6f9865-afc1-47cd-b14f-3084c2727530 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep 20 13:44:28.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5298" for this suite. 09/20/23 13:44:29.035
------------------------------
â€¢ [SLOW TEST] [8.004 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:44:21.074
    Sep 20 13:44:21.074: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename downward-api 09/20/23 13:44:21.074
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:44:21.497
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:44:21.501
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 09/20/23 13:44:21.505
    Sep 20 13:44:22.745: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5f6f9865-afc1-47cd-b14f-3084c2727530" in namespace "downward-api-5298" to be "Succeeded or Failed"
    Sep 20 13:44:22.754: INFO: Pod "downwardapi-volume-5f6f9865-afc1-47cd-b14f-3084c2727530": Phase="Pending", Reason="", readiness=false. Elapsed: 8.445739ms
    Sep 20 13:44:25.051: INFO: Pod "downwardapi-volume-5f6f9865-afc1-47cd-b14f-3084c2727530": Phase="Pending", Reason="", readiness=false. Elapsed: 2.305425024s
    Sep 20 13:44:27.470: INFO: Pod "downwardapi-volume-5f6f9865-afc1-47cd-b14f-3084c2727530": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.724714668s
    STEP: Saw pod success 09/20/23 13:44:27.47
    Sep 20 13:44:27.470: INFO: Pod "downwardapi-volume-5f6f9865-afc1-47cd-b14f-3084c2727530" satisfied condition "Succeeded or Failed"
    Sep 20 13:44:27.476: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downwardapi-volume-5f6f9865-afc1-47cd-b14f-3084c2727530 container client-container: <nil>
    STEP: delete the pod 09/20/23 13:44:27.486
    Sep 20 13:44:28.367: INFO: Waiting for pod downwardapi-volume-5f6f9865-afc1-47cd-b14f-3084c2727530 to disappear
    Sep 20 13:44:28.462: INFO: Pod downwardapi-volume-5f6f9865-afc1-47cd-b14f-3084c2727530 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:44:28.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5298" for this suite. 09/20/23 13:44:29.035
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:44:29.079
Sep 20 13:44:29.079: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename projected 09/20/23 13:44:29.08
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:44:29.499
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:44:29.504
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 09/20/23 13:44:29.51
Sep 20 13:44:29.765: INFO: Waiting up to 5m0s for pod "labelsupdate8c479500-16dd-4439-b046-2137dad62f37" in namespace "projected-4890" to be "running and ready"
Sep 20 13:44:29.781: INFO: Pod "labelsupdate8c479500-16dd-4439-b046-2137dad62f37": Phase="Pending", Reason="", readiness=false. Elapsed: 15.950917ms
Sep 20 13:44:29.781: INFO: The phase of Pod labelsupdate8c479500-16dd-4439-b046-2137dad62f37 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:44:32.396: INFO: Pod "labelsupdate8c479500-16dd-4439-b046-2137dad62f37": Phase="Running", Reason="", readiness=true. Elapsed: 2.631263287s
Sep 20 13:44:32.396: INFO: The phase of Pod labelsupdate8c479500-16dd-4439-b046-2137dad62f37 is Running (Ready = true)
Sep 20 13:44:32.396: INFO: Pod "labelsupdate8c479500-16dd-4439-b046-2137dad62f37" satisfied condition "running and ready"
Sep 20 13:44:33.339: INFO: Successfully updated pod "labelsupdate8c479500-16dd-4439-b046-2137dad62f37"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep 20 13:44:38.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4890" for this suite. 09/20/23 13:44:39.25
------------------------------
â€¢ [SLOW TEST] [10.261 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:44:29.079
    Sep 20 13:44:29.079: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename projected 09/20/23 13:44:29.08
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:44:29.499
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:44:29.504
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 09/20/23 13:44:29.51
    Sep 20 13:44:29.765: INFO: Waiting up to 5m0s for pod "labelsupdate8c479500-16dd-4439-b046-2137dad62f37" in namespace "projected-4890" to be "running and ready"
    Sep 20 13:44:29.781: INFO: Pod "labelsupdate8c479500-16dd-4439-b046-2137dad62f37": Phase="Pending", Reason="", readiness=false. Elapsed: 15.950917ms
    Sep 20 13:44:29.781: INFO: The phase of Pod labelsupdate8c479500-16dd-4439-b046-2137dad62f37 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:44:32.396: INFO: Pod "labelsupdate8c479500-16dd-4439-b046-2137dad62f37": Phase="Running", Reason="", readiness=true. Elapsed: 2.631263287s
    Sep 20 13:44:32.396: INFO: The phase of Pod labelsupdate8c479500-16dd-4439-b046-2137dad62f37 is Running (Ready = true)
    Sep 20 13:44:32.396: INFO: Pod "labelsupdate8c479500-16dd-4439-b046-2137dad62f37" satisfied condition "running and ready"
    Sep 20 13:44:33.339: INFO: Successfully updated pod "labelsupdate8c479500-16dd-4439-b046-2137dad62f37"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:44:38.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4890" for this suite. 09/20/23 13:44:39.25
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:44:39.344
Sep 20 13:44:39.344: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename runtimeclass 09/20/23 13:44:39.345
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:44:39.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:44:39.623
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-4711-delete-me 09/20/23 13:44:39.709
STEP: Waiting for the RuntimeClass to disappear 09/20/23 13:44:39.718
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Sep 20 13:44:39.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-4711" for this suite. 09/20/23 13:44:39.736
------------------------------
â€¢ [0.438 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:44:39.344
    Sep 20 13:44:39.344: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename runtimeclass 09/20/23 13:44:39.345
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:44:39.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:44:39.623
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-4711-delete-me 09/20/23 13:44:39.709
    STEP: Waiting for the RuntimeClass to disappear 09/20/23 13:44:39.718
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:44:39.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-4711" for this suite. 09/20/23 13:44:39.736
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:44:39.782
Sep 20 13:44:39.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename pods 09/20/23 13:44:39.783
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:44:40.602
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:44:40.606
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 09/20/23 13:44:40.609
Sep 20 13:44:41.341: INFO: created test-pod-1
Sep 20 13:44:41.474: INFO: created test-pod-2
Sep 20 13:44:41.762: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 09/20/23 13:44:41.762
Sep 20 13:44:41.762: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-2337' to be running and ready
Sep 20 13:44:41.830: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 13:44:41.830: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 13:44:41.830: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 13:44:41.830: INFO: 0 / 3 pods in namespace 'pods-2337' are running and ready (0 seconds elapsed)
Sep 20 13:44:41.830: INFO: expected 0 pod replicas in namespace 'pods-2337', 0 are Running and Ready.
Sep 20 13:44:41.830: INFO: POD         NODE                           PHASE    GRACE  CONDITIONS
Sep 20 13:44:41.830: INFO: test-pod-1  mycluster-ww3cg64etuwi-node-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC  }]
Sep 20 13:44:41.830: INFO: test-pod-2  mycluster-ww3cg64etuwi-node-0  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC  }]
Sep 20 13:44:41.830: INFO: test-pod-3  mycluster-ww3cg64etuwi-node-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC  }]
Sep 20 13:44:41.830: INFO: 
Sep 20 13:44:44.025: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 13:44:44.026: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 13:44:44.026: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep 20 13:44:44.026: INFO: 0 / 3 pods in namespace 'pods-2337' are running and ready (2 seconds elapsed)
Sep 20 13:44:44.026: INFO: expected 0 pod replicas in namespace 'pods-2337', 0 are Running and Ready.
Sep 20 13:44:44.026: INFO: POD         NODE                           PHASE    GRACE  CONDITIONS
Sep 20 13:44:44.026: INFO: test-pod-1  mycluster-ww3cg64etuwi-node-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC  }]
Sep 20 13:44:44.026: INFO: test-pod-2  mycluster-ww3cg64etuwi-node-0  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC  }]
Sep 20 13:44:44.026: INFO: test-pod-3  mycluster-ww3cg64etuwi-node-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC  }]
Sep 20 13:44:44.026: INFO: 
Sep 20 13:44:46.171: INFO: 3 / 3 pods in namespace 'pods-2337' are running and ready (4 seconds elapsed)
Sep 20 13:44:46.171: INFO: expected 0 pod replicas in namespace 'pods-2337', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 09/20/23 13:44:46.676
Sep 20 13:44:46.738: INFO: Pod quantity 3 is different from expected quantity 0
Sep 20 13:44:47.792: INFO: Pod quantity 3 is different from expected quantity 0
Sep 20 13:44:48.959: INFO: Pod quantity 1 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep 20 13:44:49.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2337" for this suite. 09/20/23 13:44:49.749
------------------------------
â€¢ [SLOW TEST] [9.993 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:44:39.782
    Sep 20 13:44:39.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename pods 09/20/23 13:44:39.783
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:44:40.602
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:44:40.606
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 09/20/23 13:44:40.609
    Sep 20 13:44:41.341: INFO: created test-pod-1
    Sep 20 13:44:41.474: INFO: created test-pod-2
    Sep 20 13:44:41.762: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 09/20/23 13:44:41.762
    Sep 20 13:44:41.762: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-2337' to be running and ready
    Sep 20 13:44:41.830: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Sep 20 13:44:41.830: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Sep 20 13:44:41.830: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Sep 20 13:44:41.830: INFO: 0 / 3 pods in namespace 'pods-2337' are running and ready (0 seconds elapsed)
    Sep 20 13:44:41.830: INFO: expected 0 pod replicas in namespace 'pods-2337', 0 are Running and Ready.
    Sep 20 13:44:41.830: INFO: POD         NODE                           PHASE    GRACE  CONDITIONS
    Sep 20 13:44:41.830: INFO: test-pod-1  mycluster-ww3cg64etuwi-node-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC  }]
    Sep 20 13:44:41.830: INFO: test-pod-2  mycluster-ww3cg64etuwi-node-0  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC  }]
    Sep 20 13:44:41.830: INFO: test-pod-3  mycluster-ww3cg64etuwi-node-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC  }]
    Sep 20 13:44:41.830: INFO: 
    Sep 20 13:44:44.025: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Sep 20 13:44:44.026: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Sep 20 13:44:44.026: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Sep 20 13:44:44.026: INFO: 0 / 3 pods in namespace 'pods-2337' are running and ready (2 seconds elapsed)
    Sep 20 13:44:44.026: INFO: expected 0 pod replicas in namespace 'pods-2337', 0 are Running and Ready.
    Sep 20 13:44:44.026: INFO: POD         NODE                           PHASE    GRACE  CONDITIONS
    Sep 20 13:44:44.026: INFO: test-pod-1  mycluster-ww3cg64etuwi-node-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC  }]
    Sep 20 13:44:44.026: INFO: test-pod-2  mycluster-ww3cg64etuwi-node-0  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC  }]
    Sep 20 13:44:44.026: INFO: test-pod-3  mycluster-ww3cg64etuwi-node-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-20 13:44:41 +0000 UTC  }]
    Sep 20 13:44:44.026: INFO: 
    Sep 20 13:44:46.171: INFO: 3 / 3 pods in namespace 'pods-2337' are running and ready (4 seconds elapsed)
    Sep 20 13:44:46.171: INFO: expected 0 pod replicas in namespace 'pods-2337', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 09/20/23 13:44:46.676
    Sep 20 13:44:46.738: INFO: Pod quantity 3 is different from expected quantity 0
    Sep 20 13:44:47.792: INFO: Pod quantity 3 is different from expected quantity 0
    Sep 20 13:44:48.959: INFO: Pod quantity 1 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:44:49.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2337" for this suite. 09/20/23 13:44:49.749
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:44:49.776
Sep 20 13:44:49.776: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename containers 09/20/23 13:44:49.777
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:44:50.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:44:50.284
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 09/20/23 13:44:50.288
Sep 20 13:44:50.651: INFO: Waiting up to 5m0s for pod "client-containers-9effa141-d8bb-47b5-9983-c875a7975039" in namespace "containers-3333" to be "Succeeded or Failed"
Sep 20 13:44:50.659: INFO: Pod "client-containers-9effa141-d8bb-47b5-9983-c875a7975039": Phase="Pending", Reason="", readiness=false. Elapsed: 7.821463ms
Sep 20 13:44:52.665: INFO: Pod "client-containers-9effa141-d8bb-47b5-9983-c875a7975039": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014100151s
Sep 20 13:44:55.308: INFO: Pod "client-containers-9effa141-d8bb-47b5-9983-c875a7975039": Phase="Running", Reason="", readiness=false. Elapsed: 4.656907362s
Sep 20 13:44:56.870: INFO: Pod "client-containers-9effa141-d8bb-47b5-9983-c875a7975039": Phase="Running", Reason="", readiness=false. Elapsed: 6.219579255s
Sep 20 13:44:58.667: INFO: Pod "client-containers-9effa141-d8bb-47b5-9983-c875a7975039": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.016444224s
STEP: Saw pod success 09/20/23 13:44:58.667
Sep 20 13:44:58.667: INFO: Pod "client-containers-9effa141-d8bb-47b5-9983-c875a7975039" satisfied condition "Succeeded or Failed"
Sep 20 13:44:58.716: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod client-containers-9effa141-d8bb-47b5-9983-c875a7975039 container agnhost-container: <nil>
STEP: delete the pod 09/20/23 13:44:58.728
Sep 20 13:44:59.168: INFO: Waiting for pod client-containers-9effa141-d8bb-47b5-9983-c875a7975039 to disappear
Sep 20 13:44:59.173: INFO: Pod client-containers-9effa141-d8bb-47b5-9983-c875a7975039 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Sep 20 13:44:59.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-3333" for this suite. 09/20/23 13:44:59.179
------------------------------
â€¢ [SLOW TEST] [9.903 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:44:49.776
    Sep 20 13:44:49.776: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename containers 09/20/23 13:44:49.777
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:44:50.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:44:50.284
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 09/20/23 13:44:50.288
    Sep 20 13:44:50.651: INFO: Waiting up to 5m0s for pod "client-containers-9effa141-d8bb-47b5-9983-c875a7975039" in namespace "containers-3333" to be "Succeeded or Failed"
    Sep 20 13:44:50.659: INFO: Pod "client-containers-9effa141-d8bb-47b5-9983-c875a7975039": Phase="Pending", Reason="", readiness=false. Elapsed: 7.821463ms
    Sep 20 13:44:52.665: INFO: Pod "client-containers-9effa141-d8bb-47b5-9983-c875a7975039": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014100151s
    Sep 20 13:44:55.308: INFO: Pod "client-containers-9effa141-d8bb-47b5-9983-c875a7975039": Phase="Running", Reason="", readiness=false. Elapsed: 4.656907362s
    Sep 20 13:44:56.870: INFO: Pod "client-containers-9effa141-d8bb-47b5-9983-c875a7975039": Phase="Running", Reason="", readiness=false. Elapsed: 6.219579255s
    Sep 20 13:44:58.667: INFO: Pod "client-containers-9effa141-d8bb-47b5-9983-c875a7975039": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.016444224s
    STEP: Saw pod success 09/20/23 13:44:58.667
    Sep 20 13:44:58.667: INFO: Pod "client-containers-9effa141-d8bb-47b5-9983-c875a7975039" satisfied condition "Succeeded or Failed"
    Sep 20 13:44:58.716: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod client-containers-9effa141-d8bb-47b5-9983-c875a7975039 container agnhost-container: <nil>
    STEP: delete the pod 09/20/23 13:44:58.728
    Sep 20 13:44:59.168: INFO: Waiting for pod client-containers-9effa141-d8bb-47b5-9983-c875a7975039 to disappear
    Sep 20 13:44:59.173: INFO: Pod client-containers-9effa141-d8bb-47b5-9983-c875a7975039 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:44:59.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-3333" for this suite. 09/20/23 13:44:59.179
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:44:59.681
Sep 20 13:44:59.681: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename sysctl 09/20/23 13:44:59.682
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:45:00.04
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:45:00.045
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 09/20/23 13:45:00.051
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep 20 13:45:00.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-1453" for this suite. 09/20/23 13:45:00.068
------------------------------
â€¢ [0.430 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:44:59.681
    Sep 20 13:44:59.681: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename sysctl 09/20/23 13:44:59.682
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:45:00.04
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:45:00.045
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 09/20/23 13:45:00.051
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:45:00.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-1453" for this suite. 09/20/23 13:45:00.068
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:45:00.114
Sep 20 13:45:00.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename pod-network-test 09/20/23 13:45:00.115
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:45:00.46
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:45:00.464
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-1253 09/20/23 13:45:00.469
STEP: creating a selector 09/20/23 13:45:00.469
STEP: Creating the service pods in kubernetes 09/20/23 13:45:00.469
Sep 20 13:45:00.470: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep 20 13:45:02.532: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1253" to be "running and ready"
Sep 20 13:45:02.711: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 178.344378ms
Sep 20 13:45:02.711: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:45:04.891: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.359066397s
Sep 20 13:45:04.891: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:45:08.349: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 5.816547981s
Sep 20 13:45:08.349: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep 20 13:45:09.387: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.854709856s
Sep 20 13:45:09.387: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep 20 13:45:11.476: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.943946858s
Sep 20 13:45:11.476: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep 20 13:45:12.928: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.395210237s
Sep 20 13:45:12.928: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep 20 13:45:14.715: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.182348925s
Sep 20 13:45:14.715: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep 20 13:45:16.715: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.182284365s
Sep 20 13:45:16.715: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep 20 13:45:19.241: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.708226625s
Sep 20 13:45:19.241: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep 20 13:45:20.720: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.187794619s
Sep 20 13:45:20.720: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep 20 13:45:22.858: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.325397186s
Sep 20 13:45:22.858: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep 20 13:45:25.054: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.521680967s
Sep 20 13:45:25.054: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Sep 20 13:45:25.054: INFO: Pod "netserver-0" satisfied condition "running and ready"
Sep 20 13:45:25.284: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1253" to be "running and ready"
Sep 20 13:45:25.288: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.772526ms
Sep 20 13:45:25.288: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Sep 20 13:45:25.288: INFO: Pod "netserver-1" satisfied condition "running and ready"
Sep 20 13:45:25.291: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1253" to be "running and ready"
Sep 20 13:45:25.295: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.791361ms
Sep 20 13:45:25.295: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Sep 20 13:45:25.295: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 09/20/23 13:45:25.299
Sep 20 13:45:25.793: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1253" to be "running"
Sep 20 13:45:25.798: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.808167ms
Sep 20 13:45:28.439: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.645660817s
Sep 20 13:45:28.439: INFO: Pod "test-container-pod" satisfied condition "running"
Sep 20 13:45:28.855: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-1253" to be "running"
Sep 20 13:45:28.858: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.808834ms
Sep 20 13:45:28.858: INFO: Pod "host-test-container-pod" satisfied condition "running"
Sep 20 13:45:28.863: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Sep 20 13:45:28.863: INFO: Going to poll 10.100.5.160 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Sep 20 13:45:29.514: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.5.160:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1253 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 20 13:45:29.514: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
Sep 20 13:45:29.514: INFO: ExecWithOptions: Clientset creation
Sep 20 13:45:29.515: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-1253/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.100.5.160%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep 20 13:45:29.668: INFO: Found all 1 expected endpoints: [netserver-0]
Sep 20 13:45:29.668: INFO: Going to poll 10.100.4.87 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Sep 20 13:45:29.684: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.4.87:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1253 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 20 13:45:29.684: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
Sep 20 13:45:29.685: INFO: ExecWithOptions: Clientset creation
Sep 20 13:45:29.685: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-1253/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.100.4.87%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep 20 13:45:29.798: INFO: Found all 1 expected endpoints: [netserver-1]
Sep 20 13:45:29.798: INFO: Going to poll 10.100.3.199 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Sep 20 13:45:29.922: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.3.199:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1253 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 20 13:45:29.922: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
Sep 20 13:45:29.922: INFO: ExecWithOptions: Clientset creation
Sep 20 13:45:29.922: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-1253/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.100.3.199%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep 20 13:45:30.026: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Sep 20 13:45:30.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-1253" for this suite. 09/20/23 13:45:30.032
------------------------------
â€¢ [SLOW TEST] [30.154 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:45:00.114
    Sep 20 13:45:00.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename pod-network-test 09/20/23 13:45:00.115
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:45:00.46
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:45:00.464
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-1253 09/20/23 13:45:00.469
    STEP: creating a selector 09/20/23 13:45:00.469
    STEP: Creating the service pods in kubernetes 09/20/23 13:45:00.469
    Sep 20 13:45:00.470: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Sep 20 13:45:02.532: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1253" to be "running and ready"
    Sep 20 13:45:02.711: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 178.344378ms
    Sep 20 13:45:02.711: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:45:04.891: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.359066397s
    Sep 20 13:45:04.891: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:45:08.349: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 5.816547981s
    Sep 20 13:45:08.349: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep 20 13:45:09.387: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.854709856s
    Sep 20 13:45:09.387: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep 20 13:45:11.476: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.943946858s
    Sep 20 13:45:11.476: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep 20 13:45:12.928: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.395210237s
    Sep 20 13:45:12.928: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep 20 13:45:14.715: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.182348925s
    Sep 20 13:45:14.715: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep 20 13:45:16.715: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.182284365s
    Sep 20 13:45:16.715: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep 20 13:45:19.241: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.708226625s
    Sep 20 13:45:19.241: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep 20 13:45:20.720: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.187794619s
    Sep 20 13:45:20.720: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep 20 13:45:22.858: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.325397186s
    Sep 20 13:45:22.858: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep 20 13:45:25.054: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.521680967s
    Sep 20 13:45:25.054: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Sep 20 13:45:25.054: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Sep 20 13:45:25.284: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1253" to be "running and ready"
    Sep 20 13:45:25.288: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.772526ms
    Sep 20 13:45:25.288: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Sep 20 13:45:25.288: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Sep 20 13:45:25.291: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1253" to be "running and ready"
    Sep 20 13:45:25.295: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.791361ms
    Sep 20 13:45:25.295: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Sep 20 13:45:25.295: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 09/20/23 13:45:25.299
    Sep 20 13:45:25.793: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1253" to be "running"
    Sep 20 13:45:25.798: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.808167ms
    Sep 20 13:45:28.439: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.645660817s
    Sep 20 13:45:28.439: INFO: Pod "test-container-pod" satisfied condition "running"
    Sep 20 13:45:28.855: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-1253" to be "running"
    Sep 20 13:45:28.858: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.808834ms
    Sep 20 13:45:28.858: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Sep 20 13:45:28.863: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Sep 20 13:45:28.863: INFO: Going to poll 10.100.5.160 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Sep 20 13:45:29.514: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.5.160:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1253 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep 20 13:45:29.514: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    Sep 20 13:45:29.514: INFO: ExecWithOptions: Clientset creation
    Sep 20 13:45:29.515: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-1253/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.100.5.160%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep 20 13:45:29.668: INFO: Found all 1 expected endpoints: [netserver-0]
    Sep 20 13:45:29.668: INFO: Going to poll 10.100.4.87 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Sep 20 13:45:29.684: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.4.87:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1253 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep 20 13:45:29.684: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    Sep 20 13:45:29.685: INFO: ExecWithOptions: Clientset creation
    Sep 20 13:45:29.685: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-1253/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.100.4.87%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep 20 13:45:29.798: INFO: Found all 1 expected endpoints: [netserver-1]
    Sep 20 13:45:29.798: INFO: Going to poll 10.100.3.199 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Sep 20 13:45:29.922: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.3.199:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1253 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep 20 13:45:29.922: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    Sep 20 13:45:29.922: INFO: ExecWithOptions: Clientset creation
    Sep 20 13:45:29.922: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-1253/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.100.3.199%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep 20 13:45:30.026: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:45:30.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-1253" for this suite. 09/20/23 13:45:30.032
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:45:30.27
Sep 20 13:45:30.270: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 09/20/23 13:45:30.271
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:45:31.436
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:45:31.44
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 09/20/23 13:45:31.444
STEP: Creating hostNetwork=false pod 09/20/23 13:45:31.444
Sep 20 13:45:31.839: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-3491" to be "running and ready"
Sep 20 13:45:33.306: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.466741333s
Sep 20 13:45:33.306: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:45:35.370: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.531699173s
Sep 20 13:45:35.371: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:45:37.801: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.962193182s
Sep 20 13:45:37.801: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:45:39.310: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.471339312s
Sep 20 13:45:39.310: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:45:41.311: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 9.471897125s
Sep 20 13:45:41.311: INFO: The phase of Pod test-pod is Running (Ready = true)
Sep 20 13:45:41.311: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 09/20/23 13:45:41.327
Sep 20 13:45:41.382: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-3491" to be "running and ready"
Sep 20 13:45:41.386: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.966371ms
Sep 20 13:45:41.386: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:45:43.604: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.222206512s
Sep 20 13:45:43.604: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:45:45.531: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.148936108s
Sep 20 13:45:45.531: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Sep 20 13:45:45.531: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 09/20/23 13:45:45.536
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 09/20/23 13:45:45.536
Sep 20 13:45:45.536: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3491 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 20 13:45:45.536: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
Sep 20 13:45:45.537: INFO: ExecWithOptions: Clientset creation
Sep 20 13:45:45.537: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3491/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Sep 20 13:45:45.660: INFO: Exec stderr: ""
Sep 20 13:45:45.660: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3491 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 20 13:45:45.660: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
Sep 20 13:45:45.660: INFO: ExecWithOptions: Clientset creation
Sep 20 13:45:45.661: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3491/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Sep 20 13:45:45.777: INFO: Exec stderr: ""
Sep 20 13:45:45.778: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3491 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 20 13:45:45.778: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
Sep 20 13:45:45.779: INFO: ExecWithOptions: Clientset creation
Sep 20 13:45:45.779: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3491/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Sep 20 13:45:46.133: INFO: Exec stderr: ""
Sep 20 13:45:46.133: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3491 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 20 13:45:46.133: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
Sep 20 13:45:46.134: INFO: ExecWithOptions: Clientset creation
Sep 20 13:45:46.134: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3491/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Sep 20 13:45:46.307: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 09/20/23 13:45:46.307
Sep 20 13:45:46.308: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3491 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 20 13:45:46.308: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
Sep 20 13:45:46.308: INFO: ExecWithOptions: Clientset creation
Sep 20 13:45:46.308: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3491/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Sep 20 13:45:46.408: INFO: Exec stderr: ""
Sep 20 13:45:46.408: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3491 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 20 13:45:46.408: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
Sep 20 13:45:46.409: INFO: ExecWithOptions: Clientset creation
Sep 20 13:45:46.409: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3491/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Sep 20 13:45:46.507: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 09/20/23 13:45:46.507
Sep 20 13:45:46.507: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3491 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 20 13:45:46.507: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
Sep 20 13:45:46.508: INFO: ExecWithOptions: Clientset creation
Sep 20 13:45:46.508: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3491/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Sep 20 13:45:46.641: INFO: Exec stderr: ""
Sep 20 13:45:46.641: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3491 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 20 13:45:46.641: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
Sep 20 13:45:46.642: INFO: ExecWithOptions: Clientset creation
Sep 20 13:45:46.642: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3491/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Sep 20 13:45:46.735: INFO: Exec stderr: ""
Sep 20 13:45:46.735: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3491 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 20 13:45:46.735: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
Sep 20 13:45:46.736: INFO: ExecWithOptions: Clientset creation
Sep 20 13:45:46.736: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3491/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Sep 20 13:45:46.836: INFO: Exec stderr: ""
Sep 20 13:45:46.836: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3491 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 20 13:45:46.836: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
Sep 20 13:45:46.836: INFO: ExecWithOptions: Clientset creation
Sep 20 13:45:46.836: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3491/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Sep 20 13:45:46.929: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Sep 20 13:45:46.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-3491" for this suite. 09/20/23 13:45:46.935
------------------------------
â€¢ [SLOW TEST] [17.246 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:45:30.27
    Sep 20 13:45:30.270: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 09/20/23 13:45:30.271
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:45:31.436
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:45:31.44
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 09/20/23 13:45:31.444
    STEP: Creating hostNetwork=false pod 09/20/23 13:45:31.444
    Sep 20 13:45:31.839: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-3491" to be "running and ready"
    Sep 20 13:45:33.306: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.466741333s
    Sep 20 13:45:33.306: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:45:35.370: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.531699173s
    Sep 20 13:45:35.371: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:45:37.801: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.962193182s
    Sep 20 13:45:37.801: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:45:39.310: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.471339312s
    Sep 20 13:45:39.310: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:45:41.311: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 9.471897125s
    Sep 20 13:45:41.311: INFO: The phase of Pod test-pod is Running (Ready = true)
    Sep 20 13:45:41.311: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 09/20/23 13:45:41.327
    Sep 20 13:45:41.382: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-3491" to be "running and ready"
    Sep 20 13:45:41.386: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.966371ms
    Sep 20 13:45:41.386: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:45:43.604: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.222206512s
    Sep 20 13:45:43.604: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:45:45.531: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.148936108s
    Sep 20 13:45:45.531: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Sep 20 13:45:45.531: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 09/20/23 13:45:45.536
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 09/20/23 13:45:45.536
    Sep 20 13:45:45.536: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3491 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep 20 13:45:45.536: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    Sep 20 13:45:45.537: INFO: ExecWithOptions: Clientset creation
    Sep 20 13:45:45.537: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3491/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Sep 20 13:45:45.660: INFO: Exec stderr: ""
    Sep 20 13:45:45.660: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3491 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep 20 13:45:45.660: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    Sep 20 13:45:45.660: INFO: ExecWithOptions: Clientset creation
    Sep 20 13:45:45.661: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3491/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Sep 20 13:45:45.777: INFO: Exec stderr: ""
    Sep 20 13:45:45.778: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3491 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep 20 13:45:45.778: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    Sep 20 13:45:45.779: INFO: ExecWithOptions: Clientset creation
    Sep 20 13:45:45.779: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3491/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Sep 20 13:45:46.133: INFO: Exec stderr: ""
    Sep 20 13:45:46.133: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3491 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep 20 13:45:46.133: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    Sep 20 13:45:46.134: INFO: ExecWithOptions: Clientset creation
    Sep 20 13:45:46.134: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3491/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Sep 20 13:45:46.307: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 09/20/23 13:45:46.307
    Sep 20 13:45:46.308: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3491 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep 20 13:45:46.308: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    Sep 20 13:45:46.308: INFO: ExecWithOptions: Clientset creation
    Sep 20 13:45:46.308: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3491/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Sep 20 13:45:46.408: INFO: Exec stderr: ""
    Sep 20 13:45:46.408: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3491 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep 20 13:45:46.408: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    Sep 20 13:45:46.409: INFO: ExecWithOptions: Clientset creation
    Sep 20 13:45:46.409: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3491/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Sep 20 13:45:46.507: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 09/20/23 13:45:46.507
    Sep 20 13:45:46.507: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3491 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep 20 13:45:46.507: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    Sep 20 13:45:46.508: INFO: ExecWithOptions: Clientset creation
    Sep 20 13:45:46.508: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3491/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Sep 20 13:45:46.641: INFO: Exec stderr: ""
    Sep 20 13:45:46.641: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3491 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep 20 13:45:46.641: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    Sep 20 13:45:46.642: INFO: ExecWithOptions: Clientset creation
    Sep 20 13:45:46.642: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3491/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Sep 20 13:45:46.735: INFO: Exec stderr: ""
    Sep 20 13:45:46.735: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3491 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep 20 13:45:46.735: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    Sep 20 13:45:46.736: INFO: ExecWithOptions: Clientset creation
    Sep 20 13:45:46.736: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3491/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Sep 20 13:45:46.836: INFO: Exec stderr: ""
    Sep 20 13:45:46.836: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3491 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep 20 13:45:46.836: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    Sep 20 13:45:46.836: INFO: ExecWithOptions: Clientset creation
    Sep 20 13:45:46.836: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3491/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Sep 20 13:45:46.929: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:45:46.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-3491" for this suite. 09/20/23 13:45:46.935
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:45:47.518
Sep 20 13:45:47.518: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename proxy 09/20/23 13:45:47.519
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:45:47.958
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:45:47.962
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Sep 20 13:45:47.969: INFO: Creating pod...
Sep 20 13:45:48.169: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8429" to be "running"
Sep 20 13:45:48.174: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.737613ms
Sep 20 13:45:50.773: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.60409505s
Sep 20 13:45:52.178: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.009221174s
Sep 20 13:45:52.179: INFO: Pod "agnhost" satisfied condition "running"
Sep 20 13:45:52.179: INFO: Creating service...
Sep 20 13:45:52.643: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8429/pods/agnhost/proxy?method=DELETE
Sep 20 13:45:52.723: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep 20 13:45:52.723: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8429/pods/agnhost/proxy?method=OPTIONS
Sep 20 13:45:52.729: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep 20 13:45:52.729: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8429/pods/agnhost/proxy?method=PATCH
Sep 20 13:45:52.735: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep 20 13:45:52.735: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8429/pods/agnhost/proxy?method=POST
Sep 20 13:45:52.740: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep 20 13:45:52.740: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8429/pods/agnhost/proxy?method=PUT
Sep 20 13:45:52.745: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Sep 20 13:45:52.745: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8429/services/e2e-proxy-test-service/proxy?method=DELETE
Sep 20 13:45:52.749: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
Sep 20 13:45:52.762: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
Sep 20 13:45:52.772: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
Sep 20 13:45:52.782: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
Sep 20 13:45:52.792: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
Sep 20 13:45:52.801: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
Sep 20 13:45:52.813: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
Sep 20 13:45:52.823: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
Sep 20 13:45:52.833: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
Sep 20 13:45:52.843: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
Sep 20 13:45:52.995: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep 20 13:45:52.995: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8429/services/e2e-proxy-test-service/proxy?method=OPTIONS
Sep 20 13:45:53.053: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep 20 13:45:53.053: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8429/services/e2e-proxy-test-service/proxy?method=PATCH
Sep 20 13:45:53.058: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep 20 13:45:53.058: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8429/services/e2e-proxy-test-service/proxy?method=POST
Sep 20 13:45:53.063: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep 20 13:45:53.063: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8429/services/e2e-proxy-test-service/proxy?method=PUT
Sep 20 13:45:53.069: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Sep 20 13:45:53.070: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8429/pods/agnhost/proxy?method=GET
Sep 20 13:45:53.074: INFO: http.Client request:GET StatusCode:301
Sep 20 13:45:53.074: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8429/services/e2e-proxy-test-service/proxy?method=GET
Sep 20 13:45:53.079: INFO: http.Client request:GET StatusCode:301
Sep 20 13:45:53.079: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8429/pods/agnhost/proxy?method=HEAD
Sep 20 13:45:53.082: INFO: http.Client request:HEAD StatusCode:301
Sep 20 13:45:53.082: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8429/services/e2e-proxy-test-service/proxy?method=HEAD
Sep 20 13:45:53.965: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Sep 20 13:45:53.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-8429" for this suite. 09/20/23 13:45:54.136
------------------------------
â€¢ [SLOW TEST] [6.626 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:45:47.518
    Sep 20 13:45:47.518: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename proxy 09/20/23 13:45:47.519
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:45:47.958
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:45:47.962
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Sep 20 13:45:47.969: INFO: Creating pod...
    Sep 20 13:45:48.169: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8429" to be "running"
    Sep 20 13:45:48.174: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.737613ms
    Sep 20 13:45:50.773: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.60409505s
    Sep 20 13:45:52.178: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.009221174s
    Sep 20 13:45:52.179: INFO: Pod "agnhost" satisfied condition "running"
    Sep 20 13:45:52.179: INFO: Creating service...
    Sep 20 13:45:52.643: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8429/pods/agnhost/proxy?method=DELETE
    Sep 20 13:45:52.723: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Sep 20 13:45:52.723: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8429/pods/agnhost/proxy?method=OPTIONS
    Sep 20 13:45:52.729: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Sep 20 13:45:52.729: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8429/pods/agnhost/proxy?method=PATCH
    Sep 20 13:45:52.735: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Sep 20 13:45:52.735: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8429/pods/agnhost/proxy?method=POST
    Sep 20 13:45:52.740: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Sep 20 13:45:52.740: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8429/pods/agnhost/proxy?method=PUT
    Sep 20 13:45:52.745: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Sep 20 13:45:52.745: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8429/services/e2e-proxy-test-service/proxy?method=DELETE
    Sep 20 13:45:52.749: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
    Sep 20 13:45:52.762: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
    Sep 20 13:45:52.772: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
    Sep 20 13:45:52.782: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
    Sep 20 13:45:52.792: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
    Sep 20 13:45:52.801: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
    Sep 20 13:45:52.813: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
    Sep 20 13:45:52.823: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
    Sep 20 13:45:52.833: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
    Sep 20 13:45:52.843: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
    Sep 20 13:45:52.995: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Sep 20 13:45:52.995: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8429/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Sep 20 13:45:53.053: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Sep 20 13:45:53.053: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8429/services/e2e-proxy-test-service/proxy?method=PATCH
    Sep 20 13:45:53.058: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Sep 20 13:45:53.058: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8429/services/e2e-proxy-test-service/proxy?method=POST
    Sep 20 13:45:53.063: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Sep 20 13:45:53.063: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8429/services/e2e-proxy-test-service/proxy?method=PUT
    Sep 20 13:45:53.069: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Sep 20 13:45:53.070: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8429/pods/agnhost/proxy?method=GET
    Sep 20 13:45:53.074: INFO: http.Client request:GET StatusCode:301
    Sep 20 13:45:53.074: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8429/services/e2e-proxy-test-service/proxy?method=GET
    Sep 20 13:45:53.079: INFO: http.Client request:GET StatusCode:301
    Sep 20 13:45:53.079: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8429/pods/agnhost/proxy?method=HEAD
    Sep 20 13:45:53.082: INFO: http.Client request:HEAD StatusCode:301
    Sep 20 13:45:53.082: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-8429/services/e2e-proxy-test-service/proxy?method=HEAD
    Sep 20 13:45:53.965: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:45:53.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-8429" for this suite. 09/20/23 13:45:54.136
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:45:54.146
Sep 20 13:45:54.146: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename daemonsets 09/20/23 13:45:54.147
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:45:54.527
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:45:54.53
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873
STEP: Creating simple DaemonSet "daemon-set" 09/20/23 13:45:54.764
STEP: Check that daemon pods launch on every node of the cluster. 09/20/23 13:45:55.139
Sep 20 13:45:55.251: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:45:55.251: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:45:55.251: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:45:55.333: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 13:45:55.333: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 13:45:56.377: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:45:56.377: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:45:56.377: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:45:56.382: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 13:45:56.382: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 13:45:57.764: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:45:57.764: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:45:57.764: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:45:58.302: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep 20 13:45:58.302: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 13:45:58.486: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:45:58.486: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:45:58.486: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:45:58.492: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep 20 13:45:58.492: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 13:45:59.419: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:45:59.419: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:45:59.419: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:45:59.485: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Sep 20 13:45:59.485: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 09/20/23 13:45:59.492
Sep 20 13:45:59.498: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 09/20/23 13:45:59.498
Sep 20 13:45:59.541: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 09/20/23 13:45:59.541
Sep 20 13:45:59.546: INFO: Observed &DaemonSet event: ADDED
Sep 20 13:45:59.546: INFO: Observed &DaemonSet event: MODIFIED
Sep 20 13:45:59.546: INFO: Observed &DaemonSet event: MODIFIED
Sep 20 13:45:59.546: INFO: Observed &DaemonSet event: MODIFIED
Sep 20 13:45:59.546: INFO: Observed &DaemonSet event: MODIFIED
Sep 20 13:45:59.547: INFO: Found daemon set daemon-set in namespace daemonsets-4320 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep 20 13:45:59.547: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 09/20/23 13:45:59.547
STEP: watching for the daemon set status to be patched 09/20/23 13:45:59.633
Sep 20 13:45:59.637: INFO: Observed &DaemonSet event: ADDED
Sep 20 13:45:59.637: INFO: Observed &DaemonSet event: MODIFIED
Sep 20 13:45:59.637: INFO: Observed &DaemonSet event: MODIFIED
Sep 20 13:45:59.637: INFO: Observed &DaemonSet event: MODIFIED
Sep 20 13:45:59.638: INFO: Observed &DaemonSet event: MODIFIED
Sep 20 13:45:59.638: INFO: Observed daemon set daemon-set in namespace daemonsets-4320 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep 20 13:45:59.638: INFO: Observed &DaemonSet event: MODIFIED
Sep 20 13:45:59.638: INFO: Found daemon set daemon-set in namespace daemonsets-4320 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Sep 20 13:45:59.638: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 09/20/23 13:45:59.659
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4320, will wait for the garbage collector to delete the pods 09/20/23 13:45:59.66
Sep 20 13:46:00.877: INFO: Deleting DaemonSet.extensions daemon-set took: 1.159299173s
Sep 20 13:46:01.077: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.69153ms
Sep 20 13:46:03.998: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 13:46:03.998: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep 20 13:46:04.362: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"44729"},"items":null}

Sep 20 13:46:05.182: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"44732"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep 20 13:46:05.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4320" for this suite. 09/20/23 13:46:05.456
------------------------------
â€¢ [SLOW TEST] [11.400 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:45:54.146
    Sep 20 13:45:54.146: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename daemonsets 09/20/23 13:45:54.147
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:45:54.527
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:45:54.53
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:873
    STEP: Creating simple DaemonSet "daemon-set" 09/20/23 13:45:54.764
    STEP: Check that daemon pods launch on every node of the cluster. 09/20/23 13:45:55.139
    Sep 20 13:45:55.251: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:45:55.251: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:45:55.251: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:45:55.333: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 13:45:55.333: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 13:45:56.377: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:45:56.377: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:45:56.377: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:45:56.382: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 13:45:56.382: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 13:45:57.764: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:45:57.764: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:45:57.764: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:45:58.302: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep 20 13:45:58.302: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 13:45:58.486: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:45:58.486: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:45:58.486: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:45:58.492: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep 20 13:45:58.492: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 13:45:59.419: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:45:59.419: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:45:59.419: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:45:59.485: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Sep 20 13:45:59.485: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 09/20/23 13:45:59.492
    Sep 20 13:45:59.498: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 09/20/23 13:45:59.498
    Sep 20 13:45:59.541: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 09/20/23 13:45:59.541
    Sep 20 13:45:59.546: INFO: Observed &DaemonSet event: ADDED
    Sep 20 13:45:59.546: INFO: Observed &DaemonSet event: MODIFIED
    Sep 20 13:45:59.546: INFO: Observed &DaemonSet event: MODIFIED
    Sep 20 13:45:59.546: INFO: Observed &DaemonSet event: MODIFIED
    Sep 20 13:45:59.546: INFO: Observed &DaemonSet event: MODIFIED
    Sep 20 13:45:59.547: INFO: Found daemon set daemon-set in namespace daemonsets-4320 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Sep 20 13:45:59.547: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 09/20/23 13:45:59.547
    STEP: watching for the daemon set status to be patched 09/20/23 13:45:59.633
    Sep 20 13:45:59.637: INFO: Observed &DaemonSet event: ADDED
    Sep 20 13:45:59.637: INFO: Observed &DaemonSet event: MODIFIED
    Sep 20 13:45:59.637: INFO: Observed &DaemonSet event: MODIFIED
    Sep 20 13:45:59.637: INFO: Observed &DaemonSet event: MODIFIED
    Sep 20 13:45:59.638: INFO: Observed &DaemonSet event: MODIFIED
    Sep 20 13:45:59.638: INFO: Observed daemon set daemon-set in namespace daemonsets-4320 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Sep 20 13:45:59.638: INFO: Observed &DaemonSet event: MODIFIED
    Sep 20 13:45:59.638: INFO: Found daemon set daemon-set in namespace daemonsets-4320 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Sep 20 13:45:59.638: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 09/20/23 13:45:59.659
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4320, will wait for the garbage collector to delete the pods 09/20/23 13:45:59.66
    Sep 20 13:46:00.877: INFO: Deleting DaemonSet.extensions daemon-set took: 1.159299173s
    Sep 20 13:46:01.077: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.69153ms
    Sep 20 13:46:03.998: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 13:46:03.998: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep 20 13:46:04.362: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"44729"},"items":null}

    Sep 20 13:46:05.182: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"44732"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:46:05.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4320" for this suite. 09/20/23 13:46:05.456
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:46:05.547
Sep 20 13:46:05.547: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename dns 09/20/23 13:46:05.548
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:46:05.877
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:46:05.88
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 09/20/23 13:46:05.886
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8668.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8668.svc.cluster.local; sleep 1; done
 09/20/23 13:46:06.052
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8668.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8668.svc.cluster.local; sleep 1; done
 09/20/23 13:46:06.052
STEP: creating a pod to probe DNS 09/20/23 13:46:06.052
STEP: submitting the pod to kubernetes 09/20/23 13:46:06.052
Sep 20 13:46:06.351: INFO: Waiting up to 15m0s for pod "dns-test-833a7f65-410b-4a33-b816-1becbea7451a" in namespace "dns-8668" to be "running"
Sep 20 13:46:06.389: INFO: Pod "dns-test-833a7f65-410b-4a33-b816-1becbea7451a": Phase="Pending", Reason="", readiness=false. Elapsed: 37.3375ms
Sep 20 13:46:08.452: INFO: Pod "dns-test-833a7f65-410b-4a33-b816-1becbea7451a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.100648659s
Sep 20 13:46:10.413: INFO: Pod "dns-test-833a7f65-410b-4a33-b816-1becbea7451a": Phase="Running", Reason="", readiness=true. Elapsed: 4.062277423s
Sep 20 13:46:10.414: INFO: Pod "dns-test-833a7f65-410b-4a33-b816-1becbea7451a" satisfied condition "running"
STEP: retrieving the pod 09/20/23 13:46:10.414
STEP: looking for the results for each expected name from probers 09/20/23 13:46:10.416
Sep 20 13:46:10.429: INFO: DNS probes using dns-test-833a7f65-410b-4a33-b816-1becbea7451a succeeded

STEP: deleting the pod 09/20/23 13:46:10.429
STEP: changing the externalName to bar.example.com 09/20/23 13:46:11.128
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8668.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8668.svc.cluster.local; sleep 1; done
 09/20/23 13:46:11.559
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8668.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8668.svc.cluster.local; sleep 1; done
 09/20/23 13:46:11.559
STEP: creating a second pod to probe DNS 09/20/23 13:46:11.559
STEP: submitting the pod to kubernetes 09/20/23 13:46:11.559
Sep 20 13:46:11.644: INFO: Waiting up to 15m0s for pod "dns-test-bb0e1572-570f-415f-8fee-5e9cd8dc7212" in namespace "dns-8668" to be "running"
Sep 20 13:46:12.459: INFO: Pod "dns-test-bb0e1572-570f-415f-8fee-5e9cd8dc7212": Phase="Pending", Reason="", readiness=false. Elapsed: 815.390452ms
Sep 20 13:46:14.801: INFO: Pod "dns-test-bb0e1572-570f-415f-8fee-5e9cd8dc7212": Phase="Pending", Reason="", readiness=false. Elapsed: 3.156962525s
Sep 20 13:46:16.549: INFO: Pod "dns-test-bb0e1572-570f-415f-8fee-5e9cd8dc7212": Phase="Pending", Reason="", readiness=false. Elapsed: 4.905432325s
Sep 20 13:46:18.465: INFO: Pod "dns-test-bb0e1572-570f-415f-8fee-5e9cd8dc7212": Phase="Running", Reason="", readiness=true. Elapsed: 6.820794651s
Sep 20 13:46:18.465: INFO: Pod "dns-test-bb0e1572-570f-415f-8fee-5e9cd8dc7212" satisfied condition "running"
STEP: retrieving the pod 09/20/23 13:46:18.465
STEP: looking for the results for each expected name from probers 09/20/23 13:46:18.468
Sep 20 13:46:18.477: INFO: DNS probes using dns-test-bb0e1572-570f-415f-8fee-5e9cd8dc7212 succeeded

STEP: deleting the pod 09/20/23 13:46:18.477
STEP: changing the service to type=ClusterIP 09/20/23 13:46:19.475
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8668.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8668.svc.cluster.local; sleep 1; done
 09/20/23 13:46:19.536
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8668.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8668.svc.cluster.local; sleep 1; done
 09/20/23 13:46:19.536
STEP: creating a third pod to probe DNS 09/20/23 13:46:19.536
STEP: submitting the pod to kubernetes 09/20/23 13:46:19.544
Sep 20 13:46:19.833: INFO: Waiting up to 15m0s for pod "dns-test-5f0fadb4-58fa-4a29-ade5-00d6109ef66b" in namespace "dns-8668" to be "running"
Sep 20 13:46:19.876: INFO: Pod "dns-test-5f0fadb4-58fa-4a29-ade5-00d6109ef66b": Phase="Pending", Reason="", readiness=false. Elapsed: 42.657862ms
Sep 20 13:46:22.089: INFO: Pod "dns-test-5f0fadb4-58fa-4a29-ade5-00d6109ef66b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.255964869s
Sep 20 13:46:23.976: INFO: Pod "dns-test-5f0fadb4-58fa-4a29-ade5-00d6109ef66b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.143035002s
Sep 20 13:46:25.881: INFO: Pod "dns-test-5f0fadb4-58fa-4a29-ade5-00d6109ef66b": Phase="Running", Reason="", readiness=true. Elapsed: 6.04795639s
Sep 20 13:46:25.881: INFO: Pod "dns-test-5f0fadb4-58fa-4a29-ade5-00d6109ef66b" satisfied condition "running"
STEP: retrieving the pod 09/20/23 13:46:25.881
STEP: looking for the results for each expected name from probers 09/20/23 13:46:25.883
Sep 20 13:46:25.891: INFO: DNS probes using dns-test-5f0fadb4-58fa-4a29-ade5-00d6109ef66b succeeded

STEP: deleting the pod 09/20/23 13:46:25.891
STEP: deleting the test externalName service 09/20/23 13:46:26.31
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep 20 13:46:27.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8668" for this suite. 09/20/23 13:46:27.472
------------------------------
â€¢ [SLOW TEST] [22.605 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:46:05.547
    Sep 20 13:46:05.547: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename dns 09/20/23 13:46:05.548
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:46:05.877
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:46:05.88
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 09/20/23 13:46:05.886
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8668.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8668.svc.cluster.local; sleep 1; done
     09/20/23 13:46:06.052
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8668.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8668.svc.cluster.local; sleep 1; done
     09/20/23 13:46:06.052
    STEP: creating a pod to probe DNS 09/20/23 13:46:06.052
    STEP: submitting the pod to kubernetes 09/20/23 13:46:06.052
    Sep 20 13:46:06.351: INFO: Waiting up to 15m0s for pod "dns-test-833a7f65-410b-4a33-b816-1becbea7451a" in namespace "dns-8668" to be "running"
    Sep 20 13:46:06.389: INFO: Pod "dns-test-833a7f65-410b-4a33-b816-1becbea7451a": Phase="Pending", Reason="", readiness=false. Elapsed: 37.3375ms
    Sep 20 13:46:08.452: INFO: Pod "dns-test-833a7f65-410b-4a33-b816-1becbea7451a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.100648659s
    Sep 20 13:46:10.413: INFO: Pod "dns-test-833a7f65-410b-4a33-b816-1becbea7451a": Phase="Running", Reason="", readiness=true. Elapsed: 4.062277423s
    Sep 20 13:46:10.414: INFO: Pod "dns-test-833a7f65-410b-4a33-b816-1becbea7451a" satisfied condition "running"
    STEP: retrieving the pod 09/20/23 13:46:10.414
    STEP: looking for the results for each expected name from probers 09/20/23 13:46:10.416
    Sep 20 13:46:10.429: INFO: DNS probes using dns-test-833a7f65-410b-4a33-b816-1becbea7451a succeeded

    STEP: deleting the pod 09/20/23 13:46:10.429
    STEP: changing the externalName to bar.example.com 09/20/23 13:46:11.128
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8668.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8668.svc.cluster.local; sleep 1; done
     09/20/23 13:46:11.559
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8668.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8668.svc.cluster.local; sleep 1; done
     09/20/23 13:46:11.559
    STEP: creating a second pod to probe DNS 09/20/23 13:46:11.559
    STEP: submitting the pod to kubernetes 09/20/23 13:46:11.559
    Sep 20 13:46:11.644: INFO: Waiting up to 15m0s for pod "dns-test-bb0e1572-570f-415f-8fee-5e9cd8dc7212" in namespace "dns-8668" to be "running"
    Sep 20 13:46:12.459: INFO: Pod "dns-test-bb0e1572-570f-415f-8fee-5e9cd8dc7212": Phase="Pending", Reason="", readiness=false. Elapsed: 815.390452ms
    Sep 20 13:46:14.801: INFO: Pod "dns-test-bb0e1572-570f-415f-8fee-5e9cd8dc7212": Phase="Pending", Reason="", readiness=false. Elapsed: 3.156962525s
    Sep 20 13:46:16.549: INFO: Pod "dns-test-bb0e1572-570f-415f-8fee-5e9cd8dc7212": Phase="Pending", Reason="", readiness=false. Elapsed: 4.905432325s
    Sep 20 13:46:18.465: INFO: Pod "dns-test-bb0e1572-570f-415f-8fee-5e9cd8dc7212": Phase="Running", Reason="", readiness=true. Elapsed: 6.820794651s
    Sep 20 13:46:18.465: INFO: Pod "dns-test-bb0e1572-570f-415f-8fee-5e9cd8dc7212" satisfied condition "running"
    STEP: retrieving the pod 09/20/23 13:46:18.465
    STEP: looking for the results for each expected name from probers 09/20/23 13:46:18.468
    Sep 20 13:46:18.477: INFO: DNS probes using dns-test-bb0e1572-570f-415f-8fee-5e9cd8dc7212 succeeded

    STEP: deleting the pod 09/20/23 13:46:18.477
    STEP: changing the service to type=ClusterIP 09/20/23 13:46:19.475
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8668.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8668.svc.cluster.local; sleep 1; done
     09/20/23 13:46:19.536
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8668.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8668.svc.cluster.local; sleep 1; done
     09/20/23 13:46:19.536
    STEP: creating a third pod to probe DNS 09/20/23 13:46:19.536
    STEP: submitting the pod to kubernetes 09/20/23 13:46:19.544
    Sep 20 13:46:19.833: INFO: Waiting up to 15m0s for pod "dns-test-5f0fadb4-58fa-4a29-ade5-00d6109ef66b" in namespace "dns-8668" to be "running"
    Sep 20 13:46:19.876: INFO: Pod "dns-test-5f0fadb4-58fa-4a29-ade5-00d6109ef66b": Phase="Pending", Reason="", readiness=false. Elapsed: 42.657862ms
    Sep 20 13:46:22.089: INFO: Pod "dns-test-5f0fadb4-58fa-4a29-ade5-00d6109ef66b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.255964869s
    Sep 20 13:46:23.976: INFO: Pod "dns-test-5f0fadb4-58fa-4a29-ade5-00d6109ef66b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.143035002s
    Sep 20 13:46:25.881: INFO: Pod "dns-test-5f0fadb4-58fa-4a29-ade5-00d6109ef66b": Phase="Running", Reason="", readiness=true. Elapsed: 6.04795639s
    Sep 20 13:46:25.881: INFO: Pod "dns-test-5f0fadb4-58fa-4a29-ade5-00d6109ef66b" satisfied condition "running"
    STEP: retrieving the pod 09/20/23 13:46:25.881
    STEP: looking for the results for each expected name from probers 09/20/23 13:46:25.883
    Sep 20 13:46:25.891: INFO: DNS probes using dns-test-5f0fadb4-58fa-4a29-ade5-00d6109ef66b succeeded

    STEP: deleting the pod 09/20/23 13:46:25.891
    STEP: deleting the test externalName service 09/20/23 13:46:26.31
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:46:27.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8668" for this suite. 09/20/23 13:46:27.472
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:46:28.153
Sep 20 13:46:28.153: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename kubectl 09/20/23 13:46:28.154
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:46:28.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:46:28.395
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/20/23 13:46:28.399
Sep 20 13:46:28.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-729 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Sep 20 13:46:28.495: INFO: stderr: ""
Sep 20 13:46:28.495: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 09/20/23 13:46:28.495
STEP: verifying the pod e2e-test-httpd-pod was created 09/20/23 13:46:33.547
Sep 20 13:46:33.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-729 get pod e2e-test-httpd-pod -o json'
Sep 20 13:46:34.175: INFO: stderr: ""
Sep 20 13:46:34.175: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-09-20T13:46:28Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-729\",\n        \"resourceVersion\": \"44925\",\n        \"uid\": \"e661e206-e961-483e-84ce-a6c19a2e8673\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-qdn2s\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"mycluster-ww3cg64etuwi-node-1\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-qdn2s\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-20T13:46:28Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-20T13:46:32Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-20T13:46:32Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-20T13:46:28Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://55b74a1da258b45eb3a29c64bcc189da06e9581e3fd4b235e878fc00204c7016\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-09-20T13:46:31Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.10.64\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.100.4.94\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.100.4.94\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-09-20T13:46:28Z\"\n    }\n}\n"
STEP: replace the image in the pod 09/20/23 13:46:34.175
Sep 20 13:46:34.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-729 replace -f -'
Sep 20 13:46:35.308: INFO: stderr: ""
Sep 20 13:46:35.308: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 09/20/23 13:46:35.308
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Sep 20 13:46:36.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-729 delete pods e2e-test-httpd-pod'
Sep 20 13:46:39.974: INFO: stderr: ""
Sep 20 13:46:39.974: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep 20 13:46:39.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-729" for this suite. 09/20/23 13:46:39.987
------------------------------
â€¢ [SLOW TEST] [11.847 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:46:28.153
    Sep 20 13:46:28.153: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename kubectl 09/20/23 13:46:28.154
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:46:28.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:46:28.395
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/20/23 13:46:28.399
    Sep 20 13:46:28.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-729 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Sep 20 13:46:28.495: INFO: stderr: ""
    Sep 20 13:46:28.495: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 09/20/23 13:46:28.495
    STEP: verifying the pod e2e-test-httpd-pod was created 09/20/23 13:46:33.547
    Sep 20 13:46:33.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-729 get pod e2e-test-httpd-pod -o json'
    Sep 20 13:46:34.175: INFO: stderr: ""
    Sep 20 13:46:34.175: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-09-20T13:46:28Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-729\",\n        \"resourceVersion\": \"44925\",\n        \"uid\": \"e661e206-e961-483e-84ce-a6c19a2e8673\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-qdn2s\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"mycluster-ww3cg64etuwi-node-1\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-qdn2s\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-20T13:46:28Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-20T13:46:32Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-20T13:46:32Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-20T13:46:28Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://55b74a1da258b45eb3a29c64bcc189da06e9581e3fd4b235e878fc00204c7016\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-09-20T13:46:31Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.10.64\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.100.4.94\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.100.4.94\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-09-20T13:46:28Z\"\n    }\n}\n"
    STEP: replace the image in the pod 09/20/23 13:46:34.175
    Sep 20 13:46:34.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-729 replace -f -'
    Sep 20 13:46:35.308: INFO: stderr: ""
    Sep 20 13:46:35.308: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 09/20/23 13:46:35.308
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Sep 20 13:46:36.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-729 delete pods e2e-test-httpd-pod'
    Sep 20 13:46:39.974: INFO: stderr: ""
    Sep 20 13:46:39.974: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:46:39.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-729" for this suite. 09/20/23 13:46:39.987
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:46:39.999
Sep 20 13:46:39.999: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename webhook 09/20/23 13:46:40
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:46:40.349
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:46:40.354
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/20/23 13:46:40.744
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/20/23 13:46:41.305
STEP: Deploying the webhook pod 09/20/23 13:46:41.651
STEP: Wait for the deployment to be ready 09/20/23 13:46:41.932
Sep 20 13:46:41.945: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep 20 13:46:44.690: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 46, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 46, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 46, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 46, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 13:46:46.826: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 46, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 46, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 46, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 46, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/20/23 13:46:48.696
STEP: Verifying the service has paired with the endpoint 09/20/23 13:46:49.355
Sep 20 13:46:50.356: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 09/20/23 13:46:50.814
STEP: Creating a configMap that does not comply to the validation webhook rules 09/20/23 13:46:50.859
STEP: Deleting the collection of validation webhooks 09/20/23 13:46:50.896
STEP: Creating a configMap that does not comply to the validation webhook rules 09/20/23 13:46:50.981
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep 20 13:46:51.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3423" for this suite. 09/20/23 13:46:52.391
STEP: Destroying namespace "webhook-3423-markers" for this suite. 09/20/23 13:46:52.444
------------------------------
â€¢ [SLOW TEST] [12.613 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:46:39.999
    Sep 20 13:46:39.999: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename webhook 09/20/23 13:46:40
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:46:40.349
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:46:40.354
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/20/23 13:46:40.744
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/20/23 13:46:41.305
    STEP: Deploying the webhook pod 09/20/23 13:46:41.651
    STEP: Wait for the deployment to be ready 09/20/23 13:46:41.932
    Sep 20 13:46:41.945: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Sep 20 13:46:44.690: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 46, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 46, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 46, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 46, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 13:46:46.826: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 46, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 46, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 46, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 46, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/20/23 13:46:48.696
    STEP: Verifying the service has paired with the endpoint 09/20/23 13:46:49.355
    Sep 20 13:46:50.356: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 09/20/23 13:46:50.814
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/20/23 13:46:50.859
    STEP: Deleting the collection of validation webhooks 09/20/23 13:46:50.896
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/20/23 13:46:50.981
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:46:51.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3423" for this suite. 09/20/23 13:46:52.391
    STEP: Destroying namespace "webhook-3423-markers" for this suite. 09/20/23 13:46:52.444
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:46:52.614
Sep 20 13:46:52.615: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename events 09/20/23 13:46:52.615
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:46:53.118
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:46:53.123
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 09/20/23 13:46:53.13
STEP: get a list of Events with a label in the current namespace 09/20/23 13:46:53.639
STEP: delete a list of events 09/20/23 13:46:53.724
Sep 20 13:46:53.724: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 09/20/23 13:46:53.943
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Sep 20 13:46:53.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-8803" for this suite. 09/20/23 13:46:53.95
------------------------------
â€¢ [1.346 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:46:52.614
    Sep 20 13:46:52.615: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename events 09/20/23 13:46:52.615
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:46:53.118
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:46:53.123
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 09/20/23 13:46:53.13
    STEP: get a list of Events with a label in the current namespace 09/20/23 13:46:53.639
    STEP: delete a list of events 09/20/23 13:46:53.724
    Sep 20 13:46:53.724: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 09/20/23 13:46:53.943
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:46:53.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-8803" for this suite. 09/20/23 13:46:53.95
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:46:53.962
Sep 20 13:46:53.962: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename webhook 09/20/23 13:46:53.963
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:46:53.987
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:46:53.991
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/20/23 13:46:54.053
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/20/23 13:46:54.256
STEP: Deploying the webhook pod 09/20/23 13:46:54.486
STEP: Wait for the deployment to be ready 09/20/23 13:46:54.585
Sep 20 13:46:54.595: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep 20 13:46:56.816: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 46, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 46, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 46, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 46, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 13:46:58.821: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 46, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 46, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 46, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 46, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/20/23 13:47:00.967
STEP: Verifying the service has paired with the endpoint 09/20/23 13:47:01.449
Sep 20 13:47:02.449: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 09/20/23 13:47:02.455
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 09/20/23 13:47:02.727
STEP: Creating a dummy validating-webhook-configuration object 09/20/23 13:47:03.025
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 09/20/23 13:47:03.039
STEP: Creating a dummy mutating-webhook-configuration object 09/20/23 13:47:03.051
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 09/20/23 13:47:03.197
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep 20 13:47:04.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4206" for this suite. 09/20/23 13:47:04.182
STEP: Destroying namespace "webhook-4206-markers" for this suite. 09/20/23 13:47:04.193
------------------------------
â€¢ [SLOW TEST] [10.272 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:46:53.962
    Sep 20 13:46:53.962: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename webhook 09/20/23 13:46:53.963
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:46:53.987
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:46:53.991
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/20/23 13:46:54.053
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/20/23 13:46:54.256
    STEP: Deploying the webhook pod 09/20/23 13:46:54.486
    STEP: Wait for the deployment to be ready 09/20/23 13:46:54.585
    Sep 20 13:46:54.595: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Sep 20 13:46:56.816: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 46, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 46, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 46, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 46, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 13:46:58.821: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 46, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 46, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 46, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 46, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/20/23 13:47:00.967
    STEP: Verifying the service has paired with the endpoint 09/20/23 13:47:01.449
    Sep 20 13:47:02.449: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 09/20/23 13:47:02.455
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 09/20/23 13:47:02.727
    STEP: Creating a dummy validating-webhook-configuration object 09/20/23 13:47:03.025
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 09/20/23 13:47:03.039
    STEP: Creating a dummy mutating-webhook-configuration object 09/20/23 13:47:03.051
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 09/20/23 13:47:03.197
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:47:04.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4206" for this suite. 09/20/23 13:47:04.182
    STEP: Destroying namespace "webhook-4206-markers" for this suite. 09/20/23 13:47:04.193
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:47:04.234
Sep 20 13:47:04.234: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename downward-api 09/20/23 13:47:04.235
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:47:05.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:47:05.044
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 09/20/23 13:47:05.052
Sep 20 13:47:05.067: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8101bee8-be11-45e5-ab9f-37408a7890ac" in namespace "downward-api-553" to be "Succeeded or Failed"
Sep 20 13:47:05.074: INFO: Pod "downwardapi-volume-8101bee8-be11-45e5-ab9f-37408a7890ac": Phase="Pending", Reason="", readiness=false. Elapsed: 7.158274ms
Sep 20 13:47:07.528: INFO: Pod "downwardapi-volume-8101bee8-be11-45e5-ab9f-37408a7890ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.461125378s
Sep 20 13:47:09.344: INFO: Pod "downwardapi-volume-8101bee8-be11-45e5-ab9f-37408a7890ac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.277420687s
Sep 20 13:47:11.080: INFO: Pod "downwardapi-volume-8101bee8-be11-45e5-ab9f-37408a7890ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013232785s
STEP: Saw pod success 09/20/23 13:47:11.08
Sep 20 13:47:11.080: INFO: Pod "downwardapi-volume-8101bee8-be11-45e5-ab9f-37408a7890ac" satisfied condition "Succeeded or Failed"
Sep 20 13:47:11.086: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downwardapi-volume-8101bee8-be11-45e5-ab9f-37408a7890ac container client-container: <nil>
STEP: delete the pod 09/20/23 13:47:11.157
Sep 20 13:47:11.507: INFO: Waiting for pod downwardapi-volume-8101bee8-be11-45e5-ab9f-37408a7890ac to disappear
Sep 20 13:47:11.512: INFO: Pod downwardapi-volume-8101bee8-be11-45e5-ab9f-37408a7890ac no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep 20 13:47:11.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-553" for this suite. 09/20/23 13:47:11.518
------------------------------
â€¢ [SLOW TEST] [7.294 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:47:04.234
    Sep 20 13:47:04.234: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename downward-api 09/20/23 13:47:04.235
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:47:05.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:47:05.044
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 09/20/23 13:47:05.052
    Sep 20 13:47:05.067: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8101bee8-be11-45e5-ab9f-37408a7890ac" in namespace "downward-api-553" to be "Succeeded or Failed"
    Sep 20 13:47:05.074: INFO: Pod "downwardapi-volume-8101bee8-be11-45e5-ab9f-37408a7890ac": Phase="Pending", Reason="", readiness=false. Elapsed: 7.158274ms
    Sep 20 13:47:07.528: INFO: Pod "downwardapi-volume-8101bee8-be11-45e5-ab9f-37408a7890ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.461125378s
    Sep 20 13:47:09.344: INFO: Pod "downwardapi-volume-8101bee8-be11-45e5-ab9f-37408a7890ac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.277420687s
    Sep 20 13:47:11.080: INFO: Pod "downwardapi-volume-8101bee8-be11-45e5-ab9f-37408a7890ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013232785s
    STEP: Saw pod success 09/20/23 13:47:11.08
    Sep 20 13:47:11.080: INFO: Pod "downwardapi-volume-8101bee8-be11-45e5-ab9f-37408a7890ac" satisfied condition "Succeeded or Failed"
    Sep 20 13:47:11.086: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downwardapi-volume-8101bee8-be11-45e5-ab9f-37408a7890ac container client-container: <nil>
    STEP: delete the pod 09/20/23 13:47:11.157
    Sep 20 13:47:11.507: INFO: Waiting for pod downwardapi-volume-8101bee8-be11-45e5-ab9f-37408a7890ac to disappear
    Sep 20 13:47:11.512: INFO: Pod downwardapi-volume-8101bee8-be11-45e5-ab9f-37408a7890ac no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:47:11.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-553" for this suite. 09/20/23 13:47:11.518
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:47:11.529
Sep 20 13:47:11.529: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename dns 09/20/23 13:47:11.53
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:47:11.736
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:47:11.74
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 09/20/23 13:47:11.745
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1179 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1179;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1179 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1179;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1179.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1179.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1179.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1179.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1179.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1179.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1179.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1179.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1179.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1179.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1179.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1179.svc;check="$$(dig +notcp +noall +answer +search 33.106.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.106.33_udp@PTR;check="$$(dig +tcp +noall +answer +search 33.106.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.106.33_tcp@PTR;sleep 1; done
 09/20/23 13:47:11.999
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1179 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1179;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1179 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1179;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1179.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1179.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1179.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1179.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1179.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1179.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1179.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1179.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1179.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1179.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1179.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1179.svc;check="$$(dig +notcp +noall +answer +search 33.106.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.106.33_udp@PTR;check="$$(dig +tcp +noall +answer +search 33.106.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.106.33_tcp@PTR;sleep 1; done
 09/20/23 13:47:12
STEP: creating a pod to probe DNS 09/20/23 13:47:12
STEP: submitting the pod to kubernetes 09/20/23 13:47:12
Sep 20 13:47:12.228: INFO: Waiting up to 15m0s for pod "dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61" in namespace "dns-1179" to be "running"
Sep 20 13:47:12.468: INFO: Pod "dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61": Phase="Pending", Reason="", readiness=false. Elapsed: 239.892001ms
Sep 20 13:47:14.471: INFO: Pod "dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.243867472s
Sep 20 13:47:16.606: INFO: Pod "dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61": Phase="Pending", Reason="", readiness=false. Elapsed: 4.378349319s
Sep 20 13:47:18.477: INFO: Pod "dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61": Phase="Running", Reason="", readiness=true. Elapsed: 6.249550973s
Sep 20 13:47:18.477: INFO: Pod "dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61" satisfied condition "running"
STEP: retrieving the pod 09/20/23 13:47:18.477
STEP: looking for the results for each expected name from probers 09/20/23 13:47:18.618
Sep 20 13:47:18.643: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1179/dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61: the server could not find the requested resource (get pods dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61)
Sep 20 13:47:18.681: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1179/dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61: the server could not find the requested resource (get pods dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61)
Sep 20 13:47:18.686: INFO: Unable to read wheezy_udp@dns-test-service.dns-1179 from pod dns-1179/dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61: the server could not find the requested resource (get pods dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61)
Sep 20 13:47:18.693: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1179 from pod dns-1179/dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61: the server could not find the requested resource (get pods dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61)
Sep 20 13:47:18.698: INFO: Unable to read wheezy_udp@dns-test-service.dns-1179.svc from pod dns-1179/dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61: the server could not find the requested resource (get pods dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61)
Sep 20 13:47:18.703: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1179.svc from pod dns-1179/dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61: the server could not find the requested resource (get pods dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61)
Sep 20 13:47:18.707: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1179.svc from pod dns-1179/dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61: the server could not find the requested resource (get pods dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61)
Sep 20 13:47:18.711: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1179.svc from pod dns-1179/dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61: the server could not find the requested resource (get pods dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61)
Sep 20 13:47:19.107: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1179/dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61: the server could not find the requested resource (get pods dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61)
Sep 20 13:47:19.120: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1179/dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61: the server could not find the requested resource (get pods dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61)
Sep 20 13:47:19.140: INFO: Unable to read jessie_udp@dns-test-service.dns-1179 from pod dns-1179/dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61: the server could not find the requested resource (get pods dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61)
Sep 20 13:47:19.144: INFO: Unable to read jessie_tcp@dns-test-service.dns-1179 from pod dns-1179/dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61: the server could not find the requested resource (get pods dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61)
Sep 20 13:47:19.160: INFO: Unable to read jessie_udp@dns-test-service.dns-1179.svc from pod dns-1179/dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61: the server could not find the requested resource (get pods dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61)
Sep 20 13:47:19.168: INFO: Unable to read jessie_tcp@dns-test-service.dns-1179.svc from pod dns-1179/dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61: the server could not find the requested resource (get pods dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61)
Sep 20 13:47:19.173: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1179.svc from pod dns-1179/dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61: the server could not find the requested resource (get pods dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61)
Sep 20 13:47:19.178: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1179.svc from pod dns-1179/dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61: the server could not find the requested resource (get pods dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61)
Sep 20 13:47:19.195: INFO: Lookups using dns-1179/dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1179 wheezy_tcp@dns-test-service.dns-1179 wheezy_udp@dns-test-service.dns-1179.svc wheezy_tcp@dns-test-service.dns-1179.svc wheezy_udp@_http._tcp.dns-test-service.dns-1179.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1179.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1179 jessie_tcp@dns-test-service.dns-1179 jessie_udp@dns-test-service.dns-1179.svc jessie_tcp@dns-test-service.dns-1179.svc jessie_udp@_http._tcp.dns-test-service.dns-1179.svc jessie_tcp@_http._tcp.dns-test-service.dns-1179.svc]

Sep 20 13:47:25.485: INFO: DNS probes using dns-1179/dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61 succeeded

STEP: deleting the pod 09/20/23 13:47:25.485
STEP: deleting the test service 09/20/23 13:47:25.87
STEP: deleting the test headless service 09/20/23 13:47:25.892
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep 20 13:47:25.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-1179" for this suite. 09/20/23 13:47:25.909
------------------------------
â€¢ [SLOW TEST] [14.839 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:47:11.529
    Sep 20 13:47:11.529: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename dns 09/20/23 13:47:11.53
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:47:11.736
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:47:11.74
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 09/20/23 13:47:11.745
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1179 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1179;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1179 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1179;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1179.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1179.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1179.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1179.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1179.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1179.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1179.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1179.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1179.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1179.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1179.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1179.svc;check="$$(dig +notcp +noall +answer +search 33.106.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.106.33_udp@PTR;check="$$(dig +tcp +noall +answer +search 33.106.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.106.33_tcp@PTR;sleep 1; done
     09/20/23 13:47:11.999
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1179 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1179;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1179 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1179;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1179.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1179.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1179.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1179.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1179.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1179.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1179.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1179.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1179.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1179.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1179.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1179.svc;check="$$(dig +notcp +noall +answer +search 33.106.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.106.33_udp@PTR;check="$$(dig +tcp +noall +answer +search 33.106.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.106.33_tcp@PTR;sleep 1; done
     09/20/23 13:47:12
    STEP: creating a pod to probe DNS 09/20/23 13:47:12
    STEP: submitting the pod to kubernetes 09/20/23 13:47:12
    Sep 20 13:47:12.228: INFO: Waiting up to 15m0s for pod "dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61" in namespace "dns-1179" to be "running"
    Sep 20 13:47:12.468: INFO: Pod "dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61": Phase="Pending", Reason="", readiness=false. Elapsed: 239.892001ms
    Sep 20 13:47:14.471: INFO: Pod "dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.243867472s
    Sep 20 13:47:16.606: INFO: Pod "dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61": Phase="Pending", Reason="", readiness=false. Elapsed: 4.378349319s
    Sep 20 13:47:18.477: INFO: Pod "dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61": Phase="Running", Reason="", readiness=true. Elapsed: 6.249550973s
    Sep 20 13:47:18.477: INFO: Pod "dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61" satisfied condition "running"
    STEP: retrieving the pod 09/20/23 13:47:18.477
    STEP: looking for the results for each expected name from probers 09/20/23 13:47:18.618
    Sep 20 13:47:18.643: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1179/dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61: the server could not find the requested resource (get pods dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61)
    Sep 20 13:47:18.681: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1179/dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61: the server could not find the requested resource (get pods dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61)
    Sep 20 13:47:18.686: INFO: Unable to read wheezy_udp@dns-test-service.dns-1179 from pod dns-1179/dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61: the server could not find the requested resource (get pods dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61)
    Sep 20 13:47:18.693: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1179 from pod dns-1179/dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61: the server could not find the requested resource (get pods dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61)
    Sep 20 13:47:18.698: INFO: Unable to read wheezy_udp@dns-test-service.dns-1179.svc from pod dns-1179/dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61: the server could not find the requested resource (get pods dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61)
    Sep 20 13:47:18.703: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1179.svc from pod dns-1179/dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61: the server could not find the requested resource (get pods dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61)
    Sep 20 13:47:18.707: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1179.svc from pod dns-1179/dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61: the server could not find the requested resource (get pods dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61)
    Sep 20 13:47:18.711: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1179.svc from pod dns-1179/dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61: the server could not find the requested resource (get pods dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61)
    Sep 20 13:47:19.107: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1179/dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61: the server could not find the requested resource (get pods dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61)
    Sep 20 13:47:19.120: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1179/dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61: the server could not find the requested resource (get pods dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61)
    Sep 20 13:47:19.140: INFO: Unable to read jessie_udp@dns-test-service.dns-1179 from pod dns-1179/dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61: the server could not find the requested resource (get pods dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61)
    Sep 20 13:47:19.144: INFO: Unable to read jessie_tcp@dns-test-service.dns-1179 from pod dns-1179/dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61: the server could not find the requested resource (get pods dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61)
    Sep 20 13:47:19.160: INFO: Unable to read jessie_udp@dns-test-service.dns-1179.svc from pod dns-1179/dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61: the server could not find the requested resource (get pods dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61)
    Sep 20 13:47:19.168: INFO: Unable to read jessie_tcp@dns-test-service.dns-1179.svc from pod dns-1179/dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61: the server could not find the requested resource (get pods dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61)
    Sep 20 13:47:19.173: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1179.svc from pod dns-1179/dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61: the server could not find the requested resource (get pods dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61)
    Sep 20 13:47:19.178: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1179.svc from pod dns-1179/dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61: the server could not find the requested resource (get pods dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61)
    Sep 20 13:47:19.195: INFO: Lookups using dns-1179/dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1179 wheezy_tcp@dns-test-service.dns-1179 wheezy_udp@dns-test-service.dns-1179.svc wheezy_tcp@dns-test-service.dns-1179.svc wheezy_udp@_http._tcp.dns-test-service.dns-1179.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1179.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1179 jessie_tcp@dns-test-service.dns-1179 jessie_udp@dns-test-service.dns-1179.svc jessie_tcp@dns-test-service.dns-1179.svc jessie_udp@_http._tcp.dns-test-service.dns-1179.svc jessie_tcp@_http._tcp.dns-test-service.dns-1179.svc]

    Sep 20 13:47:25.485: INFO: DNS probes using dns-1179/dns-test-a705236d-5b4f-4c3a-b160-3270834c0f61 succeeded

    STEP: deleting the pod 09/20/23 13:47:25.485
    STEP: deleting the test service 09/20/23 13:47:25.87
    STEP: deleting the test headless service 09/20/23 13:47:25.892
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:47:25.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-1179" for this suite. 09/20/23 13:47:25.909
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:47:26.371
Sep 20 13:47:26.371: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename downward-api 09/20/23 13:47:26.371
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:47:26.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:47:26.455
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 09/20/23 13:47:26.464
Sep 20 13:47:26.570: INFO: Waiting up to 5m0s for pod "downward-api-320823b7-7993-478a-a4d7-8bca7e508fd2" in namespace "downward-api-7478" to be "Succeeded or Failed"
Sep 20 13:47:26.577: INFO: Pod "downward-api-320823b7-7993-478a-a4d7-8bca7e508fd2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.852374ms
Sep 20 13:47:28.583: INFO: Pod "downward-api-320823b7-7993-478a-a4d7-8bca7e508fd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011573454s
Sep 20 13:47:30.629: INFO: Pod "downward-api-320823b7-7993-478a-a4d7-8bca7e508fd2": Phase="Running", Reason="", readiness=false. Elapsed: 4.05768827s
Sep 20 13:47:32.666: INFO: Pod "downward-api-320823b7-7993-478a-a4d7-8bca7e508fd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.094308443s
STEP: Saw pod success 09/20/23 13:47:32.666
Sep 20 13:47:32.666: INFO: Pod "downward-api-320823b7-7993-478a-a4d7-8bca7e508fd2" satisfied condition "Succeeded or Failed"
Sep 20 13:47:32.669: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downward-api-320823b7-7993-478a-a4d7-8bca7e508fd2 container dapi-container: <nil>
STEP: delete the pod 09/20/23 13:47:32.675
Sep 20 13:47:32.685: INFO: Waiting for pod downward-api-320823b7-7993-478a-a4d7-8bca7e508fd2 to disappear
Sep 20 13:47:32.689: INFO: Pod downward-api-320823b7-7993-478a-a4d7-8bca7e508fd2 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Sep 20 13:47:32.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7478" for this suite. 09/20/23 13:47:32.693
------------------------------
â€¢ [SLOW TEST] [6.329 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:47:26.371
    Sep 20 13:47:26.371: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename downward-api 09/20/23 13:47:26.371
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:47:26.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:47:26.455
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 09/20/23 13:47:26.464
    Sep 20 13:47:26.570: INFO: Waiting up to 5m0s for pod "downward-api-320823b7-7993-478a-a4d7-8bca7e508fd2" in namespace "downward-api-7478" to be "Succeeded or Failed"
    Sep 20 13:47:26.577: INFO: Pod "downward-api-320823b7-7993-478a-a4d7-8bca7e508fd2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.852374ms
    Sep 20 13:47:28.583: INFO: Pod "downward-api-320823b7-7993-478a-a4d7-8bca7e508fd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011573454s
    Sep 20 13:47:30.629: INFO: Pod "downward-api-320823b7-7993-478a-a4d7-8bca7e508fd2": Phase="Running", Reason="", readiness=false. Elapsed: 4.05768827s
    Sep 20 13:47:32.666: INFO: Pod "downward-api-320823b7-7993-478a-a4d7-8bca7e508fd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.094308443s
    STEP: Saw pod success 09/20/23 13:47:32.666
    Sep 20 13:47:32.666: INFO: Pod "downward-api-320823b7-7993-478a-a4d7-8bca7e508fd2" satisfied condition "Succeeded or Failed"
    Sep 20 13:47:32.669: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downward-api-320823b7-7993-478a-a4d7-8bca7e508fd2 container dapi-container: <nil>
    STEP: delete the pod 09/20/23 13:47:32.675
    Sep 20 13:47:32.685: INFO: Waiting for pod downward-api-320823b7-7993-478a-a4d7-8bca7e508fd2 to disappear
    Sep 20 13:47:32.689: INFO: Pod downward-api-320823b7-7993-478a-a4d7-8bca7e508fd2 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:47:32.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7478" for this suite. 09/20/23 13:47:32.693
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:47:32.703
Sep 20 13:47:32.703: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename secrets 09/20/23 13:47:32.704
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:47:32.934
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:47:32.938
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-1cbbaf5a-cc7d-4b3b-b463-3caa63d4785d 09/20/23 13:47:32.942
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Sep 20 13:47:32.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2223" for this suite. 09/20/23 13:47:33.246
------------------------------
â€¢ [1.047 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:47:32.703
    Sep 20 13:47:32.703: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename secrets 09/20/23 13:47:32.704
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:47:32.934
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:47:32.938
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-1cbbaf5a-cc7d-4b3b-b463-3caa63d4785d 09/20/23 13:47:32.942
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:47:32.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2223" for this suite. 09/20/23 13:47:33.246
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:47:33.751
Sep 20 13:47:33.751: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename crd-webhook 09/20/23 13:47:33.752
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:47:34.42
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:47:34.424
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 09/20/23 13:47:34.428
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 09/20/23 13:47:34.689
STEP: Deploying the custom resource conversion webhook pod 09/20/23 13:47:34.725
STEP: Wait for the deployment to be ready 09/20/23 13:47:35.445
Sep 20 13:47:35.719: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Sep 20 13:47:37.735: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 47, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 47, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 47, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 47, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/20/23 13:47:40.131
STEP: Verifying the service has paired with the endpoint 09/20/23 13:47:40.471
Sep 20 13:47:41.472: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Sep 20 13:47:41.533: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Creating a v1 custom resource 09/20/23 13:47:44.266
STEP: v2 custom resource should be converted 09/20/23 13:47:44.575
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep 20 13:47:45.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-1795" for this suite. 09/20/23 13:47:45.455
------------------------------
â€¢ [SLOW TEST] [11.712 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:47:33.751
    Sep 20 13:47:33.751: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename crd-webhook 09/20/23 13:47:33.752
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:47:34.42
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:47:34.424
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 09/20/23 13:47:34.428
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 09/20/23 13:47:34.689
    STEP: Deploying the custom resource conversion webhook pod 09/20/23 13:47:34.725
    STEP: Wait for the deployment to be ready 09/20/23 13:47:35.445
    Sep 20 13:47:35.719: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    Sep 20 13:47:37.735: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 47, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 47, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 47, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 47, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/20/23 13:47:40.131
    STEP: Verifying the service has paired with the endpoint 09/20/23 13:47:40.471
    Sep 20 13:47:41.472: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Sep 20 13:47:41.533: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Creating a v1 custom resource 09/20/23 13:47:44.266
    STEP: v2 custom resource should be converted 09/20/23 13:47:44.575
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:47:45.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-1795" for this suite. 09/20/23 13:47:45.455
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:47:45.464
Sep 20 13:47:45.464: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename runtimeclass 09/20/23 13:47:45.465
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:47:46.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:47:46.291
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Sep 20 13:47:46.322: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-4776 to be scheduled
Sep 20 13:47:46.328: INFO: 1 pods are not scheduled: [runtimeclass-4776/test-runtimeclass-runtimeclass-4776-preconfigured-handler-8xv7h(75af2dca-9090-4d32-858b-73684e37bda3)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Sep 20 13:47:48.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-4776" for this suite. 09/20/23 13:47:48.373
------------------------------
â€¢ [3.003 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:47:45.464
    Sep 20 13:47:45.464: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename runtimeclass 09/20/23 13:47:45.465
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:47:46.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:47:46.291
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Sep 20 13:47:46.322: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-4776 to be scheduled
    Sep 20 13:47:46.328: INFO: 1 pods are not scheduled: [runtimeclass-4776/test-runtimeclass-runtimeclass-4776-preconfigured-handler-8xv7h(75af2dca-9090-4d32-858b-73684e37bda3)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:47:48.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-4776" for this suite. 09/20/23 13:47:48.373
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:47:48.469
Sep 20 13:47:48.469: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename services 09/20/23 13:47:48.469
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:47:50.594
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:47:50.6
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-4580 09/20/23 13:47:50.608
STEP: creating service affinity-nodeport in namespace services-4580 09/20/23 13:47:50.608
STEP: creating replication controller affinity-nodeport in namespace services-4580 09/20/23 13:47:50.865
I0920 13:47:51.098036      20 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-4580, replica count: 3
I0920 13:47:54.149533      20 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0920 13:47:57.149817      20 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 20 13:47:58.932: INFO: Creating new exec pod
Sep 20 13:47:59.237: INFO: Waiting up to 5m0s for pod "execpod-affinityt485j" in namespace "services-4580" to be "running"
Sep 20 13:47:59.563: INFO: Pod "execpod-affinityt485j": Phase="Pending", Reason="", readiness=false. Elapsed: 325.931989ms
Sep 20 13:48:01.601: INFO: Pod "execpod-affinityt485j": Phase="Pending", Reason="", readiness=false. Elapsed: 2.363883749s
Sep 20 13:48:03.571: INFO: Pod "execpod-affinityt485j": Phase="Pending", Reason="", readiness=false. Elapsed: 4.333957933s
Sep 20 13:48:06.133: INFO: Pod "execpod-affinityt485j": Phase="Pending", Reason="", readiness=false. Elapsed: 6.89584572s
Sep 20 13:48:07.702: INFO: Pod "execpod-affinityt485j": Phase="Pending", Reason="", readiness=false. Elapsed: 8.464917919s
Sep 20 13:48:10.106: INFO: Pod "execpod-affinityt485j": Phase="Running", Reason="", readiness=true. Elapsed: 10.86907576s
Sep 20 13:48:10.106: INFO: Pod "execpod-affinityt485j" satisfied condition "running"
Sep 20 13:48:11.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-4580 exec execpod-affinityt485j -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Sep 20 13:48:11.305: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Sep 20 13:48:11.305: INFO: stdout: ""
Sep 20 13:48:11.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-4580 exec execpod-affinityt485j -- /bin/sh -x -c nc -v -z -w 2 10.254.234.228 80'
Sep 20 13:48:11.496: INFO: stderr: "+ nc -v -z -w 2 10.254.234.228 80\nConnection to 10.254.234.228 80 port [tcp/http] succeeded!\n"
Sep 20 13:48:11.496: INFO: stdout: ""
Sep 20 13:48:11.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-4580 exec execpod-affinityt485j -- /bin/sh -x -c nc -v -z -w 2 192.168.10.64 31563'
Sep 20 13:48:11.721: INFO: stderr: "+ nc -v -z -w 2 192.168.10.64 31563\nConnection to 192.168.10.64 31563 port [tcp/*] succeeded!\n"
Sep 20 13:48:11.721: INFO: stdout: ""
Sep 20 13:48:11.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-4580 exec execpod-affinityt485j -- /bin/sh -x -c nc -v -z -w 2 192.168.10.172 31563'
Sep 20 13:48:11.955: INFO: stderr: "+ nc -v -z -w 2 192.168.10.172 31563\nConnection to 192.168.10.172 31563 port [tcp/*] succeeded!\n"
Sep 20 13:48:11.955: INFO: stdout: ""
Sep 20 13:48:11.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-4580 exec execpod-affinityt485j -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.10.173:31563/ ; done'
Sep 20 13:48:12.227: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31563/\n"
Sep 20 13:48:12.227: INFO: stdout: "\naffinity-nodeport-w949j\naffinity-nodeport-w949j\naffinity-nodeport-w949j\naffinity-nodeport-w949j\naffinity-nodeport-w949j\naffinity-nodeport-w949j\naffinity-nodeport-w949j\naffinity-nodeport-w949j\naffinity-nodeport-w949j\naffinity-nodeport-w949j\naffinity-nodeport-w949j\naffinity-nodeport-w949j\naffinity-nodeport-w949j\naffinity-nodeport-w949j\naffinity-nodeport-w949j\naffinity-nodeport-w949j"
Sep 20 13:48:12.227: INFO: Received response from host: affinity-nodeport-w949j
Sep 20 13:48:12.227: INFO: Received response from host: affinity-nodeport-w949j
Sep 20 13:48:12.227: INFO: Received response from host: affinity-nodeport-w949j
Sep 20 13:48:12.227: INFO: Received response from host: affinity-nodeport-w949j
Sep 20 13:48:12.227: INFO: Received response from host: affinity-nodeport-w949j
Sep 20 13:48:12.227: INFO: Received response from host: affinity-nodeport-w949j
Sep 20 13:48:12.227: INFO: Received response from host: affinity-nodeport-w949j
Sep 20 13:48:12.227: INFO: Received response from host: affinity-nodeport-w949j
Sep 20 13:48:12.227: INFO: Received response from host: affinity-nodeport-w949j
Sep 20 13:48:12.227: INFO: Received response from host: affinity-nodeport-w949j
Sep 20 13:48:12.227: INFO: Received response from host: affinity-nodeport-w949j
Sep 20 13:48:12.227: INFO: Received response from host: affinity-nodeport-w949j
Sep 20 13:48:12.227: INFO: Received response from host: affinity-nodeport-w949j
Sep 20 13:48:12.227: INFO: Received response from host: affinity-nodeport-w949j
Sep 20 13:48:12.227: INFO: Received response from host: affinity-nodeport-w949j
Sep 20 13:48:12.227: INFO: Received response from host: affinity-nodeport-w949j
Sep 20 13:48:12.228: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-4580, will wait for the garbage collector to delete the pods 09/20/23 13:48:12.512
Sep 20 13:48:12.573: INFO: Deleting ReplicationController affinity-nodeport took: 7.786075ms
Sep 20 13:48:12.674: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.698ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep 20 13:48:16.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4580" for this suite. 09/20/23 13:48:16.307
------------------------------
â€¢ [SLOW TEST] [27.849 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:47:48.469
    Sep 20 13:47:48.469: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename services 09/20/23 13:47:48.469
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:47:50.594
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:47:50.6
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-4580 09/20/23 13:47:50.608
    STEP: creating service affinity-nodeport in namespace services-4580 09/20/23 13:47:50.608
    STEP: creating replication controller affinity-nodeport in namespace services-4580 09/20/23 13:47:50.865
    I0920 13:47:51.098036      20 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-4580, replica count: 3
    I0920 13:47:54.149533      20 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0920 13:47:57.149817      20 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep 20 13:47:58.932: INFO: Creating new exec pod
    Sep 20 13:47:59.237: INFO: Waiting up to 5m0s for pod "execpod-affinityt485j" in namespace "services-4580" to be "running"
    Sep 20 13:47:59.563: INFO: Pod "execpod-affinityt485j": Phase="Pending", Reason="", readiness=false. Elapsed: 325.931989ms
    Sep 20 13:48:01.601: INFO: Pod "execpod-affinityt485j": Phase="Pending", Reason="", readiness=false. Elapsed: 2.363883749s
    Sep 20 13:48:03.571: INFO: Pod "execpod-affinityt485j": Phase="Pending", Reason="", readiness=false. Elapsed: 4.333957933s
    Sep 20 13:48:06.133: INFO: Pod "execpod-affinityt485j": Phase="Pending", Reason="", readiness=false. Elapsed: 6.89584572s
    Sep 20 13:48:07.702: INFO: Pod "execpod-affinityt485j": Phase="Pending", Reason="", readiness=false. Elapsed: 8.464917919s
    Sep 20 13:48:10.106: INFO: Pod "execpod-affinityt485j": Phase="Running", Reason="", readiness=true. Elapsed: 10.86907576s
    Sep 20 13:48:10.106: INFO: Pod "execpod-affinityt485j" satisfied condition "running"
    Sep 20 13:48:11.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-4580 exec execpod-affinityt485j -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Sep 20 13:48:11.305: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Sep 20 13:48:11.305: INFO: stdout: ""
    Sep 20 13:48:11.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-4580 exec execpod-affinityt485j -- /bin/sh -x -c nc -v -z -w 2 10.254.234.228 80'
    Sep 20 13:48:11.496: INFO: stderr: "+ nc -v -z -w 2 10.254.234.228 80\nConnection to 10.254.234.228 80 port [tcp/http] succeeded!\n"
    Sep 20 13:48:11.496: INFO: stdout: ""
    Sep 20 13:48:11.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-4580 exec execpod-affinityt485j -- /bin/sh -x -c nc -v -z -w 2 192.168.10.64 31563'
    Sep 20 13:48:11.721: INFO: stderr: "+ nc -v -z -w 2 192.168.10.64 31563\nConnection to 192.168.10.64 31563 port [tcp/*] succeeded!\n"
    Sep 20 13:48:11.721: INFO: stdout: ""
    Sep 20 13:48:11.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-4580 exec execpod-affinityt485j -- /bin/sh -x -c nc -v -z -w 2 192.168.10.172 31563'
    Sep 20 13:48:11.955: INFO: stderr: "+ nc -v -z -w 2 192.168.10.172 31563\nConnection to 192.168.10.172 31563 port [tcp/*] succeeded!\n"
    Sep 20 13:48:11.955: INFO: stdout: ""
    Sep 20 13:48:11.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-4580 exec execpod-affinityt485j -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.10.173:31563/ ; done'
    Sep 20 13:48:12.227: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.10.173:31563/\n"
    Sep 20 13:48:12.227: INFO: stdout: "\naffinity-nodeport-w949j\naffinity-nodeport-w949j\naffinity-nodeport-w949j\naffinity-nodeport-w949j\naffinity-nodeport-w949j\naffinity-nodeport-w949j\naffinity-nodeport-w949j\naffinity-nodeport-w949j\naffinity-nodeport-w949j\naffinity-nodeport-w949j\naffinity-nodeport-w949j\naffinity-nodeport-w949j\naffinity-nodeport-w949j\naffinity-nodeport-w949j\naffinity-nodeport-w949j\naffinity-nodeport-w949j"
    Sep 20 13:48:12.227: INFO: Received response from host: affinity-nodeport-w949j
    Sep 20 13:48:12.227: INFO: Received response from host: affinity-nodeport-w949j
    Sep 20 13:48:12.227: INFO: Received response from host: affinity-nodeport-w949j
    Sep 20 13:48:12.227: INFO: Received response from host: affinity-nodeport-w949j
    Sep 20 13:48:12.227: INFO: Received response from host: affinity-nodeport-w949j
    Sep 20 13:48:12.227: INFO: Received response from host: affinity-nodeport-w949j
    Sep 20 13:48:12.227: INFO: Received response from host: affinity-nodeport-w949j
    Sep 20 13:48:12.227: INFO: Received response from host: affinity-nodeport-w949j
    Sep 20 13:48:12.227: INFO: Received response from host: affinity-nodeport-w949j
    Sep 20 13:48:12.227: INFO: Received response from host: affinity-nodeport-w949j
    Sep 20 13:48:12.227: INFO: Received response from host: affinity-nodeport-w949j
    Sep 20 13:48:12.227: INFO: Received response from host: affinity-nodeport-w949j
    Sep 20 13:48:12.227: INFO: Received response from host: affinity-nodeport-w949j
    Sep 20 13:48:12.227: INFO: Received response from host: affinity-nodeport-w949j
    Sep 20 13:48:12.227: INFO: Received response from host: affinity-nodeport-w949j
    Sep 20 13:48:12.227: INFO: Received response from host: affinity-nodeport-w949j
    Sep 20 13:48:12.228: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-4580, will wait for the garbage collector to delete the pods 09/20/23 13:48:12.512
    Sep 20 13:48:12.573: INFO: Deleting ReplicationController affinity-nodeport took: 7.786075ms
    Sep 20 13:48:12.674: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.698ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:48:16.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4580" for this suite. 09/20/23 13:48:16.307
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:48:16.318
Sep 20 13:48:16.318: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename job 09/20/23 13:48:16.319
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:48:16.372
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:48:16.377
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 09/20/23 13:48:16.385
STEP: Ensuring active pods == parallelism 09/20/23 13:48:17.088
STEP: delete a job 09/20/23 13:48:23.188
STEP: deleting Job.batch foo in namespace job-7293, will wait for the garbage collector to delete the pods 09/20/23 13:48:23.189
Sep 20 13:48:23.259: INFO: Deleting Job.batch foo took: 7.107629ms
Sep 20 13:48:23.460: INFO: Terminating Job.batch foo pods took: 201.061588ms
STEP: Ensuring job was deleted 09/20/23 13:48:55.761
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep 20 13:48:55.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-7293" for this suite. 09/20/23 13:48:57.037
------------------------------
â€¢ [SLOW TEST] [40.873 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:48:16.318
    Sep 20 13:48:16.318: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename job 09/20/23 13:48:16.319
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:48:16.372
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:48:16.377
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 09/20/23 13:48:16.385
    STEP: Ensuring active pods == parallelism 09/20/23 13:48:17.088
    STEP: delete a job 09/20/23 13:48:23.188
    STEP: deleting Job.batch foo in namespace job-7293, will wait for the garbage collector to delete the pods 09/20/23 13:48:23.189
    Sep 20 13:48:23.259: INFO: Deleting Job.batch foo took: 7.107629ms
    Sep 20 13:48:23.460: INFO: Terminating Job.batch foo pods took: 201.061588ms
    STEP: Ensuring job was deleted 09/20/23 13:48:55.761
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:48:55.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-7293" for this suite. 09/20/23 13:48:57.037
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:48:57.192
Sep 20 13:48:57.192: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename projected 09/20/23 13:48:57.193
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:48:57.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:48:57.292
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 09/20/23 13:48:57.297
Sep 20 13:48:58.132: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a1827c77-6b89-4762-874c-875d316fd08a" in namespace "projected-5447" to be "Succeeded or Failed"
Sep 20 13:48:58.138: INFO: Pod "downwardapi-volume-a1827c77-6b89-4762-874c-875d316fd08a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.72256ms
Sep 20 13:49:00.382: INFO: Pod "downwardapi-volume-a1827c77-6b89-4762-874c-875d316fd08a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.249971363s
Sep 20 13:49:02.202: INFO: Pod "downwardapi-volume-a1827c77-6b89-4762-874c-875d316fd08a": Phase="Running", Reason="", readiness=true. Elapsed: 4.069125981s
Sep 20 13:49:04.145: INFO: Pod "downwardapi-volume-a1827c77-6b89-4762-874c-875d316fd08a": Phase="Running", Reason="", readiness=false. Elapsed: 6.012622092s
Sep 20 13:49:06.188: INFO: Pod "downwardapi-volume-a1827c77-6b89-4762-874c-875d316fd08a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.055464288s
STEP: Saw pod success 09/20/23 13:49:06.188
Sep 20 13:49:06.188: INFO: Pod "downwardapi-volume-a1827c77-6b89-4762-874c-875d316fd08a" satisfied condition "Succeeded or Failed"
Sep 20 13:49:06.192: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downwardapi-volume-a1827c77-6b89-4762-874c-875d316fd08a container client-container: <nil>
STEP: delete the pod 09/20/23 13:49:06.243
Sep 20 13:49:07.455: INFO: Waiting for pod downwardapi-volume-a1827c77-6b89-4762-874c-875d316fd08a to disappear
Sep 20 13:49:07.469: INFO: Pod downwardapi-volume-a1827c77-6b89-4762-874c-875d316fd08a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep 20 13:49:07.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5447" for this suite. 09/20/23 13:49:07.474
------------------------------
â€¢ [SLOW TEST] [10.292 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:48:57.192
    Sep 20 13:48:57.192: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename projected 09/20/23 13:48:57.193
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:48:57.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:48:57.292
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 09/20/23 13:48:57.297
    Sep 20 13:48:58.132: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a1827c77-6b89-4762-874c-875d316fd08a" in namespace "projected-5447" to be "Succeeded or Failed"
    Sep 20 13:48:58.138: INFO: Pod "downwardapi-volume-a1827c77-6b89-4762-874c-875d316fd08a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.72256ms
    Sep 20 13:49:00.382: INFO: Pod "downwardapi-volume-a1827c77-6b89-4762-874c-875d316fd08a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.249971363s
    Sep 20 13:49:02.202: INFO: Pod "downwardapi-volume-a1827c77-6b89-4762-874c-875d316fd08a": Phase="Running", Reason="", readiness=true. Elapsed: 4.069125981s
    Sep 20 13:49:04.145: INFO: Pod "downwardapi-volume-a1827c77-6b89-4762-874c-875d316fd08a": Phase="Running", Reason="", readiness=false. Elapsed: 6.012622092s
    Sep 20 13:49:06.188: INFO: Pod "downwardapi-volume-a1827c77-6b89-4762-874c-875d316fd08a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.055464288s
    STEP: Saw pod success 09/20/23 13:49:06.188
    Sep 20 13:49:06.188: INFO: Pod "downwardapi-volume-a1827c77-6b89-4762-874c-875d316fd08a" satisfied condition "Succeeded or Failed"
    Sep 20 13:49:06.192: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downwardapi-volume-a1827c77-6b89-4762-874c-875d316fd08a container client-container: <nil>
    STEP: delete the pod 09/20/23 13:49:06.243
    Sep 20 13:49:07.455: INFO: Waiting for pod downwardapi-volume-a1827c77-6b89-4762-874c-875d316fd08a to disappear
    Sep 20 13:49:07.469: INFO: Pod downwardapi-volume-a1827c77-6b89-4762-874c-875d316fd08a no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:49:07.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5447" for this suite. 09/20/23 13:49:07.474
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:49:07.489
Sep 20 13:49:07.489: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename secrets 09/20/23 13:49:07.49
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:49:07.51
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:49:07.514
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-39c4b8d6-a133-41bb-ba1e-f3a9e280bae2 09/20/23 13:49:07.519
STEP: Creating a pod to test consume secrets 09/20/23 13:49:07.526
Sep 20 13:49:07.536: INFO: Waiting up to 5m0s for pod "pod-secrets-6e5fef17-e199-44a7-a244-7f1bb13111b6" in namespace "secrets-7756" to be "Succeeded or Failed"
Sep 20 13:49:07.543: INFO: Pod "pod-secrets-6e5fef17-e199-44a7-a244-7f1bb13111b6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.385772ms
Sep 20 13:49:09.550: INFO: Pod "pod-secrets-6e5fef17-e199-44a7-a244-7f1bb13111b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01420311s
Sep 20 13:49:11.551: INFO: Pod "pod-secrets-6e5fef17-e199-44a7-a244-7f1bb13111b6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015274173s
Sep 20 13:49:13.562: INFO: Pod "pod-secrets-6e5fef17-e199-44a7-a244-7f1bb13111b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026670324s
STEP: Saw pod success 09/20/23 13:49:13.562
Sep 20 13:49:13.563: INFO: Pod "pod-secrets-6e5fef17-e199-44a7-a244-7f1bb13111b6" satisfied condition "Succeeded or Failed"
Sep 20 13:49:13.566: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-secrets-6e5fef17-e199-44a7-a244-7f1bb13111b6 container secret-env-test: <nil>
STEP: delete the pod 09/20/23 13:49:13.575
Sep 20 13:49:14.111: INFO: Waiting for pod pod-secrets-6e5fef17-e199-44a7-a244-7f1bb13111b6 to disappear
Sep 20 13:49:14.537: INFO: Pod pod-secrets-6e5fef17-e199-44a7-a244-7f1bb13111b6 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Sep 20 13:49:14.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7756" for this suite. 09/20/23 13:49:14.542
------------------------------
â€¢ [SLOW TEST] [7.116 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:49:07.489
    Sep 20 13:49:07.489: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename secrets 09/20/23 13:49:07.49
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:49:07.51
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:49:07.514
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-39c4b8d6-a133-41bb-ba1e-f3a9e280bae2 09/20/23 13:49:07.519
    STEP: Creating a pod to test consume secrets 09/20/23 13:49:07.526
    Sep 20 13:49:07.536: INFO: Waiting up to 5m0s for pod "pod-secrets-6e5fef17-e199-44a7-a244-7f1bb13111b6" in namespace "secrets-7756" to be "Succeeded or Failed"
    Sep 20 13:49:07.543: INFO: Pod "pod-secrets-6e5fef17-e199-44a7-a244-7f1bb13111b6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.385772ms
    Sep 20 13:49:09.550: INFO: Pod "pod-secrets-6e5fef17-e199-44a7-a244-7f1bb13111b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01420311s
    Sep 20 13:49:11.551: INFO: Pod "pod-secrets-6e5fef17-e199-44a7-a244-7f1bb13111b6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015274173s
    Sep 20 13:49:13.562: INFO: Pod "pod-secrets-6e5fef17-e199-44a7-a244-7f1bb13111b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026670324s
    STEP: Saw pod success 09/20/23 13:49:13.562
    Sep 20 13:49:13.563: INFO: Pod "pod-secrets-6e5fef17-e199-44a7-a244-7f1bb13111b6" satisfied condition "Succeeded or Failed"
    Sep 20 13:49:13.566: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-secrets-6e5fef17-e199-44a7-a244-7f1bb13111b6 container secret-env-test: <nil>
    STEP: delete the pod 09/20/23 13:49:13.575
    Sep 20 13:49:14.111: INFO: Waiting for pod pod-secrets-6e5fef17-e199-44a7-a244-7f1bb13111b6 to disappear
    Sep 20 13:49:14.537: INFO: Pod pod-secrets-6e5fef17-e199-44a7-a244-7f1bb13111b6 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:49:14.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7756" for this suite. 09/20/23 13:49:14.542
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:49:14.607
Sep 20 13:49:14.607: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename dns 09/20/23 13:49:14.608
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:49:15.034
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:49:15.039
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 09/20/23 13:49:15.048
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2065.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2065.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2065.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2065.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2065.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2065.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2065.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2065.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2065.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2065.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2065.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2065.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 67.244.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.244.67_udp@PTR;check="$$(dig +tcp +noall +answer +search 67.244.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.244.67_tcp@PTR;sleep 1; done
 09/20/23 13:49:15.136
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2065.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2065.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2065.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2065.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2065.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2065.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2065.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2065.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2065.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2065.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2065.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2065.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 67.244.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.244.67_udp@PTR;check="$$(dig +tcp +noall +answer +search 67.244.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.244.67_tcp@PTR;sleep 1; done
 09/20/23 13:49:15.136
STEP: creating a pod to probe DNS 09/20/23 13:49:15.136
STEP: submitting the pod to kubernetes 09/20/23 13:49:15.136
Sep 20 13:49:15.166: INFO: Waiting up to 15m0s for pod "dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f" in namespace "dns-2065" to be "running"
Sep 20 13:49:15.178: INFO: Pod "dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.79132ms
Sep 20 13:49:17.377: INFO: Pod "dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.210732821s
Sep 20 13:49:19.463: INFO: Pod "dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f": Phase="Running", Reason="", readiness=true. Elapsed: 4.296715167s
Sep 20 13:49:19.463: INFO: Pod "dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f" satisfied condition "running"
STEP: retrieving the pod 09/20/23 13:49:19.463
STEP: looking for the results for each expected name from probers 09/20/23 13:49:19.468
Sep 20 13:49:19.478: INFO: Unable to read wheezy_udp@dns-test-service.dns-2065.svc.cluster.local from pod dns-2065/dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f: the server could not find the requested resource (get pods dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f)
Sep 20 13:49:19.654: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2065.svc.cluster.local from pod dns-2065/dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f: the server could not find the requested resource (get pods dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f)
Sep 20 13:49:19.658: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2065.svc.cluster.local from pod dns-2065/dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f: the server could not find the requested resource (get pods dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f)
Sep 20 13:49:19.669: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2065.svc.cluster.local from pod dns-2065/dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f: the server could not find the requested resource (get pods dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f)
Sep 20 13:49:19.698: INFO: Unable to read jessie_udp@dns-test-service.dns-2065.svc.cluster.local from pod dns-2065/dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f: the server could not find the requested resource (get pods dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f)
Sep 20 13:49:19.703: INFO: Unable to read jessie_tcp@dns-test-service.dns-2065.svc.cluster.local from pod dns-2065/dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f: the server could not find the requested resource (get pods dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f)
Sep 20 13:49:19.709: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2065.svc.cluster.local from pod dns-2065/dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f: the server could not find the requested resource (get pods dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f)
Sep 20 13:49:19.713: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2065.svc.cluster.local from pod dns-2065/dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f: the server could not find the requested resource (get pods dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f)
Sep 20 13:49:19.733: INFO: Lookups using dns-2065/dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f failed for: [wheezy_udp@dns-test-service.dns-2065.svc.cluster.local wheezy_tcp@dns-test-service.dns-2065.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2065.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2065.svc.cluster.local jessie_udp@dns-test-service.dns-2065.svc.cluster.local jessie_tcp@dns-test-service.dns-2065.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2065.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2065.svc.cluster.local]

Sep 20 13:49:24.777: INFO: DNS probes using dns-2065/dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f succeeded

STEP: deleting the pod 09/20/23 13:49:24.777
STEP: deleting the test service 09/20/23 13:49:25.01
STEP: deleting the test headless service 09/20/23 13:49:25.065
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep 20 13:49:25.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-2065" for this suite. 09/20/23 13:49:25.09
------------------------------
â€¢ [SLOW TEST] [10.538 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:49:14.607
    Sep 20 13:49:14.607: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename dns 09/20/23 13:49:14.608
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:49:15.034
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:49:15.039
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 09/20/23 13:49:15.048
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2065.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2065.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2065.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2065.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2065.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2065.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2065.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2065.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2065.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2065.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2065.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2065.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 67.244.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.244.67_udp@PTR;check="$$(dig +tcp +noall +answer +search 67.244.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.244.67_tcp@PTR;sleep 1; done
     09/20/23 13:49:15.136
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2065.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2065.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2065.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2065.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2065.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2065.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2065.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2065.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2065.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2065.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2065.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2065.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 67.244.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.244.67_udp@PTR;check="$$(dig +tcp +noall +answer +search 67.244.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.244.67_tcp@PTR;sleep 1; done
     09/20/23 13:49:15.136
    STEP: creating a pod to probe DNS 09/20/23 13:49:15.136
    STEP: submitting the pod to kubernetes 09/20/23 13:49:15.136
    Sep 20 13:49:15.166: INFO: Waiting up to 15m0s for pod "dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f" in namespace "dns-2065" to be "running"
    Sep 20 13:49:15.178: INFO: Pod "dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.79132ms
    Sep 20 13:49:17.377: INFO: Pod "dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.210732821s
    Sep 20 13:49:19.463: INFO: Pod "dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f": Phase="Running", Reason="", readiness=true. Elapsed: 4.296715167s
    Sep 20 13:49:19.463: INFO: Pod "dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f" satisfied condition "running"
    STEP: retrieving the pod 09/20/23 13:49:19.463
    STEP: looking for the results for each expected name from probers 09/20/23 13:49:19.468
    Sep 20 13:49:19.478: INFO: Unable to read wheezy_udp@dns-test-service.dns-2065.svc.cluster.local from pod dns-2065/dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f: the server could not find the requested resource (get pods dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f)
    Sep 20 13:49:19.654: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2065.svc.cluster.local from pod dns-2065/dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f: the server could not find the requested resource (get pods dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f)
    Sep 20 13:49:19.658: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2065.svc.cluster.local from pod dns-2065/dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f: the server could not find the requested resource (get pods dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f)
    Sep 20 13:49:19.669: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2065.svc.cluster.local from pod dns-2065/dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f: the server could not find the requested resource (get pods dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f)
    Sep 20 13:49:19.698: INFO: Unable to read jessie_udp@dns-test-service.dns-2065.svc.cluster.local from pod dns-2065/dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f: the server could not find the requested resource (get pods dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f)
    Sep 20 13:49:19.703: INFO: Unable to read jessie_tcp@dns-test-service.dns-2065.svc.cluster.local from pod dns-2065/dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f: the server could not find the requested resource (get pods dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f)
    Sep 20 13:49:19.709: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2065.svc.cluster.local from pod dns-2065/dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f: the server could not find the requested resource (get pods dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f)
    Sep 20 13:49:19.713: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2065.svc.cluster.local from pod dns-2065/dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f: the server could not find the requested resource (get pods dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f)
    Sep 20 13:49:19.733: INFO: Lookups using dns-2065/dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f failed for: [wheezy_udp@dns-test-service.dns-2065.svc.cluster.local wheezy_tcp@dns-test-service.dns-2065.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2065.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2065.svc.cluster.local jessie_udp@dns-test-service.dns-2065.svc.cluster.local jessie_tcp@dns-test-service.dns-2065.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2065.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2065.svc.cluster.local]

    Sep 20 13:49:24.777: INFO: DNS probes using dns-2065/dns-test-9e47eba6-1fd2-462c-8c20-bdbd733ee91f succeeded

    STEP: deleting the pod 09/20/23 13:49:24.777
    STEP: deleting the test service 09/20/23 13:49:25.01
    STEP: deleting the test headless service 09/20/23 13:49:25.065
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:49:25.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-2065" for this suite. 09/20/23 13:49:25.09
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:49:25.145
Sep 20 13:49:25.146: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename sched-preemption 09/20/23 13:49:25.147
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:49:25.191
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:49:25.195
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Sep 20 13:49:25.248: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 20 13:50:25.286: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 09/20/23 13:50:25.29
Sep 20 13:50:25.345: INFO: Created pod: pod0-0-sched-preemption-low-priority
Sep 20 13:50:25.373: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Sep 20 13:50:25.566: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Sep 20 13:50:25.854: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Sep 20 13:50:26.094: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Sep 20 13:50:26.109: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 09/20/23 13:50:26.109
Sep 20 13:50:26.109: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5051" to be "running"
Sep 20 13:50:26.114: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.542881ms
Sep 20 13:50:28.132: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023339197s
Sep 20 13:50:30.121: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.011612454s
Sep 20 13:50:30.121: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Sep 20 13:50:30.121: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5051" to be "running"
Sep 20 13:50:30.125: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.942706ms
Sep 20 13:50:30.125: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Sep 20 13:50:30.125: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5051" to be "running"
Sep 20 13:50:30.128: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.279077ms
Sep 20 13:50:30.128: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Sep 20 13:50:30.128: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5051" to be "running"
Sep 20 13:50:30.132: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.607426ms
Sep 20 13:50:30.132: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Sep 20 13:50:30.132: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-5051" to be "running"
Sep 20 13:50:30.137: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.003284ms
Sep 20 13:50:30.137: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Sep 20 13:50:30.137: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-5051" to be "running"
Sep 20 13:50:30.278: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 141.081369ms
Sep 20 13:50:30.278: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 09/20/23 13:50:30.278
Sep 20 13:50:30.389: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-5051" to be "running"
Sep 20 13:50:30.394: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.626636ms
Sep 20 13:50:33.179: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.790226972s
Sep 20 13:50:34.899: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.5101598s
Sep 20 13:50:37.545: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.156027308s
Sep 20 13:50:39.176: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.787159599s
Sep 20 13:50:40.536: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.146831805s
Sep 20 13:50:42.398: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 12.0095658s
Sep 20 13:50:42.399: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Sep 20 13:50:42.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-5051" for this suite. 09/20/23 13:50:43.129
------------------------------
â€¢ [SLOW TEST] [77.992 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:49:25.145
    Sep 20 13:49:25.146: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename sched-preemption 09/20/23 13:49:25.147
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:49:25.191
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:49:25.195
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Sep 20 13:49:25.248: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep 20 13:50:25.286: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 09/20/23 13:50:25.29
    Sep 20 13:50:25.345: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Sep 20 13:50:25.373: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Sep 20 13:50:25.566: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Sep 20 13:50:25.854: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Sep 20 13:50:26.094: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Sep 20 13:50:26.109: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 09/20/23 13:50:26.109
    Sep 20 13:50:26.109: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5051" to be "running"
    Sep 20 13:50:26.114: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.542881ms
    Sep 20 13:50:28.132: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023339197s
    Sep 20 13:50:30.121: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.011612454s
    Sep 20 13:50:30.121: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Sep 20 13:50:30.121: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5051" to be "running"
    Sep 20 13:50:30.125: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.942706ms
    Sep 20 13:50:30.125: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Sep 20 13:50:30.125: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5051" to be "running"
    Sep 20 13:50:30.128: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.279077ms
    Sep 20 13:50:30.128: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Sep 20 13:50:30.128: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5051" to be "running"
    Sep 20 13:50:30.132: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.607426ms
    Sep 20 13:50:30.132: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Sep 20 13:50:30.132: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-5051" to be "running"
    Sep 20 13:50:30.137: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.003284ms
    Sep 20 13:50:30.137: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Sep 20 13:50:30.137: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-5051" to be "running"
    Sep 20 13:50:30.278: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 141.081369ms
    Sep 20 13:50:30.278: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 09/20/23 13:50:30.278
    Sep 20 13:50:30.389: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-5051" to be "running"
    Sep 20 13:50:30.394: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.626636ms
    Sep 20 13:50:33.179: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.790226972s
    Sep 20 13:50:34.899: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.5101598s
    Sep 20 13:50:37.545: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.156027308s
    Sep 20 13:50:39.176: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.787159599s
    Sep 20 13:50:40.536: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.146831805s
    Sep 20 13:50:42.398: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 12.0095658s
    Sep 20 13:50:42.399: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:50:42.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-5051" for this suite. 09/20/23 13:50:43.129
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:50:43.139
Sep 20 13:50:43.139: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename daemonsets 09/20/23 13:50:43.14
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:50:43.176
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:50:43.179
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177
STEP: Creating simple DaemonSet "daemon-set" 09/20/23 13:50:43.219
STEP: Check that daemon pods launch on every node of the cluster. 09/20/23 13:50:43.333
Sep 20 13:50:43.379: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:43.379: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:43.379: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:43.401: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 13:50:43.401: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 13:50:44.737: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:44.737: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:44.737: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:44.763: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 13:50:44.764: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 13:50:45.406: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:45.406: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:45.406: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:45.409: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 13:50:45.409: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 13:50:46.411: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:46.411: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:46.411: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:46.417: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep 20 13:50:46.417: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 13:50:47.406: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:47.406: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:47.406: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:47.408: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep 20 13:50:47.408: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 13:50:48.437: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:48.437: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:48.437: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:48.455: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep 20 13:50:48.455: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 13:50:49.429: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:49.429: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:49.429: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:49.432: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Sep 20 13:50:49.432: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 09/20/23 13:50:49.436
Sep 20 13:50:49.804: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:49.804: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:49.804: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:49.807: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep 20 13:50:49.807: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 13:50:50.815: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:50.815: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:50.815: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:50.819: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep 20 13:50:50.819: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 13:50:51.823: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:51.823: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:51.823: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:51.826: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep 20 13:50:51.826: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 13:50:52.824: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:52.824: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:52.824: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:52.827: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep 20 13:50:52.827: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 13:50:53.963: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:53.963: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:53.963: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:53.967: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep 20 13:50:53.967: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 13:50:55.100: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:55.100: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:55.100: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:55.267: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep 20 13:50:55.267: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 13:50:56.031: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:56.032: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:56.032: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:56.037: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep 20 13:50:56.037: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 13:50:56.814: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:56.814: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:56.814: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:56.816: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep 20 13:50:56.816: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 13:50:58.174: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:58.174: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:58.175: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:58.419: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep 20 13:50:58.419: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 13:50:59.203: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:59.203: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:59.203: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:50:59.295: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Sep 20 13:50:59.295: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 09/20/23 13:50:59.404
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4365, will wait for the garbage collector to delete the pods 09/20/23 13:50:59.404
Sep 20 13:50:59.595: INFO: Deleting DaemonSet.extensions daemon-set took: 105.339712ms
Sep 20 13:50:59.795: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.332566ms
Sep 20 13:51:03.402: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 13:51:03.402: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep 20 13:51:03.405: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"46800"},"items":null}

Sep 20 13:51:03.407: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"46800"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep 20 13:51:04.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4365" for this suite. 09/20/23 13:51:04.342
------------------------------
â€¢ [SLOW TEST] [21.222 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:50:43.139
    Sep 20 13:50:43.139: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename daemonsets 09/20/23 13:50:43.14
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:50:43.176
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:50:43.179
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:177
    STEP: Creating simple DaemonSet "daemon-set" 09/20/23 13:50:43.219
    STEP: Check that daemon pods launch on every node of the cluster. 09/20/23 13:50:43.333
    Sep 20 13:50:43.379: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:43.379: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:43.379: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:43.401: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 13:50:43.401: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 13:50:44.737: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:44.737: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:44.737: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:44.763: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 13:50:44.764: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 13:50:45.406: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:45.406: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:45.406: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:45.409: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 13:50:45.409: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 13:50:46.411: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:46.411: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:46.411: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:46.417: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep 20 13:50:46.417: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 13:50:47.406: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:47.406: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:47.406: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:47.408: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep 20 13:50:47.408: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 13:50:48.437: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:48.437: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:48.437: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:48.455: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep 20 13:50:48.455: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 13:50:49.429: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:49.429: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:49.429: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:49.432: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Sep 20 13:50:49.432: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 09/20/23 13:50:49.436
    Sep 20 13:50:49.804: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:49.804: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:49.804: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:49.807: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep 20 13:50:49.807: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 13:50:50.815: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:50.815: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:50.815: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:50.819: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep 20 13:50:50.819: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 13:50:51.823: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:51.823: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:51.823: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:51.826: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep 20 13:50:51.826: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 13:50:52.824: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:52.824: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:52.824: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:52.827: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep 20 13:50:52.827: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 13:50:53.963: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:53.963: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:53.963: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:53.967: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep 20 13:50:53.967: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 13:50:55.100: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:55.100: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:55.100: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:55.267: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep 20 13:50:55.267: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 13:50:56.031: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:56.032: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:56.032: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:56.037: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep 20 13:50:56.037: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 13:50:56.814: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:56.814: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:56.814: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:56.816: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep 20 13:50:56.816: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 13:50:58.174: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:58.174: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:58.175: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:58.419: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep 20 13:50:58.419: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 13:50:59.203: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:59.203: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:59.203: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:50:59.295: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Sep 20 13:50:59.295: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 09/20/23 13:50:59.404
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4365, will wait for the garbage collector to delete the pods 09/20/23 13:50:59.404
    Sep 20 13:50:59.595: INFO: Deleting DaemonSet.extensions daemon-set took: 105.339712ms
    Sep 20 13:50:59.795: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.332566ms
    Sep 20 13:51:03.402: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 13:51:03.402: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep 20 13:51:03.405: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"46800"},"items":null}

    Sep 20 13:51:03.407: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"46800"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:51:04.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4365" for this suite. 09/20/23 13:51:04.342
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:51:04.363
Sep 20 13:51:04.363: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename kubectl 09/20/23 13:51:04.364
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:51:04.545
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:51:04.55
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 09/20/23 13:51:04.556
Sep 20 13:51:04.556: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8419 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 09/20/23 13:51:04.599
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep 20 13:51:04.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8419" for this suite. 09/20/23 13:51:04.632
------------------------------
â€¢ [0.279 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:51:04.363
    Sep 20 13:51:04.363: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename kubectl 09/20/23 13:51:04.364
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:51:04.545
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:51:04.55
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 09/20/23 13:51:04.556
    Sep 20 13:51:04.556: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-8419 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 09/20/23 13:51:04.599
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:51:04.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8419" for this suite. 09/20/23 13:51:04.632
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:51:04.642
Sep 20 13:51:04.642: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename services 09/20/23 13:51:04.643
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:51:04.712
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:51:04.715
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-2972 09/20/23 13:51:04.721
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 09/20/23 13:51:04.832
STEP: creating service externalsvc in namespace services-2972 09/20/23 13:51:04.832
STEP: creating replication controller externalsvc in namespace services-2972 09/20/23 13:51:05.421
I0920 13:51:05.650696      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2972, replica count: 2
I0920 13:51:08.702823      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0920 13:51:11.703074      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 09/20/23 13:51:11.826
Sep 20 13:51:12.306: INFO: Creating new exec pod
Sep 20 13:51:13.254: INFO: Waiting up to 5m0s for pod "execpoddv8lh" in namespace "services-2972" to be "running"
Sep 20 13:51:13.437: INFO: Pod "execpoddv8lh": Phase="Pending", Reason="", readiness=false. Elapsed: 182.590548ms
Sep 20 13:51:15.441: INFO: Pod "execpoddv8lh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.186753363s
Sep 20 13:51:17.448: INFO: Pod "execpoddv8lh": Phase="Running", Reason="", readiness=true. Elapsed: 4.193481663s
Sep 20 13:51:17.448: INFO: Pod "execpoddv8lh" satisfied condition "running"
Sep 20 13:51:17.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-2972 exec execpoddv8lh -- /bin/sh -x -c nslookup nodeport-service.services-2972.svc.cluster.local'
Sep 20 13:51:17.685: INFO: stderr: "+ nslookup nodeport-service.services-2972.svc.cluster.local\n"
Sep 20 13:51:17.685: INFO: stdout: "Server:\t\t10.254.0.10\nAddress:\t10.254.0.10#53\n\nnodeport-service.services-2972.svc.cluster.local\tcanonical name = externalsvc.services-2972.svc.cluster.local.\nName:\texternalsvc.services-2972.svc.cluster.local\nAddress: 10.254.230.248\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2972, will wait for the garbage collector to delete the pods 09/20/23 13:51:17.685
Sep 20 13:51:17.749: INFO: Deleting ReplicationController externalsvc took: 7.211214ms
Sep 20 13:51:18.650: INFO: Terminating ReplicationController externalsvc pods took: 900.99971ms
Sep 20 13:51:23.082: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep 20 13:51:23.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2972" for this suite. 09/20/23 13:51:23.292
------------------------------
â€¢ [SLOW TEST] [18.670 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:51:04.642
    Sep 20 13:51:04.642: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename services 09/20/23 13:51:04.643
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:51:04.712
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:51:04.715
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-2972 09/20/23 13:51:04.721
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 09/20/23 13:51:04.832
    STEP: creating service externalsvc in namespace services-2972 09/20/23 13:51:04.832
    STEP: creating replication controller externalsvc in namespace services-2972 09/20/23 13:51:05.421
    I0920 13:51:05.650696      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2972, replica count: 2
    I0920 13:51:08.702823      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0920 13:51:11.703074      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 09/20/23 13:51:11.826
    Sep 20 13:51:12.306: INFO: Creating new exec pod
    Sep 20 13:51:13.254: INFO: Waiting up to 5m0s for pod "execpoddv8lh" in namespace "services-2972" to be "running"
    Sep 20 13:51:13.437: INFO: Pod "execpoddv8lh": Phase="Pending", Reason="", readiness=false. Elapsed: 182.590548ms
    Sep 20 13:51:15.441: INFO: Pod "execpoddv8lh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.186753363s
    Sep 20 13:51:17.448: INFO: Pod "execpoddv8lh": Phase="Running", Reason="", readiness=true. Elapsed: 4.193481663s
    Sep 20 13:51:17.448: INFO: Pod "execpoddv8lh" satisfied condition "running"
    Sep 20 13:51:17.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-2972 exec execpoddv8lh -- /bin/sh -x -c nslookup nodeport-service.services-2972.svc.cluster.local'
    Sep 20 13:51:17.685: INFO: stderr: "+ nslookup nodeport-service.services-2972.svc.cluster.local\n"
    Sep 20 13:51:17.685: INFO: stdout: "Server:\t\t10.254.0.10\nAddress:\t10.254.0.10#53\n\nnodeport-service.services-2972.svc.cluster.local\tcanonical name = externalsvc.services-2972.svc.cluster.local.\nName:\texternalsvc.services-2972.svc.cluster.local\nAddress: 10.254.230.248\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-2972, will wait for the garbage collector to delete the pods 09/20/23 13:51:17.685
    Sep 20 13:51:17.749: INFO: Deleting ReplicationController externalsvc took: 7.211214ms
    Sep 20 13:51:18.650: INFO: Terminating ReplicationController externalsvc pods took: 900.99971ms
    Sep 20 13:51:23.082: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:51:23.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2972" for this suite. 09/20/23 13:51:23.292
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:51:23.314
Sep 20 13:51:23.314: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename var-expansion 09/20/23 13:51:23.315
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:51:24.435
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:51:24.443
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 09/20/23 13:51:24.449
Sep 20 13:51:24.626: INFO: Waiting up to 5m0s for pod "var-expansion-04b35db1-c639-4940-98c0-7c5abc9caa5b" in namespace "var-expansion-5097" to be "Succeeded or Failed"
Sep 20 13:51:24.767: INFO: Pod "var-expansion-04b35db1-c639-4940-98c0-7c5abc9caa5b": Phase="Pending", Reason="", readiness=false. Elapsed: 140.697833ms
Sep 20 13:51:26.772: INFO: Pod "var-expansion-04b35db1-c639-4940-98c0-7c5abc9caa5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.146184503s
Sep 20 13:51:28.909: INFO: Pod "var-expansion-04b35db1-c639-4940-98c0-7c5abc9caa5b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.28259746s
Sep 20 13:51:30.797: INFO: Pod "var-expansion-04b35db1-c639-4940-98c0-7c5abc9caa5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.170501327s
STEP: Saw pod success 09/20/23 13:51:30.797
Sep 20 13:51:30.797: INFO: Pod "var-expansion-04b35db1-c639-4940-98c0-7c5abc9caa5b" satisfied condition "Succeeded or Failed"
Sep 20 13:51:30.800: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod var-expansion-04b35db1-c639-4940-98c0-7c5abc9caa5b container dapi-container: <nil>
STEP: delete the pod 09/20/23 13:51:30.854
Sep 20 13:51:31.919: INFO: Waiting for pod var-expansion-04b35db1-c639-4940-98c0-7c5abc9caa5b to disappear
Sep 20 13:51:31.924: INFO: Pod var-expansion-04b35db1-c639-4940-98c0-7c5abc9caa5b no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep 20 13:51:31.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5097" for this suite. 09/20/23 13:51:31.93
------------------------------
â€¢ [SLOW TEST] [8.629 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:51:23.314
    Sep 20 13:51:23.314: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename var-expansion 09/20/23 13:51:23.315
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:51:24.435
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:51:24.443
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 09/20/23 13:51:24.449
    Sep 20 13:51:24.626: INFO: Waiting up to 5m0s for pod "var-expansion-04b35db1-c639-4940-98c0-7c5abc9caa5b" in namespace "var-expansion-5097" to be "Succeeded or Failed"
    Sep 20 13:51:24.767: INFO: Pod "var-expansion-04b35db1-c639-4940-98c0-7c5abc9caa5b": Phase="Pending", Reason="", readiness=false. Elapsed: 140.697833ms
    Sep 20 13:51:26.772: INFO: Pod "var-expansion-04b35db1-c639-4940-98c0-7c5abc9caa5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.146184503s
    Sep 20 13:51:28.909: INFO: Pod "var-expansion-04b35db1-c639-4940-98c0-7c5abc9caa5b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.28259746s
    Sep 20 13:51:30.797: INFO: Pod "var-expansion-04b35db1-c639-4940-98c0-7c5abc9caa5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.170501327s
    STEP: Saw pod success 09/20/23 13:51:30.797
    Sep 20 13:51:30.797: INFO: Pod "var-expansion-04b35db1-c639-4940-98c0-7c5abc9caa5b" satisfied condition "Succeeded or Failed"
    Sep 20 13:51:30.800: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod var-expansion-04b35db1-c639-4940-98c0-7c5abc9caa5b container dapi-container: <nil>
    STEP: delete the pod 09/20/23 13:51:30.854
    Sep 20 13:51:31.919: INFO: Waiting for pod var-expansion-04b35db1-c639-4940-98c0-7c5abc9caa5b to disappear
    Sep 20 13:51:31.924: INFO: Pod var-expansion-04b35db1-c639-4940-98c0-7c5abc9caa5b no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:51:31.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5097" for this suite. 09/20/23 13:51:31.93
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:51:31.946
Sep 20 13:51:31.947: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename services 09/20/23 13:51:31.947
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:51:32.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:51:32.178
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 09/20/23 13:51:32.183
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep 20 13:51:32.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3822" for this suite. 09/20/23 13:51:32.192
------------------------------
â€¢ [0.259 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:51:31.946
    Sep 20 13:51:31.947: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename services 09/20/23 13:51:31.947
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:51:32.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:51:32.178
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 09/20/23 13:51:32.183
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:51:32.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3822" for this suite. 09/20/23 13:51:32.192
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:51:32.206
Sep 20 13:51:32.206: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename resourcequota 09/20/23 13:51:32.207
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:51:32.385
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:51:32.389
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 09/20/23 13:51:32.398
STEP: Ensuring ResourceQuota status is calculated 09/20/23 13:51:32.405
STEP: Creating a ResourceQuota with not terminating scope 09/20/23 13:51:35.123
STEP: Ensuring ResourceQuota status is calculated 09/20/23 13:51:35.545
STEP: Creating a long running pod 09/20/23 13:51:37.551
STEP: Ensuring resource quota with not terminating scope captures the pod usage 09/20/23 13:51:38.223
STEP: Ensuring resource quota with terminating scope ignored the pod usage 09/20/23 13:51:40.282
STEP: Deleting the pod 09/20/23 13:51:42.288
STEP: Ensuring resource quota status released the pod usage 09/20/23 13:51:42.663
STEP: Creating a terminating pod 09/20/23 13:51:44.668
STEP: Ensuring resource quota with terminating scope captures the pod usage 09/20/23 13:51:45.291
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 09/20/23 13:51:47.547
STEP: Deleting the pod 09/20/23 13:51:49.553
STEP: Ensuring resource quota status released the pod usage 09/20/23 13:51:50.386
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep 20 13:51:52.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1682" for this suite. 09/20/23 13:51:52.58
------------------------------
â€¢ [SLOW TEST] [20.447 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:51:32.206
    Sep 20 13:51:32.206: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename resourcequota 09/20/23 13:51:32.207
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:51:32.385
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:51:32.389
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 09/20/23 13:51:32.398
    STEP: Ensuring ResourceQuota status is calculated 09/20/23 13:51:32.405
    STEP: Creating a ResourceQuota with not terminating scope 09/20/23 13:51:35.123
    STEP: Ensuring ResourceQuota status is calculated 09/20/23 13:51:35.545
    STEP: Creating a long running pod 09/20/23 13:51:37.551
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 09/20/23 13:51:38.223
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 09/20/23 13:51:40.282
    STEP: Deleting the pod 09/20/23 13:51:42.288
    STEP: Ensuring resource quota status released the pod usage 09/20/23 13:51:42.663
    STEP: Creating a terminating pod 09/20/23 13:51:44.668
    STEP: Ensuring resource quota with terminating scope captures the pod usage 09/20/23 13:51:45.291
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 09/20/23 13:51:47.547
    STEP: Deleting the pod 09/20/23 13:51:49.553
    STEP: Ensuring resource quota status released the pod usage 09/20/23 13:51:50.386
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:51:52.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1682" for this suite. 09/20/23 13:51:52.58
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:51:52.654
Sep 20 13:51:52.654: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename container-probe 09/20/23 13:51:52.655
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:51:52.767
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:51:52.773
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-4e25de4e-2a4c-4d87-9668-0061603e9652 in namespace container-probe-820 09/20/23 13:51:52.787
Sep 20 13:51:53.164: INFO: Waiting up to 5m0s for pod "liveness-4e25de4e-2a4c-4d87-9668-0061603e9652" in namespace "container-probe-820" to be "not pending"
Sep 20 13:51:53.948: INFO: Pod "liveness-4e25de4e-2a4c-4d87-9668-0061603e9652": Phase="Pending", Reason="", readiness=false. Elapsed: 784.594422ms
Sep 20 13:51:55.953: INFO: Pod "liveness-4e25de4e-2a4c-4d87-9668-0061603e9652": Phase="Pending", Reason="", readiness=false. Elapsed: 2.789655845s
Sep 20 13:51:58.314: INFO: Pod "liveness-4e25de4e-2a4c-4d87-9668-0061603e9652": Phase="Running", Reason="", readiness=true. Elapsed: 5.150740133s
Sep 20 13:51:58.315: INFO: Pod "liveness-4e25de4e-2a4c-4d87-9668-0061603e9652" satisfied condition "not pending"
Sep 20 13:51:58.315: INFO: Started pod liveness-4e25de4e-2a4c-4d87-9668-0061603e9652 in namespace container-probe-820
STEP: checking the pod's current state and verifying that restartCount is present 09/20/23 13:51:58.315
Sep 20 13:51:58.329: INFO: Initial restart count of pod liveness-4e25de4e-2a4c-4d87-9668-0061603e9652 is 0
Sep 20 13:52:17.488: INFO: Restart count of pod container-probe-820/liveness-4e25de4e-2a4c-4d87-9668-0061603e9652 is now 1 (19.158778384s elapsed)
STEP: deleting the pod 09/20/23 13:52:17.488
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep 20 13:52:17.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-820" for this suite. 09/20/23 13:52:18.018
------------------------------
â€¢ [SLOW TEST] [25.408 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:51:52.654
    Sep 20 13:51:52.654: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename container-probe 09/20/23 13:51:52.655
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:51:52.767
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:51:52.773
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-4e25de4e-2a4c-4d87-9668-0061603e9652 in namespace container-probe-820 09/20/23 13:51:52.787
    Sep 20 13:51:53.164: INFO: Waiting up to 5m0s for pod "liveness-4e25de4e-2a4c-4d87-9668-0061603e9652" in namespace "container-probe-820" to be "not pending"
    Sep 20 13:51:53.948: INFO: Pod "liveness-4e25de4e-2a4c-4d87-9668-0061603e9652": Phase="Pending", Reason="", readiness=false. Elapsed: 784.594422ms
    Sep 20 13:51:55.953: INFO: Pod "liveness-4e25de4e-2a4c-4d87-9668-0061603e9652": Phase="Pending", Reason="", readiness=false. Elapsed: 2.789655845s
    Sep 20 13:51:58.314: INFO: Pod "liveness-4e25de4e-2a4c-4d87-9668-0061603e9652": Phase="Running", Reason="", readiness=true. Elapsed: 5.150740133s
    Sep 20 13:51:58.315: INFO: Pod "liveness-4e25de4e-2a4c-4d87-9668-0061603e9652" satisfied condition "not pending"
    Sep 20 13:51:58.315: INFO: Started pod liveness-4e25de4e-2a4c-4d87-9668-0061603e9652 in namespace container-probe-820
    STEP: checking the pod's current state and verifying that restartCount is present 09/20/23 13:51:58.315
    Sep 20 13:51:58.329: INFO: Initial restart count of pod liveness-4e25de4e-2a4c-4d87-9668-0061603e9652 is 0
    Sep 20 13:52:17.488: INFO: Restart count of pod container-probe-820/liveness-4e25de4e-2a4c-4d87-9668-0061603e9652 is now 1 (19.158778384s elapsed)
    STEP: deleting the pod 09/20/23 13:52:17.488
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:52:17.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-820" for this suite. 09/20/23 13:52:18.018
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:52:18.063
Sep 20 13:52:18.063: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename crd-publish-openapi 09/20/23 13:52:18.063
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:52:18.163
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:52:18.167
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 09/20/23 13:52:18.171
Sep 20 13:52:18.172: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: rename a version 09/20/23 13:52:24.254
STEP: check the new version name is served 09/20/23 13:52:24.929
STEP: check the old version name is removed 09/20/23 13:52:26.426
STEP: check the other version is not changed 09/20/23 13:52:27.373
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep 20 13:52:32.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8118" for this suite. 09/20/23 13:52:32.062
------------------------------
â€¢ [SLOW TEST] [14.007 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:52:18.063
    Sep 20 13:52:18.063: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename crd-publish-openapi 09/20/23 13:52:18.063
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:52:18.163
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:52:18.167
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 09/20/23 13:52:18.171
    Sep 20 13:52:18.172: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: rename a version 09/20/23 13:52:24.254
    STEP: check the new version name is served 09/20/23 13:52:24.929
    STEP: check the old version name is removed 09/20/23 13:52:26.426
    STEP: check the other version is not changed 09/20/23 13:52:27.373
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:52:32.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8118" for this suite. 09/20/23 13:52:32.062
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:52:32.072
Sep 20 13:52:32.073: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename disruption 09/20/23 13:52:32.073
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:52:32.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:52:32.171
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 09/20/23 13:52:32.175
STEP: Waiting for the pdb to be processed 09/20/23 13:52:32.181
STEP: updating the pdb 09/20/23 13:52:34.187
STEP: Waiting for the pdb to be processed 09/20/23 13:52:34.441
STEP: patching the pdb 09/20/23 13:52:34.598
STEP: Waiting for the pdb to be processed 09/20/23 13:52:34.941
STEP: Waiting for the pdb to be deleted 09/20/23 13:52:36.955
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Sep 20 13:52:36.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-1468" for this suite. 09/20/23 13:52:36.962
------------------------------
â€¢ [4.896 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:52:32.072
    Sep 20 13:52:32.073: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename disruption 09/20/23 13:52:32.073
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:52:32.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:52:32.171
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 09/20/23 13:52:32.175
    STEP: Waiting for the pdb to be processed 09/20/23 13:52:32.181
    STEP: updating the pdb 09/20/23 13:52:34.187
    STEP: Waiting for the pdb to be processed 09/20/23 13:52:34.441
    STEP: patching the pdb 09/20/23 13:52:34.598
    STEP: Waiting for the pdb to be processed 09/20/23 13:52:34.941
    STEP: Waiting for the pdb to be deleted 09/20/23 13:52:36.955
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:52:36.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-1468" for this suite. 09/20/23 13:52:36.962
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:52:36.969
Sep 20 13:52:36.969: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename gc 09/20/23 13:52:36.97
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:52:37.014
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:52:37.017
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 09/20/23 13:52:37.02
STEP: delete the rc 09/20/23 13:52:42.151
STEP: wait for all pods to be garbage collected 09/20/23 13:52:42.157
STEP: Gathering metrics 09/20/23 13:52:47.163
W0920 13:52:47.170928      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Sep 20 13:52:47.170: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep 20 13:52:47.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1972" for this suite. 09/20/23 13:52:47.174
------------------------------
â€¢ [SLOW TEST] [10.215 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:52:36.969
    Sep 20 13:52:36.969: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename gc 09/20/23 13:52:36.97
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:52:37.014
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:52:37.017
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 09/20/23 13:52:37.02
    STEP: delete the rc 09/20/23 13:52:42.151
    STEP: wait for all pods to be garbage collected 09/20/23 13:52:42.157
    STEP: Gathering metrics 09/20/23 13:52:47.163
    W0920 13:52:47.170928      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Sep 20 13:52:47.170: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:52:47.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1972" for this suite. 09/20/23 13:52:47.174
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:52:47.186
Sep 20 13:52:47.186: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename namespaces 09/20/23 13:52:47.187
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:52:47.466
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:52:47.468
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 09/20/23 13:52:47.472
STEP: patching the Namespace 09/20/23 13:52:47.498
STEP: get the Namespace and ensuring it has the label 09/20/23 13:52:47.504
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep 20 13:52:47.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-8137" for this suite. 09/20/23 13:52:47.51
STEP: Destroying namespace "nspatchtest-6dd1e601-b4a3-4cfb-b9a0-f44bdc6e999f-9354" for this suite. 09/20/23 13:52:47.755
------------------------------
â€¢ [0.577 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:52:47.186
    Sep 20 13:52:47.186: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename namespaces 09/20/23 13:52:47.187
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:52:47.466
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:52:47.468
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 09/20/23 13:52:47.472
    STEP: patching the Namespace 09/20/23 13:52:47.498
    STEP: get the Namespace and ensuring it has the label 09/20/23 13:52:47.504
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:52:47.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-8137" for this suite. 09/20/23 13:52:47.51
    STEP: Destroying namespace "nspatchtest-6dd1e601-b4a3-4cfb-b9a0-f44bdc6e999f-9354" for this suite. 09/20/23 13:52:47.755
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:52:47.764
Sep 20 13:52:47.764: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename container-probe 09/20/23 13:52:47.765
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:52:47.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:52:47.79
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Sep 20 13:52:47.800: INFO: Waiting up to 5m0s for pod "test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1" in namespace "container-probe-2992" to be "running and ready"
Sep 20 13:52:47.804: INFO: Pod "test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.740075ms
Sep 20 13:52:47.804: INFO: The phase of Pod test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:52:49.810: INFO: Pod "test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009242425s
Sep 20 13:52:49.810: INFO: The phase of Pod test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:52:51.808: INFO: Pod "test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007396494s
Sep 20 13:52:51.808: INFO: The phase of Pod test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:52:53.812: INFO: Pod "test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1": Phase="Running", Reason="", readiness=false. Elapsed: 6.011911915s
Sep 20 13:52:53.812: INFO: The phase of Pod test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1 is Running (Ready = false)
Sep 20 13:52:55.807: INFO: Pod "test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1": Phase="Running", Reason="", readiness=false. Elapsed: 8.007046474s
Sep 20 13:52:55.808: INFO: The phase of Pod test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1 is Running (Ready = false)
Sep 20 13:52:57.809: INFO: Pod "test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1": Phase="Running", Reason="", readiness=false. Elapsed: 10.008536787s
Sep 20 13:52:57.809: INFO: The phase of Pod test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1 is Running (Ready = false)
Sep 20 13:52:59.808: INFO: Pod "test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1": Phase="Running", Reason="", readiness=false. Elapsed: 12.00801485s
Sep 20 13:52:59.808: INFO: The phase of Pod test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1 is Running (Ready = false)
Sep 20 13:53:01.808: INFO: Pod "test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1": Phase="Running", Reason="", readiness=false. Elapsed: 14.007727985s
Sep 20 13:53:01.808: INFO: The phase of Pod test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1 is Running (Ready = false)
Sep 20 13:53:06.145: INFO: Pod "test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1": Phase="Running", Reason="", readiness=false. Elapsed: 18.344542644s
Sep 20 13:53:06.145: INFO: The phase of Pod test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1 is Running (Ready = false)
Sep 20 13:53:07.807: INFO: Pod "test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1": Phase="Running", Reason="", readiness=false. Elapsed: 20.006895493s
Sep 20 13:53:07.807: INFO: The phase of Pod test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1 is Running (Ready = false)
Sep 20 13:53:10.013: INFO: Pod "test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1": Phase="Running", Reason="", readiness=true. Elapsed: 22.212943408s
Sep 20 13:53:10.013: INFO: The phase of Pod test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1 is Running (Ready = true)
Sep 20 13:53:10.013: INFO: Pod "test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1" satisfied condition "running and ready"
Sep 20 13:53:10.016: INFO: Container started at 2023-09-20 13:52:51 +0000 UTC, pod became ready at 2023-09-20 13:53:08 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep 20 13:53:10.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2992" for this suite. 09/20/23 13:53:10.021
------------------------------
â€¢ [SLOW TEST] [22.264 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:52:47.764
    Sep 20 13:52:47.764: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename container-probe 09/20/23 13:52:47.765
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:52:47.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:52:47.79
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Sep 20 13:52:47.800: INFO: Waiting up to 5m0s for pod "test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1" in namespace "container-probe-2992" to be "running and ready"
    Sep 20 13:52:47.804: INFO: Pod "test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.740075ms
    Sep 20 13:52:47.804: INFO: The phase of Pod test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:52:49.810: INFO: Pod "test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009242425s
    Sep 20 13:52:49.810: INFO: The phase of Pod test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:52:51.808: INFO: Pod "test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007396494s
    Sep 20 13:52:51.808: INFO: The phase of Pod test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:52:53.812: INFO: Pod "test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1": Phase="Running", Reason="", readiness=false. Elapsed: 6.011911915s
    Sep 20 13:52:53.812: INFO: The phase of Pod test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1 is Running (Ready = false)
    Sep 20 13:52:55.807: INFO: Pod "test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1": Phase="Running", Reason="", readiness=false. Elapsed: 8.007046474s
    Sep 20 13:52:55.808: INFO: The phase of Pod test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1 is Running (Ready = false)
    Sep 20 13:52:57.809: INFO: Pod "test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1": Phase="Running", Reason="", readiness=false. Elapsed: 10.008536787s
    Sep 20 13:52:57.809: INFO: The phase of Pod test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1 is Running (Ready = false)
    Sep 20 13:52:59.808: INFO: Pod "test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1": Phase="Running", Reason="", readiness=false. Elapsed: 12.00801485s
    Sep 20 13:52:59.808: INFO: The phase of Pod test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1 is Running (Ready = false)
    Sep 20 13:53:01.808: INFO: Pod "test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1": Phase="Running", Reason="", readiness=false. Elapsed: 14.007727985s
    Sep 20 13:53:01.808: INFO: The phase of Pod test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1 is Running (Ready = false)
    Sep 20 13:53:06.145: INFO: Pod "test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1": Phase="Running", Reason="", readiness=false. Elapsed: 18.344542644s
    Sep 20 13:53:06.145: INFO: The phase of Pod test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1 is Running (Ready = false)
    Sep 20 13:53:07.807: INFO: Pod "test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1": Phase="Running", Reason="", readiness=false. Elapsed: 20.006895493s
    Sep 20 13:53:07.807: INFO: The phase of Pod test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1 is Running (Ready = false)
    Sep 20 13:53:10.013: INFO: Pod "test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1": Phase="Running", Reason="", readiness=true. Elapsed: 22.212943408s
    Sep 20 13:53:10.013: INFO: The phase of Pod test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1 is Running (Ready = true)
    Sep 20 13:53:10.013: INFO: Pod "test-webserver-4af9fac2-5809-4cc0-930a-c82e2946abe1" satisfied condition "running and ready"
    Sep 20 13:53:10.016: INFO: Container started at 2023-09-20 13:52:51 +0000 UTC, pod became ready at 2023-09-20 13:53:08 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:53:10.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2992" for this suite. 09/20/23 13:53:10.021
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:53:10.028
Sep 20 13:53:10.028: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename crd-publish-openapi 09/20/23 13:53:10.029
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:53:10.247
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:53:10.25
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Sep 20 13:53:10.266: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 09/20/23 13:53:12.703
Sep 20 13:53:12.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-6653 --namespace=crd-publish-openapi-6653 create -f -'
Sep 20 13:53:13.921: INFO: stderr: ""
Sep 20 13:53:13.921: INFO: stdout: "e2e-test-crd-publish-openapi-6638-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep 20 13:53:13.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-6653 --namespace=crd-publish-openapi-6653 delete e2e-test-crd-publish-openapi-6638-crds test-foo'
Sep 20 13:53:14.262: INFO: stderr: ""
Sep 20 13:53:14.262: INFO: stdout: "e2e-test-crd-publish-openapi-6638-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Sep 20 13:53:14.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-6653 --namespace=crd-publish-openapi-6653 apply -f -'
Sep 20 13:53:14.570: INFO: stderr: ""
Sep 20 13:53:14.570: INFO: stdout: "e2e-test-crd-publish-openapi-6638-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep 20 13:53:14.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-6653 --namespace=crd-publish-openapi-6653 delete e2e-test-crd-publish-openapi-6638-crds test-foo'
Sep 20 13:53:14.669: INFO: stderr: ""
Sep 20 13:53:14.669: INFO: stdout: "e2e-test-crd-publish-openapi-6638-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 09/20/23 13:53:14.669
Sep 20 13:53:14.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-6653 --namespace=crd-publish-openapi-6653 create -f -'
Sep 20 13:53:15.339: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 09/20/23 13:53:15.339
Sep 20 13:53:15.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-6653 --namespace=crd-publish-openapi-6653 create -f -'
Sep 20 13:53:15.529: INFO: rc: 1
Sep 20 13:53:15.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-6653 --namespace=crd-publish-openapi-6653 apply -f -'
Sep 20 13:53:16.019: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 09/20/23 13:53:16.019
Sep 20 13:53:16.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-6653 --namespace=crd-publish-openapi-6653 create -f -'
Sep 20 13:53:16.234: INFO: rc: 1
Sep 20 13:53:16.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-6653 --namespace=crd-publish-openapi-6653 apply -f -'
Sep 20 13:53:17.259: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 09/20/23 13:53:17.259
Sep 20 13:53:17.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-6653 explain e2e-test-crd-publish-openapi-6638-crds'
Sep 20 13:53:17.428: INFO: stderr: ""
Sep 20 13:53:17.428: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6638-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 09/20/23 13:53:17.428
Sep 20 13:53:17.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-6653 explain e2e-test-crd-publish-openapi-6638-crds.metadata'
Sep 20 13:53:17.596: INFO: stderr: ""
Sep 20 13:53:17.596: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6638-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Sep 20 13:53:17.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-6653 explain e2e-test-crd-publish-openapi-6638-crds.spec'
Sep 20 13:53:17.778: INFO: stderr: ""
Sep 20 13:53:17.778: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6638-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Sep 20 13:53:17.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-6653 explain e2e-test-crd-publish-openapi-6638-crds.spec.bars'
Sep 20 13:53:17.960: INFO: stderr: ""
Sep 20 13:53:17.960: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6638-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 09/20/23 13:53:17.96
Sep 20 13:53:17.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-6653 explain e2e-test-crd-publish-openapi-6638-crds.spec.bars2'
Sep 20 13:53:18.129: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep 20 13:53:20.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6653" for this suite. 09/20/23 13:53:20.367
------------------------------
â€¢ [SLOW TEST] [10.373 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:53:10.028
    Sep 20 13:53:10.028: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename crd-publish-openapi 09/20/23 13:53:10.029
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:53:10.247
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:53:10.25
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Sep 20 13:53:10.266: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 09/20/23 13:53:12.703
    Sep 20 13:53:12.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-6653 --namespace=crd-publish-openapi-6653 create -f -'
    Sep 20 13:53:13.921: INFO: stderr: ""
    Sep 20 13:53:13.921: INFO: stdout: "e2e-test-crd-publish-openapi-6638-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Sep 20 13:53:13.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-6653 --namespace=crd-publish-openapi-6653 delete e2e-test-crd-publish-openapi-6638-crds test-foo'
    Sep 20 13:53:14.262: INFO: stderr: ""
    Sep 20 13:53:14.262: INFO: stdout: "e2e-test-crd-publish-openapi-6638-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Sep 20 13:53:14.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-6653 --namespace=crd-publish-openapi-6653 apply -f -'
    Sep 20 13:53:14.570: INFO: stderr: ""
    Sep 20 13:53:14.570: INFO: stdout: "e2e-test-crd-publish-openapi-6638-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Sep 20 13:53:14.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-6653 --namespace=crd-publish-openapi-6653 delete e2e-test-crd-publish-openapi-6638-crds test-foo'
    Sep 20 13:53:14.669: INFO: stderr: ""
    Sep 20 13:53:14.669: INFO: stdout: "e2e-test-crd-publish-openapi-6638-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 09/20/23 13:53:14.669
    Sep 20 13:53:14.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-6653 --namespace=crd-publish-openapi-6653 create -f -'
    Sep 20 13:53:15.339: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 09/20/23 13:53:15.339
    Sep 20 13:53:15.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-6653 --namespace=crd-publish-openapi-6653 create -f -'
    Sep 20 13:53:15.529: INFO: rc: 1
    Sep 20 13:53:15.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-6653 --namespace=crd-publish-openapi-6653 apply -f -'
    Sep 20 13:53:16.019: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 09/20/23 13:53:16.019
    Sep 20 13:53:16.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-6653 --namespace=crd-publish-openapi-6653 create -f -'
    Sep 20 13:53:16.234: INFO: rc: 1
    Sep 20 13:53:16.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-6653 --namespace=crd-publish-openapi-6653 apply -f -'
    Sep 20 13:53:17.259: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 09/20/23 13:53:17.259
    Sep 20 13:53:17.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-6653 explain e2e-test-crd-publish-openapi-6638-crds'
    Sep 20 13:53:17.428: INFO: stderr: ""
    Sep 20 13:53:17.428: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6638-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 09/20/23 13:53:17.428
    Sep 20 13:53:17.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-6653 explain e2e-test-crd-publish-openapi-6638-crds.metadata'
    Sep 20 13:53:17.596: INFO: stderr: ""
    Sep 20 13:53:17.596: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6638-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Sep 20 13:53:17.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-6653 explain e2e-test-crd-publish-openapi-6638-crds.spec'
    Sep 20 13:53:17.778: INFO: stderr: ""
    Sep 20 13:53:17.778: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6638-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Sep 20 13:53:17.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-6653 explain e2e-test-crd-publish-openapi-6638-crds.spec.bars'
    Sep 20 13:53:17.960: INFO: stderr: ""
    Sep 20 13:53:17.960: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6638-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 09/20/23 13:53:17.96
    Sep 20 13:53:17.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=crd-publish-openapi-6653 explain e2e-test-crd-publish-openapi-6638-crds.spec.bars2'
    Sep 20 13:53:18.129: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:53:20.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6653" for this suite. 09/20/23 13:53:20.367
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:53:20.406
Sep 20 13:53:20.406: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename secrets 09/20/23 13:53:20.406
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:53:20.775
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:53:20.779
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-c3a74015-bcf4-4688-947e-eea55a632422 09/20/23 13:53:21.277
STEP: Creating secret with name s-test-opt-upd-972aa43d-e9a3-49ea-92b7-6da48ffe76cd 09/20/23 13:53:21.667
STEP: Creating the pod 09/20/23 13:53:21.862
Sep 20 13:53:22.096: INFO: Waiting up to 5m0s for pod "pod-secrets-a713fa7a-9ffb-49cd-9adf-f6d95af7879e" in namespace "secrets-9056" to be "running and ready"
Sep 20 13:53:22.100: INFO: Pod "pod-secrets-a713fa7a-9ffb-49cd-9adf-f6d95af7879e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.950802ms
Sep 20 13:53:22.100: INFO: The phase of Pod pod-secrets-a713fa7a-9ffb-49cd-9adf-f6d95af7879e is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:53:24.106: INFO: Pod "pod-secrets-a713fa7a-9ffb-49cd-9adf-f6d95af7879e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009843688s
Sep 20 13:53:24.106: INFO: The phase of Pod pod-secrets-a713fa7a-9ffb-49cd-9adf-f6d95af7879e is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:53:26.274: INFO: Pod "pod-secrets-a713fa7a-9ffb-49cd-9adf-f6d95af7879e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.177511197s
Sep 20 13:53:26.274: INFO: The phase of Pod pod-secrets-a713fa7a-9ffb-49cd-9adf-f6d95af7879e is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:53:28.105: INFO: Pod "pod-secrets-a713fa7a-9ffb-49cd-9adf-f6d95af7879e": Phase="Running", Reason="", readiness=true. Elapsed: 6.008700103s
Sep 20 13:53:28.105: INFO: The phase of Pod pod-secrets-a713fa7a-9ffb-49cd-9adf-f6d95af7879e is Running (Ready = true)
Sep 20 13:53:28.105: INFO: Pod "pod-secrets-a713fa7a-9ffb-49cd-9adf-f6d95af7879e" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-c3a74015-bcf4-4688-947e-eea55a632422 09/20/23 13:53:28.194
STEP: Updating secret s-test-opt-upd-972aa43d-e9a3-49ea-92b7-6da48ffe76cd 09/20/23 13:53:28.229
STEP: Creating secret with name s-test-opt-create-17fb3547-1a54-42c8-aa8e-6b8585755e43 09/20/23 13:53:28.369
STEP: waiting to observe update in volume 09/20/23 13:53:28.401
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep 20 13:54:42.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9056" for this suite. 09/20/23 13:54:42.339
------------------------------
â€¢ [SLOW TEST] [81.964 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:53:20.406
    Sep 20 13:53:20.406: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename secrets 09/20/23 13:53:20.406
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:53:20.775
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:53:20.779
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-c3a74015-bcf4-4688-947e-eea55a632422 09/20/23 13:53:21.277
    STEP: Creating secret with name s-test-opt-upd-972aa43d-e9a3-49ea-92b7-6da48ffe76cd 09/20/23 13:53:21.667
    STEP: Creating the pod 09/20/23 13:53:21.862
    Sep 20 13:53:22.096: INFO: Waiting up to 5m0s for pod "pod-secrets-a713fa7a-9ffb-49cd-9adf-f6d95af7879e" in namespace "secrets-9056" to be "running and ready"
    Sep 20 13:53:22.100: INFO: Pod "pod-secrets-a713fa7a-9ffb-49cd-9adf-f6d95af7879e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.950802ms
    Sep 20 13:53:22.100: INFO: The phase of Pod pod-secrets-a713fa7a-9ffb-49cd-9adf-f6d95af7879e is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:53:24.106: INFO: Pod "pod-secrets-a713fa7a-9ffb-49cd-9adf-f6d95af7879e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009843688s
    Sep 20 13:53:24.106: INFO: The phase of Pod pod-secrets-a713fa7a-9ffb-49cd-9adf-f6d95af7879e is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:53:26.274: INFO: Pod "pod-secrets-a713fa7a-9ffb-49cd-9adf-f6d95af7879e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.177511197s
    Sep 20 13:53:26.274: INFO: The phase of Pod pod-secrets-a713fa7a-9ffb-49cd-9adf-f6d95af7879e is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:53:28.105: INFO: Pod "pod-secrets-a713fa7a-9ffb-49cd-9adf-f6d95af7879e": Phase="Running", Reason="", readiness=true. Elapsed: 6.008700103s
    Sep 20 13:53:28.105: INFO: The phase of Pod pod-secrets-a713fa7a-9ffb-49cd-9adf-f6d95af7879e is Running (Ready = true)
    Sep 20 13:53:28.105: INFO: Pod "pod-secrets-a713fa7a-9ffb-49cd-9adf-f6d95af7879e" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-c3a74015-bcf4-4688-947e-eea55a632422 09/20/23 13:53:28.194
    STEP: Updating secret s-test-opt-upd-972aa43d-e9a3-49ea-92b7-6da48ffe76cd 09/20/23 13:53:28.229
    STEP: Creating secret with name s-test-opt-create-17fb3547-1a54-42c8-aa8e-6b8585755e43 09/20/23 13:53:28.369
    STEP: waiting to observe update in volume 09/20/23 13:53:28.401
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:54:42.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9056" for this suite. 09/20/23 13:54:42.339
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:54:42.372
Sep 20 13:54:42.372: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename container-lifecycle-hook 09/20/23 13:54:42.373
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:54:42.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:54:42.749
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 09/20/23 13:54:42.768
Sep 20 13:54:42.827: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8358" to be "running and ready"
Sep 20 13:54:43.223: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 396.150807ms
Sep 20 13:54:43.223: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:54:45.228: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.401723599s
Sep 20 13:54:45.228: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:54:47.229: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.402660048s
Sep 20 13:54:47.229: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Sep 20 13:54:47.229: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 09/20/23 13:54:47.231
Sep 20 13:54:47.464: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-8358" to be "running and ready"
Sep 20 13:54:47.484: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 19.672325ms
Sep 20 13:54:47.484: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:54:49.501: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03682023s
Sep 20 13:54:49.501: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:54:51.878: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.413517168s
Sep 20 13:54:51.878: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Sep 20 13:54:51.878: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 09/20/23 13:54:52.311
STEP: delete the pod with lifecycle hook 09/20/23 13:54:52.419
Sep 20 13:54:52.432: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 20 13:54:52.441: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 20 13:54:54.441: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 20 13:54:54.634: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Sep 20 13:54:54.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-8358" for this suite. 09/20/23 13:54:54.639
------------------------------
â€¢ [SLOW TEST] [12.342 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:54:42.372
    Sep 20 13:54:42.372: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename container-lifecycle-hook 09/20/23 13:54:42.373
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:54:42.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:54:42.749
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 09/20/23 13:54:42.768
    Sep 20 13:54:42.827: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8358" to be "running and ready"
    Sep 20 13:54:43.223: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 396.150807ms
    Sep 20 13:54:43.223: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:54:45.228: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.401723599s
    Sep 20 13:54:45.228: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:54:47.229: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.402660048s
    Sep 20 13:54:47.229: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Sep 20 13:54:47.229: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 09/20/23 13:54:47.231
    Sep 20 13:54:47.464: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-8358" to be "running and ready"
    Sep 20 13:54:47.484: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 19.672325ms
    Sep 20 13:54:47.484: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:54:49.501: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03682023s
    Sep 20 13:54:49.501: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:54:51.878: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.413517168s
    Sep 20 13:54:51.878: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Sep 20 13:54:51.878: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 09/20/23 13:54:52.311
    STEP: delete the pod with lifecycle hook 09/20/23 13:54:52.419
    Sep 20 13:54:52.432: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Sep 20 13:54:52.441: INFO: Pod pod-with-poststart-exec-hook still exists
    Sep 20 13:54:54.441: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Sep 20 13:54:54.634: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:54:54.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-8358" for this suite. 09/20/23 13:54:54.639
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:54:54.715
Sep 20 13:54:54.715: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename crd-watch 09/20/23 13:54:54.715
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:54:54.752
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:54:54.758
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Sep 20 13:54:54.765: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Creating first CR  09/20/23 13:54:57.328
Sep 20 13:54:57.347: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-20T13:54:57Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-20T13:54:57Z]] name:name1 resourceVersion:48082 uid:a83b1670-d225-4f15-8f7a-bc527ac274f5] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 09/20/23 13:55:07.347
Sep 20 13:55:07.600: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-20T13:55:07Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-20T13:55:07Z]] name:name2 resourceVersion:48136 uid:0f3961c9-737a-41d8-b39f-1d9d43099d45] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 09/20/23 13:55:17.6
Sep 20 13:55:17.794: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-20T13:54:57Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-20T13:55:17Z]] name:name1 resourceVersion:48170 uid:a83b1670-d225-4f15-8f7a-bc527ac274f5] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 09/20/23 13:55:27.794
Sep 20 13:55:27.823: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-20T13:55:07Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-20T13:55:27Z]] name:name2 resourceVersion:48207 uid:0f3961c9-737a-41d8-b39f-1d9d43099d45] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 09/20/23 13:55:37.825
Sep 20 13:55:37.835: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-20T13:54:57Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-20T13:55:17Z]] name:name1 resourceVersion:48241 uid:a83b1670-d225-4f15-8f7a-bc527ac274f5] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 09/20/23 13:55:47.835
Sep 20 13:55:48.059: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-20T13:55:07Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-20T13:55:27Z]] name:name2 resourceVersion:48278 uid:0f3961c9-737a-41d8-b39f-1d9d43099d45] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep 20 13:55:58.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-7774" for this suite. 09/20/23 13:55:58.944
------------------------------
â€¢ [SLOW TEST] [64.239 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:54:54.715
    Sep 20 13:54:54.715: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename crd-watch 09/20/23 13:54:54.715
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:54:54.752
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:54:54.758
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Sep 20 13:54:54.765: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Creating first CR  09/20/23 13:54:57.328
    Sep 20 13:54:57.347: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-20T13:54:57Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-20T13:54:57Z]] name:name1 resourceVersion:48082 uid:a83b1670-d225-4f15-8f7a-bc527ac274f5] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 09/20/23 13:55:07.347
    Sep 20 13:55:07.600: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-20T13:55:07Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-20T13:55:07Z]] name:name2 resourceVersion:48136 uid:0f3961c9-737a-41d8-b39f-1d9d43099d45] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 09/20/23 13:55:17.6
    Sep 20 13:55:17.794: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-20T13:54:57Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-20T13:55:17Z]] name:name1 resourceVersion:48170 uid:a83b1670-d225-4f15-8f7a-bc527ac274f5] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 09/20/23 13:55:27.794
    Sep 20 13:55:27.823: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-20T13:55:07Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-20T13:55:27Z]] name:name2 resourceVersion:48207 uid:0f3961c9-737a-41d8-b39f-1d9d43099d45] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 09/20/23 13:55:37.825
    Sep 20 13:55:37.835: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-20T13:54:57Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-20T13:55:17Z]] name:name1 resourceVersion:48241 uid:a83b1670-d225-4f15-8f7a-bc527ac274f5] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 09/20/23 13:55:47.835
    Sep 20 13:55:48.059: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-20T13:55:07Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-20T13:55:27Z]] name:name2 resourceVersion:48278 uid:0f3961c9-737a-41d8-b39f-1d9d43099d45] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:55:58.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-7774" for this suite. 09/20/23 13:55:58.944
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:55:58.955
Sep 20 13:55:58.955: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename events 09/20/23 13:55:58.956
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:55:59.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:55:59.408
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 09/20/23 13:55:59.414
STEP: listing events in all namespaces 09/20/23 13:55:59.449
STEP: listing events in test namespace 09/20/23 13:55:59.463
STEP: listing events with field selection filtering on source 09/20/23 13:55:59.468
STEP: listing events with field selection filtering on reportingController 09/20/23 13:55:59.473
STEP: getting the test event 09/20/23 13:55:59.478
STEP: patching the test event 09/20/23 13:55:59.507
STEP: getting the test event 09/20/23 13:55:59.52
STEP: updating the test event 09/20/23 13:55:59.523
STEP: getting the test event 09/20/23 13:55:59.531
STEP: deleting the test event 09/20/23 13:55:59.534
STEP: listing events in all namespaces 09/20/23 13:55:59.54
STEP: listing events in test namespace 09/20/23 13:55:59.55
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Sep 20 13:55:59.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-3332" for this suite. 09/20/23 13:55:59.557
------------------------------
â€¢ [0.738 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:55:58.955
    Sep 20 13:55:58.955: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename events 09/20/23 13:55:58.956
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:55:59.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:55:59.408
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 09/20/23 13:55:59.414
    STEP: listing events in all namespaces 09/20/23 13:55:59.449
    STEP: listing events in test namespace 09/20/23 13:55:59.463
    STEP: listing events with field selection filtering on source 09/20/23 13:55:59.468
    STEP: listing events with field selection filtering on reportingController 09/20/23 13:55:59.473
    STEP: getting the test event 09/20/23 13:55:59.478
    STEP: patching the test event 09/20/23 13:55:59.507
    STEP: getting the test event 09/20/23 13:55:59.52
    STEP: updating the test event 09/20/23 13:55:59.523
    STEP: getting the test event 09/20/23 13:55:59.531
    STEP: deleting the test event 09/20/23 13:55:59.534
    STEP: listing events in all namespaces 09/20/23 13:55:59.54
    STEP: listing events in test namespace 09/20/23 13:55:59.55
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:55:59.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-3332" for this suite. 09/20/23 13:55:59.557
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:55:59.696
Sep 20 13:55:59.696: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename daemonsets 09/20/23 13:55:59.696
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:55:59.901
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:55:59.905
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834
STEP: Creating simple DaemonSet "daemon-set" 09/20/23 13:56:00.06
STEP: Check that daemon pods launch on every node of the cluster. 09/20/23 13:56:00.145
Sep 20 13:56:00.237: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:56:00.237: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:56:00.237: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:56:00.243: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 13:56:00.243: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 13:56:01.249: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:56:01.250: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:56:01.250: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:56:01.252: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep 20 13:56:01.252: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 13:56:02.839: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:56:02.839: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:56:02.839: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:56:02.845: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep 20 13:56:02.846: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 13:56:03.395: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:56:03.395: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:56:03.395: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:56:03.401: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep 20 13:56:03.401: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 13:56:04.248: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:56:04.248: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:56:04.248: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:56:04.251: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep 20 13:56:04.251: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
Sep 20 13:56:05.511: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:56:05.511: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:56:05.511: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 20 13:56:05.514: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Sep 20 13:56:05.515: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 09/20/23 13:56:05.518
STEP: DeleteCollection of the DaemonSets 09/20/23 13:56:05.522
STEP: Verify that ReplicaSets have been deleted 09/20/23 13:56:05.532
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
Sep 20 13:56:05.540: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"48402"},"items":null}

Sep 20 13:56:05.543: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"48402"},"items":[{"metadata":{"name":"daemon-set-8lw9h","generateName":"daemon-set-","namespace":"daemonsets-956","uid":"6c284587-285e-4b0f-ae18-8bd6e3d7ee7d","resourceVersion":"48369","creationTimestamp":"2023-09-20T13:56:00Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"44f009dc-f74c-4e45-80d3-eca8721a61ca","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-09-20T13:56:00Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44f009dc-f74c-4e45-80d3-eca8721a61ca\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-09-20T13:56:02Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.4.118\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-cfcbd","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-cfcbd","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"mycluster-ww3cg64etuwi-node-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["mycluster-ww3cg64etuwi-node-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-20T13:56:00Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-20T13:56:02Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-20T13:56:02Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-20T13:56:00Z"}],"hostIP":"192.168.10.64","podIP":"10.100.4.118","podIPs":[{"ip":"10.100.4.118"}],"startTime":"2023-09-20T13:56:00Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-09-20T13:56:01Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://4c23dac4f7cecb689aafcffdef9c900e3917bac5e1a5aab7b82ff2785373734b","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-f2m2b","generateName":"daemon-set-","namespace":"daemonsets-956","uid":"2d795a34-9c92-484f-9a0e-2ea86ce68821","resourceVersion":"48375","creationTimestamp":"2023-09-20T13:56:00Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"44f009dc-f74c-4e45-80d3-eca8721a61ca","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-09-20T13:56:00Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44f009dc-f74c-4e45-80d3-eca8721a61ca\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-09-20T13:56:02Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.3.208\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-5c8m7","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-5c8m7","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"mycluster-ww3cg64etuwi-node-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["mycluster-ww3cg64etuwi-node-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-20T13:56:00Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-20T13:56:02Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-20T13:56:02Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-20T13:56:00Z"}],"hostIP":"192.168.10.172","podIP":"10.100.3.208","podIPs":[{"ip":"10.100.3.208"}],"startTime":"2023-09-20T13:56:00Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-09-20T13:56:02Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://80db762b9a309f2686105bcc4909034c4585f136d01a5f087bbaebd645a014aa","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-hv2f7","generateName":"daemon-set-","namespace":"daemonsets-956","uid":"a2483367-fd75-49ce-b59f-4536dded0cc5","resourceVersion":"48392","creationTimestamp":"2023-09-20T13:56:00Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"44f009dc-f74c-4e45-80d3-eca8721a61ca","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-09-20T13:56:00Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44f009dc-f74c-4e45-80d3-eca8721a61ca\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-09-20T13:56:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.5.170\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-th2xd","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-th2xd","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"mycluster-ww3cg64etuwi-node-0","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["mycluster-ww3cg64etuwi-node-0"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-20T13:56:00Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-20T13:56:04Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-20T13:56:04Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-20T13:56:00Z"}],"hostIP":"192.168.10.173","podIP":"10.100.5.170","podIPs":[{"ip":"10.100.5.170"}],"startTime":"2023-09-20T13:56:00Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-09-20T13:56:03Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://9bc941dcc44ec8d8313e5bc5370e1af8d02e73f765ee6e57757cb26ea56d0439","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep 20 13:56:05.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-956" for this suite. 09/20/23 13:56:05.558
------------------------------
â€¢ [SLOW TEST] [6.061 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:55:59.696
    Sep 20 13:55:59.696: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename daemonsets 09/20/23 13:55:59.696
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:55:59.901
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:55:59.905
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:834
    STEP: Creating simple DaemonSet "daemon-set" 09/20/23 13:56:00.06
    STEP: Check that daemon pods launch on every node of the cluster. 09/20/23 13:56:00.145
    Sep 20 13:56:00.237: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:56:00.237: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:56:00.237: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:56:00.243: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 13:56:00.243: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 13:56:01.249: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:56:01.250: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:56:01.250: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:56:01.252: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep 20 13:56:01.252: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 13:56:02.839: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:56:02.839: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:56:02.839: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:56:02.845: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep 20 13:56:02.846: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 13:56:03.395: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:56:03.395: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:56:03.395: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:56:03.401: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep 20 13:56:03.401: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 13:56:04.248: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:56:04.248: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:56:04.248: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:56:04.251: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep 20 13:56:04.251: INFO: Node mycluster-ww3cg64etuwi-node-0 is running 0 daemon pod, expected 1
    Sep 20 13:56:05.511: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:56:05.511: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:56:05.511: INFO: DaemonSet pods can't tolerate node mycluster-ww3cg64etuwi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep 20 13:56:05.514: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Sep 20 13:56:05.515: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 09/20/23 13:56:05.518
    STEP: DeleteCollection of the DaemonSets 09/20/23 13:56:05.522
    STEP: Verify that ReplicaSets have been deleted 09/20/23 13:56:05.532
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    Sep 20 13:56:05.540: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"48402"},"items":null}

    Sep 20 13:56:05.543: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"48402"},"items":[{"metadata":{"name":"daemon-set-8lw9h","generateName":"daemon-set-","namespace":"daemonsets-956","uid":"6c284587-285e-4b0f-ae18-8bd6e3d7ee7d","resourceVersion":"48369","creationTimestamp":"2023-09-20T13:56:00Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"44f009dc-f74c-4e45-80d3-eca8721a61ca","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-09-20T13:56:00Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44f009dc-f74c-4e45-80d3-eca8721a61ca\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-09-20T13:56:02Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.4.118\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-cfcbd","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-cfcbd","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"mycluster-ww3cg64etuwi-node-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["mycluster-ww3cg64etuwi-node-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-20T13:56:00Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-20T13:56:02Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-20T13:56:02Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-20T13:56:00Z"}],"hostIP":"192.168.10.64","podIP":"10.100.4.118","podIPs":[{"ip":"10.100.4.118"}],"startTime":"2023-09-20T13:56:00Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-09-20T13:56:01Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://4c23dac4f7cecb689aafcffdef9c900e3917bac5e1a5aab7b82ff2785373734b","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-f2m2b","generateName":"daemon-set-","namespace":"daemonsets-956","uid":"2d795a34-9c92-484f-9a0e-2ea86ce68821","resourceVersion":"48375","creationTimestamp":"2023-09-20T13:56:00Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"44f009dc-f74c-4e45-80d3-eca8721a61ca","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-09-20T13:56:00Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44f009dc-f74c-4e45-80d3-eca8721a61ca\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-09-20T13:56:02Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.3.208\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-5c8m7","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-5c8m7","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"mycluster-ww3cg64etuwi-node-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["mycluster-ww3cg64etuwi-node-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-20T13:56:00Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-20T13:56:02Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-20T13:56:02Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-20T13:56:00Z"}],"hostIP":"192.168.10.172","podIP":"10.100.3.208","podIPs":[{"ip":"10.100.3.208"}],"startTime":"2023-09-20T13:56:00Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-09-20T13:56:02Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://80db762b9a309f2686105bcc4909034c4585f136d01a5f087bbaebd645a014aa","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-hv2f7","generateName":"daemon-set-","namespace":"daemonsets-956","uid":"a2483367-fd75-49ce-b59f-4536dded0cc5","resourceVersion":"48392","creationTimestamp":"2023-09-20T13:56:00Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"44f009dc-f74c-4e45-80d3-eca8721a61ca","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-09-20T13:56:00Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44f009dc-f74c-4e45-80d3-eca8721a61ca\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-09-20T13:56:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.5.170\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-th2xd","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-th2xd","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"mycluster-ww3cg64etuwi-node-0","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["mycluster-ww3cg64etuwi-node-0"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-20T13:56:00Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-20T13:56:04Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-20T13:56:04Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-20T13:56:00Z"}],"hostIP":"192.168.10.173","podIP":"10.100.5.170","podIPs":[{"ip":"10.100.5.170"}],"startTime":"2023-09-20T13:56:00Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-09-20T13:56:03Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://9bc941dcc44ec8d8313e5bc5370e1af8d02e73f765ee6e57757cb26ea56d0439","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:56:05.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-956" for this suite. 09/20/23 13:56:05.558
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:56:05.761
Sep 20 13:56:05.761: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename aggregator 09/20/23 13:56:05.762
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:56:06.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:56:06.417
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Sep 20 13:56:06.422: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 09/20/23 13:56:06.423
Sep 20 13:56:08.033: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Sep 20 13:56:10.460: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 13:56:12.504: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 13:56:14.466: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 13:56:17.252: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 13:56:18.465: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 13:56:20.798: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 13:56:22.465: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 13:56:24.501: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 13:56:26.466: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 13:56:28.849: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 13:56:30.467: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 13:56:32.467: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 13:56:34.666: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 13:56:36.464: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 13:56:38.524: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 13:56:41.726: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 13:56:42.464: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 13:56:44.480: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 13:56:46.623: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 13:56:48.464: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 13:56:50.464: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 13:56:52.465: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 13:56:54.465: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 13:56:56.466: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 13:56:59.434: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 13:57:02.231: INFO: Waited 765.633991ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 09/20/23 13:57:02.783
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 09/20/23 13:57:02.787
STEP: List APIServices 09/20/23 13:57:02.818
Sep 20 13:57:02.826: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Sep 20 13:57:06.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-8330" for this suite. 09/20/23 13:57:06.794
------------------------------
â€¢ [SLOW TEST] [61.044 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:56:05.761
    Sep 20 13:56:05.761: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename aggregator 09/20/23 13:56:05.762
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:56:06.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:56:06.417
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Sep 20 13:56:06.422: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 09/20/23 13:56:06.423
    Sep 20 13:56:08.033: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Sep 20 13:56:10.460: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 13:56:12.504: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 13:56:14.466: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 13:56:17.252: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 13:56:18.465: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 13:56:20.798: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 13:56:22.465: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 13:56:24.501: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 13:56:26.466: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 13:56:28.849: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 13:56:30.467: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 13:56:32.467: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 13:56:34.666: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 13:56:36.464: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 13:56:38.524: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 13:56:41.726: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 13:56:42.464: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 13:56:44.480: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 13:56:46.623: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 13:56:48.464: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 13:56:50.464: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 13:56:52.465: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 13:56:54.465: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 13:56:56.466: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 13:56:59.434: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 13, 56, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 13:57:02.231: INFO: Waited 765.633991ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 09/20/23 13:57:02.783
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 09/20/23 13:57:02.787
    STEP: List APIServices 09/20/23 13:57:02.818
    Sep 20 13:57:02.826: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:57:06.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-8330" for this suite. 09/20/23 13:57:06.794
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:57:06.819
Sep 20 13:57:06.819: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename subpath 09/20/23 13:57:06.82
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:57:06.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:57:06.875
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/20/23 13:57:06.881
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-x5dq 09/20/23 13:57:06.899
STEP: Creating a pod to test atomic-volume-subpath 09/20/23 13:57:06.899
Sep 20 13:57:06.912: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-x5dq" in namespace "subpath-6065" to be "Succeeded or Failed"
Sep 20 13:57:07.125: INFO: Pod "pod-subpath-test-secret-x5dq": Phase="Pending", Reason="", readiness=false. Elapsed: 213.613701ms
Sep 20 13:57:09.296: INFO: Pod "pod-subpath-test-secret-x5dq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.384492761s
Sep 20 13:57:11.220: INFO: Pod "pod-subpath-test-secret-x5dq": Phase="Running", Reason="", readiness=true. Elapsed: 4.308632023s
Sep 20 13:57:13.778: INFO: Pod "pod-subpath-test-secret-x5dq": Phase="Running", Reason="", readiness=true. Elapsed: 6.866127147s
Sep 20 13:57:15.669: INFO: Pod "pod-subpath-test-secret-x5dq": Phase="Running", Reason="", readiness=true. Elapsed: 8.757096939s
Sep 20 13:57:17.676: INFO: Pod "pod-subpath-test-secret-x5dq": Phase="Running", Reason="", readiness=true. Elapsed: 10.763898347s
Sep 20 13:57:19.130: INFO: Pod "pod-subpath-test-secret-x5dq": Phase="Running", Reason="", readiness=true. Elapsed: 12.21809874s
Sep 20 13:57:21.213: INFO: Pod "pod-subpath-test-secret-x5dq": Phase="Running", Reason="", readiness=true. Elapsed: 14.301120248s
Sep 20 13:57:23.187: INFO: Pod "pod-subpath-test-secret-x5dq": Phase="Running", Reason="", readiness=true. Elapsed: 16.275650498s
Sep 20 13:57:25.162: INFO: Pod "pod-subpath-test-secret-x5dq": Phase="Running", Reason="", readiness=true. Elapsed: 18.250526839s
Sep 20 13:57:27.130: INFO: Pod "pod-subpath-test-secret-x5dq": Phase="Running", Reason="", readiness=true. Elapsed: 20.218726122s
Sep 20 13:57:29.544: INFO: Pod "pod-subpath-test-secret-x5dq": Phase="Running", Reason="", readiness=true. Elapsed: 22.631937891s
Sep 20 13:57:31.131: INFO: Pod "pod-subpath-test-secret-x5dq": Phase="Running", Reason="", readiness=false. Elapsed: 24.219118482s
Sep 20 13:57:33.130: INFO: Pod "pod-subpath-test-secret-x5dq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.218062358s
STEP: Saw pod success 09/20/23 13:57:33.13
Sep 20 13:57:33.130: INFO: Pod "pod-subpath-test-secret-x5dq" satisfied condition "Succeeded or Failed"
Sep 20 13:57:33.132: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-subpath-test-secret-x5dq container test-container-subpath-secret-x5dq: <nil>
STEP: delete the pod 09/20/23 13:57:33.19
Sep 20 13:57:33.332: INFO: Waiting for pod pod-subpath-test-secret-x5dq to disappear
Sep 20 13:57:33.336: INFO: Pod pod-subpath-test-secret-x5dq no longer exists
STEP: Deleting pod pod-subpath-test-secret-x5dq 09/20/23 13:57:33.336
Sep 20 13:57:33.336: INFO: Deleting pod "pod-subpath-test-secret-x5dq" in namespace "subpath-6065"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Sep 20 13:57:33.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-6065" for this suite. 09/20/23 13:57:33.344
------------------------------
â€¢ [SLOW TEST] [26.533 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:57:06.819
    Sep 20 13:57:06.819: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename subpath 09/20/23 13:57:06.82
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:57:06.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:57:06.875
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/20/23 13:57:06.881
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-x5dq 09/20/23 13:57:06.899
    STEP: Creating a pod to test atomic-volume-subpath 09/20/23 13:57:06.899
    Sep 20 13:57:06.912: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-x5dq" in namespace "subpath-6065" to be "Succeeded or Failed"
    Sep 20 13:57:07.125: INFO: Pod "pod-subpath-test-secret-x5dq": Phase="Pending", Reason="", readiness=false. Elapsed: 213.613701ms
    Sep 20 13:57:09.296: INFO: Pod "pod-subpath-test-secret-x5dq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.384492761s
    Sep 20 13:57:11.220: INFO: Pod "pod-subpath-test-secret-x5dq": Phase="Running", Reason="", readiness=true. Elapsed: 4.308632023s
    Sep 20 13:57:13.778: INFO: Pod "pod-subpath-test-secret-x5dq": Phase="Running", Reason="", readiness=true. Elapsed: 6.866127147s
    Sep 20 13:57:15.669: INFO: Pod "pod-subpath-test-secret-x5dq": Phase="Running", Reason="", readiness=true. Elapsed: 8.757096939s
    Sep 20 13:57:17.676: INFO: Pod "pod-subpath-test-secret-x5dq": Phase="Running", Reason="", readiness=true. Elapsed: 10.763898347s
    Sep 20 13:57:19.130: INFO: Pod "pod-subpath-test-secret-x5dq": Phase="Running", Reason="", readiness=true. Elapsed: 12.21809874s
    Sep 20 13:57:21.213: INFO: Pod "pod-subpath-test-secret-x5dq": Phase="Running", Reason="", readiness=true. Elapsed: 14.301120248s
    Sep 20 13:57:23.187: INFO: Pod "pod-subpath-test-secret-x5dq": Phase="Running", Reason="", readiness=true. Elapsed: 16.275650498s
    Sep 20 13:57:25.162: INFO: Pod "pod-subpath-test-secret-x5dq": Phase="Running", Reason="", readiness=true. Elapsed: 18.250526839s
    Sep 20 13:57:27.130: INFO: Pod "pod-subpath-test-secret-x5dq": Phase="Running", Reason="", readiness=true. Elapsed: 20.218726122s
    Sep 20 13:57:29.544: INFO: Pod "pod-subpath-test-secret-x5dq": Phase="Running", Reason="", readiness=true. Elapsed: 22.631937891s
    Sep 20 13:57:31.131: INFO: Pod "pod-subpath-test-secret-x5dq": Phase="Running", Reason="", readiness=false. Elapsed: 24.219118482s
    Sep 20 13:57:33.130: INFO: Pod "pod-subpath-test-secret-x5dq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.218062358s
    STEP: Saw pod success 09/20/23 13:57:33.13
    Sep 20 13:57:33.130: INFO: Pod "pod-subpath-test-secret-x5dq" satisfied condition "Succeeded or Failed"
    Sep 20 13:57:33.132: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-subpath-test-secret-x5dq container test-container-subpath-secret-x5dq: <nil>
    STEP: delete the pod 09/20/23 13:57:33.19
    Sep 20 13:57:33.332: INFO: Waiting for pod pod-subpath-test-secret-x5dq to disappear
    Sep 20 13:57:33.336: INFO: Pod pod-subpath-test-secret-x5dq no longer exists
    STEP: Deleting pod pod-subpath-test-secret-x5dq 09/20/23 13:57:33.336
    Sep 20 13:57:33.336: INFO: Deleting pod "pod-subpath-test-secret-x5dq" in namespace "subpath-6065"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:57:33.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-6065" for this suite. 09/20/23 13:57:33.344
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:57:33.353
Sep 20 13:57:33.353: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename crd-publish-openapi 09/20/23 13:57:33.353
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:57:33.371
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:57:33.373
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 09/20/23 13:57:33.377
Sep 20 13:57:33.378: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
Sep 20 13:57:35.839: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep 20 13:57:46.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6899" for this suite. 09/20/23 13:57:46.862
------------------------------
â€¢ [SLOW TEST] [13.658 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:57:33.353
    Sep 20 13:57:33.353: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename crd-publish-openapi 09/20/23 13:57:33.353
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:57:33.371
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:57:33.373
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 09/20/23 13:57:33.377
    Sep 20 13:57:33.378: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    Sep 20 13:57:35.839: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:57:46.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6899" for this suite. 09/20/23 13:57:46.862
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:57:47.011
Sep 20 13:57:47.011: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename kubelet-test 09/20/23 13:57:47.012
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:57:47.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:57:47.044
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Sep 20 13:57:55.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-4526" for this suite. 09/20/23 13:57:55.123
------------------------------
â€¢ [SLOW TEST] [8.124 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:57:47.011
    Sep 20 13:57:47.011: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename kubelet-test 09/20/23 13:57:47.012
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:57:47.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:57:47.044
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:57:55.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-4526" for this suite. 09/20/23 13:57:55.123
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:57:55.137
Sep 20 13:57:55.137: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename resourcequota 09/20/23 13:57:55.138
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:57:55.368
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:57:55.371
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 09/20/23 13:57:55.376
STEP: Creating a ResourceQuota 09/20/23 13:58:00.416
STEP: Ensuring resource quota status is calculated 09/20/23 13:58:00.799
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep 20 13:58:02.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3072" for this suite. 09/20/23 13:58:02.811
------------------------------
â€¢ [SLOW TEST] [7.863 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:57:55.137
    Sep 20 13:57:55.137: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename resourcequota 09/20/23 13:57:55.138
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:57:55.368
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:57:55.371
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 09/20/23 13:57:55.376
    STEP: Creating a ResourceQuota 09/20/23 13:58:00.416
    STEP: Ensuring resource quota status is calculated 09/20/23 13:58:00.799
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:58:02.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3072" for this suite. 09/20/23 13:58:02.811
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:58:03.002
Sep 20 13:58:03.002: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename containers 09/20/23 13:58:03.003
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:58:03.029
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:58:03.032
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 09/20/23 13:58:03.036
Sep 20 13:58:03.044: INFO: Waiting up to 5m0s for pod "client-containers-78567fcf-2c87-4a8c-8237-8aad4be1c3e5" in namespace "containers-3063" to be "Succeeded or Failed"
Sep 20 13:58:03.049: INFO: Pod "client-containers-78567fcf-2c87-4a8c-8237-8aad4be1c3e5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.323994ms
Sep 20 13:58:05.053: INFO: Pod "client-containers-78567fcf-2c87-4a8c-8237-8aad4be1c3e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009067094s
Sep 20 13:58:07.057: INFO: Pod "client-containers-78567fcf-2c87-4a8c-8237-8aad4be1c3e5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01292175s
Sep 20 13:58:09.053: INFO: Pod "client-containers-78567fcf-2c87-4a8c-8237-8aad4be1c3e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009050123s
STEP: Saw pod success 09/20/23 13:58:09.053
Sep 20 13:58:09.054: INFO: Pod "client-containers-78567fcf-2c87-4a8c-8237-8aad4be1c3e5" satisfied condition "Succeeded or Failed"
Sep 20 13:58:09.059: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod client-containers-78567fcf-2c87-4a8c-8237-8aad4be1c3e5 container agnhost-container: <nil>
STEP: delete the pod 09/20/23 13:58:09.116
Sep 20 13:58:09.296: INFO: Waiting for pod client-containers-78567fcf-2c87-4a8c-8237-8aad4be1c3e5 to disappear
Sep 20 13:58:09.301: INFO: Pod client-containers-78567fcf-2c87-4a8c-8237-8aad4be1c3e5 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Sep 20 13:58:09.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-3063" for this suite. 09/20/23 13:58:09.306
------------------------------
â€¢ [SLOW TEST] [6.384 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:58:03.002
    Sep 20 13:58:03.002: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename containers 09/20/23 13:58:03.003
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:58:03.029
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:58:03.032
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 09/20/23 13:58:03.036
    Sep 20 13:58:03.044: INFO: Waiting up to 5m0s for pod "client-containers-78567fcf-2c87-4a8c-8237-8aad4be1c3e5" in namespace "containers-3063" to be "Succeeded or Failed"
    Sep 20 13:58:03.049: INFO: Pod "client-containers-78567fcf-2c87-4a8c-8237-8aad4be1c3e5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.323994ms
    Sep 20 13:58:05.053: INFO: Pod "client-containers-78567fcf-2c87-4a8c-8237-8aad4be1c3e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009067094s
    Sep 20 13:58:07.057: INFO: Pod "client-containers-78567fcf-2c87-4a8c-8237-8aad4be1c3e5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01292175s
    Sep 20 13:58:09.053: INFO: Pod "client-containers-78567fcf-2c87-4a8c-8237-8aad4be1c3e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009050123s
    STEP: Saw pod success 09/20/23 13:58:09.053
    Sep 20 13:58:09.054: INFO: Pod "client-containers-78567fcf-2c87-4a8c-8237-8aad4be1c3e5" satisfied condition "Succeeded or Failed"
    Sep 20 13:58:09.059: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod client-containers-78567fcf-2c87-4a8c-8237-8aad4be1c3e5 container agnhost-container: <nil>
    STEP: delete the pod 09/20/23 13:58:09.116
    Sep 20 13:58:09.296: INFO: Waiting for pod client-containers-78567fcf-2c87-4a8c-8237-8aad4be1c3e5 to disappear
    Sep 20 13:58:09.301: INFO: Pod client-containers-78567fcf-2c87-4a8c-8237-8aad4be1c3e5 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:58:09.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-3063" for this suite. 09/20/23 13:58:09.306
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:58:09.388
Sep 20 13:58:09.389: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename limitrange 09/20/23 13:58:09.389
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:58:09.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:58:09.415
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-sk6xh" in namespace "limitrange-8465" 09/20/23 13:58:09.581
STEP: Creating another limitRange in another namespace 09/20/23 13:58:09.597
Sep 20 13:58:09.619: INFO: Namespace "e2e-limitrange-sk6xh-9584" created
Sep 20 13:58:09.619: INFO: Creating LimitRange "e2e-limitrange-sk6xh" in namespace "e2e-limitrange-sk6xh-9584"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-sk6xh" 09/20/23 13:58:09.626
Sep 20 13:58:09.630: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-sk6xh" in "limitrange-8465" namespace 09/20/23 13:58:09.63
Sep 20 13:58:09.711: INFO: LimitRange "e2e-limitrange-sk6xh" has been patched
STEP: Delete LimitRange "e2e-limitrange-sk6xh" by Collection with labelSelector: "e2e-limitrange-sk6xh=patched" 09/20/23 13:58:09.711
STEP: Confirm that the limitRange "e2e-limitrange-sk6xh" has been deleted 09/20/23 13:58:09.725
Sep 20 13:58:09.725: INFO: Requesting list of LimitRange to confirm quantity
Sep 20 13:58:09.731: INFO: Found 0 LimitRange with label "e2e-limitrange-sk6xh=patched"
Sep 20 13:58:09.731: INFO: LimitRange "e2e-limitrange-sk6xh" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-sk6xh" 09/20/23 13:58:09.731
Sep 20 13:58:09.736: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Sep 20 13:58:09.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-8465" for this suite. 09/20/23 13:58:09.743
STEP: Destroying namespace "e2e-limitrange-sk6xh-9584" for this suite. 09/20/23 13:58:09.755
------------------------------
â€¢ [0.381 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:58:09.388
    Sep 20 13:58:09.389: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename limitrange 09/20/23 13:58:09.389
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:58:09.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:58:09.415
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-sk6xh" in namespace "limitrange-8465" 09/20/23 13:58:09.581
    STEP: Creating another limitRange in another namespace 09/20/23 13:58:09.597
    Sep 20 13:58:09.619: INFO: Namespace "e2e-limitrange-sk6xh-9584" created
    Sep 20 13:58:09.619: INFO: Creating LimitRange "e2e-limitrange-sk6xh" in namespace "e2e-limitrange-sk6xh-9584"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-sk6xh" 09/20/23 13:58:09.626
    Sep 20 13:58:09.630: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-sk6xh" in "limitrange-8465" namespace 09/20/23 13:58:09.63
    Sep 20 13:58:09.711: INFO: LimitRange "e2e-limitrange-sk6xh" has been patched
    STEP: Delete LimitRange "e2e-limitrange-sk6xh" by Collection with labelSelector: "e2e-limitrange-sk6xh=patched" 09/20/23 13:58:09.711
    STEP: Confirm that the limitRange "e2e-limitrange-sk6xh" has been deleted 09/20/23 13:58:09.725
    Sep 20 13:58:09.725: INFO: Requesting list of LimitRange to confirm quantity
    Sep 20 13:58:09.731: INFO: Found 0 LimitRange with label "e2e-limitrange-sk6xh=patched"
    Sep 20 13:58:09.731: INFO: LimitRange "e2e-limitrange-sk6xh" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-sk6xh" 09/20/23 13:58:09.731
    Sep 20 13:58:09.736: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:58:09.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-8465" for this suite. 09/20/23 13:58:09.743
    STEP: Destroying namespace "e2e-limitrange-sk6xh-9584" for this suite. 09/20/23 13:58:09.755
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:58:09.773
Sep 20 13:58:09.773: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename projected 09/20/23 13:58:09.774
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:58:09.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:58:09.818
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-b4cd3b76-597b-4585-ba5f-80a3735452a9 09/20/23 13:58:09.834
STEP: Creating a pod to test consume configMaps 09/20/23 13:58:09.848
Sep 20 13:58:09.869: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c9a09d5a-c07b-4f23-ad74-a83db1e0b383" in namespace "projected-1173" to be "Succeeded or Failed"
Sep 20 13:58:09.877: INFO: Pod "pod-projected-configmaps-c9a09d5a-c07b-4f23-ad74-a83db1e0b383": Phase="Pending", Reason="", readiness=false. Elapsed: 8.331273ms
Sep 20 13:58:11.991: INFO: Pod "pod-projected-configmaps-c9a09d5a-c07b-4f23-ad74-a83db1e0b383": Phase="Pending", Reason="", readiness=false. Elapsed: 2.12191034s
Sep 20 13:58:13.882: INFO: Pod "pod-projected-configmaps-c9a09d5a-c07b-4f23-ad74-a83db1e0b383": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013278031s
Sep 20 13:58:15.925: INFO: Pod "pod-projected-configmaps-c9a09d5a-c07b-4f23-ad74-a83db1e0b383": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.056378643s
STEP: Saw pod success 09/20/23 13:58:15.926
Sep 20 13:58:15.926: INFO: Pod "pod-projected-configmaps-c9a09d5a-c07b-4f23-ad74-a83db1e0b383" satisfied condition "Succeeded or Failed"
Sep 20 13:58:15.931: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-projected-configmaps-c9a09d5a-c07b-4f23-ad74-a83db1e0b383 container agnhost-container: <nil>
STEP: delete the pod 09/20/23 13:58:15.947
Sep 20 13:58:16.298: INFO: Waiting for pod pod-projected-configmaps-c9a09d5a-c07b-4f23-ad74-a83db1e0b383 to disappear
Sep 20 13:58:16.665: INFO: Pod pod-projected-configmaps-c9a09d5a-c07b-4f23-ad74-a83db1e0b383 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep 20 13:58:16.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1173" for this suite. 09/20/23 13:58:16.675
------------------------------
â€¢ [SLOW TEST] [6.912 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:58:09.773
    Sep 20 13:58:09.773: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename projected 09/20/23 13:58:09.774
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:58:09.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:58:09.818
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-b4cd3b76-597b-4585-ba5f-80a3735452a9 09/20/23 13:58:09.834
    STEP: Creating a pod to test consume configMaps 09/20/23 13:58:09.848
    Sep 20 13:58:09.869: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c9a09d5a-c07b-4f23-ad74-a83db1e0b383" in namespace "projected-1173" to be "Succeeded or Failed"
    Sep 20 13:58:09.877: INFO: Pod "pod-projected-configmaps-c9a09d5a-c07b-4f23-ad74-a83db1e0b383": Phase="Pending", Reason="", readiness=false. Elapsed: 8.331273ms
    Sep 20 13:58:11.991: INFO: Pod "pod-projected-configmaps-c9a09d5a-c07b-4f23-ad74-a83db1e0b383": Phase="Pending", Reason="", readiness=false. Elapsed: 2.12191034s
    Sep 20 13:58:13.882: INFO: Pod "pod-projected-configmaps-c9a09d5a-c07b-4f23-ad74-a83db1e0b383": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013278031s
    Sep 20 13:58:15.925: INFO: Pod "pod-projected-configmaps-c9a09d5a-c07b-4f23-ad74-a83db1e0b383": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.056378643s
    STEP: Saw pod success 09/20/23 13:58:15.926
    Sep 20 13:58:15.926: INFO: Pod "pod-projected-configmaps-c9a09d5a-c07b-4f23-ad74-a83db1e0b383" satisfied condition "Succeeded or Failed"
    Sep 20 13:58:15.931: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-projected-configmaps-c9a09d5a-c07b-4f23-ad74-a83db1e0b383 container agnhost-container: <nil>
    STEP: delete the pod 09/20/23 13:58:15.947
    Sep 20 13:58:16.298: INFO: Waiting for pod pod-projected-configmaps-c9a09d5a-c07b-4f23-ad74-a83db1e0b383 to disappear
    Sep 20 13:58:16.665: INFO: Pod pod-projected-configmaps-c9a09d5a-c07b-4f23-ad74-a83db1e0b383 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:58:16.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1173" for this suite. 09/20/23 13:58:16.675
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:58:16.689
Sep 20 13:58:16.689: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename limitrange 09/20/23 13:58:16.689
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:58:17.012
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:58:17.017
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 09/20/23 13:58:17.024
STEP: Setting up watch 09/20/23 13:58:17.024
STEP: Submitting a LimitRange 09/20/23 13:58:17.129
STEP: Verifying LimitRange creation was observed 09/20/23 13:58:17.137
STEP: Fetching the LimitRange to ensure it has proper values 09/20/23 13:58:17.137
Sep 20 13:58:17.146: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep 20 13:58:17.146: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 09/20/23 13:58:17.146
STEP: Ensuring Pod has resource requirements applied from LimitRange 09/20/23 13:58:17.154
Sep 20 13:58:17.160: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep 20 13:58:17.160: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 09/20/23 13:58:17.16
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 09/20/23 13:58:17.317
Sep 20 13:58:17.474: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Sep 20 13:58:17.474: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 09/20/23 13:58:17.474
STEP: Failing to create a Pod with more than max resources 09/20/23 13:58:17.476
STEP: Updating a LimitRange 09/20/23 13:58:17.478
STEP: Verifying LimitRange updating is effective 09/20/23 13:58:17.566
STEP: Creating a Pod with less than former min resources 09/20/23 13:58:19.57
STEP: Failing to create a Pod with more than max resources 09/20/23 13:58:19.601
STEP: Deleting a LimitRange 09/20/23 13:58:19.604
STEP: Verifying the LimitRange was deleted 09/20/23 13:58:19.622
Sep 20 13:58:24.679: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 09/20/23 13:58:24.679
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Sep 20 13:58:24.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-5344" for this suite. 09/20/23 13:58:24.714
------------------------------
â€¢ [SLOW TEST] [8.465 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:58:16.689
    Sep 20 13:58:16.689: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename limitrange 09/20/23 13:58:16.689
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:58:17.012
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:58:17.017
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 09/20/23 13:58:17.024
    STEP: Setting up watch 09/20/23 13:58:17.024
    STEP: Submitting a LimitRange 09/20/23 13:58:17.129
    STEP: Verifying LimitRange creation was observed 09/20/23 13:58:17.137
    STEP: Fetching the LimitRange to ensure it has proper values 09/20/23 13:58:17.137
    Sep 20 13:58:17.146: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Sep 20 13:58:17.146: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 09/20/23 13:58:17.146
    STEP: Ensuring Pod has resource requirements applied from LimitRange 09/20/23 13:58:17.154
    Sep 20 13:58:17.160: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Sep 20 13:58:17.160: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 09/20/23 13:58:17.16
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 09/20/23 13:58:17.317
    Sep 20 13:58:17.474: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Sep 20 13:58:17.474: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 09/20/23 13:58:17.474
    STEP: Failing to create a Pod with more than max resources 09/20/23 13:58:17.476
    STEP: Updating a LimitRange 09/20/23 13:58:17.478
    STEP: Verifying LimitRange updating is effective 09/20/23 13:58:17.566
    STEP: Creating a Pod with less than former min resources 09/20/23 13:58:19.57
    STEP: Failing to create a Pod with more than max resources 09/20/23 13:58:19.601
    STEP: Deleting a LimitRange 09/20/23 13:58:19.604
    STEP: Verifying the LimitRange was deleted 09/20/23 13:58:19.622
    Sep 20 13:58:24.679: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 09/20/23 13:58:24.679
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:58:24.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-5344" for this suite. 09/20/23 13:58:24.714
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:58:25.154
Sep 20 13:58:25.154: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename projected 09/20/23 13:58:25.155
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:58:25.225
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:58:25.228
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-03edab79-c36e-48bf-83f6-ada24214e325 09/20/23 13:58:25.256
STEP: Creating a pod to test consume configMaps 09/20/23 13:58:25.509
Sep 20 13:58:25.532: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0b05116e-6c84-4a1e-a733-04df4c1de575" in namespace "projected-7249" to be "Succeeded or Failed"
Sep 20 13:58:25.539: INFO: Pod "pod-projected-configmaps-0b05116e-6c84-4a1e-a733-04df4c1de575": Phase="Pending", Reason="", readiness=false. Elapsed: 7.089835ms
Sep 20 13:58:27.543: INFO: Pod "pod-projected-configmaps-0b05116e-6c84-4a1e-a733-04df4c1de575": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011281786s
Sep 20 13:58:29.545: INFO: Pod "pod-projected-configmaps-0b05116e-6c84-4a1e-a733-04df4c1de575": Phase="Running", Reason="", readiness=true. Elapsed: 4.013938226s
Sep 20 13:58:31.544: INFO: Pod "pod-projected-configmaps-0b05116e-6c84-4a1e-a733-04df4c1de575": Phase="Running", Reason="", readiness=false. Elapsed: 6.012474904s
Sep 20 13:58:33.659: INFO: Pod "pod-projected-configmaps-0b05116e-6c84-4a1e-a733-04df4c1de575": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.127822353s
STEP: Saw pod success 09/20/23 13:58:33.659
Sep 20 13:58:33.660: INFO: Pod "pod-projected-configmaps-0b05116e-6c84-4a1e-a733-04df4c1de575" satisfied condition "Succeeded or Failed"
Sep 20 13:58:33.662: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-projected-configmaps-0b05116e-6c84-4a1e-a733-04df4c1de575 container agnhost-container: <nil>
STEP: delete the pod 09/20/23 13:58:33.673
Sep 20 13:58:33.707: INFO: Waiting for pod pod-projected-configmaps-0b05116e-6c84-4a1e-a733-04df4c1de575 to disappear
Sep 20 13:58:33.711: INFO: Pod pod-projected-configmaps-0b05116e-6c84-4a1e-a733-04df4c1de575 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep 20 13:58:33.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7249" for this suite. 09/20/23 13:58:33.716
------------------------------
â€¢ [SLOW TEST] [8.570 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:58:25.154
    Sep 20 13:58:25.154: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename projected 09/20/23 13:58:25.155
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:58:25.225
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:58:25.228
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-03edab79-c36e-48bf-83f6-ada24214e325 09/20/23 13:58:25.256
    STEP: Creating a pod to test consume configMaps 09/20/23 13:58:25.509
    Sep 20 13:58:25.532: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0b05116e-6c84-4a1e-a733-04df4c1de575" in namespace "projected-7249" to be "Succeeded or Failed"
    Sep 20 13:58:25.539: INFO: Pod "pod-projected-configmaps-0b05116e-6c84-4a1e-a733-04df4c1de575": Phase="Pending", Reason="", readiness=false. Elapsed: 7.089835ms
    Sep 20 13:58:27.543: INFO: Pod "pod-projected-configmaps-0b05116e-6c84-4a1e-a733-04df4c1de575": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011281786s
    Sep 20 13:58:29.545: INFO: Pod "pod-projected-configmaps-0b05116e-6c84-4a1e-a733-04df4c1de575": Phase="Running", Reason="", readiness=true. Elapsed: 4.013938226s
    Sep 20 13:58:31.544: INFO: Pod "pod-projected-configmaps-0b05116e-6c84-4a1e-a733-04df4c1de575": Phase="Running", Reason="", readiness=false. Elapsed: 6.012474904s
    Sep 20 13:58:33.659: INFO: Pod "pod-projected-configmaps-0b05116e-6c84-4a1e-a733-04df4c1de575": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.127822353s
    STEP: Saw pod success 09/20/23 13:58:33.659
    Sep 20 13:58:33.660: INFO: Pod "pod-projected-configmaps-0b05116e-6c84-4a1e-a733-04df4c1de575" satisfied condition "Succeeded or Failed"
    Sep 20 13:58:33.662: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-projected-configmaps-0b05116e-6c84-4a1e-a733-04df4c1de575 container agnhost-container: <nil>
    STEP: delete the pod 09/20/23 13:58:33.673
    Sep 20 13:58:33.707: INFO: Waiting for pod pod-projected-configmaps-0b05116e-6c84-4a1e-a733-04df4c1de575 to disappear
    Sep 20 13:58:33.711: INFO: Pod pod-projected-configmaps-0b05116e-6c84-4a1e-a733-04df4c1de575 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:58:33.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7249" for this suite. 09/20/23 13:58:33.716
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:58:33.729
Sep 20 13:58:33.729: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename kubectl 09/20/23 13:58:33.73
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:58:33.757
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:58:33.763
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 09/20/23 13:58:33.768
Sep 20 13:58:33.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-4841 cluster-info'
Sep 20 13:58:33.921: INFO: stderr: ""
Sep 20 13:58:33.921: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.254.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep 20 13:58:33.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4841" for this suite. 09/20/23 13:58:33.928
------------------------------
â€¢ [0.208 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:58:33.729
    Sep 20 13:58:33.729: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename kubectl 09/20/23 13:58:33.73
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:58:33.757
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:58:33.763
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 09/20/23 13:58:33.768
    Sep 20 13:58:33.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-4841 cluster-info'
    Sep 20 13:58:33.921: INFO: stderr: ""
    Sep 20 13:58:33.921: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.254.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:58:33.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4841" for this suite. 09/20/23 13:58:33.928
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:58:33.939
Sep 20 13:58:33.939: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename configmap 09/20/23 13:58:33.939
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:58:34.183
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:58:34.186
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-ff49f419-69d7-49f7-8037-6126ccaf3196 09/20/23 13:58:34.19
STEP: Creating a pod to test consume configMaps 09/20/23 13:58:34.195
Sep 20 13:58:34.213: INFO: Waiting up to 5m0s for pod "pod-configmaps-faad3460-e646-48b0-ba6a-a91ea28bb6bb" in namespace "configmap-3260" to be "Succeeded or Failed"
Sep 20 13:58:34.219: INFO: Pod "pod-configmaps-faad3460-e646-48b0-ba6a-a91ea28bb6bb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.639042ms
Sep 20 13:58:36.224: INFO: Pod "pod-configmaps-faad3460-e646-48b0-ba6a-a91ea28bb6bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010920055s
Sep 20 13:58:38.296: INFO: Pod "pod-configmaps-faad3460-e646-48b0-ba6a-a91ea28bb6bb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.083119058s
Sep 20 13:58:40.226: INFO: Pod "pod-configmaps-faad3460-e646-48b0-ba6a-a91ea28bb6bb": Phase="Running", Reason="", readiness=false. Elapsed: 6.013276964s
Sep 20 13:58:42.627: INFO: Pod "pod-configmaps-faad3460-e646-48b0-ba6a-a91ea28bb6bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.413876768s
STEP: Saw pod success 09/20/23 13:58:42.627
Sep 20 13:58:42.627: INFO: Pod "pod-configmaps-faad3460-e646-48b0-ba6a-a91ea28bb6bb" satisfied condition "Succeeded or Failed"
Sep 20 13:58:42.631: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-configmaps-faad3460-e646-48b0-ba6a-a91ea28bb6bb container agnhost-container: <nil>
STEP: delete the pod 09/20/23 13:58:42.636
Sep 20 13:58:42.765: INFO: Waiting for pod pod-configmaps-faad3460-e646-48b0-ba6a-a91ea28bb6bb to disappear
Sep 20 13:58:42.769: INFO: Pod pod-configmaps-faad3460-e646-48b0-ba6a-a91ea28bb6bb no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep 20 13:58:42.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3260" for this suite. 09/20/23 13:58:42.773
------------------------------
â€¢ [SLOW TEST] [8.947 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:58:33.939
    Sep 20 13:58:33.939: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename configmap 09/20/23 13:58:33.939
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:58:34.183
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:58:34.186
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-ff49f419-69d7-49f7-8037-6126ccaf3196 09/20/23 13:58:34.19
    STEP: Creating a pod to test consume configMaps 09/20/23 13:58:34.195
    Sep 20 13:58:34.213: INFO: Waiting up to 5m0s for pod "pod-configmaps-faad3460-e646-48b0-ba6a-a91ea28bb6bb" in namespace "configmap-3260" to be "Succeeded or Failed"
    Sep 20 13:58:34.219: INFO: Pod "pod-configmaps-faad3460-e646-48b0-ba6a-a91ea28bb6bb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.639042ms
    Sep 20 13:58:36.224: INFO: Pod "pod-configmaps-faad3460-e646-48b0-ba6a-a91ea28bb6bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010920055s
    Sep 20 13:58:38.296: INFO: Pod "pod-configmaps-faad3460-e646-48b0-ba6a-a91ea28bb6bb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.083119058s
    Sep 20 13:58:40.226: INFO: Pod "pod-configmaps-faad3460-e646-48b0-ba6a-a91ea28bb6bb": Phase="Running", Reason="", readiness=false. Elapsed: 6.013276964s
    Sep 20 13:58:42.627: INFO: Pod "pod-configmaps-faad3460-e646-48b0-ba6a-a91ea28bb6bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.413876768s
    STEP: Saw pod success 09/20/23 13:58:42.627
    Sep 20 13:58:42.627: INFO: Pod "pod-configmaps-faad3460-e646-48b0-ba6a-a91ea28bb6bb" satisfied condition "Succeeded or Failed"
    Sep 20 13:58:42.631: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-configmaps-faad3460-e646-48b0-ba6a-a91ea28bb6bb container agnhost-container: <nil>
    STEP: delete the pod 09/20/23 13:58:42.636
    Sep 20 13:58:42.765: INFO: Waiting for pod pod-configmaps-faad3460-e646-48b0-ba6a-a91ea28bb6bb to disappear
    Sep 20 13:58:42.769: INFO: Pod pod-configmaps-faad3460-e646-48b0-ba6a-a91ea28bb6bb no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:58:42.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3260" for this suite. 09/20/23 13:58:42.773
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:58:42.886
Sep 20 13:58:42.886: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename downward-api 09/20/23 13:58:42.887
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:58:43.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:58:43.394
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 09/20/23 13:58:43.398
Sep 20 13:58:43.450: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2b5fb278-c633-435d-9547-a1b49320766d" in namespace "downward-api-7038" to be "Succeeded or Failed"
Sep 20 13:58:43.455: INFO: Pod "downwardapi-volume-2b5fb278-c633-435d-9547-a1b49320766d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.170005ms
Sep 20 13:58:45.458: INFO: Pod "downwardapi-volume-2b5fb278-c633-435d-9547-a1b49320766d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00781175s
Sep 20 13:58:47.709: INFO: Pod "downwardapi-volume-2b5fb278-c633-435d-9547-a1b49320766d": Phase="Running", Reason="", readiness=true. Elapsed: 4.258266713s
Sep 20 13:58:49.463: INFO: Pod "downwardapi-volume-2b5fb278-c633-435d-9547-a1b49320766d": Phase="Running", Reason="", readiness=false. Elapsed: 6.012030908s
Sep 20 13:58:51.460: INFO: Pod "downwardapi-volume-2b5fb278-c633-435d-9547-a1b49320766d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009045829s
STEP: Saw pod success 09/20/23 13:58:51.46
Sep 20 13:58:51.460: INFO: Pod "downwardapi-volume-2b5fb278-c633-435d-9547-a1b49320766d" satisfied condition "Succeeded or Failed"
Sep 20 13:58:51.464: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downwardapi-volume-2b5fb278-c633-435d-9547-a1b49320766d container client-container: <nil>
STEP: delete the pod 09/20/23 13:58:51.471
Sep 20 13:58:51.596: INFO: Waiting for pod downwardapi-volume-2b5fb278-c633-435d-9547-a1b49320766d to disappear
Sep 20 13:58:51.601: INFO: Pod downwardapi-volume-2b5fb278-c633-435d-9547-a1b49320766d no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep 20 13:58:51.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7038" for this suite. 09/20/23 13:58:51.607
------------------------------
â€¢ [SLOW TEST] [8.730 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:58:42.886
    Sep 20 13:58:42.886: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename downward-api 09/20/23 13:58:42.887
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:58:43.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:58:43.394
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 09/20/23 13:58:43.398
    Sep 20 13:58:43.450: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2b5fb278-c633-435d-9547-a1b49320766d" in namespace "downward-api-7038" to be "Succeeded or Failed"
    Sep 20 13:58:43.455: INFO: Pod "downwardapi-volume-2b5fb278-c633-435d-9547-a1b49320766d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.170005ms
    Sep 20 13:58:45.458: INFO: Pod "downwardapi-volume-2b5fb278-c633-435d-9547-a1b49320766d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00781175s
    Sep 20 13:58:47.709: INFO: Pod "downwardapi-volume-2b5fb278-c633-435d-9547-a1b49320766d": Phase="Running", Reason="", readiness=true. Elapsed: 4.258266713s
    Sep 20 13:58:49.463: INFO: Pod "downwardapi-volume-2b5fb278-c633-435d-9547-a1b49320766d": Phase="Running", Reason="", readiness=false. Elapsed: 6.012030908s
    Sep 20 13:58:51.460: INFO: Pod "downwardapi-volume-2b5fb278-c633-435d-9547-a1b49320766d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009045829s
    STEP: Saw pod success 09/20/23 13:58:51.46
    Sep 20 13:58:51.460: INFO: Pod "downwardapi-volume-2b5fb278-c633-435d-9547-a1b49320766d" satisfied condition "Succeeded or Failed"
    Sep 20 13:58:51.464: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downwardapi-volume-2b5fb278-c633-435d-9547-a1b49320766d container client-container: <nil>
    STEP: delete the pod 09/20/23 13:58:51.471
    Sep 20 13:58:51.596: INFO: Waiting for pod downwardapi-volume-2b5fb278-c633-435d-9547-a1b49320766d to disappear
    Sep 20 13:58:51.601: INFO: Pod downwardapi-volume-2b5fb278-c633-435d-9547-a1b49320766d no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:58:51.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7038" for this suite. 09/20/23 13:58:51.607
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:58:51.618
Sep 20 13:58:51.618: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename job 09/20/23 13:58:51.619
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:58:51.645
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:58:51.647
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 09/20/23 13:58:51.732
STEP: Ensuring active pods == parallelism 09/20/23 13:58:51.741
STEP: Orphaning one of the Job's Pods 09/20/23 13:58:55.746
Sep 20 13:58:56.268: INFO: Successfully updated pod "adopt-release-vz2bq"
STEP: Checking that the Job readopts the Pod 09/20/23 13:58:56.268
Sep 20 13:58:56.268: INFO: Waiting up to 15m0s for pod "adopt-release-vz2bq" in namespace "job-1060" to be "adopted"
Sep 20 13:58:56.275: INFO: Pod "adopt-release-vz2bq": Phase="Running", Reason="", readiness=true. Elapsed: 6.832801ms
Sep 20 13:58:58.279: INFO: Pod "adopt-release-vz2bq": Phase="Running", Reason="", readiness=true. Elapsed: 2.011588143s
Sep 20 13:58:58.280: INFO: Pod "adopt-release-vz2bq" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 09/20/23 13:58:58.28
Sep 20 13:58:58.798: INFO: Successfully updated pod "adopt-release-vz2bq"
STEP: Checking that the Job releases the Pod 09/20/23 13:58:58.798
Sep 20 13:58:58.798: INFO: Waiting up to 15m0s for pod "adopt-release-vz2bq" in namespace "job-1060" to be "released"
Sep 20 13:58:58.802: INFO: Pod "adopt-release-vz2bq": Phase="Running", Reason="", readiness=true. Elapsed: 3.97667ms
Sep 20 13:59:00.810: INFO: Pod "adopt-release-vz2bq": Phase="Running", Reason="", readiness=true. Elapsed: 2.011072793s
Sep 20 13:59:00.810: INFO: Pod "adopt-release-vz2bq" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep 20 13:59:00.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-1060" for this suite. 09/20/23 13:59:00.816
------------------------------
â€¢ [SLOW TEST] [9.210 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:58:51.618
    Sep 20 13:58:51.618: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename job 09/20/23 13:58:51.619
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:58:51.645
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:58:51.647
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 09/20/23 13:58:51.732
    STEP: Ensuring active pods == parallelism 09/20/23 13:58:51.741
    STEP: Orphaning one of the Job's Pods 09/20/23 13:58:55.746
    Sep 20 13:58:56.268: INFO: Successfully updated pod "adopt-release-vz2bq"
    STEP: Checking that the Job readopts the Pod 09/20/23 13:58:56.268
    Sep 20 13:58:56.268: INFO: Waiting up to 15m0s for pod "adopt-release-vz2bq" in namespace "job-1060" to be "adopted"
    Sep 20 13:58:56.275: INFO: Pod "adopt-release-vz2bq": Phase="Running", Reason="", readiness=true. Elapsed: 6.832801ms
    Sep 20 13:58:58.279: INFO: Pod "adopt-release-vz2bq": Phase="Running", Reason="", readiness=true. Elapsed: 2.011588143s
    Sep 20 13:58:58.280: INFO: Pod "adopt-release-vz2bq" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 09/20/23 13:58:58.28
    Sep 20 13:58:58.798: INFO: Successfully updated pod "adopt-release-vz2bq"
    STEP: Checking that the Job releases the Pod 09/20/23 13:58:58.798
    Sep 20 13:58:58.798: INFO: Waiting up to 15m0s for pod "adopt-release-vz2bq" in namespace "job-1060" to be "released"
    Sep 20 13:58:58.802: INFO: Pod "adopt-release-vz2bq": Phase="Running", Reason="", readiness=true. Elapsed: 3.97667ms
    Sep 20 13:59:00.810: INFO: Pod "adopt-release-vz2bq": Phase="Running", Reason="", readiness=true. Elapsed: 2.011072793s
    Sep 20 13:59:00.810: INFO: Pod "adopt-release-vz2bq" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:59:00.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-1060" for this suite. 09/20/23 13:59:00.816
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:59:00.83
Sep 20 13:59:00.831: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename pod-network-test 09/20/23 13:59:00.831
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:59:00.855
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:59:00.858
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-9924 09/20/23 13:59:00.862
STEP: creating a selector 09/20/23 13:59:00.863
STEP: Creating the service pods in kubernetes 09/20/23 13:59:00.863
Sep 20 13:59:00.863: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep 20 13:59:00.913: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9924" to be "running and ready"
Sep 20 13:59:00.929: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.262262ms
Sep 20 13:59:00.929: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:59:02.934: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021477803s
Sep 20 13:59:02.934: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:59:04.980: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.067577272s
Sep 20 13:59:04.981: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep 20 13:59:06.934: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.020921665s
Sep 20 13:59:06.934: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep 20 13:59:08.934: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.020818627s
Sep 20 13:59:08.934: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep 20 13:59:10.936: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.023444008s
Sep 20 13:59:10.936: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep 20 13:59:12.933: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.020525215s
Sep 20 13:59:12.933: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep 20 13:59:14.937: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.024095536s
Sep 20 13:59:14.937: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep 20 13:59:16.972: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.059190067s
Sep 20 13:59:16.972: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep 20 13:59:18.934: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.020922551s
Sep 20 13:59:18.934: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep 20 13:59:20.935: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.02193379s
Sep 20 13:59:20.935: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep 20 13:59:22.935: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.022531401s
Sep 20 13:59:22.935: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Sep 20 13:59:22.935: INFO: Pod "netserver-0" satisfied condition "running and ready"
Sep 20 13:59:22.938: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9924" to be "running and ready"
Sep 20 13:59:22.943: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.640551ms
Sep 20 13:59:22.943: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Sep 20 13:59:22.943: INFO: Pod "netserver-1" satisfied condition "running and ready"
Sep 20 13:59:22.946: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9924" to be "running and ready"
Sep 20 13:59:22.950: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.190794ms
Sep 20 13:59:22.950: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Sep 20 13:59:22.950: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 09/20/23 13:59:22.953
Sep 20 13:59:23.677: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9924" to be "running"
Sep 20 13:59:23.868: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 190.885189ms
Sep 20 13:59:25.873: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.195884431s
Sep 20 13:59:27.872: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.1943205s
Sep 20 13:59:29.873: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.195935016s
Sep 20 13:59:29.873: INFO: Pod "test-container-pod" satisfied condition "running"
Sep 20 13:59:29.876: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Sep 20 13:59:29.876: INFO: Breadth first check of 10.100.5.171 on host 192.168.10.173...
Sep 20 13:59:29.879: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.5.172:9080/dial?request=hostname&protocol=http&host=10.100.5.171&port=8083&tries=1'] Namespace:pod-network-test-9924 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 20 13:59:29.879: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
Sep 20 13:59:29.879: INFO: ExecWithOptions: Clientset creation
Sep 20 13:59:29.879: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-9924/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.5.172%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.100.5.171%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Sep 20 13:59:30.033: INFO: Waiting for responses: map[]
Sep 20 13:59:30.033: INFO: reached 10.100.5.171 after 0/1 tries
Sep 20 13:59:30.033: INFO: Breadth first check of 10.100.4.130 on host 192.168.10.64...
Sep 20 13:59:30.137: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.5.172:9080/dial?request=hostname&protocol=http&host=10.100.4.130&port=8083&tries=1'] Namespace:pod-network-test-9924 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 20 13:59:30.137: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
Sep 20 13:59:30.138: INFO: ExecWithOptions: Clientset creation
Sep 20 13:59:30.138: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-9924/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.5.172%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.100.4.130%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Sep 20 13:59:30.260: INFO: Waiting for responses: map[]
Sep 20 13:59:30.260: INFO: reached 10.100.4.130 after 0/1 tries
Sep 20 13:59:30.260: INFO: Breadth first check of 10.100.3.209 on host 192.168.10.172...
Sep 20 13:59:30.264: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.5.172:9080/dial?request=hostname&protocol=http&host=10.100.3.209&port=8083&tries=1'] Namespace:pod-network-test-9924 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 20 13:59:30.264: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
Sep 20 13:59:30.265: INFO: ExecWithOptions: Clientset creation
Sep 20 13:59:30.265: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-9924/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.5.172%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.100.3.209%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Sep 20 13:59:30.378: INFO: Waiting for responses: map[]
Sep 20 13:59:30.378: INFO: reached 10.100.3.209 after 0/1 tries
Sep 20 13:59:30.378: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Sep 20 13:59:30.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-9924" for this suite. 09/20/23 13:59:30.385
------------------------------
â€¢ [SLOW TEST] [29.593 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:59:00.83
    Sep 20 13:59:00.831: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename pod-network-test 09/20/23 13:59:00.831
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:59:00.855
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:59:00.858
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-9924 09/20/23 13:59:00.862
    STEP: creating a selector 09/20/23 13:59:00.863
    STEP: Creating the service pods in kubernetes 09/20/23 13:59:00.863
    Sep 20 13:59:00.863: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Sep 20 13:59:00.913: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9924" to be "running and ready"
    Sep 20 13:59:00.929: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.262262ms
    Sep 20 13:59:00.929: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:59:02.934: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021477803s
    Sep 20 13:59:02.934: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:59:04.980: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.067577272s
    Sep 20 13:59:04.981: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep 20 13:59:06.934: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.020921665s
    Sep 20 13:59:06.934: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep 20 13:59:08.934: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.020818627s
    Sep 20 13:59:08.934: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep 20 13:59:10.936: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.023444008s
    Sep 20 13:59:10.936: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep 20 13:59:12.933: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.020525215s
    Sep 20 13:59:12.933: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep 20 13:59:14.937: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.024095536s
    Sep 20 13:59:14.937: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep 20 13:59:16.972: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.059190067s
    Sep 20 13:59:16.972: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep 20 13:59:18.934: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.020922551s
    Sep 20 13:59:18.934: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep 20 13:59:20.935: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.02193379s
    Sep 20 13:59:20.935: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep 20 13:59:22.935: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.022531401s
    Sep 20 13:59:22.935: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Sep 20 13:59:22.935: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Sep 20 13:59:22.938: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9924" to be "running and ready"
    Sep 20 13:59:22.943: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.640551ms
    Sep 20 13:59:22.943: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Sep 20 13:59:22.943: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Sep 20 13:59:22.946: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9924" to be "running and ready"
    Sep 20 13:59:22.950: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.190794ms
    Sep 20 13:59:22.950: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Sep 20 13:59:22.950: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 09/20/23 13:59:22.953
    Sep 20 13:59:23.677: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9924" to be "running"
    Sep 20 13:59:23.868: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 190.885189ms
    Sep 20 13:59:25.873: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.195884431s
    Sep 20 13:59:27.872: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.1943205s
    Sep 20 13:59:29.873: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.195935016s
    Sep 20 13:59:29.873: INFO: Pod "test-container-pod" satisfied condition "running"
    Sep 20 13:59:29.876: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Sep 20 13:59:29.876: INFO: Breadth first check of 10.100.5.171 on host 192.168.10.173...
    Sep 20 13:59:29.879: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.5.172:9080/dial?request=hostname&protocol=http&host=10.100.5.171&port=8083&tries=1'] Namespace:pod-network-test-9924 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep 20 13:59:29.879: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    Sep 20 13:59:29.879: INFO: ExecWithOptions: Clientset creation
    Sep 20 13:59:29.879: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-9924/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.5.172%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.100.5.171%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Sep 20 13:59:30.033: INFO: Waiting for responses: map[]
    Sep 20 13:59:30.033: INFO: reached 10.100.5.171 after 0/1 tries
    Sep 20 13:59:30.033: INFO: Breadth first check of 10.100.4.130 on host 192.168.10.64...
    Sep 20 13:59:30.137: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.5.172:9080/dial?request=hostname&protocol=http&host=10.100.4.130&port=8083&tries=1'] Namespace:pod-network-test-9924 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep 20 13:59:30.137: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    Sep 20 13:59:30.138: INFO: ExecWithOptions: Clientset creation
    Sep 20 13:59:30.138: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-9924/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.5.172%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.100.4.130%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Sep 20 13:59:30.260: INFO: Waiting for responses: map[]
    Sep 20 13:59:30.260: INFO: reached 10.100.4.130 after 0/1 tries
    Sep 20 13:59:30.260: INFO: Breadth first check of 10.100.3.209 on host 192.168.10.172...
    Sep 20 13:59:30.264: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.5.172:9080/dial?request=hostname&protocol=http&host=10.100.3.209&port=8083&tries=1'] Namespace:pod-network-test-9924 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep 20 13:59:30.264: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    Sep 20 13:59:30.265: INFO: ExecWithOptions: Clientset creation
    Sep 20 13:59:30.265: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-9924/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.5.172%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.100.3.209%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Sep 20 13:59:30.378: INFO: Waiting for responses: map[]
    Sep 20 13:59:30.378: INFO: reached 10.100.3.209 after 0/1 tries
    Sep 20 13:59:30.378: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:59:30.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-9924" for this suite. 09/20/23 13:59:30.385
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:59:30.424
Sep 20 13:59:30.424: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename security-context 09/20/23 13:59:30.425
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:59:30.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:59:30.443
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 09/20/23 13:59:30.449
Sep 20 13:59:30.461: INFO: Waiting up to 5m0s for pod "security-context-822dd984-117a-4f68-bd97-8524d5313d72" in namespace "security-context-7982" to be "Succeeded or Failed"
Sep 20 13:59:30.466: INFO: Pod "security-context-822dd984-117a-4f68-bd97-8524d5313d72": Phase="Pending", Reason="", readiness=false. Elapsed: 5.300205ms
Sep 20 13:59:32.470: INFO: Pod "security-context-822dd984-117a-4f68-bd97-8524d5313d72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009014978s
Sep 20 13:59:34.472: INFO: Pod "security-context-822dd984-117a-4f68-bd97-8524d5313d72": Phase="Running", Reason="", readiness=false. Elapsed: 4.01061733s
Sep 20 13:59:36.472: INFO: Pod "security-context-822dd984-117a-4f68-bd97-8524d5313d72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010705481s
STEP: Saw pod success 09/20/23 13:59:36.472
Sep 20 13:59:36.472: INFO: Pod "security-context-822dd984-117a-4f68-bd97-8524d5313d72" satisfied condition "Succeeded or Failed"
Sep 20 13:59:36.475: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-2 pod security-context-822dd984-117a-4f68-bd97-8524d5313d72 container test-container: <nil>
STEP: delete the pod 09/20/23 13:59:36.536
Sep 20 13:59:36.552: INFO: Waiting for pod security-context-822dd984-117a-4f68-bd97-8524d5313d72 to disappear
Sep 20 13:59:36.555: INFO: Pod security-context-822dd984-117a-4f68-bd97-8524d5313d72 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep 20 13:59:36.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-7982" for this suite. 09/20/23 13:59:36.561
------------------------------
â€¢ [SLOW TEST] [6.144 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:59:30.424
    Sep 20 13:59:30.424: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename security-context 09/20/23 13:59:30.425
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:59:30.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:59:30.443
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 09/20/23 13:59:30.449
    Sep 20 13:59:30.461: INFO: Waiting up to 5m0s for pod "security-context-822dd984-117a-4f68-bd97-8524d5313d72" in namespace "security-context-7982" to be "Succeeded or Failed"
    Sep 20 13:59:30.466: INFO: Pod "security-context-822dd984-117a-4f68-bd97-8524d5313d72": Phase="Pending", Reason="", readiness=false. Elapsed: 5.300205ms
    Sep 20 13:59:32.470: INFO: Pod "security-context-822dd984-117a-4f68-bd97-8524d5313d72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009014978s
    Sep 20 13:59:34.472: INFO: Pod "security-context-822dd984-117a-4f68-bd97-8524d5313d72": Phase="Running", Reason="", readiness=false. Elapsed: 4.01061733s
    Sep 20 13:59:36.472: INFO: Pod "security-context-822dd984-117a-4f68-bd97-8524d5313d72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010705481s
    STEP: Saw pod success 09/20/23 13:59:36.472
    Sep 20 13:59:36.472: INFO: Pod "security-context-822dd984-117a-4f68-bd97-8524d5313d72" satisfied condition "Succeeded or Failed"
    Sep 20 13:59:36.475: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-2 pod security-context-822dd984-117a-4f68-bd97-8524d5313d72 container test-container: <nil>
    STEP: delete the pod 09/20/23 13:59:36.536
    Sep 20 13:59:36.552: INFO: Waiting for pod security-context-822dd984-117a-4f68-bd97-8524d5313d72 to disappear
    Sep 20 13:59:36.555: INFO: Pod security-context-822dd984-117a-4f68-bd97-8524d5313d72 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:59:36.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-7982" for this suite. 09/20/23 13:59:36.561
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:59:36.569
Sep 20 13:59:36.569: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename emptydir 09/20/23 13:59:36.57
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:59:36.587
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:59:36.59
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 09/20/23 13:59:36.594
Sep 20 13:59:36.603: INFO: Waiting up to 5m0s for pod "pod-7e501208-ef72-47b1-862a-fca35a0076fd" in namespace "emptydir-364" to be "Succeeded or Failed"
Sep 20 13:59:36.820: INFO: Pod "pod-7e501208-ef72-47b1-862a-fca35a0076fd": Phase="Pending", Reason="", readiness=false. Elapsed: 217.379756ms
Sep 20 13:59:39.129: INFO: Pod "pod-7e501208-ef72-47b1-862a-fca35a0076fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.526045087s
Sep 20 13:59:40.825: INFO: Pod "pod-7e501208-ef72-47b1-862a-fca35a0076fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.22195768s
Sep 20 13:59:42.824: INFO: Pod "pod-7e501208-ef72-47b1-862a-fca35a0076fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.22080304s
STEP: Saw pod success 09/20/23 13:59:42.824
Sep 20 13:59:42.824: INFO: Pod "pod-7e501208-ef72-47b1-862a-fca35a0076fd" satisfied condition "Succeeded or Failed"
Sep 20 13:59:42.826: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-2 pod pod-7e501208-ef72-47b1-862a-fca35a0076fd container test-container: <nil>
STEP: delete the pod 09/20/23 13:59:42.832
Sep 20 13:59:42.874: INFO: Waiting for pod pod-7e501208-ef72-47b1-862a-fca35a0076fd to disappear
Sep 20 13:59:42.877: INFO: Pod pod-7e501208-ef72-47b1-862a-fca35a0076fd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep 20 13:59:42.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-364" for this suite. 09/20/23 13:59:42.882
------------------------------
â€¢ [SLOW TEST] [6.320 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:59:36.569
    Sep 20 13:59:36.569: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename emptydir 09/20/23 13:59:36.57
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:59:36.587
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:59:36.59
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 09/20/23 13:59:36.594
    Sep 20 13:59:36.603: INFO: Waiting up to 5m0s for pod "pod-7e501208-ef72-47b1-862a-fca35a0076fd" in namespace "emptydir-364" to be "Succeeded or Failed"
    Sep 20 13:59:36.820: INFO: Pod "pod-7e501208-ef72-47b1-862a-fca35a0076fd": Phase="Pending", Reason="", readiness=false. Elapsed: 217.379756ms
    Sep 20 13:59:39.129: INFO: Pod "pod-7e501208-ef72-47b1-862a-fca35a0076fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.526045087s
    Sep 20 13:59:40.825: INFO: Pod "pod-7e501208-ef72-47b1-862a-fca35a0076fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.22195768s
    Sep 20 13:59:42.824: INFO: Pod "pod-7e501208-ef72-47b1-862a-fca35a0076fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.22080304s
    STEP: Saw pod success 09/20/23 13:59:42.824
    Sep 20 13:59:42.824: INFO: Pod "pod-7e501208-ef72-47b1-862a-fca35a0076fd" satisfied condition "Succeeded or Failed"
    Sep 20 13:59:42.826: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-2 pod pod-7e501208-ef72-47b1-862a-fca35a0076fd container test-container: <nil>
    STEP: delete the pod 09/20/23 13:59:42.832
    Sep 20 13:59:42.874: INFO: Waiting for pod pod-7e501208-ef72-47b1-862a-fca35a0076fd to disappear
    Sep 20 13:59:42.877: INFO: Pod pod-7e501208-ef72-47b1-862a-fca35a0076fd no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:59:42.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-364" for this suite. 09/20/23 13:59:42.882
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:59:42.889
Sep 20 13:59:42.889: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename ephemeral-containers-test 09/20/23 13:59:42.89
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:59:42.941
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:59:42.944
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 09/20/23 13:59:43.084
Sep 20 13:59:43.092: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9869" to be "running and ready"
Sep 20 13:59:43.097: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.724059ms
Sep 20 13:59:43.097: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:59:45.103: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011013802s
Sep 20 13:59:45.103: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Sep 20 13:59:47.412: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.319585275s
Sep 20 13:59:47.412: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Sep 20 13:59:47.412: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 09/20/23 13:59:47.415
Sep 20 13:59:47.464: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9869" to be "container debugger running"
Sep 20 13:59:47.468: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.332146ms
Sep 20 13:59:49.474: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00942572s
Sep 20 13:59:49.474: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 09/20/23 13:59:49.474
Sep 20 13:59:49.474: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-9869 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 20 13:59:49.474: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
Sep 20 13:59:49.475: INFO: ExecWithOptions: Clientset creation
Sep 20 13:59:49.475: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/ephemeral-containers-test-9869/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Sep 20 13:59:49.592: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep 20 13:59:49.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-9869" for this suite. 09/20/23 13:59:49.616
------------------------------
â€¢ [SLOW TEST] [6.931 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:59:42.889
    Sep 20 13:59:42.889: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename ephemeral-containers-test 09/20/23 13:59:42.89
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:59:42.941
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:59:42.944
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 09/20/23 13:59:43.084
    Sep 20 13:59:43.092: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9869" to be "running and ready"
    Sep 20 13:59:43.097: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.724059ms
    Sep 20 13:59:43.097: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:59:45.103: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011013802s
    Sep 20 13:59:45.103: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 13:59:47.412: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.319585275s
    Sep 20 13:59:47.412: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Sep 20 13:59:47.412: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 09/20/23 13:59:47.415
    Sep 20 13:59:47.464: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9869" to be "container debugger running"
    Sep 20 13:59:47.468: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.332146ms
    Sep 20 13:59:49.474: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00942572s
    Sep 20 13:59:49.474: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 09/20/23 13:59:49.474
    Sep 20 13:59:49.474: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-9869 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep 20 13:59:49.474: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    Sep 20 13:59:49.475: INFO: ExecWithOptions: Clientset creation
    Sep 20 13:59:49.475: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/ephemeral-containers-test-9869/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Sep 20 13:59:49.592: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep 20 13:59:49.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-9869" for this suite. 09/20/23 13:59:49.616
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 13:59:49.823
Sep 20 13:59:49.823: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename taint-single-pod 09/20/23 13:59:49.824
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:59:49.92
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:59:49.923
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Sep 20 13:59:49.933: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 20 14:00:49.963: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Sep 20 14:00:49.967: INFO: Starting informer...
STEP: Starting pod... 09/20/23 14:00:49.967
Sep 20 14:00:50.423: INFO: Pod is running on mycluster-ww3cg64etuwi-node-1. Tainting Node
STEP: Trying to apply a taint on the Node 09/20/23 14:00:50.423
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/20/23 14:00:50.44
STEP: Waiting short time to make sure Pod is queued for deletion 09/20/23 14:00:50.448
Sep 20 14:00:50.448: INFO: Pod wasn't evicted. Proceeding
Sep 20 14:00:50.448: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/20/23 14:00:50.475
STEP: Waiting some time to make sure that toleration time passed. 09/20/23 14:00:50.501
Sep 20 14:02:05.502: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Sep 20 14:02:05.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-9957" for this suite. 09/20/23 14:02:05.513
------------------------------
â€¢ [SLOW TEST] [135.716 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 13:59:49.823
    Sep 20 13:59:49.823: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename taint-single-pod 09/20/23 13:59:49.824
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 13:59:49.92
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 13:59:49.923
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Sep 20 13:59:49.933: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep 20 14:00:49.963: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Sep 20 14:00:49.967: INFO: Starting informer...
    STEP: Starting pod... 09/20/23 14:00:49.967
    Sep 20 14:00:50.423: INFO: Pod is running on mycluster-ww3cg64etuwi-node-1. Tainting Node
    STEP: Trying to apply a taint on the Node 09/20/23 14:00:50.423
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/20/23 14:00:50.44
    STEP: Waiting short time to make sure Pod is queued for deletion 09/20/23 14:00:50.448
    Sep 20 14:00:50.448: INFO: Pod wasn't evicted. Proceeding
    Sep 20 14:00:50.448: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/20/23 14:00:50.475
    STEP: Waiting some time to make sure that toleration time passed. 09/20/23 14:00:50.501
    Sep 20 14:02:05.502: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep 20 14:02:05.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-9957" for this suite. 09/20/23 14:02:05.513
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 14:02:05.54
Sep 20 14:02:05.540: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename configmap 09/20/23 14:02:05.541
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:02:05.561
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:02:05.563
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-76d56b75-2b49-4cc8-bf60-5d295b9c40e1 09/20/23 14:02:06.145
STEP: Creating configMap with name cm-test-opt-upd-0d0011c0-d426-4f29-9cf2-c70f94141bd9 09/20/23 14:02:06.195
STEP: Creating the pod 09/20/23 14:02:06.724
Sep 20 14:02:07.079: INFO: Waiting up to 5m0s for pod "pod-configmaps-ef4f967d-b50c-4330-b840-176350b77206" in namespace "configmap-9666" to be "running and ready"
Sep 20 14:02:07.092: INFO: Pod "pod-configmaps-ef4f967d-b50c-4330-b840-176350b77206": Phase="Pending", Reason="", readiness=false. Elapsed: 13.566914ms
Sep 20 14:02:07.092: INFO: The phase of Pod pod-configmaps-ef4f967d-b50c-4330-b840-176350b77206 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 14:02:09.099: INFO: Pod "pod-configmaps-ef4f967d-b50c-4330-b840-176350b77206": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020075029s
Sep 20 14:02:09.099: INFO: The phase of Pod pod-configmaps-ef4f967d-b50c-4330-b840-176350b77206 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 14:02:11.102: INFO: Pod "pod-configmaps-ef4f967d-b50c-4330-b840-176350b77206": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022595632s
Sep 20 14:02:11.102: INFO: The phase of Pod pod-configmaps-ef4f967d-b50c-4330-b840-176350b77206 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 14:02:13.098: INFO: Pod "pod-configmaps-ef4f967d-b50c-4330-b840-176350b77206": Phase="Running", Reason="", readiness=true. Elapsed: 6.019076098s
Sep 20 14:02:13.098: INFO: The phase of Pod pod-configmaps-ef4f967d-b50c-4330-b840-176350b77206 is Running (Ready = true)
Sep 20 14:02:13.098: INFO: Pod "pod-configmaps-ef4f967d-b50c-4330-b840-176350b77206" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-76d56b75-2b49-4cc8-bf60-5d295b9c40e1 09/20/23 14:02:13.166
STEP: Updating configmap cm-test-opt-upd-0d0011c0-d426-4f29-9cf2-c70f94141bd9 09/20/23 14:02:13.457
STEP: Creating configMap with name cm-test-opt-create-291e2f4e-d471-4362-8bbd-821d3c0ba328 09/20/23 14:02:13.465
STEP: waiting to observe update in volume 09/20/23 14:02:13.471
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep 20 14:02:16.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9666" for this suite. 09/20/23 14:02:16.566
------------------------------
â€¢ [SLOW TEST] [11.030 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 14:02:05.54
    Sep 20 14:02:05.540: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename configmap 09/20/23 14:02:05.541
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:02:05.561
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:02:05.563
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-76d56b75-2b49-4cc8-bf60-5d295b9c40e1 09/20/23 14:02:06.145
    STEP: Creating configMap with name cm-test-opt-upd-0d0011c0-d426-4f29-9cf2-c70f94141bd9 09/20/23 14:02:06.195
    STEP: Creating the pod 09/20/23 14:02:06.724
    Sep 20 14:02:07.079: INFO: Waiting up to 5m0s for pod "pod-configmaps-ef4f967d-b50c-4330-b840-176350b77206" in namespace "configmap-9666" to be "running and ready"
    Sep 20 14:02:07.092: INFO: Pod "pod-configmaps-ef4f967d-b50c-4330-b840-176350b77206": Phase="Pending", Reason="", readiness=false. Elapsed: 13.566914ms
    Sep 20 14:02:07.092: INFO: The phase of Pod pod-configmaps-ef4f967d-b50c-4330-b840-176350b77206 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 14:02:09.099: INFO: Pod "pod-configmaps-ef4f967d-b50c-4330-b840-176350b77206": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020075029s
    Sep 20 14:02:09.099: INFO: The phase of Pod pod-configmaps-ef4f967d-b50c-4330-b840-176350b77206 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 14:02:11.102: INFO: Pod "pod-configmaps-ef4f967d-b50c-4330-b840-176350b77206": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022595632s
    Sep 20 14:02:11.102: INFO: The phase of Pod pod-configmaps-ef4f967d-b50c-4330-b840-176350b77206 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 14:02:13.098: INFO: Pod "pod-configmaps-ef4f967d-b50c-4330-b840-176350b77206": Phase="Running", Reason="", readiness=true. Elapsed: 6.019076098s
    Sep 20 14:02:13.098: INFO: The phase of Pod pod-configmaps-ef4f967d-b50c-4330-b840-176350b77206 is Running (Ready = true)
    Sep 20 14:02:13.098: INFO: Pod "pod-configmaps-ef4f967d-b50c-4330-b840-176350b77206" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-76d56b75-2b49-4cc8-bf60-5d295b9c40e1 09/20/23 14:02:13.166
    STEP: Updating configmap cm-test-opt-upd-0d0011c0-d426-4f29-9cf2-c70f94141bd9 09/20/23 14:02:13.457
    STEP: Creating configMap with name cm-test-opt-create-291e2f4e-d471-4362-8bbd-821d3c0ba328 09/20/23 14:02:13.465
    STEP: waiting to observe update in volume 09/20/23 14:02:13.471
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep 20 14:02:16.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9666" for this suite. 09/20/23 14:02:16.566
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 14:02:16.571
Sep 20 14:02:16.572: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename secrets 09/20/23 14:02:16.572
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:02:16.583
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:02:16.585
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-1ac11976-907f-40ee-82b6-b4ded0f0a49e 09/20/23 14:02:16.938
STEP: Creating a pod to test consume secrets 09/20/23 14:02:16.946
Sep 20 14:02:17.124: INFO: Waiting up to 5m0s for pod "pod-secrets-a4b55434-2fdb-4880-9656-b11317f4e1ae" in namespace "secrets-5484" to be "Succeeded or Failed"
Sep 20 14:02:17.129: INFO: Pod "pod-secrets-a4b55434-2fdb-4880-9656-b11317f4e1ae": Phase="Pending", Reason="", readiness=false. Elapsed: 4.081678ms
Sep 20 14:02:19.133: INFO: Pod "pod-secrets-a4b55434-2fdb-4880-9656-b11317f4e1ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008149967s
Sep 20 14:02:21.136: INFO: Pod "pod-secrets-a4b55434-2fdb-4880-9656-b11317f4e1ae": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011494232s
Sep 20 14:02:23.233: INFO: Pod "pod-secrets-a4b55434-2fdb-4880-9656-b11317f4e1ae": Phase="Pending", Reason="", readiness=false. Elapsed: 6.10807497s
Sep 20 14:02:25.510: INFO: Pod "pod-secrets-a4b55434-2fdb-4880-9656-b11317f4e1ae": Phase="Pending", Reason="", readiness=false. Elapsed: 8.386015988s
Sep 20 14:02:27.145: INFO: Pod "pod-secrets-a4b55434-2fdb-4880-9656-b11317f4e1ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.020242076s
STEP: Saw pod success 09/20/23 14:02:27.145
Sep 20 14:02:27.145: INFO: Pod "pod-secrets-a4b55434-2fdb-4880-9656-b11317f4e1ae" satisfied condition "Succeeded or Failed"
Sep 20 14:02:27.148: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-0 pod pod-secrets-a4b55434-2fdb-4880-9656-b11317f4e1ae container secret-volume-test: <nil>
STEP: delete the pod 09/20/23 14:02:27.208
Sep 20 14:02:27.327: INFO: Waiting for pod pod-secrets-a4b55434-2fdb-4880-9656-b11317f4e1ae to disappear
Sep 20 14:02:27.330: INFO: Pod pod-secrets-a4b55434-2fdb-4880-9656-b11317f4e1ae no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep 20 14:02:27.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5484" for this suite. 09/20/23 14:02:27.334
------------------------------
â€¢ [SLOW TEST] [10.769 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 14:02:16.571
    Sep 20 14:02:16.572: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename secrets 09/20/23 14:02:16.572
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:02:16.583
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:02:16.585
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-1ac11976-907f-40ee-82b6-b4ded0f0a49e 09/20/23 14:02:16.938
    STEP: Creating a pod to test consume secrets 09/20/23 14:02:16.946
    Sep 20 14:02:17.124: INFO: Waiting up to 5m0s for pod "pod-secrets-a4b55434-2fdb-4880-9656-b11317f4e1ae" in namespace "secrets-5484" to be "Succeeded or Failed"
    Sep 20 14:02:17.129: INFO: Pod "pod-secrets-a4b55434-2fdb-4880-9656-b11317f4e1ae": Phase="Pending", Reason="", readiness=false. Elapsed: 4.081678ms
    Sep 20 14:02:19.133: INFO: Pod "pod-secrets-a4b55434-2fdb-4880-9656-b11317f4e1ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008149967s
    Sep 20 14:02:21.136: INFO: Pod "pod-secrets-a4b55434-2fdb-4880-9656-b11317f4e1ae": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011494232s
    Sep 20 14:02:23.233: INFO: Pod "pod-secrets-a4b55434-2fdb-4880-9656-b11317f4e1ae": Phase="Pending", Reason="", readiness=false. Elapsed: 6.10807497s
    Sep 20 14:02:25.510: INFO: Pod "pod-secrets-a4b55434-2fdb-4880-9656-b11317f4e1ae": Phase="Pending", Reason="", readiness=false. Elapsed: 8.386015988s
    Sep 20 14:02:27.145: INFO: Pod "pod-secrets-a4b55434-2fdb-4880-9656-b11317f4e1ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.020242076s
    STEP: Saw pod success 09/20/23 14:02:27.145
    Sep 20 14:02:27.145: INFO: Pod "pod-secrets-a4b55434-2fdb-4880-9656-b11317f4e1ae" satisfied condition "Succeeded or Failed"
    Sep 20 14:02:27.148: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-0 pod pod-secrets-a4b55434-2fdb-4880-9656-b11317f4e1ae container secret-volume-test: <nil>
    STEP: delete the pod 09/20/23 14:02:27.208
    Sep 20 14:02:27.327: INFO: Waiting for pod pod-secrets-a4b55434-2fdb-4880-9656-b11317f4e1ae to disappear
    Sep 20 14:02:27.330: INFO: Pod pod-secrets-a4b55434-2fdb-4880-9656-b11317f4e1ae no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep 20 14:02:27.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5484" for this suite. 09/20/23 14:02:27.334
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 14:02:27.343
Sep 20 14:02:27.343: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename container-probe 09/20/23 14:02:27.344
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:02:27.375
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:02:27.379
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-01865e0d-60bd-46fc-b574-4a467c6dbfd5 in namespace container-probe-5487 09/20/23 14:02:27.384
Sep 20 14:02:27.392: INFO: Waiting up to 5m0s for pod "liveness-01865e0d-60bd-46fc-b574-4a467c6dbfd5" in namespace "container-probe-5487" to be "not pending"
Sep 20 14:02:27.400: INFO: Pod "liveness-01865e0d-60bd-46fc-b574-4a467c6dbfd5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.093184ms
Sep 20 14:02:29.426: INFO: Pod "liveness-01865e0d-60bd-46fc-b574-4a467c6dbfd5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033746202s
Sep 20 14:02:31.409: INFO: Pod "liveness-01865e0d-60bd-46fc-b574-4a467c6dbfd5": Phase="Running", Reason="", readiness=true. Elapsed: 4.016200808s
Sep 20 14:02:31.409: INFO: Pod "liveness-01865e0d-60bd-46fc-b574-4a467c6dbfd5" satisfied condition "not pending"
Sep 20 14:02:31.409: INFO: Started pod liveness-01865e0d-60bd-46fc-b574-4a467c6dbfd5 in namespace container-probe-5487
STEP: checking the pod's current state and verifying that restartCount is present 09/20/23 14:02:31.409
Sep 20 14:02:31.413: INFO: Initial restart count of pod liveness-01865e0d-60bd-46fc-b574-4a467c6dbfd5 is 0
STEP: deleting the pod 09/20/23 14:06:33.068
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep 20 14:06:33.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-5487" for this suite. 09/20/23 14:06:33.668
------------------------------
â€¢ [SLOW TEST] [246.333 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 14:02:27.343
    Sep 20 14:02:27.343: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename container-probe 09/20/23 14:02:27.344
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:02:27.375
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:02:27.379
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-01865e0d-60bd-46fc-b574-4a467c6dbfd5 in namespace container-probe-5487 09/20/23 14:02:27.384
    Sep 20 14:02:27.392: INFO: Waiting up to 5m0s for pod "liveness-01865e0d-60bd-46fc-b574-4a467c6dbfd5" in namespace "container-probe-5487" to be "not pending"
    Sep 20 14:02:27.400: INFO: Pod "liveness-01865e0d-60bd-46fc-b574-4a467c6dbfd5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.093184ms
    Sep 20 14:02:29.426: INFO: Pod "liveness-01865e0d-60bd-46fc-b574-4a467c6dbfd5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033746202s
    Sep 20 14:02:31.409: INFO: Pod "liveness-01865e0d-60bd-46fc-b574-4a467c6dbfd5": Phase="Running", Reason="", readiness=true. Elapsed: 4.016200808s
    Sep 20 14:02:31.409: INFO: Pod "liveness-01865e0d-60bd-46fc-b574-4a467c6dbfd5" satisfied condition "not pending"
    Sep 20 14:02:31.409: INFO: Started pod liveness-01865e0d-60bd-46fc-b574-4a467c6dbfd5 in namespace container-probe-5487
    STEP: checking the pod's current state and verifying that restartCount is present 09/20/23 14:02:31.409
    Sep 20 14:02:31.413: INFO: Initial restart count of pod liveness-01865e0d-60bd-46fc-b574-4a467c6dbfd5 is 0
    STEP: deleting the pod 09/20/23 14:06:33.068
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep 20 14:06:33.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-5487" for this suite. 09/20/23 14:06:33.668
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 14:06:33.677
Sep 20 14:06:33.677: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename projected 09/20/23 14:06:33.678
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:06:33.712
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:06:33.718
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 09/20/23 14:06:33.729
Sep 20 14:06:34.083: INFO: Waiting up to 5m0s for pod "downwardapi-volume-95d4e1c0-4fec-4983-8746-4febc2996177" in namespace "projected-7683" to be "Succeeded or Failed"
Sep 20 14:06:34.098: INFO: Pod "downwardapi-volume-95d4e1c0-4fec-4983-8746-4febc2996177": Phase="Pending", Reason="", readiness=false. Elapsed: 14.528857ms
Sep 20 14:06:36.191: INFO: Pod "downwardapi-volume-95d4e1c0-4fec-4983-8746-4febc2996177": Phase="Pending", Reason="", readiness=false. Elapsed: 2.107990472s
Sep 20 14:06:38.102: INFO: Pod "downwardapi-volume-95d4e1c0-4fec-4983-8746-4febc2996177": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018273155s
Sep 20 14:06:40.175: INFO: Pod "downwardapi-volume-95d4e1c0-4fec-4983-8746-4febc2996177": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.091337089s
STEP: Saw pod success 09/20/23 14:06:40.175
Sep 20 14:06:40.175: INFO: Pod "downwardapi-volume-95d4e1c0-4fec-4983-8746-4febc2996177" satisfied condition "Succeeded or Failed"
Sep 20 14:06:40.178: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downwardapi-volume-95d4e1c0-4fec-4983-8746-4febc2996177 container client-container: <nil>
STEP: delete the pod 09/20/23 14:06:40.236
Sep 20 14:06:40.250: INFO: Waiting for pod downwardapi-volume-95d4e1c0-4fec-4983-8746-4febc2996177 to disappear
Sep 20 14:06:40.253: INFO: Pod downwardapi-volume-95d4e1c0-4fec-4983-8746-4febc2996177 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep 20 14:06:40.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7683" for this suite. 09/20/23 14:06:40.257
------------------------------
â€¢ [SLOW TEST] [6.586 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 14:06:33.677
    Sep 20 14:06:33.677: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename projected 09/20/23 14:06:33.678
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:06:33.712
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:06:33.718
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 09/20/23 14:06:33.729
    Sep 20 14:06:34.083: INFO: Waiting up to 5m0s for pod "downwardapi-volume-95d4e1c0-4fec-4983-8746-4febc2996177" in namespace "projected-7683" to be "Succeeded or Failed"
    Sep 20 14:06:34.098: INFO: Pod "downwardapi-volume-95d4e1c0-4fec-4983-8746-4febc2996177": Phase="Pending", Reason="", readiness=false. Elapsed: 14.528857ms
    Sep 20 14:06:36.191: INFO: Pod "downwardapi-volume-95d4e1c0-4fec-4983-8746-4febc2996177": Phase="Pending", Reason="", readiness=false. Elapsed: 2.107990472s
    Sep 20 14:06:38.102: INFO: Pod "downwardapi-volume-95d4e1c0-4fec-4983-8746-4febc2996177": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018273155s
    Sep 20 14:06:40.175: INFO: Pod "downwardapi-volume-95d4e1c0-4fec-4983-8746-4febc2996177": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.091337089s
    STEP: Saw pod success 09/20/23 14:06:40.175
    Sep 20 14:06:40.175: INFO: Pod "downwardapi-volume-95d4e1c0-4fec-4983-8746-4febc2996177" satisfied condition "Succeeded or Failed"
    Sep 20 14:06:40.178: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod downwardapi-volume-95d4e1c0-4fec-4983-8746-4febc2996177 container client-container: <nil>
    STEP: delete the pod 09/20/23 14:06:40.236
    Sep 20 14:06:40.250: INFO: Waiting for pod downwardapi-volume-95d4e1c0-4fec-4983-8746-4febc2996177 to disappear
    Sep 20 14:06:40.253: INFO: Pod downwardapi-volume-95d4e1c0-4fec-4983-8746-4febc2996177 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep 20 14:06:40.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7683" for this suite. 09/20/23 14:06:40.257
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 14:06:40.263
Sep 20 14:06:40.263: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename replicaset 09/20/23 14:06:40.264
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:06:40.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:06:40.289
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 09/20/23 14:06:40.293
STEP: Verify that the required pods have come up 09/20/23 14:06:40.299
Sep 20 14:06:40.303: INFO: Pod name sample-pod: Found 0 pods out of 3
Sep 20 14:06:46.157: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 09/20/23 14:06:46.157
Sep 20 14:06:46.157: INFO: Waiting up to 5m0s for pod "test-rs-vf8xt" in namespace "replicaset-5792" to be "running"
Sep 20 14:06:46.329: INFO: Pod "test-rs-vf8xt": Phase="Running", Reason="", readiness=true. Elapsed: 172.233027ms
Sep 20 14:06:46.329: INFO: Pod "test-rs-vf8xt" satisfied condition "running"
Sep 20 14:06:46.334: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 09/20/23 14:06:46.334
STEP: DeleteCollection of the ReplicaSets 09/20/23 14:06:46.339
STEP: After DeleteCollection verify that ReplicaSets have been deleted 09/20/23 14:06:46.347
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep 20 14:06:46.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-5792" for this suite. 09/20/23 14:06:46.356
------------------------------
â€¢ [SLOW TEST] [6.114 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 14:06:40.263
    Sep 20 14:06:40.263: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename replicaset 09/20/23 14:06:40.264
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:06:40.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:06:40.289
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 09/20/23 14:06:40.293
    STEP: Verify that the required pods have come up 09/20/23 14:06:40.299
    Sep 20 14:06:40.303: INFO: Pod name sample-pod: Found 0 pods out of 3
    Sep 20 14:06:46.157: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 09/20/23 14:06:46.157
    Sep 20 14:06:46.157: INFO: Waiting up to 5m0s for pod "test-rs-vf8xt" in namespace "replicaset-5792" to be "running"
    Sep 20 14:06:46.329: INFO: Pod "test-rs-vf8xt": Phase="Running", Reason="", readiness=true. Elapsed: 172.233027ms
    Sep 20 14:06:46.329: INFO: Pod "test-rs-vf8xt" satisfied condition "running"
    Sep 20 14:06:46.334: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 09/20/23 14:06:46.334
    STEP: DeleteCollection of the ReplicaSets 09/20/23 14:06:46.339
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 09/20/23 14:06:46.347
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep 20 14:06:46.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-5792" for this suite. 09/20/23 14:06:46.356
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 14:06:46.381
Sep 20 14:06:46.381: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename kubelet-test 09/20/23 14:06:46.382
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:06:46.411
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:06:46.413
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Sep 20 14:06:46.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-632" for this suite. 09/20/23 14:06:46.63
------------------------------
â€¢ [0.728 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 14:06:46.381
    Sep 20 14:06:46.381: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename kubelet-test 09/20/23 14:06:46.382
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:06:46.411
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:06:46.413
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Sep 20 14:06:46.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-632" for this suite. 09/20/23 14:06:46.63
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 14:06:47.112
Sep 20 14:06:47.112: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename replication-controller 09/20/23 14:06:47.112
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:06:47.376
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:06:47.379
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-z956z" 09/20/23 14:06:47.623
Sep 20 14:06:47.631: INFO: Get Replication Controller "e2e-rc-z956z" to confirm replicas
Sep 20 14:06:48.638: INFO: Get Replication Controller "e2e-rc-z956z" to confirm replicas
Sep 20 14:06:48.642: INFO: Found 1 replicas for "e2e-rc-z956z" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-z956z" 09/20/23 14:06:48.642
STEP: Updating a scale subresource 09/20/23 14:06:48.646
STEP: Verifying replicas where modified for replication controller "e2e-rc-z956z" 09/20/23 14:06:48.654
Sep 20 14:06:48.654: INFO: Get Replication Controller "e2e-rc-z956z" to confirm replicas
Sep 20 14:06:49.658: INFO: Get Replication Controller "e2e-rc-z956z" to confirm replicas
Sep 20 14:06:49.663: INFO: Found 2 replicas for "e2e-rc-z956z" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep 20 14:06:49.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-1583" for this suite. 09/20/23 14:06:49.67
------------------------------
â€¢ [2.875 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 14:06:47.112
    Sep 20 14:06:47.112: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename replication-controller 09/20/23 14:06:47.112
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:06:47.376
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:06:47.379
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-z956z" 09/20/23 14:06:47.623
    Sep 20 14:06:47.631: INFO: Get Replication Controller "e2e-rc-z956z" to confirm replicas
    Sep 20 14:06:48.638: INFO: Get Replication Controller "e2e-rc-z956z" to confirm replicas
    Sep 20 14:06:48.642: INFO: Found 1 replicas for "e2e-rc-z956z" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-z956z" 09/20/23 14:06:48.642
    STEP: Updating a scale subresource 09/20/23 14:06:48.646
    STEP: Verifying replicas where modified for replication controller "e2e-rc-z956z" 09/20/23 14:06:48.654
    Sep 20 14:06:48.654: INFO: Get Replication Controller "e2e-rc-z956z" to confirm replicas
    Sep 20 14:06:49.658: INFO: Get Replication Controller "e2e-rc-z956z" to confirm replicas
    Sep 20 14:06:49.663: INFO: Found 2 replicas for "e2e-rc-z956z" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep 20 14:06:49.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-1583" for this suite. 09/20/23 14:06:49.67
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 14:06:49.988
Sep 20 14:06:49.988: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename pods 09/20/23 14:06:49.989
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:06:50.011
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:06:50.014
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 09/20/23 14:06:50.173
STEP: setting up watch 09/20/23 14:06:50.173
STEP: submitting the pod to kubernetes 09/20/23 14:06:50.278
STEP: verifying the pod is in kubernetes 09/20/23 14:06:51.404
STEP: verifying pod creation was observed 09/20/23 14:06:51.43
Sep 20 14:06:51.430: INFO: Waiting up to 5m0s for pod "pod-submit-remove-d5027885-1988-4a33-ad37-60b54c8a4e23" in namespace "pods-1794" to be "running"
Sep 20 14:06:51.445: INFO: Pod "pod-submit-remove-d5027885-1988-4a33-ad37-60b54c8a4e23": Phase="Pending", Reason="", readiness=false. Elapsed: 14.858509ms
Sep 20 14:06:53.458: INFO: Pod "pod-submit-remove-d5027885-1988-4a33-ad37-60b54c8a4e23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028159777s
Sep 20 14:06:55.448: INFO: Pod "pod-submit-remove-d5027885-1988-4a33-ad37-60b54c8a4e23": Phase="Running", Reason="", readiness=true. Elapsed: 4.017833187s
Sep 20 14:06:55.448: INFO: Pod "pod-submit-remove-d5027885-1988-4a33-ad37-60b54c8a4e23" satisfied condition "running"
STEP: deleting the pod gracefully 09/20/23 14:06:55.451
STEP: verifying pod deletion was observed 09/20/23 14:06:55.462
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep 20 14:06:58.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1794" for this suite. 09/20/23 14:06:58.317
------------------------------
â€¢ [SLOW TEST] [8.339 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 14:06:49.988
    Sep 20 14:06:49.988: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename pods 09/20/23 14:06:49.989
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:06:50.011
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:06:50.014
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 09/20/23 14:06:50.173
    STEP: setting up watch 09/20/23 14:06:50.173
    STEP: submitting the pod to kubernetes 09/20/23 14:06:50.278
    STEP: verifying the pod is in kubernetes 09/20/23 14:06:51.404
    STEP: verifying pod creation was observed 09/20/23 14:06:51.43
    Sep 20 14:06:51.430: INFO: Waiting up to 5m0s for pod "pod-submit-remove-d5027885-1988-4a33-ad37-60b54c8a4e23" in namespace "pods-1794" to be "running"
    Sep 20 14:06:51.445: INFO: Pod "pod-submit-remove-d5027885-1988-4a33-ad37-60b54c8a4e23": Phase="Pending", Reason="", readiness=false. Elapsed: 14.858509ms
    Sep 20 14:06:53.458: INFO: Pod "pod-submit-remove-d5027885-1988-4a33-ad37-60b54c8a4e23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028159777s
    Sep 20 14:06:55.448: INFO: Pod "pod-submit-remove-d5027885-1988-4a33-ad37-60b54c8a4e23": Phase="Running", Reason="", readiness=true. Elapsed: 4.017833187s
    Sep 20 14:06:55.448: INFO: Pod "pod-submit-remove-d5027885-1988-4a33-ad37-60b54c8a4e23" satisfied condition "running"
    STEP: deleting the pod gracefully 09/20/23 14:06:55.451
    STEP: verifying pod deletion was observed 09/20/23 14:06:55.462
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep 20 14:06:58.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1794" for this suite. 09/20/23 14:06:58.317
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 14:06:58.328
Sep 20 14:06:58.328: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename services 09/20/23 14:06:58.329
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:06:58.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:06:58.673
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-6020 09/20/23 14:06:58.679
STEP: creating replication controller nodeport-test in namespace services-6020 09/20/23 14:06:58.723
I0920 14:06:58.757215      20 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-6020, replica count: 2
I0920 14:07:01.808602      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0920 14:07:04.809660      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0920 14:07:07.812674      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 20 14:07:07.812: INFO: Creating new exec pod
Sep 20 14:07:07.823: INFO: Waiting up to 5m0s for pod "execpodm7ztt" in namespace "services-6020" to be "running"
Sep 20 14:07:07.832: INFO: Pod "execpodm7ztt": Phase="Pending", Reason="", readiness=false. Elapsed: 8.999432ms
Sep 20 14:07:09.836: INFO: Pod "execpodm7ztt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012610917s
Sep 20 14:07:11.837: INFO: Pod "execpodm7ztt": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0135457s
Sep 20 14:07:13.837: INFO: Pod "execpodm7ztt": Phase="Running", Reason="", readiness=true. Elapsed: 6.01364033s
Sep 20 14:07:13.837: INFO: Pod "execpodm7ztt" satisfied condition "running"
Sep 20 14:07:14.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-6020 exec execpodm7ztt -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Sep 20 14:07:15.186: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Sep 20 14:07:15.186: INFO: stdout: ""
Sep 20 14:07:15.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-6020 exec execpodm7ztt -- /bin/sh -x -c nc -v -z -w 2 10.254.100.57 80'
Sep 20 14:07:15.396: INFO: stderr: "+ nc -v -z -w 2 10.254.100.57 80\nConnection to 10.254.100.57 80 port [tcp/http] succeeded!\n"
Sep 20 14:07:15.396: INFO: stdout: ""
Sep 20 14:07:15.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-6020 exec execpodm7ztt -- /bin/sh -x -c nc -v -z -w 2 192.168.10.173 32228'
Sep 20 14:07:15.617: INFO: stderr: "+ nc -v -z -w 2 192.168.10.173 32228\nConnection to 192.168.10.173 32228 port [tcp/*] succeeded!\n"
Sep 20 14:07:15.617: INFO: stdout: ""
Sep 20 14:07:15.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-6020 exec execpodm7ztt -- /bin/sh -x -c nc -v -z -w 2 192.168.10.172 32228'
Sep 20 14:07:15.802: INFO: stderr: "+ nc -v -z -w 2 192.168.10.172 32228\nConnection to 192.168.10.172 32228 port [tcp/*] succeeded!\n"
Sep 20 14:07:15.803: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep 20 14:07:15.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6020" for this suite. 09/20/23 14:07:15.807
------------------------------
â€¢ [SLOW TEST] [17.486 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 14:06:58.328
    Sep 20 14:06:58.328: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename services 09/20/23 14:06:58.329
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:06:58.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:06:58.673
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-6020 09/20/23 14:06:58.679
    STEP: creating replication controller nodeport-test in namespace services-6020 09/20/23 14:06:58.723
    I0920 14:06:58.757215      20 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-6020, replica count: 2
    I0920 14:07:01.808602      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0920 14:07:04.809660      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0920 14:07:07.812674      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep 20 14:07:07.812: INFO: Creating new exec pod
    Sep 20 14:07:07.823: INFO: Waiting up to 5m0s for pod "execpodm7ztt" in namespace "services-6020" to be "running"
    Sep 20 14:07:07.832: INFO: Pod "execpodm7ztt": Phase="Pending", Reason="", readiness=false. Elapsed: 8.999432ms
    Sep 20 14:07:09.836: INFO: Pod "execpodm7ztt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012610917s
    Sep 20 14:07:11.837: INFO: Pod "execpodm7ztt": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0135457s
    Sep 20 14:07:13.837: INFO: Pod "execpodm7ztt": Phase="Running", Reason="", readiness=true. Elapsed: 6.01364033s
    Sep 20 14:07:13.837: INFO: Pod "execpodm7ztt" satisfied condition "running"
    Sep 20 14:07:14.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-6020 exec execpodm7ztt -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Sep 20 14:07:15.186: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Sep 20 14:07:15.186: INFO: stdout: ""
    Sep 20 14:07:15.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-6020 exec execpodm7ztt -- /bin/sh -x -c nc -v -z -w 2 10.254.100.57 80'
    Sep 20 14:07:15.396: INFO: stderr: "+ nc -v -z -w 2 10.254.100.57 80\nConnection to 10.254.100.57 80 port [tcp/http] succeeded!\n"
    Sep 20 14:07:15.396: INFO: stdout: ""
    Sep 20 14:07:15.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-6020 exec execpodm7ztt -- /bin/sh -x -c nc -v -z -w 2 192.168.10.173 32228'
    Sep 20 14:07:15.617: INFO: stderr: "+ nc -v -z -w 2 192.168.10.173 32228\nConnection to 192.168.10.173 32228 port [tcp/*] succeeded!\n"
    Sep 20 14:07:15.617: INFO: stdout: ""
    Sep 20 14:07:15.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=services-6020 exec execpodm7ztt -- /bin/sh -x -c nc -v -z -w 2 192.168.10.172 32228'
    Sep 20 14:07:15.802: INFO: stderr: "+ nc -v -z -w 2 192.168.10.172 32228\nConnection to 192.168.10.172 32228 port [tcp/*] succeeded!\n"
    Sep 20 14:07:15.803: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep 20 14:07:15.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6020" for this suite. 09/20/23 14:07:15.807
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 14:07:15.815
Sep 20 14:07:15.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename custom-resource-definition 09/20/23 14:07:15.816
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:07:15.952
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:07:15.956
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Sep 20 14:07:15.962: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep 20 14:07:17.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-5497" for this suite. 09/20/23 14:07:17.556
------------------------------
â€¢ [1.939 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 14:07:15.815
    Sep 20 14:07:15.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename custom-resource-definition 09/20/23 14:07:15.816
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:07:15.952
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:07:15.956
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Sep 20 14:07:15.962: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep 20 14:07:17.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-5497" for this suite. 09/20/23 14:07:17.556
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 14:07:17.756
Sep 20 14:07:17.756: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename webhook 09/20/23 14:07:17.756
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:07:17.775
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:07:17.778
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/20/23 14:07:17.803
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/20/23 14:07:18.153
STEP: Deploying the webhook pod 09/20/23 14:07:18.165
STEP: Wait for the deployment to be ready 09/20/23 14:07:18.18
Sep 20 14:07:18.200: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 14:07:20.210: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 14, 7, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 14, 7, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 14, 7, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 14, 7, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 20 14:07:22.298: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 14, 7, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 14, 7, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 14, 7, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 14, 7, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/20/23 14:07:24.228
STEP: Verifying the service has paired with the endpoint 09/20/23 14:07:24.304
Sep 20 14:07:25.304: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 09/20/23 14:07:25.48
STEP: Creating a custom resource definition that should be denied by the webhook 09/20/23 14:07:25.631
Sep 20 14:07:25.631: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep 20 14:07:25.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6184" for this suite. 09/20/23 14:07:26.486
STEP: Destroying namespace "webhook-6184-markers" for this suite. 09/20/23 14:07:27.292
------------------------------
â€¢ [SLOW TEST] [9.550 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 14:07:17.756
    Sep 20 14:07:17.756: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename webhook 09/20/23 14:07:17.756
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:07:17.775
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:07:17.778
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/20/23 14:07:17.803
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/20/23 14:07:18.153
    STEP: Deploying the webhook pod 09/20/23 14:07:18.165
    STEP: Wait for the deployment to be ready 09/20/23 14:07:18.18
    Sep 20 14:07:18.200: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Sep 20 14:07:20.210: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 14, 7, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 14, 7, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 14, 7, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 14, 7, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep 20 14:07:22.298: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 14, 7, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 14, 7, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 14, 7, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 14, 7, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/20/23 14:07:24.228
    STEP: Verifying the service has paired with the endpoint 09/20/23 14:07:24.304
    Sep 20 14:07:25.304: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 09/20/23 14:07:25.48
    STEP: Creating a custom resource definition that should be denied by the webhook 09/20/23 14:07:25.631
    Sep 20 14:07:25.631: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep 20 14:07:25.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6184" for this suite. 09/20/23 14:07:26.486
    STEP: Destroying namespace "webhook-6184-markers" for this suite. 09/20/23 14:07:27.292
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 14:07:27.306
Sep 20 14:07:27.306: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename pods 09/20/23 14:07:27.307
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:07:27.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:07:27.657
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Sep 20 14:07:27.661: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: creating the pod 09/20/23 14:07:27.661
STEP: submitting the pod to kubernetes 09/20/23 14:07:27.661
Sep 20 14:07:27.696: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-7a3366ac-f95c-40ca-8af2-3c1e6bcbd87f" in namespace "pods-4019" to be "running and ready"
Sep 20 14:07:27.702: INFO: Pod "pod-logs-websocket-7a3366ac-f95c-40ca-8af2-3c1e6bcbd87f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.480403ms
Sep 20 14:07:27.702: INFO: The phase of Pod pod-logs-websocket-7a3366ac-f95c-40ca-8af2-3c1e6bcbd87f is Pending, waiting for it to be Running (with Ready = true)
Sep 20 14:07:29.707: INFO: Pod "pod-logs-websocket-7a3366ac-f95c-40ca-8af2-3c1e6bcbd87f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011095072s
Sep 20 14:07:29.707: INFO: The phase of Pod pod-logs-websocket-7a3366ac-f95c-40ca-8af2-3c1e6bcbd87f is Pending, waiting for it to be Running (with Ready = true)
Sep 20 14:07:31.815: INFO: Pod "pod-logs-websocket-7a3366ac-f95c-40ca-8af2-3c1e6bcbd87f": Phase="Running", Reason="", readiness=true. Elapsed: 4.118558003s
Sep 20 14:07:31.815: INFO: The phase of Pod pod-logs-websocket-7a3366ac-f95c-40ca-8af2-3c1e6bcbd87f is Running (Ready = true)
Sep 20 14:07:31.815: INFO: Pod "pod-logs-websocket-7a3366ac-f95c-40ca-8af2-3c1e6bcbd87f" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep 20 14:07:32.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4019" for this suite. 09/20/23 14:07:32.032
------------------------------
â€¢ [4.915 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 14:07:27.306
    Sep 20 14:07:27.306: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename pods 09/20/23 14:07:27.307
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:07:27.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:07:27.657
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Sep 20 14:07:27.661: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: creating the pod 09/20/23 14:07:27.661
    STEP: submitting the pod to kubernetes 09/20/23 14:07:27.661
    Sep 20 14:07:27.696: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-7a3366ac-f95c-40ca-8af2-3c1e6bcbd87f" in namespace "pods-4019" to be "running and ready"
    Sep 20 14:07:27.702: INFO: Pod "pod-logs-websocket-7a3366ac-f95c-40ca-8af2-3c1e6bcbd87f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.480403ms
    Sep 20 14:07:27.702: INFO: The phase of Pod pod-logs-websocket-7a3366ac-f95c-40ca-8af2-3c1e6bcbd87f is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 14:07:29.707: INFO: Pod "pod-logs-websocket-7a3366ac-f95c-40ca-8af2-3c1e6bcbd87f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011095072s
    Sep 20 14:07:29.707: INFO: The phase of Pod pod-logs-websocket-7a3366ac-f95c-40ca-8af2-3c1e6bcbd87f is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 14:07:31.815: INFO: Pod "pod-logs-websocket-7a3366ac-f95c-40ca-8af2-3c1e6bcbd87f": Phase="Running", Reason="", readiness=true. Elapsed: 4.118558003s
    Sep 20 14:07:31.815: INFO: The phase of Pod pod-logs-websocket-7a3366ac-f95c-40ca-8af2-3c1e6bcbd87f is Running (Ready = true)
    Sep 20 14:07:31.815: INFO: Pod "pod-logs-websocket-7a3366ac-f95c-40ca-8af2-3c1e6bcbd87f" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep 20 14:07:32.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4019" for this suite. 09/20/23 14:07:32.032
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 14:07:32.224
Sep 20 14:07:32.224: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename container-lifecycle-hook 09/20/23 14:07:32.225
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:07:32.46
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:07:32.467
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 09/20/23 14:07:32.521
Sep 20 14:07:32.651: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9913" to be "running and ready"
Sep 20 14:07:32.691: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 40.563239ms
Sep 20 14:07:32.691: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep 20 14:07:34.697: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045943646s
Sep 20 14:07:34.697: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep 20 14:07:36.697: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04594963s
Sep 20 14:07:36.697: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep 20 14:07:39.137: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.486752706s
Sep 20 14:07:39.137: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep 20 14:07:40.957: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 8.306707988s
Sep 20 14:07:40.957: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Sep 20 14:07:40.957: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 09/20/23 14:07:41.106
Sep 20 14:07:41.161: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-9913" to be "running and ready"
Sep 20 14:07:41.168: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 7.619943ms
Sep 20 14:07:41.168: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Sep 20 14:07:43.172: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011868056s
Sep 20 14:07:43.172: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Sep 20 14:07:45.173: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.012472108s
Sep 20 14:07:45.173: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Sep 20 14:07:45.173: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 09/20/23 14:07:45.179
Sep 20 14:07:45.202: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 20 14:07:45.206: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 20 14:07:47.207: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 20 14:07:47.211: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 20 14:07:49.207: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 20 14:07:49.211: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 09/20/23 14:07:49.212
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Sep 20 14:07:49.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-9913" for this suite. 09/20/23 14:07:49.272
------------------------------
â€¢ [SLOW TEST] [17.057 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 14:07:32.224
    Sep 20 14:07:32.224: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename container-lifecycle-hook 09/20/23 14:07:32.225
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:07:32.46
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:07:32.467
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 09/20/23 14:07:32.521
    Sep 20 14:07:32.651: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9913" to be "running and ready"
    Sep 20 14:07:32.691: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 40.563239ms
    Sep 20 14:07:32.691: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 14:07:34.697: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045943646s
    Sep 20 14:07:34.697: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 14:07:36.697: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04594963s
    Sep 20 14:07:36.697: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 14:07:39.137: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.486752706s
    Sep 20 14:07:39.137: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 14:07:40.957: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 8.306707988s
    Sep 20 14:07:40.957: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Sep 20 14:07:40.957: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 09/20/23 14:07:41.106
    Sep 20 14:07:41.161: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-9913" to be "running and ready"
    Sep 20 14:07:41.168: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 7.619943ms
    Sep 20 14:07:41.168: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 14:07:43.172: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011868056s
    Sep 20 14:07:43.172: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 14:07:45.173: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.012472108s
    Sep 20 14:07:45.173: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Sep 20 14:07:45.173: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 09/20/23 14:07:45.179
    Sep 20 14:07:45.202: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Sep 20 14:07:45.206: INFO: Pod pod-with-prestop-exec-hook still exists
    Sep 20 14:07:47.207: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Sep 20 14:07:47.211: INFO: Pod pod-with-prestop-exec-hook still exists
    Sep 20 14:07:49.207: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Sep 20 14:07:49.211: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 09/20/23 14:07:49.212
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Sep 20 14:07:49.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-9913" for this suite. 09/20/23 14:07:49.272
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 14:07:49.281
Sep 20 14:07:49.281: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename emptydir 09/20/23 14:07:49.281
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:07:49.679
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:07:49.682
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 09/20/23 14:07:49.685
Sep 20 14:07:50.160: INFO: Waiting up to 5m0s for pod "pod-ecec37ce-04ff-4b4b-9cc5-38cb0f719195" in namespace "emptydir-5010" to be "Succeeded or Failed"
Sep 20 14:07:50.166: INFO: Pod "pod-ecec37ce-04ff-4b4b-9cc5-38cb0f719195": Phase="Pending", Reason="", readiness=false. Elapsed: 6.080354ms
Sep 20 14:07:52.171: INFO: Pod "pod-ecec37ce-04ff-4b4b-9cc5-38cb0f719195": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010472962s
Sep 20 14:07:54.175: INFO: Pod "pod-ecec37ce-04ff-4b4b-9cc5-38cb0f719195": Phase="Running", Reason="", readiness=true. Elapsed: 4.014518715s
Sep 20 14:07:56.353: INFO: Pod "pod-ecec37ce-04ff-4b4b-9cc5-38cb0f719195": Phase="Running", Reason="", readiness=false. Elapsed: 6.192415363s
Sep 20 14:07:58.174: INFO: Pod "pod-ecec37ce-04ff-4b4b-9cc5-38cb0f719195": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.01395002s
STEP: Saw pod success 09/20/23 14:07:58.174
Sep 20 14:07:58.174: INFO: Pod "pod-ecec37ce-04ff-4b4b-9cc5-38cb0f719195" satisfied condition "Succeeded or Failed"
Sep 20 14:07:58.179: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-ecec37ce-04ff-4b4b-9cc5-38cb0f719195 container test-container: <nil>
STEP: delete the pod 09/20/23 14:07:58.194
Sep 20 14:07:58.358: INFO: Waiting for pod pod-ecec37ce-04ff-4b4b-9cc5-38cb0f719195 to disappear
Sep 20 14:07:58.362: INFO: Pod pod-ecec37ce-04ff-4b4b-9cc5-38cb0f719195 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep 20 14:07:58.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5010" for this suite. 09/20/23 14:07:58.366
------------------------------
â€¢ [SLOW TEST] [9.091 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 14:07:49.281
    Sep 20 14:07:49.281: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename emptydir 09/20/23 14:07:49.281
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:07:49.679
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:07:49.682
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 09/20/23 14:07:49.685
    Sep 20 14:07:50.160: INFO: Waiting up to 5m0s for pod "pod-ecec37ce-04ff-4b4b-9cc5-38cb0f719195" in namespace "emptydir-5010" to be "Succeeded or Failed"
    Sep 20 14:07:50.166: INFO: Pod "pod-ecec37ce-04ff-4b4b-9cc5-38cb0f719195": Phase="Pending", Reason="", readiness=false. Elapsed: 6.080354ms
    Sep 20 14:07:52.171: INFO: Pod "pod-ecec37ce-04ff-4b4b-9cc5-38cb0f719195": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010472962s
    Sep 20 14:07:54.175: INFO: Pod "pod-ecec37ce-04ff-4b4b-9cc5-38cb0f719195": Phase="Running", Reason="", readiness=true. Elapsed: 4.014518715s
    Sep 20 14:07:56.353: INFO: Pod "pod-ecec37ce-04ff-4b4b-9cc5-38cb0f719195": Phase="Running", Reason="", readiness=false. Elapsed: 6.192415363s
    Sep 20 14:07:58.174: INFO: Pod "pod-ecec37ce-04ff-4b4b-9cc5-38cb0f719195": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.01395002s
    STEP: Saw pod success 09/20/23 14:07:58.174
    Sep 20 14:07:58.174: INFO: Pod "pod-ecec37ce-04ff-4b4b-9cc5-38cb0f719195" satisfied condition "Succeeded or Failed"
    Sep 20 14:07:58.179: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-ecec37ce-04ff-4b4b-9cc5-38cb0f719195 container test-container: <nil>
    STEP: delete the pod 09/20/23 14:07:58.194
    Sep 20 14:07:58.358: INFO: Waiting for pod pod-ecec37ce-04ff-4b4b-9cc5-38cb0f719195 to disappear
    Sep 20 14:07:58.362: INFO: Pod pod-ecec37ce-04ff-4b4b-9cc5-38cb0f719195 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep 20 14:07:58.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5010" for this suite. 09/20/23 14:07:58.366
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 14:07:58.373
Sep 20 14:07:58.373: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename pods 09/20/23 14:07:58.374
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:07:58.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:07:58.411
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 09/20/23 14:07:58.415
Sep 20 14:07:58.688: INFO: Waiting up to 5m0s for pod "pod-hostip-3bdcaf30-ccdc-4388-9b2d-38edb634d462" in namespace "pods-718" to be "running and ready"
Sep 20 14:07:58.694: INFO: Pod "pod-hostip-3bdcaf30-ccdc-4388-9b2d-38edb634d462": Phase="Pending", Reason="", readiness=false. Elapsed: 6.22086ms
Sep 20 14:07:58.694: INFO: The phase of Pod pod-hostip-3bdcaf30-ccdc-4388-9b2d-38edb634d462 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 14:08:00.698: INFO: Pod "pod-hostip-3bdcaf30-ccdc-4388-9b2d-38edb634d462": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00980304s
Sep 20 14:08:00.698: INFO: The phase of Pod pod-hostip-3bdcaf30-ccdc-4388-9b2d-38edb634d462 is Pending, waiting for it to be Running (with Ready = true)
Sep 20 14:08:02.704: INFO: Pod "pod-hostip-3bdcaf30-ccdc-4388-9b2d-38edb634d462": Phase="Running", Reason="", readiness=true. Elapsed: 4.015645067s
Sep 20 14:08:02.704: INFO: The phase of Pod pod-hostip-3bdcaf30-ccdc-4388-9b2d-38edb634d462 is Running (Ready = true)
Sep 20 14:08:02.704: INFO: Pod "pod-hostip-3bdcaf30-ccdc-4388-9b2d-38edb634d462" satisfied condition "running and ready"
Sep 20 14:08:02.715: INFO: Pod pod-hostip-3bdcaf30-ccdc-4388-9b2d-38edb634d462 has hostIP: 192.168.10.64
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep 20 14:08:02.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-718" for this suite. 09/20/23 14:08:02.722
------------------------------
â€¢ [4.434 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 14:07:58.373
    Sep 20 14:07:58.373: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename pods 09/20/23 14:07:58.374
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:07:58.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:07:58.411
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 09/20/23 14:07:58.415
    Sep 20 14:07:58.688: INFO: Waiting up to 5m0s for pod "pod-hostip-3bdcaf30-ccdc-4388-9b2d-38edb634d462" in namespace "pods-718" to be "running and ready"
    Sep 20 14:07:58.694: INFO: Pod "pod-hostip-3bdcaf30-ccdc-4388-9b2d-38edb634d462": Phase="Pending", Reason="", readiness=false. Elapsed: 6.22086ms
    Sep 20 14:07:58.694: INFO: The phase of Pod pod-hostip-3bdcaf30-ccdc-4388-9b2d-38edb634d462 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 14:08:00.698: INFO: Pod "pod-hostip-3bdcaf30-ccdc-4388-9b2d-38edb634d462": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00980304s
    Sep 20 14:08:00.698: INFO: The phase of Pod pod-hostip-3bdcaf30-ccdc-4388-9b2d-38edb634d462 is Pending, waiting for it to be Running (with Ready = true)
    Sep 20 14:08:02.704: INFO: Pod "pod-hostip-3bdcaf30-ccdc-4388-9b2d-38edb634d462": Phase="Running", Reason="", readiness=true. Elapsed: 4.015645067s
    Sep 20 14:08:02.704: INFO: The phase of Pod pod-hostip-3bdcaf30-ccdc-4388-9b2d-38edb634d462 is Running (Ready = true)
    Sep 20 14:08:02.704: INFO: Pod "pod-hostip-3bdcaf30-ccdc-4388-9b2d-38edb634d462" satisfied condition "running and ready"
    Sep 20 14:08:02.715: INFO: Pod pod-hostip-3bdcaf30-ccdc-4388-9b2d-38edb634d462 has hostIP: 192.168.10.64
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep 20 14:08:02.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-718" for this suite. 09/20/23 14:08:02.722
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 14:08:02.81
Sep 20 14:08:02.810: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename disruption 09/20/23 14:08:02.81
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:08:03.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:08:03.169
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 09/20/23 14:08:03.176
STEP: Waiting for the pdb to be processed 09/20/23 14:08:03.182
STEP: First trying to evict a pod which shouldn't be evictable 09/20/23 14:08:05.201
STEP: Waiting for all pods to be running 09/20/23 14:08:05.201
Sep 20 14:08:05.206: INFO: pods: 0 < 3
Sep 20 14:08:07.542: INFO: running pods: 0 < 3
Sep 20 14:08:09.213: INFO: running pods: 0 < 3
Sep 20 14:08:11.299: INFO: running pods: 0 < 3
Sep 20 14:08:13.219: INFO: running pods: 2 < 3
STEP: locating a running pod 09/20/23 14:08:15.263
STEP: Updating the pdb to allow a pod to be evicted 09/20/23 14:08:15.276
STEP: Waiting for the pdb to be processed 09/20/23 14:08:15.288
STEP: Trying to evict the same pod we tried earlier which should now be evictable 09/20/23 14:08:17.296
STEP: Waiting for all pods to be running 09/20/23 14:08:17.297
STEP: Waiting for the pdb to observed all healthy pods 09/20/23 14:08:17.299
STEP: Patching the pdb to disallow a pod to be evicted 09/20/23 14:08:18.008
STEP: Waiting for the pdb to be processed 09/20/23 14:08:18.066
STEP: Waiting for all pods to be running 09/20/23 14:08:20.081
Sep 20 14:08:20.084: INFO: running pods: 2 < 3
Sep 20 14:08:22.109: INFO: running pods: 2 < 3
STEP: locating a running pod 09/20/23 14:08:24.238
STEP: Deleting the pdb to allow a pod to be evicted 09/20/23 14:08:24.249
STEP: Waiting for the pdb to be deleted 09/20/23 14:08:24.278
STEP: Trying to evict the same pod we tried earlier which should now be evictable 09/20/23 14:08:24.282
STEP: Waiting for all pods to be running 09/20/23 14:08:24.282
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Sep 20 14:08:24.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-6308" for this suite. 09/20/23 14:08:24.326
------------------------------
â€¢ [SLOW TEST] [21.796 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 14:08:02.81
    Sep 20 14:08:02.810: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename disruption 09/20/23 14:08:02.81
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:08:03.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:08:03.169
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 09/20/23 14:08:03.176
    STEP: Waiting for the pdb to be processed 09/20/23 14:08:03.182
    STEP: First trying to evict a pod which shouldn't be evictable 09/20/23 14:08:05.201
    STEP: Waiting for all pods to be running 09/20/23 14:08:05.201
    Sep 20 14:08:05.206: INFO: pods: 0 < 3
    Sep 20 14:08:07.542: INFO: running pods: 0 < 3
    Sep 20 14:08:09.213: INFO: running pods: 0 < 3
    Sep 20 14:08:11.299: INFO: running pods: 0 < 3
    Sep 20 14:08:13.219: INFO: running pods: 2 < 3
    STEP: locating a running pod 09/20/23 14:08:15.263
    STEP: Updating the pdb to allow a pod to be evicted 09/20/23 14:08:15.276
    STEP: Waiting for the pdb to be processed 09/20/23 14:08:15.288
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 09/20/23 14:08:17.296
    STEP: Waiting for all pods to be running 09/20/23 14:08:17.297
    STEP: Waiting for the pdb to observed all healthy pods 09/20/23 14:08:17.299
    STEP: Patching the pdb to disallow a pod to be evicted 09/20/23 14:08:18.008
    STEP: Waiting for the pdb to be processed 09/20/23 14:08:18.066
    STEP: Waiting for all pods to be running 09/20/23 14:08:20.081
    Sep 20 14:08:20.084: INFO: running pods: 2 < 3
    Sep 20 14:08:22.109: INFO: running pods: 2 < 3
    STEP: locating a running pod 09/20/23 14:08:24.238
    STEP: Deleting the pdb to allow a pod to be evicted 09/20/23 14:08:24.249
    STEP: Waiting for the pdb to be deleted 09/20/23 14:08:24.278
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 09/20/23 14:08:24.282
    STEP: Waiting for all pods to be running 09/20/23 14:08:24.282
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Sep 20 14:08:24.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-6308" for this suite. 09/20/23 14:08:24.326
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 14:08:24.607
Sep 20 14:08:24.607: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename kubectl 09/20/23 14:08:24.608
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:08:24.638
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:08:24.641
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 09/20/23 14:08:24.648
Sep 20 14:08:24.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-5820 create -f -'
Sep 20 14:08:25.495: INFO: stderr: ""
Sep 20 14:08:25.495: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 09/20/23 14:08:25.495
Sep 20 14:08:25.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-5820 diff -f -'
Sep 20 14:08:26.287: INFO: rc: 1
Sep 20 14:08:26.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-5820 delete -f -'
Sep 20 14:08:26.591: INFO: stderr: ""
Sep 20 14:08:26.591: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep 20 14:08:26.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5820" for this suite. 09/20/23 14:08:26.597
------------------------------
â€¢ [1.998 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 14:08:24.607
    Sep 20 14:08:24.607: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename kubectl 09/20/23 14:08:24.608
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:08:24.638
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:08:24.641
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 09/20/23 14:08:24.648
    Sep 20 14:08:24.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-5820 create -f -'
    Sep 20 14:08:25.495: INFO: stderr: ""
    Sep 20 14:08:25.495: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 09/20/23 14:08:25.495
    Sep 20 14:08:25.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-5820 diff -f -'
    Sep 20 14:08:26.287: INFO: rc: 1
    Sep 20 14:08:26.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-5820 delete -f -'
    Sep 20 14:08:26.591: INFO: stderr: ""
    Sep 20 14:08:26.591: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep 20 14:08:26.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5820" for this suite. 09/20/23 14:08:26.597
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 14:08:26.608
Sep 20 14:08:26.608: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename kubectl 09/20/23 14:08:26.61
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:08:26.63
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:08:26.635
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Sep 20 14:08:26.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-9531 version'
Sep 20 14:08:26.711: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Sep 20 14:08:26.711: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.8\", GitCommit:\"395f0a2fdc940aeb9ab88849e8fa4321decbf6e1\", GitTreeState:\"clean\", BuildDate:\"2023-08-24T00:50:44Z\", GoVersion:\"go1.20.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.8\", GitCommit:\"395f0a2fdc940aeb9ab88849e8fa4321decbf6e1\", GitTreeState:\"clean\", BuildDate:\"2023-08-24T00:43:07Z\", GoVersion:\"go1.20.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep 20 14:08:26.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9531" for this suite. 09/20/23 14:08:27.155
------------------------------
â€¢ [0.598 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 14:08:26.608
    Sep 20 14:08:26.608: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename kubectl 09/20/23 14:08:26.61
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:08:26.63
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:08:26.635
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Sep 20 14:08:26.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-9531 version'
    Sep 20 14:08:26.711: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Sep 20 14:08:26.711: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.8\", GitCommit:\"395f0a2fdc940aeb9ab88849e8fa4321decbf6e1\", GitTreeState:\"clean\", BuildDate:\"2023-08-24T00:50:44Z\", GoVersion:\"go1.20.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.8\", GitCommit:\"395f0a2fdc940aeb9ab88849e8fa4321decbf6e1\", GitTreeState:\"clean\", BuildDate:\"2023-08-24T00:43:07Z\", GoVersion:\"go1.20.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep 20 14:08:26.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9531" for this suite. 09/20/23 14:08:27.155
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 14:08:27.207
Sep 20 14:08:27.208: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename webhook 09/20/23 14:08:27.208
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:08:28.089
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:08:28.092
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/20/23 14:08:28.302
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/20/23 14:08:28.51
STEP: Deploying the webhook pod 09/20/23 14:08:28.589
STEP: Wait for the deployment to be ready 09/20/23 14:08:29.521
Sep 20 14:08:29.839: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 20 14:08:32.438: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 14, 8, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 14, 8, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 14, 8, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 14, 8, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/20/23 14:08:34.572
STEP: Verifying the service has paired with the endpoint 09/20/23 14:08:34.77
Sep 20 14:08:35.770: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 09/20/23 14:08:35.777
STEP: Creating a configMap that does not comply to the validation webhook rules 09/20/23 14:08:36.089
STEP: Updating a validating webhook configuration's rules to not include the create operation 09/20/23 14:08:36.102
STEP: Creating a configMap that does not comply to the validation webhook rules 09/20/23 14:08:36.284
STEP: Patching a validating webhook configuration's rules to include the create operation 09/20/23 14:08:36.311
STEP: Creating a configMap that does not comply to the validation webhook rules 09/20/23 14:08:36.436
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep 20 14:08:36.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3908" for this suite. 09/20/23 14:08:36.617
STEP: Destroying namespace "webhook-3908-markers" for this suite. 09/20/23 14:08:36.631
------------------------------
â€¢ [SLOW TEST] [9.442 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 14:08:27.207
    Sep 20 14:08:27.208: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename webhook 09/20/23 14:08:27.208
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:08:28.089
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:08:28.092
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/20/23 14:08:28.302
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/20/23 14:08:28.51
    STEP: Deploying the webhook pod 09/20/23 14:08:28.589
    STEP: Wait for the deployment to be ready 09/20/23 14:08:29.521
    Sep 20 14:08:29.839: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Sep 20 14:08:32.438: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 20, 14, 8, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 14, 8, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 20, 14, 8, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 20, 14, 8, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/20/23 14:08:34.572
    STEP: Verifying the service has paired with the endpoint 09/20/23 14:08:34.77
    Sep 20 14:08:35.770: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 09/20/23 14:08:35.777
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/20/23 14:08:36.089
    STEP: Updating a validating webhook configuration's rules to not include the create operation 09/20/23 14:08:36.102
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/20/23 14:08:36.284
    STEP: Patching a validating webhook configuration's rules to include the create operation 09/20/23 14:08:36.311
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/20/23 14:08:36.436
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep 20 14:08:36.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3908" for this suite. 09/20/23 14:08:36.617
    STEP: Destroying namespace "webhook-3908-markers" for this suite. 09/20/23 14:08:36.631
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 14:08:36.65
Sep 20 14:08:36.650: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename sysctl 09/20/23 14:08:36.651
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:08:36.744
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:08:36.748
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 09/20/23 14:08:36.824
STEP: Watching for error events or started pod 09/20/23 14:08:36.838
STEP: Waiting for pod completion 09/20/23 14:08:40.841
Sep 20 14:08:40.841: INFO: Waiting up to 3m0s for pod "sysctl-76616bf7-12ec-487f-880a-920f8509c2c0" in namespace "sysctl-4609" to be "completed"
Sep 20 14:08:40.845: INFO: Pod "sysctl-76616bf7-12ec-487f-880a-920f8509c2c0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.179529ms
Sep 20 14:08:42.850: INFO: Pod "sysctl-76616bf7-12ec-487f-880a-920f8509c2c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008072738s
Sep 20 14:08:45.137: INFO: Pod "sysctl-76616bf7-12ec-487f-880a-920f8509c2c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.295496363s
Sep 20 14:08:45.137: INFO: Pod "sysctl-76616bf7-12ec-487f-880a-920f8509c2c0" satisfied condition "completed"
STEP: Checking that the pod succeeded 09/20/23 14:08:45.142
STEP: Getting logs from the pod 09/20/23 14:08:45.142
STEP: Checking that the sysctl is actually updated 09/20/23 14:08:45.151
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep 20 14:08:45.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-4609" for this suite. 09/20/23 14:08:45.156
------------------------------
â€¢ [SLOW TEST] [8.512 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 14:08:36.65
    Sep 20 14:08:36.650: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename sysctl 09/20/23 14:08:36.651
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:08:36.744
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:08:36.748
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 09/20/23 14:08:36.824
    STEP: Watching for error events or started pod 09/20/23 14:08:36.838
    STEP: Waiting for pod completion 09/20/23 14:08:40.841
    Sep 20 14:08:40.841: INFO: Waiting up to 3m0s for pod "sysctl-76616bf7-12ec-487f-880a-920f8509c2c0" in namespace "sysctl-4609" to be "completed"
    Sep 20 14:08:40.845: INFO: Pod "sysctl-76616bf7-12ec-487f-880a-920f8509c2c0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.179529ms
    Sep 20 14:08:42.850: INFO: Pod "sysctl-76616bf7-12ec-487f-880a-920f8509c2c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008072738s
    Sep 20 14:08:45.137: INFO: Pod "sysctl-76616bf7-12ec-487f-880a-920f8509c2c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.295496363s
    Sep 20 14:08:45.137: INFO: Pod "sysctl-76616bf7-12ec-487f-880a-920f8509c2c0" satisfied condition "completed"
    STEP: Checking that the pod succeeded 09/20/23 14:08:45.142
    STEP: Getting logs from the pod 09/20/23 14:08:45.142
    STEP: Checking that the sysctl is actually updated 09/20/23 14:08:45.151
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep 20 14:08:45.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-4609" for this suite. 09/20/23 14:08:45.156
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 14:08:45.166
Sep 20 14:08:45.166: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename statefulset 09/20/23 14:08:45.167
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:08:45.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:08:45.23
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4253 09/20/23 14:08:45.236
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-4253 09/20/23 14:08:45.264
Sep 20 14:08:45.282: INFO: Found 0 stateful pods, waiting for 1
Sep 20 14:08:55.288: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 09/20/23 14:08:55.294
STEP: updating a scale subresource 09/20/23 14:08:55.296
STEP: verifying the statefulset Spec.Replicas was modified 09/20/23 14:08:55.432
STEP: Patch a scale subresource 09/20/23 14:08:55.436
STEP: verifying the statefulset Spec.Replicas was modified 09/20/23 14:08:55.492
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep 20 14:08:55.611: INFO: Deleting all statefulset in ns statefulset-4253
Sep 20 14:08:55.618: INFO: Scaling statefulset ss to 0
Sep 20 14:09:05.982: INFO: Waiting for statefulset status.replicas updated to 0
Sep 20 14:09:05.985: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep 20 14:09:06.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4253" for this suite. 09/20/23 14:09:06.106
------------------------------
â€¢ [SLOW TEST] [20.949 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 14:08:45.166
    Sep 20 14:08:45.166: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename statefulset 09/20/23 14:08:45.167
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:08:45.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:08:45.23
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4253 09/20/23 14:08:45.236
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-4253 09/20/23 14:08:45.264
    Sep 20 14:08:45.282: INFO: Found 0 stateful pods, waiting for 1
    Sep 20 14:08:55.288: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 09/20/23 14:08:55.294
    STEP: updating a scale subresource 09/20/23 14:08:55.296
    STEP: verifying the statefulset Spec.Replicas was modified 09/20/23 14:08:55.432
    STEP: Patch a scale subresource 09/20/23 14:08:55.436
    STEP: verifying the statefulset Spec.Replicas was modified 09/20/23 14:08:55.492
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep 20 14:08:55.611: INFO: Deleting all statefulset in ns statefulset-4253
    Sep 20 14:08:55.618: INFO: Scaling statefulset ss to 0
    Sep 20 14:09:05.982: INFO: Waiting for statefulset status.replicas updated to 0
    Sep 20 14:09:05.985: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep 20 14:09:06.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4253" for this suite. 09/20/23 14:09:06.106
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 14:09:06.117
Sep 20 14:09:06.117: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename configmap 09/20/23 14:09:06.118
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:09:06.135
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:09:06.137
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-43e762f2-6cba-4782-83e3-fe0483272667 09/20/23 14:09:06.184
STEP: Creating a pod to test consume configMaps 09/20/23 14:09:06.189
Sep 20 14:09:06.258: INFO: Waiting up to 5m0s for pod "pod-configmaps-addac5c2-8f7a-41b0-9aa5-82ba6d3de2f0" in namespace "configmap-4617" to be "Succeeded or Failed"
Sep 20 14:09:06.264: INFO: Pod "pod-configmaps-addac5c2-8f7a-41b0-9aa5-82ba6d3de2f0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.448453ms
Sep 20 14:09:08.269: INFO: Pod "pod-configmaps-addac5c2-8f7a-41b0-9aa5-82ba6d3de2f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010147777s
Sep 20 14:09:10.270: INFO: Pod "pod-configmaps-addac5c2-8f7a-41b0-9aa5-82ba6d3de2f0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011237931s
Sep 20 14:09:12.414: INFO: Pod "pod-configmaps-addac5c2-8f7a-41b0-9aa5-82ba6d3de2f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.155096401s
STEP: Saw pod success 09/20/23 14:09:12.414
Sep 20 14:09:12.414: INFO: Pod "pod-configmaps-addac5c2-8f7a-41b0-9aa5-82ba6d3de2f0" satisfied condition "Succeeded or Failed"
Sep 20 14:09:12.418: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-configmaps-addac5c2-8f7a-41b0-9aa5-82ba6d3de2f0 container agnhost-container: <nil>
STEP: delete the pod 09/20/23 14:09:12.43
Sep 20 14:09:12.684: INFO: Waiting for pod pod-configmaps-addac5c2-8f7a-41b0-9aa5-82ba6d3de2f0 to disappear
Sep 20 14:09:12.687: INFO: Pod pod-configmaps-addac5c2-8f7a-41b0-9aa5-82ba6d3de2f0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep 20 14:09:12.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4617" for this suite. 09/20/23 14:09:12.691
------------------------------
â€¢ [SLOW TEST] [6.579 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 14:09:06.117
    Sep 20 14:09:06.117: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename configmap 09/20/23 14:09:06.118
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:09:06.135
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:09:06.137
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-43e762f2-6cba-4782-83e3-fe0483272667 09/20/23 14:09:06.184
    STEP: Creating a pod to test consume configMaps 09/20/23 14:09:06.189
    Sep 20 14:09:06.258: INFO: Waiting up to 5m0s for pod "pod-configmaps-addac5c2-8f7a-41b0-9aa5-82ba6d3de2f0" in namespace "configmap-4617" to be "Succeeded or Failed"
    Sep 20 14:09:06.264: INFO: Pod "pod-configmaps-addac5c2-8f7a-41b0-9aa5-82ba6d3de2f0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.448453ms
    Sep 20 14:09:08.269: INFO: Pod "pod-configmaps-addac5c2-8f7a-41b0-9aa5-82ba6d3de2f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010147777s
    Sep 20 14:09:10.270: INFO: Pod "pod-configmaps-addac5c2-8f7a-41b0-9aa5-82ba6d3de2f0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011237931s
    Sep 20 14:09:12.414: INFO: Pod "pod-configmaps-addac5c2-8f7a-41b0-9aa5-82ba6d3de2f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.155096401s
    STEP: Saw pod success 09/20/23 14:09:12.414
    Sep 20 14:09:12.414: INFO: Pod "pod-configmaps-addac5c2-8f7a-41b0-9aa5-82ba6d3de2f0" satisfied condition "Succeeded or Failed"
    Sep 20 14:09:12.418: INFO: Trying to get logs from node mycluster-ww3cg64etuwi-node-1 pod pod-configmaps-addac5c2-8f7a-41b0-9aa5-82ba6d3de2f0 container agnhost-container: <nil>
    STEP: delete the pod 09/20/23 14:09:12.43
    Sep 20 14:09:12.684: INFO: Waiting for pod pod-configmaps-addac5c2-8f7a-41b0-9aa5-82ba6d3de2f0 to disappear
    Sep 20 14:09:12.687: INFO: Pod pod-configmaps-addac5c2-8f7a-41b0-9aa5-82ba6d3de2f0 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep 20 14:09:12.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4617" for this suite. 09/20/23 14:09:12.691
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 14:09:12.698
Sep 20 14:09:12.698: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename replicaset 09/20/23 14:09:12.699
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:09:12.715
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:09:12.72
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Sep 20 14:09:12.740: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep 20 14:09:17.749: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/20/23 14:09:17.749
STEP: Scaling up "test-rs" replicaset  09/20/23 14:09:17.749
Sep 20 14:09:17.799: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 09/20/23 14:09:17.799
W0920 14:09:17.878676      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Sep 20 14:09:17.909: INFO: observed ReplicaSet test-rs in namespace replicaset-9414 with ReadyReplicas 1, AvailableReplicas 1
Sep 20 14:09:17.933: INFO: observed ReplicaSet test-rs in namespace replicaset-9414 with ReadyReplicas 1, AvailableReplicas 1
Sep 20 14:09:17.979: INFO: observed ReplicaSet test-rs in namespace replicaset-9414 with ReadyReplicas 1, AvailableReplicas 1
Sep 20 14:09:17.999: INFO: observed ReplicaSet test-rs in namespace replicaset-9414 with ReadyReplicas 1, AvailableReplicas 1
Sep 20 14:09:23.182: INFO: observed ReplicaSet test-rs in namespace replicaset-9414 with ReadyReplicas 2, AvailableReplicas 2
Sep 20 14:09:23.294: INFO: observed Replicaset test-rs in namespace replicaset-9414 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep 20 14:09:23.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-9414" for this suite. 09/20/23 14:09:23.3
------------------------------
â€¢ [SLOW TEST] [10.611 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 14:09:12.698
    Sep 20 14:09:12.698: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename replicaset 09/20/23 14:09:12.699
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:09:12.715
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:09:12.72
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Sep 20 14:09:12.740: INFO: Pod name sample-pod: Found 0 pods out of 1
    Sep 20 14:09:17.749: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/20/23 14:09:17.749
    STEP: Scaling up "test-rs" replicaset  09/20/23 14:09:17.749
    Sep 20 14:09:17.799: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 09/20/23 14:09:17.799
    W0920 14:09:17.878676      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Sep 20 14:09:17.909: INFO: observed ReplicaSet test-rs in namespace replicaset-9414 with ReadyReplicas 1, AvailableReplicas 1
    Sep 20 14:09:17.933: INFO: observed ReplicaSet test-rs in namespace replicaset-9414 with ReadyReplicas 1, AvailableReplicas 1
    Sep 20 14:09:17.979: INFO: observed ReplicaSet test-rs in namespace replicaset-9414 with ReadyReplicas 1, AvailableReplicas 1
    Sep 20 14:09:17.999: INFO: observed ReplicaSet test-rs in namespace replicaset-9414 with ReadyReplicas 1, AvailableReplicas 1
    Sep 20 14:09:23.182: INFO: observed ReplicaSet test-rs in namespace replicaset-9414 with ReadyReplicas 2, AvailableReplicas 2
    Sep 20 14:09:23.294: INFO: observed Replicaset test-rs in namespace replicaset-9414 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep 20 14:09:23.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-9414" for this suite. 09/20/23 14:09:23.3
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 14:09:23.309
Sep 20 14:09:23.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename svcaccounts 09/20/23 14:09:23.31
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:09:23.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:09:23.333
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Sep 20 14:09:23.394: INFO: created pod pod-service-account-defaultsa
Sep 20 14:09:23.394: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Sep 20 14:09:23.643: INFO: created pod pod-service-account-mountsa
Sep 20 14:09:23.643: INFO: pod pod-service-account-mountsa service account token volume mount: true
Sep 20 14:09:23.651: INFO: created pod pod-service-account-nomountsa
Sep 20 14:09:23.651: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Sep 20 14:09:23.667: INFO: created pod pod-service-account-defaultsa-mountspec
Sep 20 14:09:23.667: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Sep 20 14:09:23.679: INFO: created pod pod-service-account-mountsa-mountspec
Sep 20 14:09:23.679: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Sep 20 14:09:23.689: INFO: created pod pod-service-account-nomountsa-mountspec
Sep 20 14:09:23.689: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Sep 20 14:09:23.700: INFO: created pod pod-service-account-defaultsa-nomountspec
Sep 20 14:09:23.700: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Sep 20 14:09:23.722: INFO: created pod pod-service-account-mountsa-nomountspec
Sep 20 14:09:23.722: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Sep 20 14:09:24.376: INFO: created pod pod-service-account-nomountsa-nomountspec
Sep 20 14:09:24.376: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep 20 14:09:24.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-5162" for this suite. 09/20/23 14:09:24.39
------------------------------
â€¢ [1.144 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 14:09:23.309
    Sep 20 14:09:23.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename svcaccounts 09/20/23 14:09:23.31
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:09:23.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:09:23.333
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Sep 20 14:09:23.394: INFO: created pod pod-service-account-defaultsa
    Sep 20 14:09:23.394: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Sep 20 14:09:23.643: INFO: created pod pod-service-account-mountsa
    Sep 20 14:09:23.643: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Sep 20 14:09:23.651: INFO: created pod pod-service-account-nomountsa
    Sep 20 14:09:23.651: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Sep 20 14:09:23.667: INFO: created pod pod-service-account-defaultsa-mountspec
    Sep 20 14:09:23.667: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Sep 20 14:09:23.679: INFO: created pod pod-service-account-mountsa-mountspec
    Sep 20 14:09:23.679: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Sep 20 14:09:23.689: INFO: created pod pod-service-account-nomountsa-mountspec
    Sep 20 14:09:23.689: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Sep 20 14:09:23.700: INFO: created pod pod-service-account-defaultsa-nomountspec
    Sep 20 14:09:23.700: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Sep 20 14:09:23.722: INFO: created pod pod-service-account-mountsa-nomountspec
    Sep 20 14:09:23.722: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Sep 20 14:09:24.376: INFO: created pod pod-service-account-nomountsa-nomountspec
    Sep 20 14:09:24.376: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep 20 14:09:24.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-5162" for this suite. 09/20/23 14:09:24.39
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/20/23 14:09:24.458
Sep 20 14:09:24.458: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
STEP: Building a namespace api object, basename kubectl 09/20/23 14:09:24.458
STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:09:24.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:09:24.793
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Sep 20 14:09:24.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-3526 create -f -'
Sep 20 14:09:25.690: INFO: stderr: ""
Sep 20 14:09:25.690: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Sep 20 14:09:25.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-3526 create -f -'
Sep 20 14:09:26.725: INFO: stderr: ""
Sep 20 14:09:26.725: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 09/20/23 14:09:26.725
Sep 20 14:09:27.931: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 20 14:09:27.931: INFO: Found 0 / 1
Sep 20 14:09:28.746: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 20 14:09:28.746: INFO: Found 0 / 1
Sep 20 14:09:29.845: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 20 14:09:29.845: INFO: Found 0 / 1
Sep 20 14:09:30.916: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 20 14:09:30.916: INFO: Found 1 / 1
Sep 20 14:09:30.916: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 20 14:09:30.978: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 20 14:09:30.978: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 20 14:09:30.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-3526 describe pod agnhost-primary-cv7wt'
Sep 20 14:09:31.499: INFO: stderr: ""
Sep 20 14:09:31.499: INFO: stdout: "Name:             agnhost-primary-cv7wt\nNamespace:        kubectl-3526\nPriority:         0\nService Account:  default\nNode:             mycluster-ww3cg64etuwi-node-2/192.168.10.172\nStart Time:       Wed, 20 Sep 2023 14:09:25 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.100.3.216\nIPs:\n  IP:           10.100.3.216\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://554c8b76f07465f6465574bd6116ffc105e017129634f5601040b81c7bb05563\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 20 Sep 2023 14:09:28 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7svq4 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-7svq4:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  5s    default-scheduler  Successfully assigned kubectl-3526/agnhost-primary-cv7wt to mycluster-ww3cg64etuwi-node-2\n  Normal  Pulled     3s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    3s    kubelet            Created container agnhost-primary\n  Normal  Started    3s    kubelet            Started container agnhost-primary\n"
Sep 20 14:09:31.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-3526 describe rc agnhost-primary'
Sep 20 14:09:31.597: INFO: stderr: ""
Sep 20 14:09:31.597: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-3526\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  6s    replication-controller  Created pod: agnhost-primary-cv7wt\n"
Sep 20 14:09:31.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-3526 describe service agnhost-primary'
Sep 20 14:09:31.689: INFO: stderr: ""
Sep 20 14:09:31.689: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-3526\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.254.189.17\nIPs:               10.254.189.17\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.100.3.216:6379\nSession Affinity:  None\nEvents:            <none>\n"
Sep 20 14:09:31.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-3526 describe node mycluster-ww3cg64etuwi-master-0'
Sep 20 14:09:33.854: INFO: stderr: ""
Sep 20 14:09:33.854: INFO: stdout: "Name:               mycluster-ww3cg64etuwi-master-0\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m3.small\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=Melbourne\n                    failure-domain.beta.kubernetes.io/zone=melbourne-qh2\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=mycluster-ww3cg64etuwi-master-0\n                    kubernetes.io/os=linux\n                    magnum.openstack.org/nodegroup=default-master\n                    magnum.openstack.org/role=master\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/instance-type=m3.small\n                    topology.cinder.csi.openstack.org/zone=melbourne-qh2\n                    topology.kubernetes.io/region=Melbourne\n                    topology.kubernetes.io/zone=melbourne-qh2\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"cinder.csi.openstack.org\":\"86040e9c-745b-4668-918d-97dbf025a8bb\"}\n                    flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"e2:92:7a:8c:b9:ed\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 192.168.10.121\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 20 Sep 2023 11:47:15 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  mycluster-ww3cg64etuwi-master-0\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 20 Sep 2023 14:09:24 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 20 Sep 2023 11:48:45 +0000   Wed, 20 Sep 2023 11:48:45 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Wed, 20 Sep 2023 14:07:54 +0000   Wed, 20 Sep 2023 11:47:15 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 20 Sep 2023 14:07:54 +0000   Wed, 20 Sep 2023 11:47:15 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 20 Sep 2023 14:07:54 +0000   Wed, 20 Sep 2023 11:47:15 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 20 Sep 2023 14:07:54 +0000   Wed, 20 Sep 2023 11:48:40 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.10.121\n  Hostname:    mycluster-ww3cg64etuwi-master-0\nCapacity:\n  cpu:                2\n  ephemeral-storage:  30866412Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3999864Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  30866412Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3999864Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 86040e9c745b4668918d97dbf025a8bb\n  System UUID:                86040e9c-745b-4668-918d-97dbf025a8bb\n  Boot ID:                    41f21d22-77e3-4c65-875a-21aa90f1b132\n  Kernel Version:             6.1.18-200.fc37.x86_64\n  OS Image:                   Fedora CoreOS 37.20230322.3.0\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.20\n  Kubelet Version:            v1.26.8\n  Kube-Proxy Version:         v1.26.8\nPodCIDR:                      10.100.1.0/24\nPodCIDRs:                     10.100.1.0/24\nProviderID:                   openstack:///86040e9c-745b-4668-918d-97dbf025a8bb\nNon-terminated Pods:          (6 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 csi-cinder-nodeplugin-g4dc2                                20m (1%)      0 (0%)      0 (0%)           0 (0%)         140m\n  kube-system                 k8s-keystone-auth-5fkhq                                    200m (10%)    0 (0%)      0 (0%)           0 (0%)         140m\n  kube-system                 kube-flannel-ds-glmzp                                      100m (5%)     100m (5%)   50Mi (1%)        50Mi (1%)      141m\n  kube-system                 octavia-ingress-controller-0                               50m (2%)      0 (0%)      0 (0%)           0 (0%)         141m\n  kube-system                 openstack-cloud-controller-manager-rcwm2                   200m (10%)    0 (0%)      0 (0%)           0 (0%)         141m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-6128a9cd7eba4f1a-txq4q    0 (0%)        0 (0%)      0 (0%)           0 (0%)         125m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                570m (28%)  100m (5%)\n  memory             50Mi (1%)   50Mi (1%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:\n  Type    Reason          Age   From                   Message\n  ----    ------          ----  ----                   -------\n  Normal  Starting        142m  kube-proxy             \n  Normal  RegisteredNode  142m  node-controller        Node mycluster-ww3cg64etuwi-master-0 event: Registered Node mycluster-ww3cg64etuwi-master-0 in Controller\n  Normal  Synced          141m  cloud-node-controller  Node synced successfully\n  Normal  NodeReady       140m  kubelet                Node mycluster-ww3cg64etuwi-master-0 status is now: NodeReady\n  Normal  RegisteredNode  140m  node-controller        Node mycluster-ww3cg64etuwi-master-0 event: Registered Node mycluster-ww3cg64etuwi-master-0 in Controller\n"
Sep 20 14:09:33.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-3526 describe namespace kubectl-3526'
Sep 20 14:09:38.888: INFO: stderr: ""
Sep 20 14:09:38.888: INFO: stdout: "Name:         kubectl-3526\nLabels:       e2e-framework=kubectl\n              e2e-run=e878e702-44f2-4f20-b5dd-9ccb6fb27e1a\n              kubernetes.io/metadata.name=kubectl-3526\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep 20 14:09:38.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3526" for this suite. 09/20/23 14:09:39.045
------------------------------
â€¢ [SLOW TEST] [14.648 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/20/23 14:09:24.458
    Sep 20 14:09:24.458: INFO: >>> kubeConfig: /tmp/kubeconfig-1333120238
    STEP: Building a namespace api object, basename kubectl 09/20/23 14:09:24.458
    STEP: Waiting for a default service account to be provisioned in namespace 09/20/23 14:09:24.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/20/23 14:09:24.793
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Sep 20 14:09:24.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-3526 create -f -'
    Sep 20 14:09:25.690: INFO: stderr: ""
    Sep 20 14:09:25.690: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Sep 20 14:09:25.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-3526 create -f -'
    Sep 20 14:09:26.725: INFO: stderr: ""
    Sep 20 14:09:26.725: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 09/20/23 14:09:26.725
    Sep 20 14:09:27.931: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep 20 14:09:27.931: INFO: Found 0 / 1
    Sep 20 14:09:28.746: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep 20 14:09:28.746: INFO: Found 0 / 1
    Sep 20 14:09:29.845: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep 20 14:09:29.845: INFO: Found 0 / 1
    Sep 20 14:09:30.916: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep 20 14:09:30.916: INFO: Found 1 / 1
    Sep 20 14:09:30.916: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Sep 20 14:09:30.978: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep 20 14:09:30.978: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Sep 20 14:09:30.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-3526 describe pod agnhost-primary-cv7wt'
    Sep 20 14:09:31.499: INFO: stderr: ""
    Sep 20 14:09:31.499: INFO: stdout: "Name:             agnhost-primary-cv7wt\nNamespace:        kubectl-3526\nPriority:         0\nService Account:  default\nNode:             mycluster-ww3cg64etuwi-node-2/192.168.10.172\nStart Time:       Wed, 20 Sep 2023 14:09:25 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.100.3.216\nIPs:\n  IP:           10.100.3.216\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://554c8b76f07465f6465574bd6116ffc105e017129634f5601040b81c7bb05563\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 20 Sep 2023 14:09:28 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7svq4 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-7svq4:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  5s    default-scheduler  Successfully assigned kubectl-3526/agnhost-primary-cv7wt to mycluster-ww3cg64etuwi-node-2\n  Normal  Pulled     3s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    3s    kubelet            Created container agnhost-primary\n  Normal  Started    3s    kubelet            Started container agnhost-primary\n"
    Sep 20 14:09:31.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-3526 describe rc agnhost-primary'
    Sep 20 14:09:31.597: INFO: stderr: ""
    Sep 20 14:09:31.597: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-3526\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  6s    replication-controller  Created pod: agnhost-primary-cv7wt\n"
    Sep 20 14:09:31.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-3526 describe service agnhost-primary'
    Sep 20 14:09:31.689: INFO: stderr: ""
    Sep 20 14:09:31.689: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-3526\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.254.189.17\nIPs:               10.254.189.17\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.100.3.216:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Sep 20 14:09:31.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-3526 describe node mycluster-ww3cg64etuwi-master-0'
    Sep 20 14:09:33.854: INFO: stderr: ""
    Sep 20 14:09:33.854: INFO: stdout: "Name:               mycluster-ww3cg64etuwi-master-0\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m3.small\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=Melbourne\n                    failure-domain.beta.kubernetes.io/zone=melbourne-qh2\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=mycluster-ww3cg64etuwi-master-0\n                    kubernetes.io/os=linux\n                    magnum.openstack.org/nodegroup=default-master\n                    magnum.openstack.org/role=master\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/instance-type=m3.small\n                    topology.cinder.csi.openstack.org/zone=melbourne-qh2\n                    topology.kubernetes.io/region=Melbourne\n                    topology.kubernetes.io/zone=melbourne-qh2\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"cinder.csi.openstack.org\":\"86040e9c-745b-4668-918d-97dbf025a8bb\"}\n                    flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"e2:92:7a:8c:b9:ed\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 192.168.10.121\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 20 Sep 2023 11:47:15 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  mycluster-ww3cg64etuwi-master-0\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 20 Sep 2023 14:09:24 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 20 Sep 2023 11:48:45 +0000   Wed, 20 Sep 2023 11:48:45 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Wed, 20 Sep 2023 14:07:54 +0000   Wed, 20 Sep 2023 11:47:15 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 20 Sep 2023 14:07:54 +0000   Wed, 20 Sep 2023 11:47:15 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 20 Sep 2023 14:07:54 +0000   Wed, 20 Sep 2023 11:47:15 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 20 Sep 2023 14:07:54 +0000   Wed, 20 Sep 2023 11:48:40 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.10.121\n  Hostname:    mycluster-ww3cg64etuwi-master-0\nCapacity:\n  cpu:                2\n  ephemeral-storage:  30866412Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3999864Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  30866412Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3999864Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 86040e9c745b4668918d97dbf025a8bb\n  System UUID:                86040e9c-745b-4668-918d-97dbf025a8bb\n  Boot ID:                    41f21d22-77e3-4c65-875a-21aa90f1b132\n  Kernel Version:             6.1.18-200.fc37.x86_64\n  OS Image:                   Fedora CoreOS 37.20230322.3.0\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.20\n  Kubelet Version:            v1.26.8\n  Kube-Proxy Version:         v1.26.8\nPodCIDR:                      10.100.1.0/24\nPodCIDRs:                     10.100.1.0/24\nProviderID:                   openstack:///86040e9c-745b-4668-918d-97dbf025a8bb\nNon-terminated Pods:          (6 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 csi-cinder-nodeplugin-g4dc2                                20m (1%)      0 (0%)      0 (0%)           0 (0%)         140m\n  kube-system                 k8s-keystone-auth-5fkhq                                    200m (10%)    0 (0%)      0 (0%)           0 (0%)         140m\n  kube-system                 kube-flannel-ds-glmzp                                      100m (5%)     100m (5%)   50Mi (1%)        50Mi (1%)      141m\n  kube-system                 octavia-ingress-controller-0                               50m (2%)      0 (0%)      0 (0%)           0 (0%)         141m\n  kube-system                 openstack-cloud-controller-manager-rcwm2                   200m (10%)    0 (0%)      0 (0%)           0 (0%)         141m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-6128a9cd7eba4f1a-txq4q    0 (0%)        0 (0%)      0 (0%)           0 (0%)         125m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                570m (28%)  100m (5%)\n  memory             50Mi (1%)   50Mi (1%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:\n  Type    Reason          Age   From                   Message\n  ----    ------          ----  ----                   -------\n  Normal  Starting        142m  kube-proxy             \n  Normal  RegisteredNode  142m  node-controller        Node mycluster-ww3cg64etuwi-master-0 event: Registered Node mycluster-ww3cg64etuwi-master-0 in Controller\n  Normal  Synced          141m  cloud-node-controller  Node synced successfully\n  Normal  NodeReady       140m  kubelet                Node mycluster-ww3cg64etuwi-master-0 status is now: NodeReady\n  Normal  RegisteredNode  140m  node-controller        Node mycluster-ww3cg64etuwi-master-0 event: Registered Node mycluster-ww3cg64etuwi-master-0 in Controller\n"
    Sep 20 14:09:33.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1333120238 --namespace=kubectl-3526 describe namespace kubectl-3526'
    Sep 20 14:09:38.888: INFO: stderr: ""
    Sep 20 14:09:38.888: INFO: stdout: "Name:         kubectl-3526\nLabels:       e2e-framework=kubectl\n              e2e-run=e878e702-44f2-4f20-b5dd-9ccb6fb27e1a\n              kubernetes.io/metadata.name=kubectl-3526\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep 20 14:09:38.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3526" for this suite. 09/20/23 14:09:39.045
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Sep 20 14:09:39.107: INFO: Running AfterSuite actions on node 1
Sep 20 14:09:39.107: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Sep 20 14:09:39.107: INFO: Running AfterSuite actions on node 1
    Sep 20 14:09:39.107: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.066 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 7479.618 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 2h4m39.869200654s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

