I1115 15:25:04.884206      22 e2e.go:126] Starting e2e run "3b21b822-8b44-408e-b236-bfbd8cf999aa" on Ginkgo node 1
Nov 15 15:25:04.904: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1700061904 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Nov 15 15:25:05.060: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
Nov 15 15:25:05.062: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Nov 15 15:25:05.138: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Nov 15 15:25:05.274: INFO: 36 / 36 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Nov 15 15:25:05.274: INFO: expected 21 pod replicas in namespace 'kube-system', 21 are Running and Ready.
Nov 15 15:25:05.274: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Nov 15 15:25:05.300: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Nov 15 15:25:05.300: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Nov 15 15:25:05.300: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibmcloud-block-storage-driver' (0 seconds elapsed)
Nov 15 15:25:05.300: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'konnectivity-agent' (0 seconds elapsed)
Nov 15 15:25:05.300: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Nov 15 15:25:05.300: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-driver-installer' (0 seconds elapsed)
Nov 15 15:25:05.300: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-gpu-device-plugin' (0 seconds elapsed)
Nov 15 15:25:05.300: INFO: e2e test version: v1.26.10
Nov 15 15:25:05.308: INFO: kube-apiserver version: v1.26.10+IKS
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Nov 15 15:25:05.308: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
Nov 15 15:25:05.324: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.264 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Nov 15 15:25:05.060: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    Nov 15 15:25:05.062: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Nov 15 15:25:05.138: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Nov 15 15:25:05.274: INFO: 36 / 36 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Nov 15 15:25:05.274: INFO: expected 21 pod replicas in namespace 'kube-system', 21 are Running and Ready.
    Nov 15 15:25:05.274: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Nov 15 15:25:05.300: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Nov 15 15:25:05.300: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
    Nov 15 15:25:05.300: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibmcloud-block-storage-driver' (0 seconds elapsed)
    Nov 15 15:25:05.300: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'konnectivity-agent' (0 seconds elapsed)
    Nov 15 15:25:05.300: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
    Nov 15 15:25:05.300: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-driver-installer' (0 seconds elapsed)
    Nov 15 15:25:05.300: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-gpu-device-plugin' (0 seconds elapsed)
    Nov 15 15:25:05.300: INFO: e2e test version: v1.26.10
    Nov 15 15:25:05.308: INFO: kube-apiserver version: v1.26.10+IKS
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Nov 15 15:25:05.308: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    Nov 15 15:25:05.324: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:25:05.362
Nov 15 15:25:05.362: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename job 11/15/23 15:25:05.363
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:25:05.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:25:05.427
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 11/15/23 15:25:05.447
STEP: Ensure pods equal to parallelism count is attached to the job 11/15/23 15:25:05.472
STEP: patching /status 11/15/23 15:25:11.496
STEP: updating /status 11/15/23 15:25:11.523
STEP: get /status 11/15/23 15:25:11.568
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Nov 15 15:25:11.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-2765" for this suite. 11/15/23 15:25:11.613
------------------------------
• [SLOW TEST] [6.277 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:25:05.362
    Nov 15 15:25:05.362: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename job 11/15/23 15:25:05.363
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:25:05.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:25:05.427
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 11/15/23 15:25:05.447
    STEP: Ensure pods equal to parallelism count is attached to the job 11/15/23 15:25:05.472
    STEP: patching /status 11/15/23 15:25:11.496
    STEP: updating /status 11/15/23 15:25:11.523
    STEP: get /status 11/15/23 15:25:11.568
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:25:11.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-2765" for this suite. 11/15/23 15:25:11.613
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:25:11.642
Nov 15 15:25:11.643: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename projected 11/15/23 15:25:11.646
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:25:11.69
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:25:11.708
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-4094e2dd-3c70-480b-9dfa-03f3a30a54bb 11/15/23 15:25:11.726
STEP: Creating a pod to test consume configMaps 11/15/23 15:25:11.749
Nov 15 15:25:11.783: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6a3a4c31-faa9-4113-b7d5-cb58a28b555c" in namespace "projected-323" to be "Succeeded or Failed"
Nov 15 15:25:11.802: INFO: Pod "pod-projected-configmaps-6a3a4c31-faa9-4113-b7d5-cb58a28b555c": Phase="Pending", Reason="", readiness=false. Elapsed: 18.877269ms
Nov 15 15:25:13.825: INFO: Pod "pod-projected-configmaps-6a3a4c31-faa9-4113-b7d5-cb58a28b555c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042055354s
Nov 15 15:25:15.830: INFO: Pod "pod-projected-configmaps-6a3a4c31-faa9-4113-b7d5-cb58a28b555c": Phase="Running", Reason="", readiness=true. Elapsed: 4.046754399s
Nov 15 15:25:17.824: INFO: Pod "pod-projected-configmaps-6a3a4c31-faa9-4113-b7d5-cb58a28b555c": Phase="Running", Reason="", readiness=false. Elapsed: 6.041257911s
Nov 15 15:25:19.824: INFO: Pod "pod-projected-configmaps-6a3a4c31-faa9-4113-b7d5-cb58a28b555c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.040898425s
STEP: Saw pod success 11/15/23 15:25:19.824
Nov 15 15:25:19.824: INFO: Pod "pod-projected-configmaps-6a3a4c31-faa9-4113-b7d5-cb58a28b555c" satisfied condition "Succeeded or Failed"
Nov 15 15:25:19.863: INFO: Trying to get logs from node 10.15.40.106 pod pod-projected-configmaps-6a3a4c31-faa9-4113-b7d5-cb58a28b555c container agnhost-container: <nil>
STEP: delete the pod 11/15/23 15:25:19.98
Nov 15 15:25:20.059: INFO: Waiting for pod pod-projected-configmaps-6a3a4c31-faa9-4113-b7d5-cb58a28b555c to disappear
Nov 15 15:25:20.076: INFO: Pod pod-projected-configmaps-6a3a4c31-faa9-4113-b7d5-cb58a28b555c no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Nov 15 15:25:20.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-323" for this suite. 11/15/23 15:25:20.095
------------------------------
• [SLOW TEST] [8.472 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:25:11.642
    Nov 15 15:25:11.643: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename projected 11/15/23 15:25:11.646
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:25:11.69
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:25:11.708
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-4094e2dd-3c70-480b-9dfa-03f3a30a54bb 11/15/23 15:25:11.726
    STEP: Creating a pod to test consume configMaps 11/15/23 15:25:11.749
    Nov 15 15:25:11.783: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6a3a4c31-faa9-4113-b7d5-cb58a28b555c" in namespace "projected-323" to be "Succeeded or Failed"
    Nov 15 15:25:11.802: INFO: Pod "pod-projected-configmaps-6a3a4c31-faa9-4113-b7d5-cb58a28b555c": Phase="Pending", Reason="", readiness=false. Elapsed: 18.877269ms
    Nov 15 15:25:13.825: INFO: Pod "pod-projected-configmaps-6a3a4c31-faa9-4113-b7d5-cb58a28b555c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042055354s
    Nov 15 15:25:15.830: INFO: Pod "pod-projected-configmaps-6a3a4c31-faa9-4113-b7d5-cb58a28b555c": Phase="Running", Reason="", readiness=true. Elapsed: 4.046754399s
    Nov 15 15:25:17.824: INFO: Pod "pod-projected-configmaps-6a3a4c31-faa9-4113-b7d5-cb58a28b555c": Phase="Running", Reason="", readiness=false. Elapsed: 6.041257911s
    Nov 15 15:25:19.824: INFO: Pod "pod-projected-configmaps-6a3a4c31-faa9-4113-b7d5-cb58a28b555c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.040898425s
    STEP: Saw pod success 11/15/23 15:25:19.824
    Nov 15 15:25:19.824: INFO: Pod "pod-projected-configmaps-6a3a4c31-faa9-4113-b7d5-cb58a28b555c" satisfied condition "Succeeded or Failed"
    Nov 15 15:25:19.863: INFO: Trying to get logs from node 10.15.40.106 pod pod-projected-configmaps-6a3a4c31-faa9-4113-b7d5-cb58a28b555c container agnhost-container: <nil>
    STEP: delete the pod 11/15/23 15:25:19.98
    Nov 15 15:25:20.059: INFO: Waiting for pod pod-projected-configmaps-6a3a4c31-faa9-4113-b7d5-cb58a28b555c to disappear
    Nov 15 15:25:20.076: INFO: Pod pod-projected-configmaps-6a3a4c31-faa9-4113-b7d5-cb58a28b555c no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:25:20.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-323" for this suite. 11/15/23 15:25:20.095
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:25:20.117
Nov 15 15:25:20.117: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename dns 11/15/23 15:25:20.12
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:25:20.157
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:25:20.172
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 11/15/23 15:25:20.191
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5190.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5190.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5190.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5190.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5190.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5190.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5190.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5190.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5190.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5190.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 29.189.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.189.29_udp@PTR;check="$$(dig +tcp +noall +answer +search 29.189.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.189.29_tcp@PTR;sleep 1; done
 11/15/23 15:25:20.262
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5190.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5190.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5190.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5190.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5190.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5190.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5190.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5190.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5190.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5190.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 29.189.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.189.29_udp@PTR;check="$$(dig +tcp +noall +answer +search 29.189.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.189.29_tcp@PTR;sleep 1; done
 11/15/23 15:25:20.264
STEP: creating a pod to probe DNS 11/15/23 15:25:20.267
STEP: submitting the pod to kubernetes 11/15/23 15:25:20.267
Nov 15 15:25:20.299: INFO: Waiting up to 15m0s for pod "dns-test-18ee8f06-eaef-4765-9de1-c1027e558221" in namespace "dns-5190" to be "running"
Nov 15 15:25:20.320: INFO: Pod "dns-test-18ee8f06-eaef-4765-9de1-c1027e558221": Phase="Pending", Reason="", readiness=false. Elapsed: 20.062772ms
Nov 15 15:25:22.339: INFO: Pod "dns-test-18ee8f06-eaef-4765-9de1-c1027e558221": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039557364s
Nov 15 15:25:24.340: INFO: Pod "dns-test-18ee8f06-eaef-4765-9de1-c1027e558221": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040676685s
Nov 15 15:25:26.344: INFO: Pod "dns-test-18ee8f06-eaef-4765-9de1-c1027e558221": Phase="Pending", Reason="", readiness=false. Elapsed: 6.04425612s
Nov 15 15:25:28.340: INFO: Pod "dns-test-18ee8f06-eaef-4765-9de1-c1027e558221": Phase="Running", Reason="", readiness=true. Elapsed: 8.040695067s
Nov 15 15:25:28.341: INFO: Pod "dns-test-18ee8f06-eaef-4765-9de1-c1027e558221" satisfied condition "running"
STEP: retrieving the pod 11/15/23 15:25:28.342
STEP: looking for the results for each expected name from probers 11/15/23 15:25:28.362
Nov 15 15:25:28.452: INFO: Unable to read wheezy_udp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:28.482: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:28.512: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:28.542: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:28.690: INFO: Unable to read jessie_udp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:28.721: INFO: Unable to read jessie_tcp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:28.752: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:28.783: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:28.902: INFO: Lookups using dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221 failed for: [wheezy_udp@dns-test-service.dns-5190.svc.cluster.local wheezy_tcp@dns-test-service.dns-5190.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local jessie_udp@dns-test-service.dns-5190.svc.cluster.local jessie_tcp@dns-test-service.dns-5190.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local]

Nov 15 15:25:33.937: INFO: Unable to read wheezy_udp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:33.966: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:33.996: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:34.026: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:34.184: INFO: Unable to read jessie_udp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:34.216: INFO: Unable to read jessie_tcp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:34.244: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:34.274: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:34.396: INFO: Lookups using dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221 failed for: [wheezy_udp@dns-test-service.dns-5190.svc.cluster.local wheezy_tcp@dns-test-service.dns-5190.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local jessie_udp@dns-test-service.dns-5190.svc.cluster.local jessie_tcp@dns-test-service.dns-5190.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local]

Nov 15 15:25:38.942: INFO: Unable to read wheezy_udp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:38.973: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:39.008: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:39.038: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:39.186: INFO: Unable to read jessie_udp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:39.219: INFO: Unable to read jessie_tcp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:39.249: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:39.278: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:39.399: INFO: Lookups using dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221 failed for: [wheezy_udp@dns-test-service.dns-5190.svc.cluster.local wheezy_tcp@dns-test-service.dns-5190.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local jessie_udp@dns-test-service.dns-5190.svc.cluster.local jessie_tcp@dns-test-service.dns-5190.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local]

Nov 15 15:25:43.937: INFO: Unable to read wheezy_udp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:43.995: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:44.027: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:44.057: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:44.212: INFO: Unable to read jessie_udp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:44.244: INFO: Unable to read jessie_tcp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:44.274: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:44.304: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:44.423: INFO: Lookups using dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221 failed for: [wheezy_udp@dns-test-service.dns-5190.svc.cluster.local wheezy_tcp@dns-test-service.dns-5190.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local jessie_udp@dns-test-service.dns-5190.svc.cluster.local jessie_tcp@dns-test-service.dns-5190.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local]

Nov 15 15:25:48.939: INFO: Unable to read wheezy_udp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:49.005: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:49.039: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:49.069: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:49.222: INFO: Unable to read jessie_udp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:49.253: INFO: Unable to read jessie_tcp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:49.282: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:49.312: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:49.448: INFO: Lookups using dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221 failed for: [wheezy_udp@dns-test-service.dns-5190.svc.cluster.local wheezy_tcp@dns-test-service.dns-5190.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local jessie_udp@dns-test-service.dns-5190.svc.cluster.local jessie_tcp@dns-test-service.dns-5190.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local]

Nov 15 15:25:53.934: INFO: Unable to read wheezy_udp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:53.965: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:54.000: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:54.028: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:54.195: INFO: Unable to read jessie_udp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:54.237: INFO: Unable to read jessie_tcp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:54.266: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:54.299: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
Nov 15 15:25:54.422: INFO: Lookups using dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221 failed for: [wheezy_udp@dns-test-service.dns-5190.svc.cluster.local wheezy_tcp@dns-test-service.dns-5190.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local jessie_udp@dns-test-service.dns-5190.svc.cluster.local jessie_tcp@dns-test-service.dns-5190.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local]

Nov 15 15:25:59.395: INFO: DNS probes using dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221 succeeded

STEP: deleting the pod 11/15/23 15:25:59.395
STEP: deleting the test service 11/15/23 15:25:59.465
STEP: deleting the test headless service 11/15/23 15:25:59.54
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Nov 15 15:25:59.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5190" for this suite. 11/15/23 15:25:59.602
------------------------------
• [SLOW TEST] [39.508 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:25:20.117
    Nov 15 15:25:20.117: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename dns 11/15/23 15:25:20.12
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:25:20.157
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:25:20.172
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 11/15/23 15:25:20.191
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5190.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5190.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5190.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5190.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5190.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5190.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5190.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5190.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5190.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5190.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 29.189.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.189.29_udp@PTR;check="$$(dig +tcp +noall +answer +search 29.189.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.189.29_tcp@PTR;sleep 1; done
     11/15/23 15:25:20.262
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5190.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5190.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5190.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5190.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5190.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5190.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5190.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5190.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5190.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5190.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 29.189.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.189.29_udp@PTR;check="$$(dig +tcp +noall +answer +search 29.189.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.189.29_tcp@PTR;sleep 1; done
     11/15/23 15:25:20.264
    STEP: creating a pod to probe DNS 11/15/23 15:25:20.267
    STEP: submitting the pod to kubernetes 11/15/23 15:25:20.267
    Nov 15 15:25:20.299: INFO: Waiting up to 15m0s for pod "dns-test-18ee8f06-eaef-4765-9de1-c1027e558221" in namespace "dns-5190" to be "running"
    Nov 15 15:25:20.320: INFO: Pod "dns-test-18ee8f06-eaef-4765-9de1-c1027e558221": Phase="Pending", Reason="", readiness=false. Elapsed: 20.062772ms
    Nov 15 15:25:22.339: INFO: Pod "dns-test-18ee8f06-eaef-4765-9de1-c1027e558221": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039557364s
    Nov 15 15:25:24.340: INFO: Pod "dns-test-18ee8f06-eaef-4765-9de1-c1027e558221": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040676685s
    Nov 15 15:25:26.344: INFO: Pod "dns-test-18ee8f06-eaef-4765-9de1-c1027e558221": Phase="Pending", Reason="", readiness=false. Elapsed: 6.04425612s
    Nov 15 15:25:28.340: INFO: Pod "dns-test-18ee8f06-eaef-4765-9de1-c1027e558221": Phase="Running", Reason="", readiness=true. Elapsed: 8.040695067s
    Nov 15 15:25:28.341: INFO: Pod "dns-test-18ee8f06-eaef-4765-9de1-c1027e558221" satisfied condition "running"
    STEP: retrieving the pod 11/15/23 15:25:28.342
    STEP: looking for the results for each expected name from probers 11/15/23 15:25:28.362
    Nov 15 15:25:28.452: INFO: Unable to read wheezy_udp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:28.482: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:28.512: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:28.542: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:28.690: INFO: Unable to read jessie_udp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:28.721: INFO: Unable to read jessie_tcp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:28.752: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:28.783: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:28.902: INFO: Lookups using dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221 failed for: [wheezy_udp@dns-test-service.dns-5190.svc.cluster.local wheezy_tcp@dns-test-service.dns-5190.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local jessie_udp@dns-test-service.dns-5190.svc.cluster.local jessie_tcp@dns-test-service.dns-5190.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local]

    Nov 15 15:25:33.937: INFO: Unable to read wheezy_udp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:33.966: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:33.996: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:34.026: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:34.184: INFO: Unable to read jessie_udp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:34.216: INFO: Unable to read jessie_tcp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:34.244: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:34.274: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:34.396: INFO: Lookups using dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221 failed for: [wheezy_udp@dns-test-service.dns-5190.svc.cluster.local wheezy_tcp@dns-test-service.dns-5190.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local jessie_udp@dns-test-service.dns-5190.svc.cluster.local jessie_tcp@dns-test-service.dns-5190.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local]

    Nov 15 15:25:38.942: INFO: Unable to read wheezy_udp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:38.973: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:39.008: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:39.038: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:39.186: INFO: Unable to read jessie_udp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:39.219: INFO: Unable to read jessie_tcp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:39.249: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:39.278: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:39.399: INFO: Lookups using dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221 failed for: [wheezy_udp@dns-test-service.dns-5190.svc.cluster.local wheezy_tcp@dns-test-service.dns-5190.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local jessie_udp@dns-test-service.dns-5190.svc.cluster.local jessie_tcp@dns-test-service.dns-5190.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local]

    Nov 15 15:25:43.937: INFO: Unable to read wheezy_udp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:43.995: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:44.027: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:44.057: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:44.212: INFO: Unable to read jessie_udp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:44.244: INFO: Unable to read jessie_tcp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:44.274: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:44.304: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:44.423: INFO: Lookups using dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221 failed for: [wheezy_udp@dns-test-service.dns-5190.svc.cluster.local wheezy_tcp@dns-test-service.dns-5190.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local jessie_udp@dns-test-service.dns-5190.svc.cluster.local jessie_tcp@dns-test-service.dns-5190.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local]

    Nov 15 15:25:48.939: INFO: Unable to read wheezy_udp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:49.005: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:49.039: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:49.069: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:49.222: INFO: Unable to read jessie_udp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:49.253: INFO: Unable to read jessie_tcp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:49.282: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:49.312: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:49.448: INFO: Lookups using dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221 failed for: [wheezy_udp@dns-test-service.dns-5190.svc.cluster.local wheezy_tcp@dns-test-service.dns-5190.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local jessie_udp@dns-test-service.dns-5190.svc.cluster.local jessie_tcp@dns-test-service.dns-5190.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local]

    Nov 15 15:25:53.934: INFO: Unable to read wheezy_udp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:53.965: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:54.000: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:54.028: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:54.195: INFO: Unable to read jessie_udp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:54.237: INFO: Unable to read jessie_tcp@dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:54.266: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:54.299: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local from pod dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221: the server could not find the requested resource (get pods dns-test-18ee8f06-eaef-4765-9de1-c1027e558221)
    Nov 15 15:25:54.422: INFO: Lookups using dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221 failed for: [wheezy_udp@dns-test-service.dns-5190.svc.cluster.local wheezy_tcp@dns-test-service.dns-5190.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local jessie_udp@dns-test-service.dns-5190.svc.cluster.local jessie_tcp@dns-test-service.dns-5190.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5190.svc.cluster.local]

    Nov 15 15:25:59.395: INFO: DNS probes using dns-5190/dns-test-18ee8f06-eaef-4765-9de1-c1027e558221 succeeded

    STEP: deleting the pod 11/15/23 15:25:59.395
    STEP: deleting the test service 11/15/23 15:25:59.465
    STEP: deleting the test headless service 11/15/23 15:25:59.54
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:25:59.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5190" for this suite. 11/15/23 15:25:59.602
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:25:59.644
Nov 15 15:25:59.644: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename disruption 11/15/23 15:25:59.646
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:25:59.69
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:25:59.709
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 11/15/23 15:25:59.729
STEP: Waiting for the pdb to be processed 11/15/23 15:25:59.751
STEP: First trying to evict a pod which shouldn't be evictable 11/15/23 15:26:01.809
STEP: Waiting for all pods to be running 11/15/23 15:26:01.809
Nov 15 15:26:01.828: INFO: pods: 0 < 3
Nov 15 15:26:03.852: INFO: running pods: 0 < 3
Nov 15 15:26:05.852: INFO: running pods: 1 < 3
STEP: locating a running pod 11/15/23 15:26:07.85
STEP: Updating the pdb to allow a pod to be evicted 11/15/23 15:26:07.904
STEP: Waiting for the pdb to be processed 11/15/23 15:26:07.945
STEP: Trying to evict the same pod we tried earlier which should now be evictable 11/15/23 15:26:07.967
STEP: Waiting for all pods to be running 11/15/23 15:26:07.967
STEP: Waiting for the pdb to observed all healthy pods 11/15/23 15:26:07.987
STEP: Patching the pdb to disallow a pod to be evicted 11/15/23 15:26:08.107
STEP: Waiting for the pdb to be processed 11/15/23 15:26:08.172
STEP: Waiting for all pods to be running 11/15/23 15:26:08.189
Nov 15 15:26:08.210: INFO: running pods: 2 < 3
Nov 15 15:26:10.237: INFO: running pods: 2 < 3
STEP: locating a running pod 11/15/23 15:26:12.234
STEP: Deleting the pdb to allow a pod to be evicted 11/15/23 15:26:12.316
STEP: Waiting for the pdb to be deleted 11/15/23 15:26:12.348
STEP: Trying to evict the same pod we tried earlier which should now be evictable 11/15/23 15:26:12.367
STEP: Waiting for all pods to be running 11/15/23 15:26:12.367
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Nov 15 15:26:12.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-6879" for this suite. 11/15/23 15:26:12.5
------------------------------
• [SLOW TEST] [12.878 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:25:59.644
    Nov 15 15:25:59.644: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename disruption 11/15/23 15:25:59.646
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:25:59.69
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:25:59.709
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 11/15/23 15:25:59.729
    STEP: Waiting for the pdb to be processed 11/15/23 15:25:59.751
    STEP: First trying to evict a pod which shouldn't be evictable 11/15/23 15:26:01.809
    STEP: Waiting for all pods to be running 11/15/23 15:26:01.809
    Nov 15 15:26:01.828: INFO: pods: 0 < 3
    Nov 15 15:26:03.852: INFO: running pods: 0 < 3
    Nov 15 15:26:05.852: INFO: running pods: 1 < 3
    STEP: locating a running pod 11/15/23 15:26:07.85
    STEP: Updating the pdb to allow a pod to be evicted 11/15/23 15:26:07.904
    STEP: Waiting for the pdb to be processed 11/15/23 15:26:07.945
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 11/15/23 15:26:07.967
    STEP: Waiting for all pods to be running 11/15/23 15:26:07.967
    STEP: Waiting for the pdb to observed all healthy pods 11/15/23 15:26:07.987
    STEP: Patching the pdb to disallow a pod to be evicted 11/15/23 15:26:08.107
    STEP: Waiting for the pdb to be processed 11/15/23 15:26:08.172
    STEP: Waiting for all pods to be running 11/15/23 15:26:08.189
    Nov 15 15:26:08.210: INFO: running pods: 2 < 3
    Nov 15 15:26:10.237: INFO: running pods: 2 < 3
    STEP: locating a running pod 11/15/23 15:26:12.234
    STEP: Deleting the pdb to allow a pod to be evicted 11/15/23 15:26:12.316
    STEP: Waiting for the pdb to be deleted 11/15/23 15:26:12.348
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 11/15/23 15:26:12.367
    STEP: Waiting for all pods to be running 11/15/23 15:26:12.367
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:26:12.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-6879" for this suite. 11/15/23 15:26:12.5
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:26:12.524
Nov 15 15:26:12.525: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename container-probe 11/15/23 15:26:12.528
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:26:12.581
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:26:12.599
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Nov 15 15:27:12.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-5598" for this suite. 11/15/23 15:27:12.712
------------------------------
• [SLOW TEST] [60.210 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:26:12.524
    Nov 15 15:26:12.525: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename container-probe 11/15/23 15:26:12.528
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:26:12.581
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:26:12.599
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:27:12.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-5598" for this suite. 11/15/23 15:27:12.712
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:27:12.736
Nov 15 15:27:12.736: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename svcaccounts 11/15/23 15:27:12.738
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:27:12.784
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:27:12.799
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Nov 15 15:27:12.866: INFO: created pod
Nov 15 15:27:12.866: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-204" to be "Succeeded or Failed"
Nov 15 15:27:12.892: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 25.386819ms
Nov 15 15:27:14.914: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047402249s
Nov 15 15:27:16.919: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052168006s
STEP: Saw pod success 11/15/23 15:27:16.919
Nov 15 15:27:16.919: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Nov 15 15:27:46.920: INFO: polling logs
Nov 15 15:27:47.055: INFO: Pod logs: 
I1115 15:27:14.244569       1 log.go:198] OK: Got token
I1115 15:27:14.244614       1 log.go:198] validating with in-cluster discovery
I1115 15:27:14.245066       1 log.go:198] OK: got issuer https://kubernetes.default.svc
I1115 15:27:14.245104       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-204:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1700062633, NotBefore:1700062033, IssuedAt:1700062033, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-204", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"e9f167f4-6e1f-4cf9-8702-a78832b3e5a9"}}}
I1115 15:27:14.297425       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
I1115 15:27:14.339169       1 log.go:198] OK: Validated signature on JWT
I1115 15:27:14.340158       1 log.go:198] OK: Got valid claims from token!
I1115 15:27:14.340576       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-204:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1700062633, NotBefore:1700062033, IssuedAt:1700062033, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-204", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"e9f167f4-6e1f-4cf9-8702-a78832b3e5a9"}}}

Nov 15 15:27:47.055: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Nov 15 15:27:47.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-204" for this suite. 11/15/23 15:27:47.118
------------------------------
• [SLOW TEST] [34.405 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:27:12.736
    Nov 15 15:27:12.736: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename svcaccounts 11/15/23 15:27:12.738
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:27:12.784
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:27:12.799
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Nov 15 15:27:12.866: INFO: created pod
    Nov 15 15:27:12.866: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-204" to be "Succeeded or Failed"
    Nov 15 15:27:12.892: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 25.386819ms
    Nov 15 15:27:14.914: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047402249s
    Nov 15 15:27:16.919: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052168006s
    STEP: Saw pod success 11/15/23 15:27:16.919
    Nov 15 15:27:16.919: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Nov 15 15:27:46.920: INFO: polling logs
    Nov 15 15:27:47.055: INFO: Pod logs: 
    I1115 15:27:14.244569       1 log.go:198] OK: Got token
    I1115 15:27:14.244614       1 log.go:198] validating with in-cluster discovery
    I1115 15:27:14.245066       1 log.go:198] OK: got issuer https://kubernetes.default.svc
    I1115 15:27:14.245104       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-204:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1700062633, NotBefore:1700062033, IssuedAt:1700062033, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-204", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"e9f167f4-6e1f-4cf9-8702-a78832b3e5a9"}}}
    I1115 15:27:14.297425       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
    I1115 15:27:14.339169       1 log.go:198] OK: Validated signature on JWT
    I1115 15:27:14.340158       1 log.go:198] OK: Got valid claims from token!
    I1115 15:27:14.340576       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-204:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1700062633, NotBefore:1700062033, IssuedAt:1700062033, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-204", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"e9f167f4-6e1f-4cf9-8702-a78832b3e5a9"}}}

    Nov 15 15:27:47.055: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:27:47.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-204" for this suite. 11/15/23 15:27:47.118
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:27:47.142
Nov 15 15:27:47.143: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename projected 11/15/23 15:27:47.145
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:27:47.198
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:27:47.22
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 11/15/23 15:27:47.238
Nov 15 15:27:47.271: INFO: Waiting up to 5m0s for pod "downwardapi-volume-99d5b002-17d6-4219-b772-66b2c1357e2b" in namespace "projected-4213" to be "Succeeded or Failed"
Nov 15 15:27:47.293: INFO: Pod "downwardapi-volume-99d5b002-17d6-4219-b772-66b2c1357e2b": Phase="Pending", Reason="", readiness=false. Elapsed: 22.056983ms
Nov 15 15:27:49.313: INFO: Pod "downwardapi-volume-99d5b002-17d6-4219-b772-66b2c1357e2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041582018s
Nov 15 15:27:51.314: INFO: Pod "downwardapi-volume-99d5b002-17d6-4219-b772-66b2c1357e2b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043177749s
Nov 15 15:27:53.315: INFO: Pod "downwardapi-volume-99d5b002-17d6-4219-b772-66b2c1357e2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043888869s
STEP: Saw pod success 11/15/23 15:27:53.315
Nov 15 15:27:53.316: INFO: Pod "downwardapi-volume-99d5b002-17d6-4219-b772-66b2c1357e2b" satisfied condition "Succeeded or Failed"
Nov 15 15:27:53.335: INFO: Trying to get logs from node 10.15.40.106 pod downwardapi-volume-99d5b002-17d6-4219-b772-66b2c1357e2b container client-container: <nil>
STEP: delete the pod 11/15/23 15:27:53.437
Nov 15 15:27:53.483: INFO: Waiting for pod downwardapi-volume-99d5b002-17d6-4219-b772-66b2c1357e2b to disappear
Nov 15 15:27:53.501: INFO: Pod downwardapi-volume-99d5b002-17d6-4219-b772-66b2c1357e2b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Nov 15 15:27:53.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4213" for this suite. 11/15/23 15:27:53.524
------------------------------
• [SLOW TEST] [6.404 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:27:47.142
    Nov 15 15:27:47.143: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename projected 11/15/23 15:27:47.145
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:27:47.198
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:27:47.22
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 11/15/23 15:27:47.238
    Nov 15 15:27:47.271: INFO: Waiting up to 5m0s for pod "downwardapi-volume-99d5b002-17d6-4219-b772-66b2c1357e2b" in namespace "projected-4213" to be "Succeeded or Failed"
    Nov 15 15:27:47.293: INFO: Pod "downwardapi-volume-99d5b002-17d6-4219-b772-66b2c1357e2b": Phase="Pending", Reason="", readiness=false. Elapsed: 22.056983ms
    Nov 15 15:27:49.313: INFO: Pod "downwardapi-volume-99d5b002-17d6-4219-b772-66b2c1357e2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041582018s
    Nov 15 15:27:51.314: INFO: Pod "downwardapi-volume-99d5b002-17d6-4219-b772-66b2c1357e2b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043177749s
    Nov 15 15:27:53.315: INFO: Pod "downwardapi-volume-99d5b002-17d6-4219-b772-66b2c1357e2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043888869s
    STEP: Saw pod success 11/15/23 15:27:53.315
    Nov 15 15:27:53.316: INFO: Pod "downwardapi-volume-99d5b002-17d6-4219-b772-66b2c1357e2b" satisfied condition "Succeeded or Failed"
    Nov 15 15:27:53.335: INFO: Trying to get logs from node 10.15.40.106 pod downwardapi-volume-99d5b002-17d6-4219-b772-66b2c1357e2b container client-container: <nil>
    STEP: delete the pod 11/15/23 15:27:53.437
    Nov 15 15:27:53.483: INFO: Waiting for pod downwardapi-volume-99d5b002-17d6-4219-b772-66b2c1357e2b to disappear
    Nov 15 15:27:53.501: INFO: Pod downwardapi-volume-99d5b002-17d6-4219-b772-66b2c1357e2b no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:27:53.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4213" for this suite. 11/15/23 15:27:53.524
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:27:53.553
Nov 15 15:27:53.553: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename webhook 11/15/23 15:27:53.556
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:27:53.601
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:27:53.619
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/15/23 15:27:53.751
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 15:27:54.385
STEP: Deploying the webhook pod 11/15/23 15:27:54.414
STEP: Wait for the deployment to be ready 11/15/23 15:27:54.453
Nov 15 15:27:54.482: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 15 15:27:56.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 27, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 27, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 27, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 27, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 11/15/23 15:27:58.546
STEP: Verifying the service has paired with the endpoint 11/15/23 15:27:58.592
Nov 15 15:27:59.592: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 11/15/23 15:27:59.608
STEP: create a pod that should be denied by the webhook 11/15/23 15:27:59.734
STEP: create a pod that causes the webhook to hang 11/15/23 15:27:59.823
STEP: create a configmap that should be denied by the webhook 11/15/23 15:28:09.86
STEP: create a configmap that should be admitted by the webhook 11/15/23 15:28:09.977
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 11/15/23 15:28:10.062
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 11/15/23 15:28:10.109
STEP: create a namespace that bypass the webhook 11/15/23 15:28:10.143
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 11/15/23 15:28:10.167
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 15:28:10.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2706" for this suite. 11/15/23 15:28:10.48
STEP: Destroying namespace "webhook-2706-markers" for this suite. 11/15/23 15:28:10.502
------------------------------
• [SLOW TEST] [16.973 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:27:53.553
    Nov 15 15:27:53.553: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename webhook 11/15/23 15:27:53.556
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:27:53.601
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:27:53.619
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/15/23 15:27:53.751
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 15:27:54.385
    STEP: Deploying the webhook pod 11/15/23 15:27:54.414
    STEP: Wait for the deployment to be ready 11/15/23 15:27:54.453
    Nov 15 15:27:54.482: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Nov 15 15:27:56.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 27, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 27, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 27, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 27, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 11/15/23 15:27:58.546
    STEP: Verifying the service has paired with the endpoint 11/15/23 15:27:58.592
    Nov 15 15:27:59.592: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 11/15/23 15:27:59.608
    STEP: create a pod that should be denied by the webhook 11/15/23 15:27:59.734
    STEP: create a pod that causes the webhook to hang 11/15/23 15:27:59.823
    STEP: create a configmap that should be denied by the webhook 11/15/23 15:28:09.86
    STEP: create a configmap that should be admitted by the webhook 11/15/23 15:28:09.977
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 11/15/23 15:28:10.062
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 11/15/23 15:28:10.109
    STEP: create a namespace that bypass the webhook 11/15/23 15:28:10.143
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 11/15/23 15:28:10.167
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:28:10.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2706" for this suite. 11/15/23 15:28:10.48
    STEP: Destroying namespace "webhook-2706-markers" for this suite. 11/15/23 15:28:10.502
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:28:10.527
Nov 15 15:28:10.528: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename resourcequota 11/15/23 15:28:10.529
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:28:10.569
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:28:10.586
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 11/15/23 15:28:10.604
STEP: Creating a ResourceQuota 11/15/23 15:28:15.621
STEP: Ensuring resource quota status is calculated 11/15/23 15:28:15.638
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Nov 15 15:28:17.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4777" for this suite. 11/15/23 15:28:17.677
------------------------------
• [SLOW TEST] [7.171 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:28:10.527
    Nov 15 15:28:10.528: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename resourcequota 11/15/23 15:28:10.529
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:28:10.569
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:28:10.586
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 11/15/23 15:28:10.604
    STEP: Creating a ResourceQuota 11/15/23 15:28:15.621
    STEP: Ensuring resource quota status is calculated 11/15/23 15:28:15.638
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:28:17.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4777" for this suite. 11/15/23 15:28:17.677
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:28:17.708
Nov 15 15:28:17.708: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename disruption 11/15/23 15:28:17.713
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:28:17.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:28:17.776
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 11/15/23 15:28:17.819
STEP: Waiting for all pods to be running 11/15/23 15:28:17.931
Nov 15 15:28:17.950: INFO: running pods: 0 < 3
Nov 15 15:28:19.973: INFO: running pods: 2 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Nov 15 15:28:21.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2823" for this suite. 11/15/23 15:28:22.019
------------------------------
• [4.333 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:28:17.708
    Nov 15 15:28:17.708: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename disruption 11/15/23 15:28:17.713
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:28:17.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:28:17.776
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 11/15/23 15:28:17.819
    STEP: Waiting for all pods to be running 11/15/23 15:28:17.931
    Nov 15 15:28:17.950: INFO: running pods: 0 < 3
    Nov 15 15:28:19.973: INFO: running pods: 2 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:28:21.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2823" for this suite. 11/15/23 15:28:22.019
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:28:22.044
Nov 15 15:28:22.044: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename configmap 11/15/23 15:28:22.048
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:28:22.097
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:28:22.114
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-3d05db2d-bc71-41ca-8706-e075d2e99ed4 11/15/23 15:28:22.133
STEP: Creating a pod to test consume configMaps 11/15/23 15:28:22.155
Nov 15 15:28:22.203: INFO: Waiting up to 5m0s for pod "pod-configmaps-22091210-2a31-44f3-9aa5-554cfde4da45" in namespace "configmap-750" to be "Succeeded or Failed"
Nov 15 15:28:22.223: INFO: Pod "pod-configmaps-22091210-2a31-44f3-9aa5-554cfde4da45": Phase="Pending", Reason="", readiness=false. Elapsed: 19.582096ms
Nov 15 15:28:24.244: INFO: Pod "pod-configmaps-22091210-2a31-44f3-9aa5-554cfde4da45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040890021s
Nov 15 15:28:26.244: INFO: Pod "pod-configmaps-22091210-2a31-44f3-9aa5-554cfde4da45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040423021s
STEP: Saw pod success 11/15/23 15:28:26.244
Nov 15 15:28:26.244: INFO: Pod "pod-configmaps-22091210-2a31-44f3-9aa5-554cfde4da45" satisfied condition "Succeeded or Failed"
Nov 15 15:28:26.262: INFO: Trying to get logs from node 10.15.40.115 pod pod-configmaps-22091210-2a31-44f3-9aa5-554cfde4da45 container configmap-volume-test: <nil>
STEP: delete the pod 11/15/23 15:28:26.305
Nov 15 15:28:26.362: INFO: Waiting for pod pod-configmaps-22091210-2a31-44f3-9aa5-554cfde4da45 to disappear
Nov 15 15:28:26.382: INFO: Pod pod-configmaps-22091210-2a31-44f3-9aa5-554cfde4da45 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 15 15:28:26.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-750" for this suite. 11/15/23 15:28:26.405
------------------------------
• [4.385 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:28:22.044
    Nov 15 15:28:22.044: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename configmap 11/15/23 15:28:22.048
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:28:22.097
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:28:22.114
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-3d05db2d-bc71-41ca-8706-e075d2e99ed4 11/15/23 15:28:22.133
    STEP: Creating a pod to test consume configMaps 11/15/23 15:28:22.155
    Nov 15 15:28:22.203: INFO: Waiting up to 5m0s for pod "pod-configmaps-22091210-2a31-44f3-9aa5-554cfde4da45" in namespace "configmap-750" to be "Succeeded or Failed"
    Nov 15 15:28:22.223: INFO: Pod "pod-configmaps-22091210-2a31-44f3-9aa5-554cfde4da45": Phase="Pending", Reason="", readiness=false. Elapsed: 19.582096ms
    Nov 15 15:28:24.244: INFO: Pod "pod-configmaps-22091210-2a31-44f3-9aa5-554cfde4da45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040890021s
    Nov 15 15:28:26.244: INFO: Pod "pod-configmaps-22091210-2a31-44f3-9aa5-554cfde4da45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040423021s
    STEP: Saw pod success 11/15/23 15:28:26.244
    Nov 15 15:28:26.244: INFO: Pod "pod-configmaps-22091210-2a31-44f3-9aa5-554cfde4da45" satisfied condition "Succeeded or Failed"
    Nov 15 15:28:26.262: INFO: Trying to get logs from node 10.15.40.115 pod pod-configmaps-22091210-2a31-44f3-9aa5-554cfde4da45 container configmap-volume-test: <nil>
    STEP: delete the pod 11/15/23 15:28:26.305
    Nov 15 15:28:26.362: INFO: Waiting for pod pod-configmaps-22091210-2a31-44f3-9aa5-554cfde4da45 to disappear
    Nov 15 15:28:26.382: INFO: Pod pod-configmaps-22091210-2a31-44f3-9aa5-554cfde4da45 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:28:26.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-750" for this suite. 11/15/23 15:28:26.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:28:26.432
Nov 15 15:28:26.432: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename crd-watch 11/15/23 15:28:26.434
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:28:26.479
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:28:26.495
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Nov 15 15:28:26.514: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Creating first CR  11/15/23 15:28:29.219
Nov 15 15:28:29.243: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-11-15T15:28:29Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-11-15T15:28:29Z]] name:name1 resourceVersion:19988 uid:acb24cf6-3a21-4ea3-9111-3824b8edfb6e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 11/15/23 15:28:39.246
Nov 15 15:28:39.276: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-11-15T15:28:39Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-11-15T15:28:39Z]] name:name2 resourceVersion:20013 uid:23899373-cfe9-472d-b4a7-b541ec1eb209] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 11/15/23 15:28:49.278
Nov 15 15:28:49.335: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-11-15T15:28:29Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-11-15T15:28:49Z]] name:name1 resourceVersion:20024 uid:acb24cf6-3a21-4ea3-9111-3824b8edfb6e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 11/15/23 15:28:59.335
Nov 15 15:28:59.364: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-11-15T15:28:39Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-11-15T15:28:59Z]] name:name2 resourceVersion:20039 uid:23899373-cfe9-472d-b4a7-b541ec1eb209] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 11/15/23 15:29:09.367
Nov 15 15:29:09.407: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-11-15T15:28:29Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-11-15T15:28:49Z]] name:name1 resourceVersion:20051 uid:acb24cf6-3a21-4ea3-9111-3824b8edfb6e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 11/15/23 15:29:19.412
Nov 15 15:29:19.451: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-11-15T15:28:39Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-11-15T15:28:59Z]] name:name2 resourceVersion:20064 uid:23899373-cfe9-472d-b4a7-b541ec1eb209] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 15:29:30.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-8856" for this suite. 11/15/23 15:29:30.04
------------------------------
• [SLOW TEST] [63.629 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:28:26.432
    Nov 15 15:28:26.432: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename crd-watch 11/15/23 15:28:26.434
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:28:26.479
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:28:26.495
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Nov 15 15:28:26.514: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Creating first CR  11/15/23 15:28:29.219
    Nov 15 15:28:29.243: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-11-15T15:28:29Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-11-15T15:28:29Z]] name:name1 resourceVersion:19988 uid:acb24cf6-3a21-4ea3-9111-3824b8edfb6e] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 11/15/23 15:28:39.246
    Nov 15 15:28:39.276: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-11-15T15:28:39Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-11-15T15:28:39Z]] name:name2 resourceVersion:20013 uid:23899373-cfe9-472d-b4a7-b541ec1eb209] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 11/15/23 15:28:49.278
    Nov 15 15:28:49.335: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-11-15T15:28:29Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-11-15T15:28:49Z]] name:name1 resourceVersion:20024 uid:acb24cf6-3a21-4ea3-9111-3824b8edfb6e] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 11/15/23 15:28:59.335
    Nov 15 15:28:59.364: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-11-15T15:28:39Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-11-15T15:28:59Z]] name:name2 resourceVersion:20039 uid:23899373-cfe9-472d-b4a7-b541ec1eb209] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 11/15/23 15:29:09.367
    Nov 15 15:29:09.407: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-11-15T15:28:29Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-11-15T15:28:49Z]] name:name1 resourceVersion:20051 uid:acb24cf6-3a21-4ea3-9111-3824b8edfb6e] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 11/15/23 15:29:19.412
    Nov 15 15:29:19.451: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-11-15T15:28:39Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-11-15T15:28:59Z]] name:name2 resourceVersion:20064 uid:23899373-cfe9-472d-b4a7-b541ec1eb209] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:29:30.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-8856" for this suite. 11/15/23 15:29:30.04
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:29:30.072
Nov 15 15:29:30.072: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename pods 11/15/23 15:29:30.074
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:29:30.119
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:29:30.136
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 11/15/23 15:29:30.156
STEP: submitting the pod to kubernetes 11/15/23 15:29:30.157
Nov 15 15:29:30.187: INFO: Waiting up to 5m0s for pod "pod-update-fd6d5030-ed5b-46aa-a9bf-dfe217df5bc0" in namespace "pods-790" to be "running and ready"
Nov 15 15:29:30.212: INFO: Pod "pod-update-fd6d5030-ed5b-46aa-a9bf-dfe217df5bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 25.374498ms
Nov 15 15:29:30.213: INFO: The phase of Pod pod-update-fd6d5030-ed5b-46aa-a9bf-dfe217df5bc0 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 15:29:32.234: INFO: Pod "pod-update-fd6d5030-ed5b-46aa-a9bf-dfe217df5bc0": Phase="Running", Reason="", readiness=true. Elapsed: 2.046619028s
Nov 15 15:29:32.234: INFO: The phase of Pod pod-update-fd6d5030-ed5b-46aa-a9bf-dfe217df5bc0 is Running (Ready = true)
Nov 15 15:29:32.234: INFO: Pod "pod-update-fd6d5030-ed5b-46aa-a9bf-dfe217df5bc0" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 11/15/23 15:29:32.253
STEP: updating the pod 11/15/23 15:29:32.274
Nov 15 15:29:32.831: INFO: Successfully updated pod "pod-update-fd6d5030-ed5b-46aa-a9bf-dfe217df5bc0"
Nov 15 15:29:32.832: INFO: Waiting up to 5m0s for pod "pod-update-fd6d5030-ed5b-46aa-a9bf-dfe217df5bc0" in namespace "pods-790" to be "running"
Nov 15 15:29:32.852: INFO: Pod "pod-update-fd6d5030-ed5b-46aa-a9bf-dfe217df5bc0": Phase="Running", Reason="", readiness=true. Elapsed: 20.242081ms
Nov 15 15:29:32.852: INFO: Pod "pod-update-fd6d5030-ed5b-46aa-a9bf-dfe217df5bc0" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 11/15/23 15:29:32.853
Nov 15 15:29:32.882: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Nov 15 15:29:32.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-790" for this suite. 11/15/23 15:29:32.905
------------------------------
• [2.855 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:29:30.072
    Nov 15 15:29:30.072: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename pods 11/15/23 15:29:30.074
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:29:30.119
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:29:30.136
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 11/15/23 15:29:30.156
    STEP: submitting the pod to kubernetes 11/15/23 15:29:30.157
    Nov 15 15:29:30.187: INFO: Waiting up to 5m0s for pod "pod-update-fd6d5030-ed5b-46aa-a9bf-dfe217df5bc0" in namespace "pods-790" to be "running and ready"
    Nov 15 15:29:30.212: INFO: Pod "pod-update-fd6d5030-ed5b-46aa-a9bf-dfe217df5bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 25.374498ms
    Nov 15 15:29:30.213: INFO: The phase of Pod pod-update-fd6d5030-ed5b-46aa-a9bf-dfe217df5bc0 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 15:29:32.234: INFO: Pod "pod-update-fd6d5030-ed5b-46aa-a9bf-dfe217df5bc0": Phase="Running", Reason="", readiness=true. Elapsed: 2.046619028s
    Nov 15 15:29:32.234: INFO: The phase of Pod pod-update-fd6d5030-ed5b-46aa-a9bf-dfe217df5bc0 is Running (Ready = true)
    Nov 15 15:29:32.234: INFO: Pod "pod-update-fd6d5030-ed5b-46aa-a9bf-dfe217df5bc0" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 11/15/23 15:29:32.253
    STEP: updating the pod 11/15/23 15:29:32.274
    Nov 15 15:29:32.831: INFO: Successfully updated pod "pod-update-fd6d5030-ed5b-46aa-a9bf-dfe217df5bc0"
    Nov 15 15:29:32.832: INFO: Waiting up to 5m0s for pod "pod-update-fd6d5030-ed5b-46aa-a9bf-dfe217df5bc0" in namespace "pods-790" to be "running"
    Nov 15 15:29:32.852: INFO: Pod "pod-update-fd6d5030-ed5b-46aa-a9bf-dfe217df5bc0": Phase="Running", Reason="", readiness=true. Elapsed: 20.242081ms
    Nov 15 15:29:32.852: INFO: Pod "pod-update-fd6d5030-ed5b-46aa-a9bf-dfe217df5bc0" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 11/15/23 15:29:32.853
    Nov 15 15:29:32.882: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:29:32.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-790" for this suite. 11/15/23 15:29:32.905
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:29:32.929
Nov 15 15:29:32.929: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename downward-api 11/15/23 15:29:32.931
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:29:32.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:29:32.993
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 11/15/23 15:29:33.013
Nov 15 15:29:33.045: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ca7452a5-6cae-495a-8ac5-b8e8ea35ef38" in namespace "downward-api-3244" to be "Succeeded or Failed"
Nov 15 15:29:33.064: INFO: Pod "downwardapi-volume-ca7452a5-6cae-495a-8ac5-b8e8ea35ef38": Phase="Pending", Reason="", readiness=false. Elapsed: 19.28303ms
Nov 15 15:29:35.085: INFO: Pod "downwardapi-volume-ca7452a5-6cae-495a-8ac5-b8e8ea35ef38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039776866s
Nov 15 15:29:37.086: INFO: Pod "downwardapi-volume-ca7452a5-6cae-495a-8ac5-b8e8ea35ef38": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040860799s
Nov 15 15:29:39.088: INFO: Pod "downwardapi-volume-ca7452a5-6cae-495a-8ac5-b8e8ea35ef38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042763184s
STEP: Saw pod success 11/15/23 15:29:39.088
Nov 15 15:29:39.088: INFO: Pod "downwardapi-volume-ca7452a5-6cae-495a-8ac5-b8e8ea35ef38" satisfied condition "Succeeded or Failed"
Nov 15 15:29:39.108: INFO: Trying to get logs from node 10.15.40.115 pod downwardapi-volume-ca7452a5-6cae-495a-8ac5-b8e8ea35ef38 container client-container: <nil>
STEP: delete the pod 11/15/23 15:29:39.163
Nov 15 15:29:39.208: INFO: Waiting for pod downwardapi-volume-ca7452a5-6cae-495a-8ac5-b8e8ea35ef38 to disappear
Nov 15 15:29:39.226: INFO: Pod downwardapi-volume-ca7452a5-6cae-495a-8ac5-b8e8ea35ef38 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Nov 15 15:29:39.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3244" for this suite. 11/15/23 15:29:39.251
------------------------------
• [SLOW TEST] [6.344 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:29:32.929
    Nov 15 15:29:32.929: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename downward-api 11/15/23 15:29:32.931
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:29:32.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:29:32.993
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 11/15/23 15:29:33.013
    Nov 15 15:29:33.045: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ca7452a5-6cae-495a-8ac5-b8e8ea35ef38" in namespace "downward-api-3244" to be "Succeeded or Failed"
    Nov 15 15:29:33.064: INFO: Pod "downwardapi-volume-ca7452a5-6cae-495a-8ac5-b8e8ea35ef38": Phase="Pending", Reason="", readiness=false. Elapsed: 19.28303ms
    Nov 15 15:29:35.085: INFO: Pod "downwardapi-volume-ca7452a5-6cae-495a-8ac5-b8e8ea35ef38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039776866s
    Nov 15 15:29:37.086: INFO: Pod "downwardapi-volume-ca7452a5-6cae-495a-8ac5-b8e8ea35ef38": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040860799s
    Nov 15 15:29:39.088: INFO: Pod "downwardapi-volume-ca7452a5-6cae-495a-8ac5-b8e8ea35ef38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042763184s
    STEP: Saw pod success 11/15/23 15:29:39.088
    Nov 15 15:29:39.088: INFO: Pod "downwardapi-volume-ca7452a5-6cae-495a-8ac5-b8e8ea35ef38" satisfied condition "Succeeded or Failed"
    Nov 15 15:29:39.108: INFO: Trying to get logs from node 10.15.40.115 pod downwardapi-volume-ca7452a5-6cae-495a-8ac5-b8e8ea35ef38 container client-container: <nil>
    STEP: delete the pod 11/15/23 15:29:39.163
    Nov 15 15:29:39.208: INFO: Waiting for pod downwardapi-volume-ca7452a5-6cae-495a-8ac5-b8e8ea35ef38 to disappear
    Nov 15 15:29:39.226: INFO: Pod downwardapi-volume-ca7452a5-6cae-495a-8ac5-b8e8ea35ef38 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:29:39.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3244" for this suite. 11/15/23 15:29:39.251
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:29:39.281
Nov 15 15:29:39.282: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename job 11/15/23 15:29:39.284
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:29:39.326
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:29:39.345
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 11/15/23 15:29:39.365
STEP: Ensuring active pods == parallelism 11/15/23 15:29:39.391
STEP: Orphaning one of the Job's Pods 11/15/23 15:29:43.413
Nov 15 15:29:43.979: INFO: Successfully updated pod "adopt-release-d5pdx"
STEP: Checking that the Job readopts the Pod 11/15/23 15:29:43.98
Nov 15 15:29:43.980: INFO: Waiting up to 15m0s for pod "adopt-release-d5pdx" in namespace "job-1509" to be "adopted"
Nov 15 15:29:43.999: INFO: Pod "adopt-release-d5pdx": Phase="Running", Reason="", readiness=true. Elapsed: 18.710616ms
Nov 15 15:29:46.029: INFO: Pod "adopt-release-d5pdx": Phase="Running", Reason="", readiness=true. Elapsed: 2.049039611s
Nov 15 15:29:46.029: INFO: Pod "adopt-release-d5pdx" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 11/15/23 15:29:46.029
Nov 15 15:29:46.577: INFO: Successfully updated pod "adopt-release-d5pdx"
STEP: Checking that the Job releases the Pod 11/15/23 15:29:46.577
Nov 15 15:29:46.578: INFO: Waiting up to 15m0s for pod "adopt-release-d5pdx" in namespace "job-1509" to be "released"
Nov 15 15:29:46.597: INFO: Pod "adopt-release-d5pdx": Phase="Running", Reason="", readiness=true. Elapsed: 18.535445ms
Nov 15 15:29:48.616: INFO: Pod "adopt-release-d5pdx": Phase="Running", Reason="", readiness=true. Elapsed: 2.038495849s
Nov 15 15:29:48.617: INFO: Pod "adopt-release-d5pdx" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Nov 15 15:29:48.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-1509" for this suite. 11/15/23 15:29:48.64
------------------------------
• [SLOW TEST] [9.382 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:29:39.281
    Nov 15 15:29:39.282: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename job 11/15/23 15:29:39.284
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:29:39.326
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:29:39.345
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 11/15/23 15:29:39.365
    STEP: Ensuring active pods == parallelism 11/15/23 15:29:39.391
    STEP: Orphaning one of the Job's Pods 11/15/23 15:29:43.413
    Nov 15 15:29:43.979: INFO: Successfully updated pod "adopt-release-d5pdx"
    STEP: Checking that the Job readopts the Pod 11/15/23 15:29:43.98
    Nov 15 15:29:43.980: INFO: Waiting up to 15m0s for pod "adopt-release-d5pdx" in namespace "job-1509" to be "adopted"
    Nov 15 15:29:43.999: INFO: Pod "adopt-release-d5pdx": Phase="Running", Reason="", readiness=true. Elapsed: 18.710616ms
    Nov 15 15:29:46.029: INFO: Pod "adopt-release-d5pdx": Phase="Running", Reason="", readiness=true. Elapsed: 2.049039611s
    Nov 15 15:29:46.029: INFO: Pod "adopt-release-d5pdx" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 11/15/23 15:29:46.029
    Nov 15 15:29:46.577: INFO: Successfully updated pod "adopt-release-d5pdx"
    STEP: Checking that the Job releases the Pod 11/15/23 15:29:46.577
    Nov 15 15:29:46.578: INFO: Waiting up to 15m0s for pod "adopt-release-d5pdx" in namespace "job-1509" to be "released"
    Nov 15 15:29:46.597: INFO: Pod "adopt-release-d5pdx": Phase="Running", Reason="", readiness=true. Elapsed: 18.535445ms
    Nov 15 15:29:48.616: INFO: Pod "adopt-release-d5pdx": Phase="Running", Reason="", readiness=true. Elapsed: 2.038495849s
    Nov 15 15:29:48.617: INFO: Pod "adopt-release-d5pdx" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:29:48.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-1509" for this suite. 11/15/23 15:29:48.64
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:29:48.689
Nov 15 15:29:48.689: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename kubectl 11/15/23 15:29:48.69
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:29:48.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:29:48.756
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 11/15/23 15:29:48.776
Nov 15 15:29:48.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-3658 api-versions'
Nov 15 15:29:48.967: INFO: stderr: ""
Nov 15 15:29:48.967: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nibm.com/v1alpha1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 15 15:29:48.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3658" for this suite. 11/15/23 15:29:48.998
------------------------------
• [0.333 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:29:48.689
    Nov 15 15:29:48.689: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename kubectl 11/15/23 15:29:48.69
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:29:48.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:29:48.756
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 11/15/23 15:29:48.776
    Nov 15 15:29:48.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-3658 api-versions'
    Nov 15 15:29:48.967: INFO: stderr: ""
    Nov 15 15:29:48.967: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nibm.com/v1alpha1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:29:48.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3658" for this suite. 11/15/23 15:29:48.998
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:29:49.026
Nov 15 15:29:49.027: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename statefulset 11/15/23 15:29:49.028
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:29:49.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:29:49.089
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4652 11/15/23 15:29:49.108
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Nov 15 15:29:49.168: INFO: Found 0 stateful pods, waiting for 1
Nov 15 15:29:59.196: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=false
Nov 15 15:30:09.189: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 11/15/23 15:30:09.234
W1115 15:30:09.288252      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Nov 15 15:30:09.341: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 15:30:09.341: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
Nov 15 15:30:19.367: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 15:30:19.367: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
Nov 15 15:30:29.372: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 15:30:29.372: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 11/15/23 15:30:29.418
STEP: Delete all of the StatefulSets 11/15/23 15:30:29.438
STEP: Verify that StatefulSets have been deleted 11/15/23 15:30:29.483
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Nov 15 15:30:29.502: INFO: Deleting all statefulset in ns statefulset-4652
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Nov 15 15:30:29.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4652" for this suite. 11/15/23 15:30:29.583
------------------------------
• [SLOW TEST] [40.579 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:29:49.026
    Nov 15 15:29:49.027: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename statefulset 11/15/23 15:29:49.028
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:29:49.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:29:49.089
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4652 11/15/23 15:29:49.108
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Nov 15 15:29:49.168: INFO: Found 0 stateful pods, waiting for 1
    Nov 15 15:29:59.196: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Nov 15 15:30:09.189: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 11/15/23 15:30:09.234
    W1115 15:30:09.288252      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Nov 15 15:30:09.341: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Nov 15 15:30:09.341: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
    Nov 15 15:30:19.367: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Nov 15 15:30:19.367: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
    Nov 15 15:30:29.372: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Nov 15 15:30:29.372: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 11/15/23 15:30:29.418
    STEP: Delete all of the StatefulSets 11/15/23 15:30:29.438
    STEP: Verify that StatefulSets have been deleted 11/15/23 15:30:29.483
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Nov 15 15:30:29.502: INFO: Deleting all statefulset in ns statefulset-4652
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:30:29.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4652" for this suite. 11/15/23 15:30:29.583
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:30:29.61
Nov 15 15:30:29.611: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename downward-api 11/15/23 15:30:29.614
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:30:29.66
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:30:29.676
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 11/15/23 15:30:29.696
Nov 15 15:30:29.741: INFO: Waiting up to 5m0s for pod "annotationupdateb371804f-7bbc-470c-91b0-ad554ad07a3b" in namespace "downward-api-3188" to be "running and ready"
Nov 15 15:30:29.765: INFO: Pod "annotationupdateb371804f-7bbc-470c-91b0-ad554ad07a3b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.026675ms
Nov 15 15:30:29.765: INFO: The phase of Pod annotationupdateb371804f-7bbc-470c-91b0-ad554ad07a3b is Pending, waiting for it to be Running (with Ready = true)
Nov 15 15:30:31.785: INFO: Pod "annotationupdateb371804f-7bbc-470c-91b0-ad554ad07a3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043337412s
Nov 15 15:30:31.785: INFO: The phase of Pod annotationupdateb371804f-7bbc-470c-91b0-ad554ad07a3b is Pending, waiting for it to be Running (with Ready = true)
Nov 15 15:30:33.786: INFO: Pod "annotationupdateb371804f-7bbc-470c-91b0-ad554ad07a3b": Phase="Running", Reason="", readiness=true. Elapsed: 4.044813269s
Nov 15 15:30:33.786: INFO: The phase of Pod annotationupdateb371804f-7bbc-470c-91b0-ad554ad07a3b is Running (Ready = true)
Nov 15 15:30:33.786: INFO: Pod "annotationupdateb371804f-7bbc-470c-91b0-ad554ad07a3b" satisfied condition "running and ready"
Nov 15 15:30:34.398: INFO: Successfully updated pod "annotationupdateb371804f-7bbc-470c-91b0-ad554ad07a3b"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Nov 15 15:30:36.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3188" for this suite. 11/15/23 15:30:36.506
------------------------------
• [SLOW TEST] [6.917 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:30:29.61
    Nov 15 15:30:29.611: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename downward-api 11/15/23 15:30:29.614
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:30:29.66
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:30:29.676
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 11/15/23 15:30:29.696
    Nov 15 15:30:29.741: INFO: Waiting up to 5m0s for pod "annotationupdateb371804f-7bbc-470c-91b0-ad554ad07a3b" in namespace "downward-api-3188" to be "running and ready"
    Nov 15 15:30:29.765: INFO: Pod "annotationupdateb371804f-7bbc-470c-91b0-ad554ad07a3b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.026675ms
    Nov 15 15:30:29.765: INFO: The phase of Pod annotationupdateb371804f-7bbc-470c-91b0-ad554ad07a3b is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 15:30:31.785: INFO: Pod "annotationupdateb371804f-7bbc-470c-91b0-ad554ad07a3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043337412s
    Nov 15 15:30:31.785: INFO: The phase of Pod annotationupdateb371804f-7bbc-470c-91b0-ad554ad07a3b is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 15:30:33.786: INFO: Pod "annotationupdateb371804f-7bbc-470c-91b0-ad554ad07a3b": Phase="Running", Reason="", readiness=true. Elapsed: 4.044813269s
    Nov 15 15:30:33.786: INFO: The phase of Pod annotationupdateb371804f-7bbc-470c-91b0-ad554ad07a3b is Running (Ready = true)
    Nov 15 15:30:33.786: INFO: Pod "annotationupdateb371804f-7bbc-470c-91b0-ad554ad07a3b" satisfied condition "running and ready"
    Nov 15 15:30:34.398: INFO: Successfully updated pod "annotationupdateb371804f-7bbc-470c-91b0-ad554ad07a3b"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:30:36.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3188" for this suite. 11/15/23 15:30:36.506
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:30:36.533
Nov 15 15:30:36.533: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename var-expansion 11/15/23 15:30:36.537
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:30:36.58
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:30:36.597
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Nov 15 15:30:36.651: INFO: Waiting up to 2m0s for pod "var-expansion-0d13159f-f12f-4377-9525-615cc2211b9a" in namespace "var-expansion-7940" to be "container 0 failed with reason CreateContainerConfigError"
Nov 15 15:30:36.673: INFO: Pod "var-expansion-0d13159f-f12f-4377-9525-615cc2211b9a": Phase="Pending", Reason="", readiness=false. Elapsed: 21.210673ms
Nov 15 15:30:38.693: INFO: Pod "var-expansion-0d13159f-f12f-4377-9525-615cc2211b9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041138615s
Nov 15 15:30:38.693: INFO: Pod "var-expansion-0d13159f-f12f-4377-9525-615cc2211b9a" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Nov 15 15:30:38.693: INFO: Deleting pod "var-expansion-0d13159f-f12f-4377-9525-615cc2211b9a" in namespace "var-expansion-7940"
Nov 15 15:30:38.728: INFO: Wait up to 5m0s for pod "var-expansion-0d13159f-f12f-4377-9525-615cc2211b9a" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Nov 15 15:30:42.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7940" for this suite. 11/15/23 15:30:42.806
------------------------------
• [SLOW TEST] [6.304 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:30:36.533
    Nov 15 15:30:36.533: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename var-expansion 11/15/23 15:30:36.537
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:30:36.58
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:30:36.597
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Nov 15 15:30:36.651: INFO: Waiting up to 2m0s for pod "var-expansion-0d13159f-f12f-4377-9525-615cc2211b9a" in namespace "var-expansion-7940" to be "container 0 failed with reason CreateContainerConfigError"
    Nov 15 15:30:36.673: INFO: Pod "var-expansion-0d13159f-f12f-4377-9525-615cc2211b9a": Phase="Pending", Reason="", readiness=false. Elapsed: 21.210673ms
    Nov 15 15:30:38.693: INFO: Pod "var-expansion-0d13159f-f12f-4377-9525-615cc2211b9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041138615s
    Nov 15 15:30:38.693: INFO: Pod "var-expansion-0d13159f-f12f-4377-9525-615cc2211b9a" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Nov 15 15:30:38.693: INFO: Deleting pod "var-expansion-0d13159f-f12f-4377-9525-615cc2211b9a" in namespace "var-expansion-7940"
    Nov 15 15:30:38.728: INFO: Wait up to 5m0s for pod "var-expansion-0d13159f-f12f-4377-9525-615cc2211b9a" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:30:42.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7940" for this suite. 11/15/23 15:30:42.806
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:30:42.843
Nov 15 15:30:42.843: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename sched-preemption 11/15/23 15:30:42.846
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:30:42.891
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:30:42.909
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Nov 15 15:30:42.978: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 15 15:31:43.126: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 11/15/23 15:31:43.142
Nov 15 15:31:43.221: INFO: Created pod: pod0-0-sched-preemption-low-priority
Nov 15 15:31:43.245: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Nov 15 15:31:43.304: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Nov 15 15:31:43.326: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Nov 15 15:31:43.401: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Nov 15 15:31:43.436: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 11/15/23 15:31:43.437
Nov 15 15:31:43.437: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7075" to be "running"
Nov 15 15:31:43.459: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 22.410384ms
Nov 15 15:31:45.485: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.048059684s
Nov 15 15:31:45.485: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Nov 15 15:31:45.486: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7075" to be "running"
Nov 15 15:31:45.508: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 22.396161ms
Nov 15 15:31:47.529: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.043214105s
Nov 15 15:31:47.529: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Nov 15 15:31:47.529: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7075" to be "running"
Nov 15 15:31:47.555: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 25.629234ms
Nov 15 15:31:47.555: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Nov 15 15:31:47.555: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7075" to be "running"
Nov 15 15:31:47.575: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 19.356663ms
Nov 15 15:31:47.575: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Nov 15 15:31:47.575: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-7075" to be "running"
Nov 15 15:31:47.595: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 20.336692ms
Nov 15 15:31:47.595: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Nov 15 15:31:47.595: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-7075" to be "running"
Nov 15 15:31:47.615: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 19.887428ms
Nov 15 15:31:47.615: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 11/15/23 15:31:47.616
Nov 15 15:31:47.655: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Nov 15 15:31:47.679: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 23.360493ms
Nov 15 15:31:49.701: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045307283s
Nov 15 15:31:51.704: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.048176902s
Nov 15 15:31:51.704: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 15:31:51.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-7075" for this suite. 11/15/23 15:31:52.121
------------------------------
• [SLOW TEST] [69.301 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:30:42.843
    Nov 15 15:30:42.843: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename sched-preemption 11/15/23 15:30:42.846
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:30:42.891
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:30:42.909
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Nov 15 15:30:42.978: INFO: Waiting up to 1m0s for all nodes to be ready
    Nov 15 15:31:43.126: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 11/15/23 15:31:43.142
    Nov 15 15:31:43.221: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Nov 15 15:31:43.245: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Nov 15 15:31:43.304: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Nov 15 15:31:43.326: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Nov 15 15:31:43.401: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Nov 15 15:31:43.436: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 11/15/23 15:31:43.437
    Nov 15 15:31:43.437: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7075" to be "running"
    Nov 15 15:31:43.459: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 22.410384ms
    Nov 15 15:31:45.485: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.048059684s
    Nov 15 15:31:45.485: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Nov 15 15:31:45.486: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7075" to be "running"
    Nov 15 15:31:45.508: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 22.396161ms
    Nov 15 15:31:47.529: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.043214105s
    Nov 15 15:31:47.529: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Nov 15 15:31:47.529: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7075" to be "running"
    Nov 15 15:31:47.555: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 25.629234ms
    Nov 15 15:31:47.555: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Nov 15 15:31:47.555: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7075" to be "running"
    Nov 15 15:31:47.575: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 19.356663ms
    Nov 15 15:31:47.575: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Nov 15 15:31:47.575: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-7075" to be "running"
    Nov 15 15:31:47.595: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 20.336692ms
    Nov 15 15:31:47.595: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Nov 15 15:31:47.595: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-7075" to be "running"
    Nov 15 15:31:47.615: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 19.887428ms
    Nov 15 15:31:47.615: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 11/15/23 15:31:47.616
    Nov 15 15:31:47.655: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Nov 15 15:31:47.679: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 23.360493ms
    Nov 15 15:31:49.701: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045307283s
    Nov 15 15:31:51.704: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.048176902s
    Nov 15 15:31:51.704: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:31:51.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-7075" for this suite. 11/15/23 15:31:52.121
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:31:52.15
Nov 15 15:31:52.150: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename resourcequota 11/15/23 15:31:52.152
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:31:52.197
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:31:52.213
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 11/15/23 15:32:09.265
STEP: Creating a ResourceQuota 11/15/23 15:32:14.281
STEP: Ensuring resource quota status is calculated 11/15/23 15:32:14.299
STEP: Creating a ConfigMap 11/15/23 15:32:16.315
STEP: Ensuring resource quota status captures configMap creation 11/15/23 15:32:16.353
STEP: Deleting a ConfigMap 11/15/23 15:32:18.373
STEP: Ensuring resource quota status released usage 11/15/23 15:32:18.404
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Nov 15 15:32:20.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-827" for this suite. 11/15/23 15:32:20.441
------------------------------
• [SLOW TEST] [28.313 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:31:52.15
    Nov 15 15:31:52.150: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename resourcequota 11/15/23 15:31:52.152
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:31:52.197
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:31:52.213
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 11/15/23 15:32:09.265
    STEP: Creating a ResourceQuota 11/15/23 15:32:14.281
    STEP: Ensuring resource quota status is calculated 11/15/23 15:32:14.299
    STEP: Creating a ConfigMap 11/15/23 15:32:16.315
    STEP: Ensuring resource quota status captures configMap creation 11/15/23 15:32:16.353
    STEP: Deleting a ConfigMap 11/15/23 15:32:18.373
    STEP: Ensuring resource quota status released usage 11/15/23 15:32:18.404
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:32:20.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-827" for this suite. 11/15/23 15:32:20.441
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:32:20.469
Nov 15 15:32:20.469: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename emptydir 11/15/23 15:32:20.471
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:32:20.516
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:32:20.534
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 11/15/23 15:32:20.552
Nov 15 15:32:20.584: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-a872b6da-c130-4bc2-adf8-df906324add5" in namespace "emptydir-7643" to be "running"
Nov 15 15:32:20.604: INFO: Pod "pod-sharedvolume-a872b6da-c130-4bc2-adf8-df906324add5": Phase="Pending", Reason="", readiness=false. Elapsed: 19.884511ms
Nov 15 15:32:22.626: INFO: Pod "pod-sharedvolume-a872b6da-c130-4bc2-adf8-df906324add5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041635692s
Nov 15 15:32:24.624: INFO: Pod "pod-sharedvolume-a872b6da-c130-4bc2-adf8-df906324add5": Phase="Running", Reason="", readiness=false. Elapsed: 4.040380357s
Nov 15 15:32:24.624: INFO: Pod "pod-sharedvolume-a872b6da-c130-4bc2-adf8-df906324add5" satisfied condition "running"
STEP: Reading file content from the nginx-container 11/15/23 15:32:24.624
Nov 15 15:32:24.625: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-7643 PodName:pod-sharedvolume-a872b6da-c130-4bc2-adf8-df906324add5 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 15:32:24.625: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
Nov 15 15:32:24.628: INFO: ExecWithOptions: Clientset creation
Nov 15 15:32:24.628: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/emptydir-7643/pods/pod-sharedvolume-a872b6da-c130-4bc2-adf8-df906324add5/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Nov 15 15:32:24.922: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 15 15:32:24.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7643" for this suite. 11/15/23 15:32:24.946
------------------------------
• [4.499 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:32:20.469
    Nov 15 15:32:20.469: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename emptydir 11/15/23 15:32:20.471
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:32:20.516
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:32:20.534
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 11/15/23 15:32:20.552
    Nov 15 15:32:20.584: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-a872b6da-c130-4bc2-adf8-df906324add5" in namespace "emptydir-7643" to be "running"
    Nov 15 15:32:20.604: INFO: Pod "pod-sharedvolume-a872b6da-c130-4bc2-adf8-df906324add5": Phase="Pending", Reason="", readiness=false. Elapsed: 19.884511ms
    Nov 15 15:32:22.626: INFO: Pod "pod-sharedvolume-a872b6da-c130-4bc2-adf8-df906324add5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041635692s
    Nov 15 15:32:24.624: INFO: Pod "pod-sharedvolume-a872b6da-c130-4bc2-adf8-df906324add5": Phase="Running", Reason="", readiness=false. Elapsed: 4.040380357s
    Nov 15 15:32:24.624: INFO: Pod "pod-sharedvolume-a872b6da-c130-4bc2-adf8-df906324add5" satisfied condition "running"
    STEP: Reading file content from the nginx-container 11/15/23 15:32:24.624
    Nov 15 15:32:24.625: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-7643 PodName:pod-sharedvolume-a872b6da-c130-4bc2-adf8-df906324add5 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 15:32:24.625: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    Nov 15 15:32:24.628: INFO: ExecWithOptions: Clientset creation
    Nov 15 15:32:24.628: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/emptydir-7643/pods/pod-sharedvolume-a872b6da-c130-4bc2-adf8-df906324add5/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Nov 15 15:32:24.922: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:32:24.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7643" for this suite. 11/15/23 15:32:24.946
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:32:24.969
Nov 15 15:32:24.969: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename pod-network-test 11/15/23 15:32:24.972
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:32:25.013
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:32:25.031
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-4593 11/15/23 15:32:25.05
STEP: creating a selector 11/15/23 15:32:25.05
STEP: Creating the service pods in kubernetes 11/15/23 15:32:25.051
Nov 15 15:32:25.052: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 15 15:32:25.167: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4593" to be "running and ready"
Nov 15 15:32:25.188: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 20.862321ms
Nov 15 15:32:25.188: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 15:32:27.215: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.048167284s
Nov 15 15:32:27.216: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 15:32:29.216: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.048465173s
Nov 15 15:32:29.216: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 15:32:31.209: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.041743035s
Nov 15 15:32:31.209: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 15:32:33.209: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.041569117s
Nov 15 15:32:33.209: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 15:32:35.209: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.041488755s
Nov 15 15:32:35.209: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 15:32:37.209: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.041776036s
Nov 15 15:32:37.209: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 15:32:39.211: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.043541263s
Nov 15 15:32:39.211: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 15:32:41.212: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.045050054s
Nov 15 15:32:41.212: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 15:32:43.208: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.041058323s
Nov 15 15:32:43.209: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 15:32:45.209: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.04140195s
Nov 15 15:32:45.209: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 15:32:47.208: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.041146354s
Nov 15 15:32:47.209: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Nov 15 15:32:47.209: INFO: Pod "netserver-0" satisfied condition "running and ready"
Nov 15 15:32:47.231: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4593" to be "running and ready"
Nov 15 15:32:47.251: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 19.490784ms
Nov 15 15:32:47.251: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Nov 15 15:32:47.251: INFO: Pod "netserver-1" satisfied condition "running and ready"
Nov 15 15:32:47.271: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4593" to be "running and ready"
Nov 15 15:32:47.291: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 19.791803ms
Nov 15 15:32:47.291: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Nov 15 15:32:47.291: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 11/15/23 15:32:47.312
Nov 15 15:32:47.338: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4593" to be "running"
Nov 15 15:32:47.371: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 32.664642ms
Nov 15 15:32:49.391: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.053143257s
Nov 15 15:32:49.391: INFO: Pod "test-container-pod" satisfied condition "running"
Nov 15 15:32:49.411: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Nov 15 15:32:49.411: INFO: Breadth first check of 172.30.191.86 on host 10.15.40.106...
Nov 15 15:32:49.432: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.191.88:9080/dial?request=hostname&protocol=http&host=172.30.191.86&port=8083&tries=1'] Namespace:pod-network-test-4593 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 15:32:49.432: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
Nov 15 15:32:49.434: INFO: ExecWithOptions: Clientset creation
Nov 15 15:32:49.435: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4593/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.191.88%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.191.86%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Nov 15 15:32:49.783: INFO: Waiting for responses: map[]
Nov 15 15:32:49.783: INFO: reached 172.30.191.86 after 0/1 tries
Nov 15 15:32:49.784: INFO: Breadth first check of 172.30.205.231 on host 10.15.40.114...
Nov 15 15:32:49.802: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.191.88:9080/dial?request=hostname&protocol=http&host=172.30.205.231&port=8083&tries=1'] Namespace:pod-network-test-4593 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 15:32:49.802: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
Nov 15 15:32:49.803: INFO: ExecWithOptions: Clientset creation
Nov 15 15:32:49.803: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4593/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.191.88%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.205.231%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Nov 15 15:32:50.120: INFO: Waiting for responses: map[]
Nov 15 15:32:50.121: INFO: reached 172.30.205.231 after 0/1 tries
Nov 15 15:32:50.121: INFO: Breadth first check of 172.30.164.43 on host 10.15.40.115...
Nov 15 15:32:50.142: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.191.88:9080/dial?request=hostname&protocol=http&host=172.30.164.43&port=8083&tries=1'] Namespace:pod-network-test-4593 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 15:32:50.142: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
Nov 15 15:32:50.143: INFO: ExecWithOptions: Clientset creation
Nov 15 15:32:50.143: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4593/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.191.88%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.164.43%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Nov 15 15:32:50.491: INFO: Waiting for responses: map[]
Nov 15 15:32:50.491: INFO: reached 172.30.164.43 after 0/1 tries
Nov 15 15:32:50.491: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Nov 15 15:32:50.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-4593" for this suite. 11/15/23 15:32:50.516
------------------------------
• [SLOW TEST] [25.570 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:32:24.969
    Nov 15 15:32:24.969: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename pod-network-test 11/15/23 15:32:24.972
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:32:25.013
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:32:25.031
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-4593 11/15/23 15:32:25.05
    STEP: creating a selector 11/15/23 15:32:25.05
    STEP: Creating the service pods in kubernetes 11/15/23 15:32:25.051
    Nov 15 15:32:25.052: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Nov 15 15:32:25.167: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4593" to be "running and ready"
    Nov 15 15:32:25.188: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 20.862321ms
    Nov 15 15:32:25.188: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 15:32:27.215: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.048167284s
    Nov 15 15:32:27.216: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 15:32:29.216: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.048465173s
    Nov 15 15:32:29.216: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 15:32:31.209: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.041743035s
    Nov 15 15:32:31.209: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 15:32:33.209: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.041569117s
    Nov 15 15:32:33.209: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 15:32:35.209: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.041488755s
    Nov 15 15:32:35.209: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 15:32:37.209: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.041776036s
    Nov 15 15:32:37.209: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 15:32:39.211: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.043541263s
    Nov 15 15:32:39.211: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 15:32:41.212: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.045050054s
    Nov 15 15:32:41.212: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 15:32:43.208: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.041058323s
    Nov 15 15:32:43.209: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 15:32:45.209: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.04140195s
    Nov 15 15:32:45.209: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 15:32:47.208: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.041146354s
    Nov 15 15:32:47.209: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Nov 15 15:32:47.209: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Nov 15 15:32:47.231: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4593" to be "running and ready"
    Nov 15 15:32:47.251: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 19.490784ms
    Nov 15 15:32:47.251: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Nov 15 15:32:47.251: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Nov 15 15:32:47.271: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4593" to be "running and ready"
    Nov 15 15:32:47.291: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 19.791803ms
    Nov 15 15:32:47.291: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Nov 15 15:32:47.291: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 11/15/23 15:32:47.312
    Nov 15 15:32:47.338: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4593" to be "running"
    Nov 15 15:32:47.371: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 32.664642ms
    Nov 15 15:32:49.391: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.053143257s
    Nov 15 15:32:49.391: INFO: Pod "test-container-pod" satisfied condition "running"
    Nov 15 15:32:49.411: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Nov 15 15:32:49.411: INFO: Breadth first check of 172.30.191.86 on host 10.15.40.106...
    Nov 15 15:32:49.432: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.191.88:9080/dial?request=hostname&protocol=http&host=172.30.191.86&port=8083&tries=1'] Namespace:pod-network-test-4593 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 15:32:49.432: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    Nov 15 15:32:49.434: INFO: ExecWithOptions: Clientset creation
    Nov 15 15:32:49.435: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4593/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.191.88%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.191.86%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Nov 15 15:32:49.783: INFO: Waiting for responses: map[]
    Nov 15 15:32:49.783: INFO: reached 172.30.191.86 after 0/1 tries
    Nov 15 15:32:49.784: INFO: Breadth first check of 172.30.205.231 on host 10.15.40.114...
    Nov 15 15:32:49.802: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.191.88:9080/dial?request=hostname&protocol=http&host=172.30.205.231&port=8083&tries=1'] Namespace:pod-network-test-4593 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 15:32:49.802: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    Nov 15 15:32:49.803: INFO: ExecWithOptions: Clientset creation
    Nov 15 15:32:49.803: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4593/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.191.88%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.205.231%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Nov 15 15:32:50.120: INFO: Waiting for responses: map[]
    Nov 15 15:32:50.121: INFO: reached 172.30.205.231 after 0/1 tries
    Nov 15 15:32:50.121: INFO: Breadth first check of 172.30.164.43 on host 10.15.40.115...
    Nov 15 15:32:50.142: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.191.88:9080/dial?request=hostname&protocol=http&host=172.30.164.43&port=8083&tries=1'] Namespace:pod-network-test-4593 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 15:32:50.142: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    Nov 15 15:32:50.143: INFO: ExecWithOptions: Clientset creation
    Nov 15 15:32:50.143: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4593/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.191.88%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.164.43%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Nov 15 15:32:50.491: INFO: Waiting for responses: map[]
    Nov 15 15:32:50.491: INFO: reached 172.30.164.43 after 0/1 tries
    Nov 15 15:32:50.491: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:32:50.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-4593" for this suite. 11/15/23 15:32:50.516
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:32:50.541
Nov 15 15:32:50.541: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename kubelet-test 11/15/23 15:32:50.544
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:32:50.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:32:50.608
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Nov 15 15:32:50.659: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs8f8967c5-fe44-42e4-85be-2cf6f3353be5" in namespace "kubelet-test-9953" to be "running and ready"
Nov 15 15:32:50.679: INFO: Pod "busybox-readonly-fs8f8967c5-fe44-42e4-85be-2cf6f3353be5": Phase="Pending", Reason="", readiness=false. Elapsed: 19.501993ms
Nov 15 15:32:50.679: INFO: The phase of Pod busybox-readonly-fs8f8967c5-fe44-42e4-85be-2cf6f3353be5 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 15:32:52.705: INFO: Pod "busybox-readonly-fs8f8967c5-fe44-42e4-85be-2cf6f3353be5": Phase="Running", Reason="", readiness=true. Elapsed: 2.045432427s
Nov 15 15:32:52.705: INFO: The phase of Pod busybox-readonly-fs8f8967c5-fe44-42e4-85be-2cf6f3353be5 is Running (Ready = true)
Nov 15 15:32:52.705: INFO: Pod "busybox-readonly-fs8f8967c5-fe44-42e4-85be-2cf6f3353be5" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Nov 15 15:32:52.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-9953" for this suite. 11/15/23 15:32:52.867
------------------------------
• [2.350 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:32:50.541
    Nov 15 15:32:50.541: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename kubelet-test 11/15/23 15:32:50.544
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:32:50.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:32:50.608
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Nov 15 15:32:50.659: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs8f8967c5-fe44-42e4-85be-2cf6f3353be5" in namespace "kubelet-test-9953" to be "running and ready"
    Nov 15 15:32:50.679: INFO: Pod "busybox-readonly-fs8f8967c5-fe44-42e4-85be-2cf6f3353be5": Phase="Pending", Reason="", readiness=false. Elapsed: 19.501993ms
    Nov 15 15:32:50.679: INFO: The phase of Pod busybox-readonly-fs8f8967c5-fe44-42e4-85be-2cf6f3353be5 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 15:32:52.705: INFO: Pod "busybox-readonly-fs8f8967c5-fe44-42e4-85be-2cf6f3353be5": Phase="Running", Reason="", readiness=true. Elapsed: 2.045432427s
    Nov 15 15:32:52.705: INFO: The phase of Pod busybox-readonly-fs8f8967c5-fe44-42e4-85be-2cf6f3353be5 is Running (Ready = true)
    Nov 15 15:32:52.705: INFO: Pod "busybox-readonly-fs8f8967c5-fe44-42e4-85be-2cf6f3353be5" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:32:52.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-9953" for this suite. 11/15/23 15:32:52.867
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:32:52.899
Nov 15 15:32:52.899: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename job 11/15/23 15:32:52.902
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:32:52.943
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:32:52.96
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 11/15/23 15:32:53.005
STEP: Patching the Job 11/15/23 15:32:53.047
STEP: Watching for Job to be patched 11/15/23 15:32:53.104
Nov 15 15:32:53.114: INFO: Event ADDED observed for Job e2e-766vp in namespace job-8172 with labels: map[e2e-job-label:e2e-766vp] and annotations: map[batch.kubernetes.io/job-tracking:]
Nov 15 15:32:53.115: INFO: Event MODIFIED observed for Job e2e-766vp in namespace job-8172 with labels: map[e2e-job-label:e2e-766vp] and annotations: map[batch.kubernetes.io/job-tracking:]
Nov 15 15:32:53.115: INFO: Event MODIFIED found for Job e2e-766vp in namespace job-8172 with labels: map[e2e-766vp:patched e2e-job-label:e2e-766vp] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 11/15/23 15:32:53.116
STEP: Watching for Job to be updated 11/15/23 15:32:53.162
Nov 15 15:32:53.172: INFO: Event MODIFIED found for Job e2e-766vp in namespace job-8172 with labels: map[e2e-766vp:patched e2e-job-label:e2e-766vp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Nov 15 15:32:53.172: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 11/15/23 15:32:53.172
Nov 15 15:32:53.191: INFO: Job: e2e-766vp as labels: map[e2e-766vp:patched e2e-job-label:e2e-766vp]
STEP: Waiting for job to complete 11/15/23 15:32:53.191
STEP: Delete a job collection with a labelselector 11/15/23 15:33:03.216
STEP: Watching for Job to be deleted 11/15/23 15:33:03.258
Nov 15 15:33:03.269: INFO: Event MODIFIED observed for Job e2e-766vp in namespace job-8172 with labels: map[e2e-766vp:patched e2e-job-label:e2e-766vp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Nov 15 15:33:03.269: INFO: Event MODIFIED observed for Job e2e-766vp in namespace job-8172 with labels: map[e2e-766vp:patched e2e-job-label:e2e-766vp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Nov 15 15:33:03.270: INFO: Event MODIFIED observed for Job e2e-766vp in namespace job-8172 with labels: map[e2e-766vp:patched e2e-job-label:e2e-766vp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Nov 15 15:33:03.270: INFO: Event MODIFIED observed for Job e2e-766vp in namespace job-8172 with labels: map[e2e-766vp:patched e2e-job-label:e2e-766vp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Nov 15 15:33:03.270: INFO: Event MODIFIED observed for Job e2e-766vp in namespace job-8172 with labels: map[e2e-766vp:patched e2e-job-label:e2e-766vp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Nov 15 15:33:03.271: INFO: Event MODIFIED observed for Job e2e-766vp in namespace job-8172 with labels: map[e2e-766vp:patched e2e-job-label:e2e-766vp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Nov 15 15:33:03.274: INFO: Event MODIFIED observed for Job e2e-766vp in namespace job-8172 with labels: map[e2e-766vp:patched e2e-job-label:e2e-766vp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Nov 15 15:33:03.274: INFO: Event MODIFIED observed for Job e2e-766vp in namespace job-8172 with labels: map[e2e-766vp:patched e2e-job-label:e2e-766vp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Nov 15 15:33:03.274: INFO: Event MODIFIED observed for Job e2e-766vp in namespace job-8172 with labels: map[e2e-766vp:patched e2e-job-label:e2e-766vp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Nov 15 15:33:03.275: INFO: Event DELETED found for Job e2e-766vp in namespace job-8172 with labels: map[e2e-766vp:patched e2e-job-label:e2e-766vp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 11/15/23 15:33:03.275
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Nov 15 15:33:03.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-8172" for this suite. 11/15/23 15:33:03.363
------------------------------
• [SLOW TEST] [10.487 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:32:52.899
    Nov 15 15:32:52.899: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename job 11/15/23 15:32:52.902
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:32:52.943
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:32:52.96
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 11/15/23 15:32:53.005
    STEP: Patching the Job 11/15/23 15:32:53.047
    STEP: Watching for Job to be patched 11/15/23 15:32:53.104
    Nov 15 15:32:53.114: INFO: Event ADDED observed for Job e2e-766vp in namespace job-8172 with labels: map[e2e-job-label:e2e-766vp] and annotations: map[batch.kubernetes.io/job-tracking:]
    Nov 15 15:32:53.115: INFO: Event MODIFIED observed for Job e2e-766vp in namespace job-8172 with labels: map[e2e-job-label:e2e-766vp] and annotations: map[batch.kubernetes.io/job-tracking:]
    Nov 15 15:32:53.115: INFO: Event MODIFIED found for Job e2e-766vp in namespace job-8172 with labels: map[e2e-766vp:patched e2e-job-label:e2e-766vp] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 11/15/23 15:32:53.116
    STEP: Watching for Job to be updated 11/15/23 15:32:53.162
    Nov 15 15:32:53.172: INFO: Event MODIFIED found for Job e2e-766vp in namespace job-8172 with labels: map[e2e-766vp:patched e2e-job-label:e2e-766vp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Nov 15 15:32:53.172: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 11/15/23 15:32:53.172
    Nov 15 15:32:53.191: INFO: Job: e2e-766vp as labels: map[e2e-766vp:patched e2e-job-label:e2e-766vp]
    STEP: Waiting for job to complete 11/15/23 15:32:53.191
    STEP: Delete a job collection with a labelselector 11/15/23 15:33:03.216
    STEP: Watching for Job to be deleted 11/15/23 15:33:03.258
    Nov 15 15:33:03.269: INFO: Event MODIFIED observed for Job e2e-766vp in namespace job-8172 with labels: map[e2e-766vp:patched e2e-job-label:e2e-766vp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Nov 15 15:33:03.269: INFO: Event MODIFIED observed for Job e2e-766vp in namespace job-8172 with labels: map[e2e-766vp:patched e2e-job-label:e2e-766vp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Nov 15 15:33:03.270: INFO: Event MODIFIED observed for Job e2e-766vp in namespace job-8172 with labels: map[e2e-766vp:patched e2e-job-label:e2e-766vp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Nov 15 15:33:03.270: INFO: Event MODIFIED observed for Job e2e-766vp in namespace job-8172 with labels: map[e2e-766vp:patched e2e-job-label:e2e-766vp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Nov 15 15:33:03.270: INFO: Event MODIFIED observed for Job e2e-766vp in namespace job-8172 with labels: map[e2e-766vp:patched e2e-job-label:e2e-766vp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Nov 15 15:33:03.271: INFO: Event MODIFIED observed for Job e2e-766vp in namespace job-8172 with labels: map[e2e-766vp:patched e2e-job-label:e2e-766vp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Nov 15 15:33:03.274: INFO: Event MODIFIED observed for Job e2e-766vp in namespace job-8172 with labels: map[e2e-766vp:patched e2e-job-label:e2e-766vp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Nov 15 15:33:03.274: INFO: Event MODIFIED observed for Job e2e-766vp in namespace job-8172 with labels: map[e2e-766vp:patched e2e-job-label:e2e-766vp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Nov 15 15:33:03.274: INFO: Event MODIFIED observed for Job e2e-766vp in namespace job-8172 with labels: map[e2e-766vp:patched e2e-job-label:e2e-766vp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Nov 15 15:33:03.275: INFO: Event DELETED found for Job e2e-766vp in namespace job-8172 with labels: map[e2e-766vp:patched e2e-job-label:e2e-766vp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 11/15/23 15:33:03.275
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:33:03.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-8172" for this suite. 11/15/23 15:33:03.363
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:33:03.394
Nov 15 15:33:03.395: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename kubectl 11/15/23 15:33:03.396
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:33:03.442
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:33:03.46
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 11/15/23 15:33:03.479
Nov 15 15:33:03.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-5204 cluster-info'
Nov 15 15:33:03.740: INFO: stderr: ""
Nov 15 15:33:03.740: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 15 15:33:03.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5204" for this suite. 11/15/23 15:33:03.765
------------------------------
• [0.397 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:33:03.394
    Nov 15 15:33:03.395: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename kubectl 11/15/23 15:33:03.396
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:33:03.442
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:33:03.46
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 11/15/23 15:33:03.479
    Nov 15 15:33:03.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-5204 cluster-info'
    Nov 15 15:33:03.740: INFO: stderr: ""
    Nov 15 15:33:03.740: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:33:03.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5204" for this suite. 11/15/23 15:33:03.765
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:33:03.793
Nov 15 15:33:03.793: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename kubelet-test 11/15/23 15:33:03.794
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:33:03.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:33:03.858
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Nov 15 15:33:07.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-6100" for this suite. 11/15/23 15:33:07.976
------------------------------
• [4.206 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:33:03.793
    Nov 15 15:33:03.793: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename kubelet-test 11/15/23 15:33:03.794
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:33:03.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:33:03.858
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:33:07.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-6100" for this suite. 11/15/23 15:33:07.976
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:33:08.009
Nov 15 15:33:08.009: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename pods 11/15/23 15:33:08.011
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:33:08.056
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:33:08.079
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Nov 15 15:33:08.098: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: creating the pod 11/15/23 15:33:08.1
STEP: submitting the pod to kubernetes 11/15/23 15:33:08.101
Nov 15 15:33:08.132: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-54df9f65-e97e-4210-8a56-55a5c52f3555" in namespace "pods-3877" to be "running and ready"
Nov 15 15:33:08.152: INFO: Pod "pod-exec-websocket-54df9f65-e97e-4210-8a56-55a5c52f3555": Phase="Pending", Reason="", readiness=false. Elapsed: 19.767652ms
Nov 15 15:33:08.152: INFO: The phase of Pod pod-exec-websocket-54df9f65-e97e-4210-8a56-55a5c52f3555 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 15:33:10.192: INFO: Pod "pod-exec-websocket-54df9f65-e97e-4210-8a56-55a5c52f3555": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059903418s
Nov 15 15:33:10.192: INFO: The phase of Pod pod-exec-websocket-54df9f65-e97e-4210-8a56-55a5c52f3555 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 15:33:12.175: INFO: Pod "pod-exec-websocket-54df9f65-e97e-4210-8a56-55a5c52f3555": Phase="Running", Reason="", readiness=true. Elapsed: 4.042077396s
Nov 15 15:33:12.175: INFO: The phase of Pod pod-exec-websocket-54df9f65-e97e-4210-8a56-55a5c52f3555 is Running (Ready = true)
Nov 15 15:33:12.175: INFO: Pod "pod-exec-websocket-54df9f65-e97e-4210-8a56-55a5c52f3555" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Nov 15 15:33:12.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3877" for this suite. 11/15/23 15:33:12.582
------------------------------
• [4.595 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:33:08.009
    Nov 15 15:33:08.009: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename pods 11/15/23 15:33:08.011
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:33:08.056
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:33:08.079
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Nov 15 15:33:08.098: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: creating the pod 11/15/23 15:33:08.1
    STEP: submitting the pod to kubernetes 11/15/23 15:33:08.101
    Nov 15 15:33:08.132: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-54df9f65-e97e-4210-8a56-55a5c52f3555" in namespace "pods-3877" to be "running and ready"
    Nov 15 15:33:08.152: INFO: Pod "pod-exec-websocket-54df9f65-e97e-4210-8a56-55a5c52f3555": Phase="Pending", Reason="", readiness=false. Elapsed: 19.767652ms
    Nov 15 15:33:08.152: INFO: The phase of Pod pod-exec-websocket-54df9f65-e97e-4210-8a56-55a5c52f3555 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 15:33:10.192: INFO: Pod "pod-exec-websocket-54df9f65-e97e-4210-8a56-55a5c52f3555": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059903418s
    Nov 15 15:33:10.192: INFO: The phase of Pod pod-exec-websocket-54df9f65-e97e-4210-8a56-55a5c52f3555 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 15:33:12.175: INFO: Pod "pod-exec-websocket-54df9f65-e97e-4210-8a56-55a5c52f3555": Phase="Running", Reason="", readiness=true. Elapsed: 4.042077396s
    Nov 15 15:33:12.175: INFO: The phase of Pod pod-exec-websocket-54df9f65-e97e-4210-8a56-55a5c52f3555 is Running (Ready = true)
    Nov 15 15:33:12.175: INFO: Pod "pod-exec-websocket-54df9f65-e97e-4210-8a56-55a5c52f3555" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:33:12.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3877" for this suite. 11/15/23 15:33:12.582
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:33:12.607
Nov 15 15:33:12.607: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename container-probe 11/15/23 15:33:12.609
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:33:12.688
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:33:12.737
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-30a6ba38-5022-4fb6-ba66-2530f51b8693 in namespace container-probe-1452 11/15/23 15:33:12.756
Nov 15 15:33:12.804: INFO: Waiting up to 5m0s for pod "busybox-30a6ba38-5022-4fb6-ba66-2530f51b8693" in namespace "container-probe-1452" to be "not pending"
Nov 15 15:33:12.838: INFO: Pod "busybox-30a6ba38-5022-4fb6-ba66-2530f51b8693": Phase="Pending", Reason="", readiness=false. Elapsed: 33.349649ms
Nov 15 15:33:14.863: INFO: Pod "busybox-30a6ba38-5022-4fb6-ba66-2530f51b8693": Phase="Running", Reason="", readiness=true. Elapsed: 2.058605179s
Nov 15 15:33:14.863: INFO: Pod "busybox-30a6ba38-5022-4fb6-ba66-2530f51b8693" satisfied condition "not pending"
Nov 15 15:33:14.863: INFO: Started pod busybox-30a6ba38-5022-4fb6-ba66-2530f51b8693 in namespace container-probe-1452
STEP: checking the pod's current state and verifying that restartCount is present 11/15/23 15:33:14.863
Nov 15 15:33:14.885: INFO: Initial restart count of pod busybox-30a6ba38-5022-4fb6-ba66-2530f51b8693 is 0
STEP: deleting the pod 11/15/23 15:37:15.577
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Nov 15 15:37:15.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1452" for this suite. 11/15/23 15:37:15.701
------------------------------
• [SLOW TEST] [243.117 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:33:12.607
    Nov 15 15:33:12.607: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename container-probe 11/15/23 15:33:12.609
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:33:12.688
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:33:12.737
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-30a6ba38-5022-4fb6-ba66-2530f51b8693 in namespace container-probe-1452 11/15/23 15:33:12.756
    Nov 15 15:33:12.804: INFO: Waiting up to 5m0s for pod "busybox-30a6ba38-5022-4fb6-ba66-2530f51b8693" in namespace "container-probe-1452" to be "not pending"
    Nov 15 15:33:12.838: INFO: Pod "busybox-30a6ba38-5022-4fb6-ba66-2530f51b8693": Phase="Pending", Reason="", readiness=false. Elapsed: 33.349649ms
    Nov 15 15:33:14.863: INFO: Pod "busybox-30a6ba38-5022-4fb6-ba66-2530f51b8693": Phase="Running", Reason="", readiness=true. Elapsed: 2.058605179s
    Nov 15 15:33:14.863: INFO: Pod "busybox-30a6ba38-5022-4fb6-ba66-2530f51b8693" satisfied condition "not pending"
    Nov 15 15:33:14.863: INFO: Started pod busybox-30a6ba38-5022-4fb6-ba66-2530f51b8693 in namespace container-probe-1452
    STEP: checking the pod's current state and verifying that restartCount is present 11/15/23 15:33:14.863
    Nov 15 15:33:14.885: INFO: Initial restart count of pod busybox-30a6ba38-5022-4fb6-ba66-2530f51b8693 is 0
    STEP: deleting the pod 11/15/23 15:37:15.577
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:37:15.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1452" for this suite. 11/15/23 15:37:15.701
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:37:15.727
Nov 15 15:37:15.727: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename downward-api 11/15/23 15:37:15.73
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:37:15.78
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:37:15.798
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 11/15/23 15:37:15.817
Nov 15 15:37:15.851: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9bc81626-542d-43c7-887a-a5828d28d292" in namespace "downward-api-6148" to be "Succeeded or Failed"
Nov 15 15:37:15.870: INFO: Pod "downwardapi-volume-9bc81626-542d-43c7-887a-a5828d28d292": Phase="Pending", Reason="", readiness=false. Elapsed: 18.409854ms
Nov 15 15:37:17.896: INFO: Pod "downwardapi-volume-9bc81626-542d-43c7-887a-a5828d28d292": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044347602s
Nov 15 15:37:19.893: INFO: Pod "downwardapi-volume-9bc81626-542d-43c7-887a-a5828d28d292": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041994731s
Nov 15 15:37:21.890: INFO: Pod "downwardapi-volume-9bc81626-542d-43c7-887a-a5828d28d292": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038668851s
STEP: Saw pod success 11/15/23 15:37:21.891
Nov 15 15:37:21.891: INFO: Pod "downwardapi-volume-9bc81626-542d-43c7-887a-a5828d28d292" satisfied condition "Succeeded or Failed"
Nov 15 15:37:21.921: INFO: Trying to get logs from node 10.15.40.115 pod downwardapi-volume-9bc81626-542d-43c7-887a-a5828d28d292 container client-container: <nil>
STEP: delete the pod 11/15/23 15:37:22.076
Nov 15 15:37:22.141: INFO: Waiting for pod downwardapi-volume-9bc81626-542d-43c7-887a-a5828d28d292 to disappear
Nov 15 15:37:22.163: INFO: Pod downwardapi-volume-9bc81626-542d-43c7-887a-a5828d28d292 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Nov 15 15:37:22.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6148" for this suite. 11/15/23 15:37:22.188
------------------------------
• [SLOW TEST] [6.489 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:37:15.727
    Nov 15 15:37:15.727: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename downward-api 11/15/23 15:37:15.73
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:37:15.78
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:37:15.798
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 11/15/23 15:37:15.817
    Nov 15 15:37:15.851: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9bc81626-542d-43c7-887a-a5828d28d292" in namespace "downward-api-6148" to be "Succeeded or Failed"
    Nov 15 15:37:15.870: INFO: Pod "downwardapi-volume-9bc81626-542d-43c7-887a-a5828d28d292": Phase="Pending", Reason="", readiness=false. Elapsed: 18.409854ms
    Nov 15 15:37:17.896: INFO: Pod "downwardapi-volume-9bc81626-542d-43c7-887a-a5828d28d292": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044347602s
    Nov 15 15:37:19.893: INFO: Pod "downwardapi-volume-9bc81626-542d-43c7-887a-a5828d28d292": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041994731s
    Nov 15 15:37:21.890: INFO: Pod "downwardapi-volume-9bc81626-542d-43c7-887a-a5828d28d292": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038668851s
    STEP: Saw pod success 11/15/23 15:37:21.891
    Nov 15 15:37:21.891: INFO: Pod "downwardapi-volume-9bc81626-542d-43c7-887a-a5828d28d292" satisfied condition "Succeeded or Failed"
    Nov 15 15:37:21.921: INFO: Trying to get logs from node 10.15.40.115 pod downwardapi-volume-9bc81626-542d-43c7-887a-a5828d28d292 container client-container: <nil>
    STEP: delete the pod 11/15/23 15:37:22.076
    Nov 15 15:37:22.141: INFO: Waiting for pod downwardapi-volume-9bc81626-542d-43c7-887a-a5828d28d292 to disappear
    Nov 15 15:37:22.163: INFO: Pod downwardapi-volume-9bc81626-542d-43c7-887a-a5828d28d292 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:37:22.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6148" for this suite. 11/15/23 15:37:22.188
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:37:22.222
Nov 15 15:37:22.223: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename kubectl 11/15/23 15:37:22.225
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:37:22.271
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:37:22.291
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 11/15/23 15:37:22.309
Nov 15 15:37:22.309: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-5820 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 11/15/23 15:37:22.39
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 15 15:37:22.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5820" for this suite. 11/15/23 15:37:22.449
------------------------------
• [0.250 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:37:22.222
    Nov 15 15:37:22.223: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename kubectl 11/15/23 15:37:22.225
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:37:22.271
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:37:22.291
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 11/15/23 15:37:22.309
    Nov 15 15:37:22.309: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-5820 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 11/15/23 15:37:22.39
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:37:22.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5820" for this suite. 11/15/23 15:37:22.449
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:37:22.473
Nov 15 15:37:22.473: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename gc 11/15/23 15:37:22.477
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:37:22.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:37:22.538
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 11/15/23 15:37:22.558
STEP: Wait for the Deployment to create new ReplicaSet 11/15/23 15:37:22.577
STEP: delete the deployment 11/15/23 15:37:23.145
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 11/15/23 15:37:23.17
STEP: Gathering metrics 11/15/23 15:37:23.763
W1115 15:37:23.817509      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Nov 15 15:37:23.818: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Nov 15 15:37:23.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-4562" for this suite. 11/15/23 15:37:23.845
------------------------------
• [1.396 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:37:22.473
    Nov 15 15:37:22.473: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename gc 11/15/23 15:37:22.477
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:37:22.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:37:22.538
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 11/15/23 15:37:22.558
    STEP: Wait for the Deployment to create new ReplicaSet 11/15/23 15:37:22.577
    STEP: delete the deployment 11/15/23 15:37:23.145
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 11/15/23 15:37:23.17
    STEP: Gathering metrics 11/15/23 15:37:23.763
    W1115 15:37:23.817509      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Nov 15 15:37:23.818: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:37:23.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-4562" for this suite. 11/15/23 15:37:23.845
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:37:23.888
Nov 15 15:37:23.888: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename ephemeral-containers-test 11/15/23 15:37:23.889
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:37:23.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:37:23.954
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 11/15/23 15:37:23.974
Nov 15 15:37:24.006: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-3088" to be "running and ready"
Nov 15 15:37:24.026: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 19.498462ms
Nov 15 15:37:24.026: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Nov 15 15:37:26.048: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041175912s
Nov 15 15:37:26.048: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Nov 15 15:37:28.055: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.048167773s
Nov 15 15:37:28.055: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Nov 15 15:37:28.055: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 11/15/23 15:37:28.074
Nov 15 15:37:28.123: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-3088" to be "container debugger running"
Nov 15 15:37:28.149: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 25.033758ms
Nov 15 15:37:30.177: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.053398585s
Nov 15 15:37:30.177: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 11/15/23 15:37:30.177
Nov 15 15:37:30.178: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-3088 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 15:37:30.178: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
Nov 15 15:37:30.180: INFO: ExecWithOptions: Clientset creation
Nov 15 15:37:30.180: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/ephemeral-containers-test-3088/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Nov 15 15:37:30.660: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Nov 15 15:37:30.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-3088" for this suite. 11/15/23 15:37:30.728
------------------------------
• [SLOW TEST] [6.864 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:37:23.888
    Nov 15 15:37:23.888: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename ephemeral-containers-test 11/15/23 15:37:23.889
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:37:23.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:37:23.954
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 11/15/23 15:37:23.974
    Nov 15 15:37:24.006: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-3088" to be "running and ready"
    Nov 15 15:37:24.026: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 19.498462ms
    Nov 15 15:37:24.026: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 15:37:26.048: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041175912s
    Nov 15 15:37:26.048: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 15:37:28.055: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.048167773s
    Nov 15 15:37:28.055: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Nov 15 15:37:28.055: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 11/15/23 15:37:28.074
    Nov 15 15:37:28.123: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-3088" to be "container debugger running"
    Nov 15 15:37:28.149: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 25.033758ms
    Nov 15 15:37:30.177: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.053398585s
    Nov 15 15:37:30.177: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 11/15/23 15:37:30.177
    Nov 15 15:37:30.178: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-3088 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 15:37:30.178: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    Nov 15 15:37:30.180: INFO: ExecWithOptions: Clientset creation
    Nov 15 15:37:30.180: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/ephemeral-containers-test-3088/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Nov 15 15:37:30.660: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:37:30.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-3088" for this suite. 11/15/23 15:37:30.728
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:37:30.775
Nov 15 15:37:30.775: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename kubectl 11/15/23 15:37:30.776
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:37:30.851
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:37:30.89
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 11/15/23 15:37:30.916
Nov 15 15:37:30.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4873 create -f -'
Nov 15 15:37:31.331: INFO: stderr: ""
Nov 15 15:37:31.331: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 11/15/23 15:37:31.331
Nov 15 15:37:32.354: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 15 15:37:32.355: INFO: Found 0 / 1
Nov 15 15:37:33.352: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 15 15:37:33.352: INFO: Found 0 / 1
Nov 15 15:37:34.351: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 15 15:37:34.351: INFO: Found 1 / 1
Nov 15 15:37:34.351: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 11/15/23 15:37:34.351
Nov 15 15:37:34.380: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 15 15:37:34.380: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 15 15:37:34.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4873 patch pod agnhost-primary-hvbdr -p {"metadata":{"annotations":{"x":"y"}}}'
Nov 15 15:37:34.566: INFO: stderr: ""
Nov 15 15:37:34.566: INFO: stdout: "pod/agnhost-primary-hvbdr patched\n"
STEP: checking annotations 11/15/23 15:37:34.566
Nov 15 15:37:34.586: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 15 15:37:34.586: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 15 15:37:34.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4873" for this suite. 11/15/23 15:37:34.617
------------------------------
• [3.860 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:37:30.775
    Nov 15 15:37:30.775: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename kubectl 11/15/23 15:37:30.776
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:37:30.851
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:37:30.89
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 11/15/23 15:37:30.916
    Nov 15 15:37:30.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4873 create -f -'
    Nov 15 15:37:31.331: INFO: stderr: ""
    Nov 15 15:37:31.331: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 11/15/23 15:37:31.331
    Nov 15 15:37:32.354: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 15 15:37:32.355: INFO: Found 0 / 1
    Nov 15 15:37:33.352: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 15 15:37:33.352: INFO: Found 0 / 1
    Nov 15 15:37:34.351: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 15 15:37:34.351: INFO: Found 1 / 1
    Nov 15 15:37:34.351: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 11/15/23 15:37:34.351
    Nov 15 15:37:34.380: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 15 15:37:34.380: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Nov 15 15:37:34.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4873 patch pod agnhost-primary-hvbdr -p {"metadata":{"annotations":{"x":"y"}}}'
    Nov 15 15:37:34.566: INFO: stderr: ""
    Nov 15 15:37:34.566: INFO: stdout: "pod/agnhost-primary-hvbdr patched\n"
    STEP: checking annotations 11/15/23 15:37:34.566
    Nov 15 15:37:34.586: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 15 15:37:34.586: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:37:34.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4873" for this suite. 11/15/23 15:37:34.617
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:37:34.641
Nov 15 15:37:34.641: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename replication-controller 11/15/23 15:37:34.643
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:37:34.687
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:37:34.702
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 11/15/23 15:37:34.717
Nov 15 15:37:34.744: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-9204" to be "running and ready"
Nov 15 15:37:34.762: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 17.484511ms
Nov 15 15:37:34.762: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Nov 15 15:37:36.796: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.05177191s
Nov 15 15:37:36.796: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Nov 15 15:37:36.796: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 11/15/23 15:37:36.81
STEP: Then the orphan pod is adopted 11/15/23 15:37:36.842
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Nov 15 15:37:37.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-9204" for this suite. 11/15/23 15:37:37.902
------------------------------
• [3.287 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:37:34.641
    Nov 15 15:37:34.641: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename replication-controller 11/15/23 15:37:34.643
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:37:34.687
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:37:34.702
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 11/15/23 15:37:34.717
    Nov 15 15:37:34.744: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-9204" to be "running and ready"
    Nov 15 15:37:34.762: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 17.484511ms
    Nov 15 15:37:34.762: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 15:37:36.796: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.05177191s
    Nov 15 15:37:36.796: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Nov 15 15:37:36.796: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 11/15/23 15:37:36.81
    STEP: Then the orphan pod is adopted 11/15/23 15:37:36.842
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:37:37.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-9204" for this suite. 11/15/23 15:37:37.902
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:37:37.94
Nov 15 15:37:37.940: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename kubelet-test 11/15/23 15:37:37.943
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:37:37.997
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:37:38.012
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Nov 15 15:37:38.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-9848" for this suite. 11/15/23 15:37:38.148
------------------------------
• [0.241 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:37:37.94
    Nov 15 15:37:37.940: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename kubelet-test 11/15/23 15:37:37.943
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:37:37.997
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:37:38.012
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:37:38.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-9848" for this suite. 11/15/23 15:37:38.148
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:37:38.182
Nov 15 15:37:38.183: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename webhook 11/15/23 15:37:38.186
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:37:38.24
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:37:38.252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/15/23 15:37:38.319
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 15:37:38.556
STEP: Deploying the webhook pod 11/15/23 15:37:38.586
STEP: Wait for the deployment to be ready 11/15/23 15:37:38.624
Nov 15 15:37:38.662: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 15 15:37:40.714: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 37, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 37, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 37, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 37, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 11/15/23 15:37:42.728
STEP: Verifying the service has paired with the endpoint 11/15/23 15:37:42.778
Nov 15 15:37:43.779: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 11/15/23 15:37:43.8
STEP: create a namespace for the webhook 11/15/23 15:37:43.945
STEP: create a configmap should be unconditionally rejected by the webhook 11/15/23 15:37:43.982
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 15:37:44.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8332" for this suite. 11/15/23 15:37:44.296
STEP: Destroying namespace "webhook-8332-markers" for this suite. 11/15/23 15:37:44.33
------------------------------
• [SLOW TEST] [6.175 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:37:38.182
    Nov 15 15:37:38.183: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename webhook 11/15/23 15:37:38.186
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:37:38.24
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:37:38.252
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/15/23 15:37:38.319
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 15:37:38.556
    STEP: Deploying the webhook pod 11/15/23 15:37:38.586
    STEP: Wait for the deployment to be ready 11/15/23 15:37:38.624
    Nov 15 15:37:38.662: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Nov 15 15:37:40.714: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 37, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 37, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 37, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 37, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 11/15/23 15:37:42.728
    STEP: Verifying the service has paired with the endpoint 11/15/23 15:37:42.778
    Nov 15 15:37:43.779: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 11/15/23 15:37:43.8
    STEP: create a namespace for the webhook 11/15/23 15:37:43.945
    STEP: create a configmap should be unconditionally rejected by the webhook 11/15/23 15:37:43.982
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:37:44.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8332" for this suite. 11/15/23 15:37:44.296
    STEP: Destroying namespace "webhook-8332-markers" for this suite. 11/15/23 15:37:44.33
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:37:44.361
Nov 15 15:37:44.362: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename watch 11/15/23 15:37:44.364
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:37:44.421
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:37:44.432
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 11/15/23 15:37:44.448
STEP: modifying the configmap once 11/15/23 15:37:44.464
STEP: modifying the configmap a second time 11/15/23 15:37:44.492
STEP: deleting the configmap 11/15/23 15:37:44.522
STEP: creating a watch on configmaps from the resource version returned by the first update 11/15/23 15:37:44.544
STEP: Expecting to observe notifications for all changes to the configmap after the first update 11/15/23 15:37:44.549
Nov 15 15:37:44.550: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4562  66b55715-a95e-45eb-a5ba-7c26bf32ef51 21929 0 2023-11-15 15:37:44 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-11-15 15:37:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 15 15:37:44.551: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4562  66b55715-a95e-45eb-a5ba-7c26bf32ef51 21930 0 2023-11-15 15:37:44 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-11-15 15:37:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Nov 15 15:37:44.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-4562" for this suite. 11/15/23 15:37:44.579
------------------------------
• [0.246 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:37:44.361
    Nov 15 15:37:44.362: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename watch 11/15/23 15:37:44.364
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:37:44.421
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:37:44.432
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 11/15/23 15:37:44.448
    STEP: modifying the configmap once 11/15/23 15:37:44.464
    STEP: modifying the configmap a second time 11/15/23 15:37:44.492
    STEP: deleting the configmap 11/15/23 15:37:44.522
    STEP: creating a watch on configmaps from the resource version returned by the first update 11/15/23 15:37:44.544
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 11/15/23 15:37:44.549
    Nov 15 15:37:44.550: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4562  66b55715-a95e-45eb-a5ba-7c26bf32ef51 21929 0 2023-11-15 15:37:44 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-11-15 15:37:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 15 15:37:44.551: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4562  66b55715-a95e-45eb-a5ba-7c26bf32ef51 21930 0 2023-11-15 15:37:44 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-11-15 15:37:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:37:44.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-4562" for this suite. 11/15/23 15:37:44.579
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:37:44.614
Nov 15 15:37:44.614: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename secrets 11/15/23 15:37:44.616
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:37:44.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:37:44.682
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-dbe9c890-04f0-4fe5-9da9-f5d9ede79b07 11/15/23 15:37:44.694
STEP: Creating a pod to test consume secrets 11/15/23 15:37:44.711
Nov 15 15:37:44.762: INFO: Waiting up to 5m0s for pod "pod-secrets-31dcab2d-ca46-4e40-aa86-ada91e1f429a" in namespace "secrets-801" to be "Succeeded or Failed"
Nov 15 15:37:44.776: INFO: Pod "pod-secrets-31dcab2d-ca46-4e40-aa86-ada91e1f429a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.29956ms
Nov 15 15:37:46.791: INFO: Pod "pod-secrets-31dcab2d-ca46-4e40-aa86-ada91e1f429a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029717154s
Nov 15 15:37:48.792: INFO: Pod "pod-secrets-31dcab2d-ca46-4e40-aa86-ada91e1f429a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030195675s
STEP: Saw pod success 11/15/23 15:37:48.792
Nov 15 15:37:48.792: INFO: Pod "pod-secrets-31dcab2d-ca46-4e40-aa86-ada91e1f429a" satisfied condition "Succeeded or Failed"
Nov 15 15:37:48.807: INFO: Trying to get logs from node 10.15.40.106 pod pod-secrets-31dcab2d-ca46-4e40-aa86-ada91e1f429a container secret-volume-test: <nil>
STEP: delete the pod 11/15/23 15:37:48.936
Nov 15 15:37:48.974: INFO: Waiting for pod pod-secrets-31dcab2d-ca46-4e40-aa86-ada91e1f429a to disappear
Nov 15 15:37:48.989: INFO: Pod pod-secrets-31dcab2d-ca46-4e40-aa86-ada91e1f429a no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 15 15:37:48.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-801" for this suite. 11/15/23 15:37:49.01
------------------------------
• [4.428 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:37:44.614
    Nov 15 15:37:44.614: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename secrets 11/15/23 15:37:44.616
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:37:44.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:37:44.682
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-dbe9c890-04f0-4fe5-9da9-f5d9ede79b07 11/15/23 15:37:44.694
    STEP: Creating a pod to test consume secrets 11/15/23 15:37:44.711
    Nov 15 15:37:44.762: INFO: Waiting up to 5m0s for pod "pod-secrets-31dcab2d-ca46-4e40-aa86-ada91e1f429a" in namespace "secrets-801" to be "Succeeded or Failed"
    Nov 15 15:37:44.776: INFO: Pod "pod-secrets-31dcab2d-ca46-4e40-aa86-ada91e1f429a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.29956ms
    Nov 15 15:37:46.791: INFO: Pod "pod-secrets-31dcab2d-ca46-4e40-aa86-ada91e1f429a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029717154s
    Nov 15 15:37:48.792: INFO: Pod "pod-secrets-31dcab2d-ca46-4e40-aa86-ada91e1f429a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030195675s
    STEP: Saw pod success 11/15/23 15:37:48.792
    Nov 15 15:37:48.792: INFO: Pod "pod-secrets-31dcab2d-ca46-4e40-aa86-ada91e1f429a" satisfied condition "Succeeded or Failed"
    Nov 15 15:37:48.807: INFO: Trying to get logs from node 10.15.40.106 pod pod-secrets-31dcab2d-ca46-4e40-aa86-ada91e1f429a container secret-volume-test: <nil>
    STEP: delete the pod 11/15/23 15:37:48.936
    Nov 15 15:37:48.974: INFO: Waiting for pod pod-secrets-31dcab2d-ca46-4e40-aa86-ada91e1f429a to disappear
    Nov 15 15:37:48.989: INFO: Pod pod-secrets-31dcab2d-ca46-4e40-aa86-ada91e1f429a no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:37:48.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-801" for this suite. 11/15/23 15:37:49.01
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:37:49.045
Nov 15 15:37:49.045: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename kubectl 11/15/23 15:37:49.046
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:37:49.096
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:37:49.108
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 11/15/23 15:37:49.12
Nov 15 15:37:49.121: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Nov 15 15:37:49.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6721 create -f -'
Nov 15 15:37:49.529: INFO: stderr: ""
Nov 15 15:37:49.529: INFO: stdout: "service/agnhost-replica created\n"
Nov 15 15:37:49.529: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Nov 15 15:37:49.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6721 create -f -'
Nov 15 15:37:49.891: INFO: stderr: ""
Nov 15 15:37:49.891: INFO: stdout: "service/agnhost-primary created\n"
Nov 15 15:37:49.891: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Nov 15 15:37:49.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6721 create -f -'
Nov 15 15:37:50.254: INFO: stderr: ""
Nov 15 15:37:50.254: INFO: stdout: "service/frontend created\n"
Nov 15 15:37:50.254: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Nov 15 15:37:50.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6721 create -f -'
Nov 15 15:37:50.633: INFO: stderr: ""
Nov 15 15:37:50.633: INFO: stdout: "deployment.apps/frontend created\n"
Nov 15 15:37:50.633: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Nov 15 15:37:50.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6721 create -f -'
Nov 15 15:37:50.994: INFO: stderr: ""
Nov 15 15:37:50.994: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Nov 15 15:37:50.995: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Nov 15 15:37:50.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6721 create -f -'
Nov 15 15:37:51.419: INFO: stderr: ""
Nov 15 15:37:51.419: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 11/15/23 15:37:51.419
Nov 15 15:37:51.420: INFO: Waiting for all frontend pods to be Running.
Nov 15 15:37:56.474: INFO: Waiting for frontend to serve content.
Nov 15 15:37:57.591: INFO: Failed to get response from guestbook. err: the server responded with the status code 417 but did not return more information (get services frontend), response: 
Nov 15 15:38:02.663: INFO: Trying to add a new entry to the guestbook.
Nov 15 15:38:02.791: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 11/15/23 15:38:02.837
Nov 15 15:38:02.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6721 delete --grace-period=0 --force -f -'
Nov 15 15:38:03.050: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 15 15:38:03.051: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 11/15/23 15:38:03.051
Nov 15 15:38:03.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6721 delete --grace-period=0 --force -f -'
Nov 15 15:38:03.307: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 15 15:38:03.307: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 11/15/23 15:38:03.307
Nov 15 15:38:03.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6721 delete --grace-period=0 --force -f -'
Nov 15 15:38:03.540: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 15 15:38:03.540: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 11/15/23 15:38:03.54
Nov 15 15:38:03.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6721 delete --grace-period=0 --force -f -'
Nov 15 15:38:03.722: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 15 15:38:03.722: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 11/15/23 15:38:03.723
Nov 15 15:38:03.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6721 delete --grace-period=0 --force -f -'
Nov 15 15:38:03.934: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 15 15:38:03.935: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 11/15/23 15:38:03.935
Nov 15 15:38:03.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6721 delete --grace-period=0 --force -f -'
Nov 15 15:38:04.090: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 15 15:38:04.090: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 15 15:38:04.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6721" for this suite. 11/15/23 15:38:04.109
------------------------------
• [SLOW TEST] [15.089 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:37:49.045
    Nov 15 15:37:49.045: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename kubectl 11/15/23 15:37:49.046
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:37:49.096
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:37:49.108
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 11/15/23 15:37:49.12
    Nov 15 15:37:49.121: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Nov 15 15:37:49.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6721 create -f -'
    Nov 15 15:37:49.529: INFO: stderr: ""
    Nov 15 15:37:49.529: INFO: stdout: "service/agnhost-replica created\n"
    Nov 15 15:37:49.529: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Nov 15 15:37:49.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6721 create -f -'
    Nov 15 15:37:49.891: INFO: stderr: ""
    Nov 15 15:37:49.891: INFO: stdout: "service/agnhost-primary created\n"
    Nov 15 15:37:49.891: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Nov 15 15:37:49.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6721 create -f -'
    Nov 15 15:37:50.254: INFO: stderr: ""
    Nov 15 15:37:50.254: INFO: stdout: "service/frontend created\n"
    Nov 15 15:37:50.254: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Nov 15 15:37:50.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6721 create -f -'
    Nov 15 15:37:50.633: INFO: stderr: ""
    Nov 15 15:37:50.633: INFO: stdout: "deployment.apps/frontend created\n"
    Nov 15 15:37:50.633: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Nov 15 15:37:50.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6721 create -f -'
    Nov 15 15:37:50.994: INFO: stderr: ""
    Nov 15 15:37:50.994: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Nov 15 15:37:50.995: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Nov 15 15:37:50.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6721 create -f -'
    Nov 15 15:37:51.419: INFO: stderr: ""
    Nov 15 15:37:51.419: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 11/15/23 15:37:51.419
    Nov 15 15:37:51.420: INFO: Waiting for all frontend pods to be Running.
    Nov 15 15:37:56.474: INFO: Waiting for frontend to serve content.
    Nov 15 15:37:57.591: INFO: Failed to get response from guestbook. err: the server responded with the status code 417 but did not return more information (get services frontend), response: 
    Nov 15 15:38:02.663: INFO: Trying to add a new entry to the guestbook.
    Nov 15 15:38:02.791: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 11/15/23 15:38:02.837
    Nov 15 15:38:02.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6721 delete --grace-period=0 --force -f -'
    Nov 15 15:38:03.050: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Nov 15 15:38:03.051: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 11/15/23 15:38:03.051
    Nov 15 15:38:03.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6721 delete --grace-period=0 --force -f -'
    Nov 15 15:38:03.307: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Nov 15 15:38:03.307: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 11/15/23 15:38:03.307
    Nov 15 15:38:03.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6721 delete --grace-period=0 --force -f -'
    Nov 15 15:38:03.540: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Nov 15 15:38:03.540: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 11/15/23 15:38:03.54
    Nov 15 15:38:03.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6721 delete --grace-period=0 --force -f -'
    Nov 15 15:38:03.722: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Nov 15 15:38:03.722: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 11/15/23 15:38:03.723
    Nov 15 15:38:03.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6721 delete --grace-period=0 --force -f -'
    Nov 15 15:38:03.934: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Nov 15 15:38:03.935: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 11/15/23 15:38:03.935
    Nov 15 15:38:03.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6721 delete --grace-period=0 --force -f -'
    Nov 15 15:38:04.090: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Nov 15 15:38:04.090: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:38:04.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6721" for this suite. 11/15/23 15:38:04.109
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:38:04.152
Nov 15 15:38:04.152: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename events 11/15/23 15:38:04.155
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:38:04.218
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:38:04.229
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 11/15/23 15:38:04.243
Nov 15 15:38:04.264: INFO: created test-event-1
Nov 15 15:38:04.280: INFO: created test-event-2
Nov 15 15:38:04.296: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 11/15/23 15:38:04.296
STEP: delete collection of events 11/15/23 15:38:04.311
Nov 15 15:38:04.312: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 11/15/23 15:38:04.395
Nov 15 15:38:04.395: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Nov 15 15:38:04.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-9976" for this suite. 11/15/23 15:38:04.427
------------------------------
• [0.305 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:38:04.152
    Nov 15 15:38:04.152: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename events 11/15/23 15:38:04.155
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:38:04.218
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:38:04.229
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 11/15/23 15:38:04.243
    Nov 15 15:38:04.264: INFO: created test-event-1
    Nov 15 15:38:04.280: INFO: created test-event-2
    Nov 15 15:38:04.296: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 11/15/23 15:38:04.296
    STEP: delete collection of events 11/15/23 15:38:04.311
    Nov 15 15:38:04.312: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 11/15/23 15:38:04.395
    Nov 15 15:38:04.395: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:38:04.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-9976" for this suite. 11/15/23 15:38:04.427
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:38:04.471
Nov 15 15:38:04.472: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename dns 11/15/23 15:38:04.474
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:38:04.526
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:38:04.537
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 11/15/23 15:38:04.548
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6098.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6098.svc.cluster.local; sleep 1; done
 11/15/23 15:38:04.57
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6098.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6098.svc.cluster.local; sleep 1; done
 11/15/23 15:38:04.571
STEP: creating a pod to probe DNS 11/15/23 15:38:04.571
STEP: submitting the pod to kubernetes 11/15/23 15:38:04.571
Nov 15 15:38:04.632: INFO: Waiting up to 15m0s for pod "dns-test-1ae82f0a-a1d3-4c76-8ae8-c6def4d8c503" in namespace "dns-6098" to be "running"
Nov 15 15:38:04.646: INFO: Pod "dns-test-1ae82f0a-a1d3-4c76-8ae8-c6def4d8c503": Phase="Pending", Reason="", readiness=false. Elapsed: 14.431932ms
Nov 15 15:38:06.661: INFO: Pod "dns-test-1ae82f0a-a1d3-4c76-8ae8-c6def4d8c503": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029115891s
Nov 15 15:38:08.665: INFO: Pod "dns-test-1ae82f0a-a1d3-4c76-8ae8-c6def4d8c503": Phase="Running", Reason="", readiness=true. Elapsed: 4.033531224s
Nov 15 15:38:08.665: INFO: Pod "dns-test-1ae82f0a-a1d3-4c76-8ae8-c6def4d8c503" satisfied condition "running"
STEP: retrieving the pod 11/15/23 15:38:08.666
STEP: looking for the results for each expected name from probers 11/15/23 15:38:08.681
Nov 15 15:38:08.774: INFO: DNS probes using dns-test-1ae82f0a-a1d3-4c76-8ae8-c6def4d8c503 succeeded

STEP: deleting the pod 11/15/23 15:38:08.774
STEP: changing the externalName to bar.example.com 11/15/23 15:38:08.811
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6098.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6098.svc.cluster.local; sleep 1; done
 11/15/23 15:38:08.848
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6098.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6098.svc.cluster.local; sleep 1; done
 11/15/23 15:38:08.849
STEP: creating a second pod to probe DNS 11/15/23 15:38:08.85
STEP: submitting the pod to kubernetes 11/15/23 15:38:08.85
Nov 15 15:38:08.872: INFO: Waiting up to 15m0s for pod "dns-test-3377682a-cf78-4060-bde8-b865702ea2a8" in namespace "dns-6098" to be "running"
Nov 15 15:38:08.887: INFO: Pod "dns-test-3377682a-cf78-4060-bde8-b865702ea2a8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.507221ms
Nov 15 15:38:10.902: INFO: Pod "dns-test-3377682a-cf78-4060-bde8-b865702ea2a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029637083s
Nov 15 15:38:12.905: INFO: Pod "dns-test-3377682a-cf78-4060-bde8-b865702ea2a8": Phase="Running", Reason="", readiness=true. Elapsed: 4.03255475s
Nov 15 15:38:12.905: INFO: Pod "dns-test-3377682a-cf78-4060-bde8-b865702ea2a8" satisfied condition "running"
STEP: retrieving the pod 11/15/23 15:38:12.905
STEP: looking for the results for each expected name from probers 11/15/23 15:38:12.92
Nov 15 15:38:12.977: INFO: File wheezy_udp@dns-test-service-3.dns-6098.svc.cluster.local from pod  dns-6098/dns-test-3377682a-cf78-4060-bde8-b865702ea2a8 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 15 15:38:13.000: INFO: Lookups using dns-6098/dns-test-3377682a-cf78-4060-bde8-b865702ea2a8 failed for: [wheezy_udp@dns-test-service-3.dns-6098.svc.cluster.local]

Nov 15 15:38:18.025: INFO: File wheezy_udp@dns-test-service-3.dns-6098.svc.cluster.local from pod  dns-6098/dns-test-3377682a-cf78-4060-bde8-b865702ea2a8 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 15 15:38:18.046: INFO: File jessie_udp@dns-test-service-3.dns-6098.svc.cluster.local from pod  dns-6098/dns-test-3377682a-cf78-4060-bde8-b865702ea2a8 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 15 15:38:18.046: INFO: Lookups using dns-6098/dns-test-3377682a-cf78-4060-bde8-b865702ea2a8 failed for: [wheezy_udp@dns-test-service-3.dns-6098.svc.cluster.local jessie_udp@dns-test-service-3.dns-6098.svc.cluster.local]

Nov 15 15:38:23.041: INFO: File wheezy_udp@dns-test-service-3.dns-6098.svc.cluster.local from pod  dns-6098/dns-test-3377682a-cf78-4060-bde8-b865702ea2a8 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 15 15:38:23.063: INFO: File jessie_udp@dns-test-service-3.dns-6098.svc.cluster.local from pod  dns-6098/dns-test-3377682a-cf78-4060-bde8-b865702ea2a8 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 15 15:38:23.063: INFO: Lookups using dns-6098/dns-test-3377682a-cf78-4060-bde8-b865702ea2a8 failed for: [wheezy_udp@dns-test-service-3.dns-6098.svc.cluster.local jessie_udp@dns-test-service-3.dns-6098.svc.cluster.local]

Nov 15 15:38:28.021: INFO: File wheezy_udp@dns-test-service-3.dns-6098.svc.cluster.local from pod  dns-6098/dns-test-3377682a-cf78-4060-bde8-b865702ea2a8 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 15 15:38:28.042: INFO: File jessie_udp@dns-test-service-3.dns-6098.svc.cluster.local from pod  dns-6098/dns-test-3377682a-cf78-4060-bde8-b865702ea2a8 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 15 15:38:28.042: INFO: Lookups using dns-6098/dns-test-3377682a-cf78-4060-bde8-b865702ea2a8 failed for: [wheezy_udp@dns-test-service-3.dns-6098.svc.cluster.local jessie_udp@dns-test-service-3.dns-6098.svc.cluster.local]

Nov 15 15:38:33.027: INFO: File wheezy_udp@dns-test-service-3.dns-6098.svc.cluster.local from pod  dns-6098/dns-test-3377682a-cf78-4060-bde8-b865702ea2a8 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 15 15:38:33.056: INFO: File jessie_udp@dns-test-service-3.dns-6098.svc.cluster.local from pod  dns-6098/dns-test-3377682a-cf78-4060-bde8-b865702ea2a8 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 15 15:38:33.056: INFO: Lookups using dns-6098/dns-test-3377682a-cf78-4060-bde8-b865702ea2a8 failed for: [wheezy_udp@dns-test-service-3.dns-6098.svc.cluster.local jessie_udp@dns-test-service-3.dns-6098.svc.cluster.local]

Nov 15 15:38:38.053: INFO: DNS probes using dns-test-3377682a-cf78-4060-bde8-b865702ea2a8 succeeded

STEP: deleting the pod 11/15/23 15:38:38.053
STEP: changing the service to type=ClusterIP 11/15/23 15:38:38.092
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6098.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6098.svc.cluster.local; sleep 1; done
 11/15/23 15:38:38.174
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6098.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6098.svc.cluster.local; sleep 1; done
 11/15/23 15:38:38.176
STEP: creating a third pod to probe DNS 11/15/23 15:38:38.176
STEP: submitting the pod to kubernetes 11/15/23 15:38:38.196
Nov 15 15:38:38.229: INFO: Waiting up to 15m0s for pod "dns-test-128b6024-b664-4f87-a203-794bf0278925" in namespace "dns-6098" to be "running"
Nov 15 15:38:38.243: INFO: Pod "dns-test-128b6024-b664-4f87-a203-794bf0278925": Phase="Pending", Reason="", readiness=false. Elapsed: 14.159371ms
Nov 15 15:38:40.258: INFO: Pod "dns-test-128b6024-b664-4f87-a203-794bf0278925": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028721585s
Nov 15 15:38:42.260: INFO: Pod "dns-test-128b6024-b664-4f87-a203-794bf0278925": Phase="Running", Reason="", readiness=true. Elapsed: 4.030676341s
Nov 15 15:38:42.260: INFO: Pod "dns-test-128b6024-b664-4f87-a203-794bf0278925" satisfied condition "running"
STEP: retrieving the pod 11/15/23 15:38:42.261
STEP: looking for the results for each expected name from probers 11/15/23 15:38:42.276
Nov 15 15:38:42.402: INFO: DNS probes using dns-test-128b6024-b664-4f87-a203-794bf0278925 succeeded

STEP: deleting the pod 11/15/23 15:38:42.402
STEP: deleting the test externalName service 11/15/23 15:38:42.443
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Nov 15 15:38:42.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6098" for this suite. 11/15/23 15:38:42.53
------------------------------
• [SLOW TEST] [38.084 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:38:04.471
    Nov 15 15:38:04.472: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename dns 11/15/23 15:38:04.474
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:38:04.526
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:38:04.537
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 11/15/23 15:38:04.548
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6098.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6098.svc.cluster.local; sleep 1; done
     11/15/23 15:38:04.57
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6098.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6098.svc.cluster.local; sleep 1; done
     11/15/23 15:38:04.571
    STEP: creating a pod to probe DNS 11/15/23 15:38:04.571
    STEP: submitting the pod to kubernetes 11/15/23 15:38:04.571
    Nov 15 15:38:04.632: INFO: Waiting up to 15m0s for pod "dns-test-1ae82f0a-a1d3-4c76-8ae8-c6def4d8c503" in namespace "dns-6098" to be "running"
    Nov 15 15:38:04.646: INFO: Pod "dns-test-1ae82f0a-a1d3-4c76-8ae8-c6def4d8c503": Phase="Pending", Reason="", readiness=false. Elapsed: 14.431932ms
    Nov 15 15:38:06.661: INFO: Pod "dns-test-1ae82f0a-a1d3-4c76-8ae8-c6def4d8c503": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029115891s
    Nov 15 15:38:08.665: INFO: Pod "dns-test-1ae82f0a-a1d3-4c76-8ae8-c6def4d8c503": Phase="Running", Reason="", readiness=true. Elapsed: 4.033531224s
    Nov 15 15:38:08.665: INFO: Pod "dns-test-1ae82f0a-a1d3-4c76-8ae8-c6def4d8c503" satisfied condition "running"
    STEP: retrieving the pod 11/15/23 15:38:08.666
    STEP: looking for the results for each expected name from probers 11/15/23 15:38:08.681
    Nov 15 15:38:08.774: INFO: DNS probes using dns-test-1ae82f0a-a1d3-4c76-8ae8-c6def4d8c503 succeeded

    STEP: deleting the pod 11/15/23 15:38:08.774
    STEP: changing the externalName to bar.example.com 11/15/23 15:38:08.811
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6098.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6098.svc.cluster.local; sleep 1; done
     11/15/23 15:38:08.848
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6098.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6098.svc.cluster.local; sleep 1; done
     11/15/23 15:38:08.849
    STEP: creating a second pod to probe DNS 11/15/23 15:38:08.85
    STEP: submitting the pod to kubernetes 11/15/23 15:38:08.85
    Nov 15 15:38:08.872: INFO: Waiting up to 15m0s for pod "dns-test-3377682a-cf78-4060-bde8-b865702ea2a8" in namespace "dns-6098" to be "running"
    Nov 15 15:38:08.887: INFO: Pod "dns-test-3377682a-cf78-4060-bde8-b865702ea2a8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.507221ms
    Nov 15 15:38:10.902: INFO: Pod "dns-test-3377682a-cf78-4060-bde8-b865702ea2a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029637083s
    Nov 15 15:38:12.905: INFO: Pod "dns-test-3377682a-cf78-4060-bde8-b865702ea2a8": Phase="Running", Reason="", readiness=true. Elapsed: 4.03255475s
    Nov 15 15:38:12.905: INFO: Pod "dns-test-3377682a-cf78-4060-bde8-b865702ea2a8" satisfied condition "running"
    STEP: retrieving the pod 11/15/23 15:38:12.905
    STEP: looking for the results for each expected name from probers 11/15/23 15:38:12.92
    Nov 15 15:38:12.977: INFO: File wheezy_udp@dns-test-service-3.dns-6098.svc.cluster.local from pod  dns-6098/dns-test-3377682a-cf78-4060-bde8-b865702ea2a8 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Nov 15 15:38:13.000: INFO: Lookups using dns-6098/dns-test-3377682a-cf78-4060-bde8-b865702ea2a8 failed for: [wheezy_udp@dns-test-service-3.dns-6098.svc.cluster.local]

    Nov 15 15:38:18.025: INFO: File wheezy_udp@dns-test-service-3.dns-6098.svc.cluster.local from pod  dns-6098/dns-test-3377682a-cf78-4060-bde8-b865702ea2a8 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Nov 15 15:38:18.046: INFO: File jessie_udp@dns-test-service-3.dns-6098.svc.cluster.local from pod  dns-6098/dns-test-3377682a-cf78-4060-bde8-b865702ea2a8 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Nov 15 15:38:18.046: INFO: Lookups using dns-6098/dns-test-3377682a-cf78-4060-bde8-b865702ea2a8 failed for: [wheezy_udp@dns-test-service-3.dns-6098.svc.cluster.local jessie_udp@dns-test-service-3.dns-6098.svc.cluster.local]

    Nov 15 15:38:23.041: INFO: File wheezy_udp@dns-test-service-3.dns-6098.svc.cluster.local from pod  dns-6098/dns-test-3377682a-cf78-4060-bde8-b865702ea2a8 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Nov 15 15:38:23.063: INFO: File jessie_udp@dns-test-service-3.dns-6098.svc.cluster.local from pod  dns-6098/dns-test-3377682a-cf78-4060-bde8-b865702ea2a8 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Nov 15 15:38:23.063: INFO: Lookups using dns-6098/dns-test-3377682a-cf78-4060-bde8-b865702ea2a8 failed for: [wheezy_udp@dns-test-service-3.dns-6098.svc.cluster.local jessie_udp@dns-test-service-3.dns-6098.svc.cluster.local]

    Nov 15 15:38:28.021: INFO: File wheezy_udp@dns-test-service-3.dns-6098.svc.cluster.local from pod  dns-6098/dns-test-3377682a-cf78-4060-bde8-b865702ea2a8 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Nov 15 15:38:28.042: INFO: File jessie_udp@dns-test-service-3.dns-6098.svc.cluster.local from pod  dns-6098/dns-test-3377682a-cf78-4060-bde8-b865702ea2a8 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Nov 15 15:38:28.042: INFO: Lookups using dns-6098/dns-test-3377682a-cf78-4060-bde8-b865702ea2a8 failed for: [wheezy_udp@dns-test-service-3.dns-6098.svc.cluster.local jessie_udp@dns-test-service-3.dns-6098.svc.cluster.local]

    Nov 15 15:38:33.027: INFO: File wheezy_udp@dns-test-service-3.dns-6098.svc.cluster.local from pod  dns-6098/dns-test-3377682a-cf78-4060-bde8-b865702ea2a8 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Nov 15 15:38:33.056: INFO: File jessie_udp@dns-test-service-3.dns-6098.svc.cluster.local from pod  dns-6098/dns-test-3377682a-cf78-4060-bde8-b865702ea2a8 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Nov 15 15:38:33.056: INFO: Lookups using dns-6098/dns-test-3377682a-cf78-4060-bde8-b865702ea2a8 failed for: [wheezy_udp@dns-test-service-3.dns-6098.svc.cluster.local jessie_udp@dns-test-service-3.dns-6098.svc.cluster.local]

    Nov 15 15:38:38.053: INFO: DNS probes using dns-test-3377682a-cf78-4060-bde8-b865702ea2a8 succeeded

    STEP: deleting the pod 11/15/23 15:38:38.053
    STEP: changing the service to type=ClusterIP 11/15/23 15:38:38.092
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6098.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6098.svc.cluster.local; sleep 1; done
     11/15/23 15:38:38.174
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6098.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6098.svc.cluster.local; sleep 1; done
     11/15/23 15:38:38.176
    STEP: creating a third pod to probe DNS 11/15/23 15:38:38.176
    STEP: submitting the pod to kubernetes 11/15/23 15:38:38.196
    Nov 15 15:38:38.229: INFO: Waiting up to 15m0s for pod "dns-test-128b6024-b664-4f87-a203-794bf0278925" in namespace "dns-6098" to be "running"
    Nov 15 15:38:38.243: INFO: Pod "dns-test-128b6024-b664-4f87-a203-794bf0278925": Phase="Pending", Reason="", readiness=false. Elapsed: 14.159371ms
    Nov 15 15:38:40.258: INFO: Pod "dns-test-128b6024-b664-4f87-a203-794bf0278925": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028721585s
    Nov 15 15:38:42.260: INFO: Pod "dns-test-128b6024-b664-4f87-a203-794bf0278925": Phase="Running", Reason="", readiness=true. Elapsed: 4.030676341s
    Nov 15 15:38:42.260: INFO: Pod "dns-test-128b6024-b664-4f87-a203-794bf0278925" satisfied condition "running"
    STEP: retrieving the pod 11/15/23 15:38:42.261
    STEP: looking for the results for each expected name from probers 11/15/23 15:38:42.276
    Nov 15 15:38:42.402: INFO: DNS probes using dns-test-128b6024-b664-4f87-a203-794bf0278925 succeeded

    STEP: deleting the pod 11/15/23 15:38:42.402
    STEP: deleting the test externalName service 11/15/23 15:38:42.443
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:38:42.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6098" for this suite. 11/15/23 15:38:42.53
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:38:42.564
Nov 15 15:38:42.565: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename resourcequota 11/15/23 15:38:42.568
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:38:42.625
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:38:42.639
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 11/15/23 15:38:42.651
STEP: Creating a ResourceQuota 11/15/23 15:38:47.67
STEP: Ensuring resource quota status is calculated 11/15/23 15:38:47.692
STEP: Creating a ReplicaSet 11/15/23 15:38:49.711
STEP: Ensuring resource quota status captures replicaset creation 11/15/23 15:38:49.754
STEP: Deleting a ReplicaSet 11/15/23 15:38:51.774
STEP: Ensuring resource quota status released usage 11/15/23 15:38:51.804
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Nov 15 15:38:53.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7344" for this suite. 11/15/23 15:38:53.852
------------------------------
• [SLOW TEST] [11.313 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:38:42.564
    Nov 15 15:38:42.565: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename resourcequota 11/15/23 15:38:42.568
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:38:42.625
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:38:42.639
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 11/15/23 15:38:42.651
    STEP: Creating a ResourceQuota 11/15/23 15:38:47.67
    STEP: Ensuring resource quota status is calculated 11/15/23 15:38:47.692
    STEP: Creating a ReplicaSet 11/15/23 15:38:49.711
    STEP: Ensuring resource quota status captures replicaset creation 11/15/23 15:38:49.754
    STEP: Deleting a ReplicaSet 11/15/23 15:38:51.774
    STEP: Ensuring resource quota status released usage 11/15/23 15:38:51.804
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:38:53.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7344" for this suite. 11/15/23 15:38:53.852
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:38:53.887
Nov 15 15:38:53.887: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename taint-single-pod 11/15/23 15:38:53.889
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:38:53.946
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:38:53.961
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Nov 15 15:38:53.975: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 15 15:39:54.098: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Nov 15 15:39:54.111: INFO: Starting informer...
STEP: Starting pod... 11/15/23 15:39:54.111
Nov 15 15:39:54.371: INFO: Pod is running on 10.15.40.115. Tainting Node
STEP: Trying to apply a taint on the Node 11/15/23 15:39:54.372
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 11/15/23 15:39:54.415
STEP: Waiting short time to make sure Pod is queued for deletion 11/15/23 15:39:54.431
Nov 15 15:39:54.432: INFO: Pod wasn't evicted. Proceeding
Nov 15 15:39:54.432: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 11/15/23 15:39:54.473
STEP: Waiting some time to make sure that toleration time passed. 11/15/23 15:39:54.489
Nov 15 15:41:09.492: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 15:41:09.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-1195" for this suite. 11/15/23 15:41:09.513
------------------------------
• [SLOW TEST] [135.650 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:38:53.887
    Nov 15 15:38:53.887: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename taint-single-pod 11/15/23 15:38:53.889
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:38:53.946
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:38:53.961
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Nov 15 15:38:53.975: INFO: Waiting up to 1m0s for all nodes to be ready
    Nov 15 15:39:54.098: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Nov 15 15:39:54.111: INFO: Starting informer...
    STEP: Starting pod... 11/15/23 15:39:54.111
    Nov 15 15:39:54.371: INFO: Pod is running on 10.15.40.115. Tainting Node
    STEP: Trying to apply a taint on the Node 11/15/23 15:39:54.372
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 11/15/23 15:39:54.415
    STEP: Waiting short time to make sure Pod is queued for deletion 11/15/23 15:39:54.431
    Nov 15 15:39:54.432: INFO: Pod wasn't evicted. Proceeding
    Nov 15 15:39:54.432: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 11/15/23 15:39:54.473
    STEP: Waiting some time to make sure that toleration time passed. 11/15/23 15:39:54.489
    Nov 15 15:41:09.492: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:41:09.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-1195" for this suite. 11/15/23 15:41:09.513
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:41:09.55
Nov 15 15:41:09.550: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename services 11/15/23 15:41:09.553
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:41:09.605
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:41:09.616
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-507 11/15/23 15:41:09.633
STEP: creating service affinity-clusterip in namespace services-507 11/15/23 15:41:09.633
STEP: creating replication controller affinity-clusterip in namespace services-507 11/15/23 15:41:09.68
I1115 15:41:09.719892      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-507, replica count: 3
I1115 15:41:12.775716      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 15 15:41:12.813: INFO: Creating new exec pod
Nov 15 15:41:12.844: INFO: Waiting up to 5m0s for pod "execpod-affinityvvtqh" in namespace "services-507" to be "running"
Nov 15 15:41:12.857: INFO: Pod "execpod-affinityvvtqh": Phase="Pending", Reason="", readiness=false. Elapsed: 12.933659ms
Nov 15 15:41:14.874: INFO: Pod "execpod-affinityvvtqh": Phase="Running", Reason="", readiness=true. Elapsed: 2.029471384s
Nov 15 15:41:14.874: INFO: Pod "execpod-affinityvvtqh" satisfied condition "running"
Nov 15 15:41:15.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-507 exec execpod-affinityvvtqh -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Nov 15 15:41:16.368: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Nov 15 15:41:16.368: INFO: stdout: ""
Nov 15 15:41:16.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-507 exec execpod-affinityvvtqh -- /bin/sh -x -c nc -v -z -w 2 172.21.199.134 80'
Nov 15 15:41:16.829: INFO: stderr: "+ nc -v -z -w 2 172.21.199.134 80\nConnection to 172.21.199.134 80 port [tcp/http] succeeded!\n"
Nov 15 15:41:16.829: INFO: stdout: ""
Nov 15 15:41:16.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-507 exec execpod-affinityvvtqh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.199.134:80/ ; done'
Nov 15 15:41:17.488: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.134:80/\n"
Nov 15 15:41:17.488: INFO: stdout: "\naffinity-clusterip-x62j5\naffinity-clusterip-x62j5\naffinity-clusterip-x62j5\naffinity-clusterip-x62j5\naffinity-clusterip-x62j5\naffinity-clusterip-x62j5\naffinity-clusterip-x62j5\naffinity-clusterip-x62j5\naffinity-clusterip-x62j5\naffinity-clusterip-x62j5\naffinity-clusterip-x62j5\naffinity-clusterip-x62j5\naffinity-clusterip-x62j5\naffinity-clusterip-x62j5\naffinity-clusterip-x62j5\naffinity-clusterip-x62j5"
Nov 15 15:41:17.488: INFO: Received response from host: affinity-clusterip-x62j5
Nov 15 15:41:17.488: INFO: Received response from host: affinity-clusterip-x62j5
Nov 15 15:41:17.488: INFO: Received response from host: affinity-clusterip-x62j5
Nov 15 15:41:17.488: INFO: Received response from host: affinity-clusterip-x62j5
Nov 15 15:41:17.488: INFO: Received response from host: affinity-clusterip-x62j5
Nov 15 15:41:17.488: INFO: Received response from host: affinity-clusterip-x62j5
Nov 15 15:41:17.488: INFO: Received response from host: affinity-clusterip-x62j5
Nov 15 15:41:17.488: INFO: Received response from host: affinity-clusterip-x62j5
Nov 15 15:41:17.488: INFO: Received response from host: affinity-clusterip-x62j5
Nov 15 15:41:17.488: INFO: Received response from host: affinity-clusterip-x62j5
Nov 15 15:41:17.488: INFO: Received response from host: affinity-clusterip-x62j5
Nov 15 15:41:17.488: INFO: Received response from host: affinity-clusterip-x62j5
Nov 15 15:41:17.488: INFO: Received response from host: affinity-clusterip-x62j5
Nov 15 15:41:17.489: INFO: Received response from host: affinity-clusterip-x62j5
Nov 15 15:41:17.489: INFO: Received response from host: affinity-clusterip-x62j5
Nov 15 15:41:17.489: INFO: Received response from host: affinity-clusterip-x62j5
Nov 15 15:41:17.489: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-507, will wait for the garbage collector to delete the pods 11/15/23 15:41:17.536
Nov 15 15:41:17.626: INFO: Deleting ReplicationController affinity-clusterip took: 23.661004ms
Nov 15 15:41:17.928: INFO: Terminating ReplicationController affinity-clusterip pods took: 301.726266ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 15 15:41:20.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-507" for this suite. 11/15/23 15:41:20.857
------------------------------
• [SLOW TEST] [11.333 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:41:09.55
    Nov 15 15:41:09.550: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename services 11/15/23 15:41:09.553
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:41:09.605
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:41:09.616
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-507 11/15/23 15:41:09.633
    STEP: creating service affinity-clusterip in namespace services-507 11/15/23 15:41:09.633
    STEP: creating replication controller affinity-clusterip in namespace services-507 11/15/23 15:41:09.68
    I1115 15:41:09.719892      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-507, replica count: 3
    I1115 15:41:12.775716      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 15 15:41:12.813: INFO: Creating new exec pod
    Nov 15 15:41:12.844: INFO: Waiting up to 5m0s for pod "execpod-affinityvvtqh" in namespace "services-507" to be "running"
    Nov 15 15:41:12.857: INFO: Pod "execpod-affinityvvtqh": Phase="Pending", Reason="", readiness=false. Elapsed: 12.933659ms
    Nov 15 15:41:14.874: INFO: Pod "execpod-affinityvvtqh": Phase="Running", Reason="", readiness=true. Elapsed: 2.029471384s
    Nov 15 15:41:14.874: INFO: Pod "execpod-affinityvvtqh" satisfied condition "running"
    Nov 15 15:41:15.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-507 exec execpod-affinityvvtqh -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Nov 15 15:41:16.368: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Nov 15 15:41:16.368: INFO: stdout: ""
    Nov 15 15:41:16.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-507 exec execpod-affinityvvtqh -- /bin/sh -x -c nc -v -z -w 2 172.21.199.134 80'
    Nov 15 15:41:16.829: INFO: stderr: "+ nc -v -z -w 2 172.21.199.134 80\nConnection to 172.21.199.134 80 port [tcp/http] succeeded!\n"
    Nov 15 15:41:16.829: INFO: stdout: ""
    Nov 15 15:41:16.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-507 exec execpod-affinityvvtqh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.199.134:80/ ; done'
    Nov 15 15:41:17.488: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.134:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.199.134:80/\n"
    Nov 15 15:41:17.488: INFO: stdout: "\naffinity-clusterip-x62j5\naffinity-clusterip-x62j5\naffinity-clusterip-x62j5\naffinity-clusterip-x62j5\naffinity-clusterip-x62j5\naffinity-clusterip-x62j5\naffinity-clusterip-x62j5\naffinity-clusterip-x62j5\naffinity-clusterip-x62j5\naffinity-clusterip-x62j5\naffinity-clusterip-x62j5\naffinity-clusterip-x62j5\naffinity-clusterip-x62j5\naffinity-clusterip-x62j5\naffinity-clusterip-x62j5\naffinity-clusterip-x62j5"
    Nov 15 15:41:17.488: INFO: Received response from host: affinity-clusterip-x62j5
    Nov 15 15:41:17.488: INFO: Received response from host: affinity-clusterip-x62j5
    Nov 15 15:41:17.488: INFO: Received response from host: affinity-clusterip-x62j5
    Nov 15 15:41:17.488: INFO: Received response from host: affinity-clusterip-x62j5
    Nov 15 15:41:17.488: INFO: Received response from host: affinity-clusterip-x62j5
    Nov 15 15:41:17.488: INFO: Received response from host: affinity-clusterip-x62j5
    Nov 15 15:41:17.488: INFO: Received response from host: affinity-clusterip-x62j5
    Nov 15 15:41:17.488: INFO: Received response from host: affinity-clusterip-x62j5
    Nov 15 15:41:17.488: INFO: Received response from host: affinity-clusterip-x62j5
    Nov 15 15:41:17.488: INFO: Received response from host: affinity-clusterip-x62j5
    Nov 15 15:41:17.488: INFO: Received response from host: affinity-clusterip-x62j5
    Nov 15 15:41:17.488: INFO: Received response from host: affinity-clusterip-x62j5
    Nov 15 15:41:17.488: INFO: Received response from host: affinity-clusterip-x62j5
    Nov 15 15:41:17.489: INFO: Received response from host: affinity-clusterip-x62j5
    Nov 15 15:41:17.489: INFO: Received response from host: affinity-clusterip-x62j5
    Nov 15 15:41:17.489: INFO: Received response from host: affinity-clusterip-x62j5
    Nov 15 15:41:17.489: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-507, will wait for the garbage collector to delete the pods 11/15/23 15:41:17.536
    Nov 15 15:41:17.626: INFO: Deleting ReplicationController affinity-clusterip took: 23.661004ms
    Nov 15 15:41:17.928: INFO: Terminating ReplicationController affinity-clusterip pods took: 301.726266ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:41:20.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-507" for this suite. 11/15/23 15:41:20.857
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:41:20.888
Nov 15 15:41:20.889: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename projected 11/15/23 15:41:20.891
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:41:20.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:41:20.972
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-9fd57b63-8873-4b61-9500-7d6de56695d2 11/15/23 15:41:20.999
STEP: Creating configMap with name cm-test-opt-upd-9656225c-2d48-4d47-8092-3c0c2f1dc171 11/15/23 15:41:21.014
STEP: Creating the pod 11/15/23 15:41:21.031
Nov 15 15:41:21.079: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d18ecd2c-bc43-4753-a14c-e2ae9a0de49b" in namespace "projected-5638" to be "running and ready"
Nov 15 15:41:21.093: INFO: Pod "pod-projected-configmaps-d18ecd2c-bc43-4753-a14c-e2ae9a0de49b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.008453ms
Nov 15 15:41:21.093: INFO: The phase of Pod pod-projected-configmaps-d18ecd2c-bc43-4753-a14c-e2ae9a0de49b is Pending, waiting for it to be Running (with Ready = true)
Nov 15 15:41:23.112: INFO: Pod "pod-projected-configmaps-d18ecd2c-bc43-4753-a14c-e2ae9a0de49b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033155406s
Nov 15 15:41:23.112: INFO: The phase of Pod pod-projected-configmaps-d18ecd2c-bc43-4753-a14c-e2ae9a0de49b is Pending, waiting for it to be Running (with Ready = true)
Nov 15 15:41:25.109: INFO: Pod "pod-projected-configmaps-d18ecd2c-bc43-4753-a14c-e2ae9a0de49b": Phase="Running", Reason="", readiness=true. Elapsed: 4.03034044s
Nov 15 15:41:25.109: INFO: The phase of Pod pod-projected-configmaps-d18ecd2c-bc43-4753-a14c-e2ae9a0de49b is Running (Ready = true)
Nov 15 15:41:25.109: INFO: Pod "pod-projected-configmaps-d18ecd2c-bc43-4753-a14c-e2ae9a0de49b" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-9fd57b63-8873-4b61-9500-7d6de56695d2 11/15/23 15:41:25.331
STEP: Updating configmap cm-test-opt-upd-9656225c-2d48-4d47-8092-3c0c2f1dc171 11/15/23 15:41:25.353
STEP: Creating configMap with name cm-test-opt-create-a1866f6b-a4b0-42cc-8677-57b07b01df3b 11/15/23 15:41:25.369
STEP: waiting to observe update in volume 11/15/23 15:41:25.385
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Nov 15 15:42:35.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5638" for this suite. 11/15/23 15:42:35.12
------------------------------
• [SLOW TEST] [74.254 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:41:20.888
    Nov 15 15:41:20.889: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename projected 11/15/23 15:41:20.891
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:41:20.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:41:20.972
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-9fd57b63-8873-4b61-9500-7d6de56695d2 11/15/23 15:41:20.999
    STEP: Creating configMap with name cm-test-opt-upd-9656225c-2d48-4d47-8092-3c0c2f1dc171 11/15/23 15:41:21.014
    STEP: Creating the pod 11/15/23 15:41:21.031
    Nov 15 15:41:21.079: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d18ecd2c-bc43-4753-a14c-e2ae9a0de49b" in namespace "projected-5638" to be "running and ready"
    Nov 15 15:41:21.093: INFO: Pod "pod-projected-configmaps-d18ecd2c-bc43-4753-a14c-e2ae9a0de49b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.008453ms
    Nov 15 15:41:21.093: INFO: The phase of Pod pod-projected-configmaps-d18ecd2c-bc43-4753-a14c-e2ae9a0de49b is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 15:41:23.112: INFO: Pod "pod-projected-configmaps-d18ecd2c-bc43-4753-a14c-e2ae9a0de49b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033155406s
    Nov 15 15:41:23.112: INFO: The phase of Pod pod-projected-configmaps-d18ecd2c-bc43-4753-a14c-e2ae9a0de49b is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 15:41:25.109: INFO: Pod "pod-projected-configmaps-d18ecd2c-bc43-4753-a14c-e2ae9a0de49b": Phase="Running", Reason="", readiness=true. Elapsed: 4.03034044s
    Nov 15 15:41:25.109: INFO: The phase of Pod pod-projected-configmaps-d18ecd2c-bc43-4753-a14c-e2ae9a0de49b is Running (Ready = true)
    Nov 15 15:41:25.109: INFO: Pod "pod-projected-configmaps-d18ecd2c-bc43-4753-a14c-e2ae9a0de49b" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-9fd57b63-8873-4b61-9500-7d6de56695d2 11/15/23 15:41:25.331
    STEP: Updating configmap cm-test-opt-upd-9656225c-2d48-4d47-8092-3c0c2f1dc171 11/15/23 15:41:25.353
    STEP: Creating configMap with name cm-test-opt-create-a1866f6b-a4b0-42cc-8677-57b07b01df3b 11/15/23 15:41:25.369
    STEP: waiting to observe update in volume 11/15/23 15:41:25.385
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:42:35.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5638" for this suite. 11/15/23 15:42:35.12
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:42:35.154
Nov 15 15:42:35.154: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename replicaset 11/15/23 15:42:35.156
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:42:35.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:42:35.218
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 11/15/23 15:42:35.257
STEP: Verify that the required pods have come up. 11/15/23 15:42:35.276
Nov 15 15:42:35.291: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov 15 15:42:40.306: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 11/15/23 15:42:40.306
STEP: Getting /status 11/15/23 15:42:40.306
Nov 15 15:42:40.340: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 11/15/23 15:42:40.341
Nov 15 15:42:40.378: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 11/15/23 15:42:40.378
Nov 15 15:42:40.384: INFO: Observed &ReplicaSet event: ADDED
Nov 15 15:42:40.385: INFO: Observed &ReplicaSet event: MODIFIED
Nov 15 15:42:40.385: INFO: Observed &ReplicaSet event: MODIFIED
Nov 15 15:42:40.387: INFO: Observed &ReplicaSet event: MODIFIED
Nov 15 15:42:40.387: INFO: Found replicaset test-rs in namespace replicaset-3501 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Nov 15 15:42:40.387: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 11/15/23 15:42:40.387
Nov 15 15:42:40.388: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Nov 15 15:42:40.409: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 11/15/23 15:42:40.409
Nov 15 15:42:40.416: INFO: Observed &ReplicaSet event: ADDED
Nov 15 15:42:40.417: INFO: Observed &ReplicaSet event: MODIFIED
Nov 15 15:42:40.418: INFO: Observed &ReplicaSet event: MODIFIED
Nov 15 15:42:40.418: INFO: Observed &ReplicaSet event: MODIFIED
Nov 15 15:42:40.419: INFO: Observed replicaset test-rs in namespace replicaset-3501 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Nov 15 15:42:40.420: INFO: Observed &ReplicaSet event: MODIFIED
Nov 15 15:42:40.420: INFO: Found replicaset test-rs in namespace replicaset-3501 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Nov 15 15:42:40.420: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Nov 15 15:42:40.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-3501" for this suite. 11/15/23 15:42:40.443
------------------------------
• [SLOW TEST] [5.315 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:42:35.154
    Nov 15 15:42:35.154: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename replicaset 11/15/23 15:42:35.156
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:42:35.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:42:35.218
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 11/15/23 15:42:35.257
    STEP: Verify that the required pods have come up. 11/15/23 15:42:35.276
    Nov 15 15:42:35.291: INFO: Pod name sample-pod: Found 0 pods out of 1
    Nov 15 15:42:40.306: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 11/15/23 15:42:40.306
    STEP: Getting /status 11/15/23 15:42:40.306
    Nov 15 15:42:40.340: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 11/15/23 15:42:40.341
    Nov 15 15:42:40.378: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 11/15/23 15:42:40.378
    Nov 15 15:42:40.384: INFO: Observed &ReplicaSet event: ADDED
    Nov 15 15:42:40.385: INFO: Observed &ReplicaSet event: MODIFIED
    Nov 15 15:42:40.385: INFO: Observed &ReplicaSet event: MODIFIED
    Nov 15 15:42:40.387: INFO: Observed &ReplicaSet event: MODIFIED
    Nov 15 15:42:40.387: INFO: Found replicaset test-rs in namespace replicaset-3501 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Nov 15 15:42:40.387: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 11/15/23 15:42:40.387
    Nov 15 15:42:40.388: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Nov 15 15:42:40.409: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 11/15/23 15:42:40.409
    Nov 15 15:42:40.416: INFO: Observed &ReplicaSet event: ADDED
    Nov 15 15:42:40.417: INFO: Observed &ReplicaSet event: MODIFIED
    Nov 15 15:42:40.418: INFO: Observed &ReplicaSet event: MODIFIED
    Nov 15 15:42:40.418: INFO: Observed &ReplicaSet event: MODIFIED
    Nov 15 15:42:40.419: INFO: Observed replicaset test-rs in namespace replicaset-3501 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Nov 15 15:42:40.420: INFO: Observed &ReplicaSet event: MODIFIED
    Nov 15 15:42:40.420: INFO: Found replicaset test-rs in namespace replicaset-3501 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Nov 15 15:42:40.420: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:42:40.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-3501" for this suite. 11/15/23 15:42:40.443
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:42:40.473
Nov 15 15:42:40.474: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename gc 11/15/23 15:42:40.476
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:42:40.531
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:42:40.542
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 11/15/23 15:42:40.555
STEP: delete the rc 11/15/23 15:42:45.594
STEP: wait for all pods to be garbage collected 11/15/23 15:42:45.618
STEP: Gathering metrics 11/15/23 15:42:50.645
W1115 15:42:50.682593      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Nov 15 15:42:50.682: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Nov 15 15:42:50.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-7949" for this suite. 11/15/23 15:42:50.702
------------------------------
• [SLOW TEST] [10.257 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:42:40.473
    Nov 15 15:42:40.474: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename gc 11/15/23 15:42:40.476
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:42:40.531
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:42:40.542
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 11/15/23 15:42:40.555
    STEP: delete the rc 11/15/23 15:42:45.594
    STEP: wait for all pods to be garbage collected 11/15/23 15:42:45.618
    STEP: Gathering metrics 11/15/23 15:42:50.645
    W1115 15:42:50.682593      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Nov 15 15:42:50.682: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:42:50.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-7949" for this suite. 11/15/23 15:42:50.702
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:42:50.734
Nov 15 15:42:50.735: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename container-probe 11/15/23 15:42:50.738
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:42:50.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:42:50.8
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-11d962bd-a4da-43f5-be11-04726ed3d410 in namespace container-probe-9003 11/15/23 15:42:50.812
Nov 15 15:42:50.842: INFO: Waiting up to 5m0s for pod "test-webserver-11d962bd-a4da-43f5-be11-04726ed3d410" in namespace "container-probe-9003" to be "not pending"
Nov 15 15:42:50.855: INFO: Pod "test-webserver-11d962bd-a4da-43f5-be11-04726ed3d410": Phase="Pending", Reason="", readiness=false. Elapsed: 12.552417ms
Nov 15 15:42:52.871: INFO: Pod "test-webserver-11d962bd-a4da-43f5-be11-04726ed3d410": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028963107s
Nov 15 15:42:54.870: INFO: Pod "test-webserver-11d962bd-a4da-43f5-be11-04726ed3d410": Phase="Running", Reason="", readiness=true. Elapsed: 4.027922836s
Nov 15 15:42:54.870: INFO: Pod "test-webserver-11d962bd-a4da-43f5-be11-04726ed3d410" satisfied condition "not pending"
Nov 15 15:42:54.871: INFO: Started pod test-webserver-11d962bd-a4da-43f5-be11-04726ed3d410 in namespace container-probe-9003
STEP: checking the pod's current state and verifying that restartCount is present 11/15/23 15:42:54.871
Nov 15 15:42:54.884: INFO: Initial restart count of pod test-webserver-11d962bd-a4da-43f5-be11-04726ed3d410 is 0
STEP: deleting the pod 11/15/23 15:46:55.007
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Nov 15 15:46:55.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9003" for this suite. 11/15/23 15:46:55.067
------------------------------
• [SLOW TEST] [244.358 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:42:50.734
    Nov 15 15:42:50.735: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename container-probe 11/15/23 15:42:50.738
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:42:50.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:42:50.8
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-11d962bd-a4da-43f5-be11-04726ed3d410 in namespace container-probe-9003 11/15/23 15:42:50.812
    Nov 15 15:42:50.842: INFO: Waiting up to 5m0s for pod "test-webserver-11d962bd-a4da-43f5-be11-04726ed3d410" in namespace "container-probe-9003" to be "not pending"
    Nov 15 15:42:50.855: INFO: Pod "test-webserver-11d962bd-a4da-43f5-be11-04726ed3d410": Phase="Pending", Reason="", readiness=false. Elapsed: 12.552417ms
    Nov 15 15:42:52.871: INFO: Pod "test-webserver-11d962bd-a4da-43f5-be11-04726ed3d410": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028963107s
    Nov 15 15:42:54.870: INFO: Pod "test-webserver-11d962bd-a4da-43f5-be11-04726ed3d410": Phase="Running", Reason="", readiness=true. Elapsed: 4.027922836s
    Nov 15 15:42:54.870: INFO: Pod "test-webserver-11d962bd-a4da-43f5-be11-04726ed3d410" satisfied condition "not pending"
    Nov 15 15:42:54.871: INFO: Started pod test-webserver-11d962bd-a4da-43f5-be11-04726ed3d410 in namespace container-probe-9003
    STEP: checking the pod's current state and verifying that restartCount is present 11/15/23 15:42:54.871
    Nov 15 15:42:54.884: INFO: Initial restart count of pod test-webserver-11d962bd-a4da-43f5-be11-04726ed3d410 is 0
    STEP: deleting the pod 11/15/23 15:46:55.007
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:46:55.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9003" for this suite. 11/15/23 15:46:55.067
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:46:55.094
Nov 15 15:46:55.095: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename configmap 11/15/23 15:46:55.098
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:46:55.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:46:55.171
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-855d6de5-9c8e-4072-b4d5-fb0adf6f53b0 11/15/23 15:46:55.227
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 15 15:46:55.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4588" for this suite. 11/15/23 15:46:55.263
------------------------------
• [0.193 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:46:55.094
    Nov 15 15:46:55.095: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename configmap 11/15/23 15:46:55.098
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:46:55.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:46:55.171
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-855d6de5-9c8e-4072-b4d5-fb0adf6f53b0 11/15/23 15:46:55.227
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:46:55.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4588" for this suite. 11/15/23 15:46:55.263
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:46:55.301
Nov 15 15:46:55.301: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename var-expansion 11/15/23 15:46:55.303
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:46:55.353
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:46:55.372
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Nov 15 15:46:55.421: INFO: Waiting up to 2m0s for pod "var-expansion-659b2c7b-36e9-4e51-aa86-1bf18dc486a5" in namespace "var-expansion-5765" to be "container 0 failed with reason CreateContainerConfigError"
Nov 15 15:46:55.436: INFO: Pod "var-expansion-659b2c7b-36e9-4e51-aa86-1bf18dc486a5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.785769ms
Nov 15 15:46:57.456: INFO: Pod "var-expansion-659b2c7b-36e9-4e51-aa86-1bf18dc486a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034962806s
Nov 15 15:46:57.456: INFO: Pod "var-expansion-659b2c7b-36e9-4e51-aa86-1bf18dc486a5" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Nov 15 15:46:57.456: INFO: Deleting pod "var-expansion-659b2c7b-36e9-4e51-aa86-1bf18dc486a5" in namespace "var-expansion-5765"
Nov 15 15:46:57.483: INFO: Wait up to 5m0s for pod "var-expansion-659b2c7b-36e9-4e51-aa86-1bf18dc486a5" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Nov 15 15:47:01.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5765" for this suite. 11/15/23 15:47:01.545
------------------------------
• [SLOW TEST] [6.269 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:46:55.301
    Nov 15 15:46:55.301: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename var-expansion 11/15/23 15:46:55.303
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:46:55.353
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:46:55.372
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Nov 15 15:46:55.421: INFO: Waiting up to 2m0s for pod "var-expansion-659b2c7b-36e9-4e51-aa86-1bf18dc486a5" in namespace "var-expansion-5765" to be "container 0 failed with reason CreateContainerConfigError"
    Nov 15 15:46:55.436: INFO: Pod "var-expansion-659b2c7b-36e9-4e51-aa86-1bf18dc486a5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.785769ms
    Nov 15 15:46:57.456: INFO: Pod "var-expansion-659b2c7b-36e9-4e51-aa86-1bf18dc486a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034962806s
    Nov 15 15:46:57.456: INFO: Pod "var-expansion-659b2c7b-36e9-4e51-aa86-1bf18dc486a5" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Nov 15 15:46:57.456: INFO: Deleting pod "var-expansion-659b2c7b-36e9-4e51-aa86-1bf18dc486a5" in namespace "var-expansion-5765"
    Nov 15 15:46:57.483: INFO: Wait up to 5m0s for pod "var-expansion-659b2c7b-36e9-4e51-aa86-1bf18dc486a5" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:47:01.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5765" for this suite. 11/15/23 15:47:01.545
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:47:01.58
Nov 15 15:47:01.580: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename sched-pred 11/15/23 15:47:01.582
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:47:01.648
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:47:01.662
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Nov 15 15:47:01.678: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 15 15:47:01.730: INFO: Waiting for terminating namespaces to be deleted...
Nov 15 15:47:01.746: INFO: 
Logging pods the apiserver thinks is on node 10.15.40.106 before test
Nov 15 15:47:01.800: INFO: ibm-cloud-provider-ip-163-109-74-98-5d59656977-bgllw from ibm-system started at 2023-11-15 15:39:54 +0000 UTC (1 container statuses recorded)
Nov 15 15:47:01.800: INFO: 	Container ibm-cloud-provider-ip-163-109-74-98 ready: true, restart count 0
Nov 15 15:47:01.800: INFO: calico-kube-controllers-7847f7647d-srqd4 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 15:47:01.800: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Nov 15 15:47:01.801: INFO: calico-node-67csc from kube-system started at 2023-11-15 13:13:21 +0000 UTC (1 container statuses recorded)
Nov 15 15:47:01.801: INFO: 	Container calico-node ready: true, restart count 0
Nov 15 15:47:01.801: INFO: calico-typha-5b5db87f55-nwmh4 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 15:47:01.801: INFO: 	Container calico-typha ready: true, restart count 0
Nov 15 15:47:01.802: INFO: coredns-5845f98d4-2lnrf from kube-system started at 2023-11-15 13:23:15 +0000 UTC (1 container statuses recorded)
Nov 15 15:47:01.802: INFO: 	Container coredns ready: true, restart count 0
Nov 15 15:47:01.802: INFO: coredns-autoscaler-85f4bdddf6-qwxmw from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 15:47:01.802: INFO: 	Container autoscaler ready: true, restart count 0
Nov 15 15:47:01.802: INFO: dashboard-metrics-scraper-7cf679fbdf-frv7t from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 15:47:01.803: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Nov 15 15:47:01.803: INFO: ibm-file-plugin-557f875d5f-jrfnv from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 15:47:01.803: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Nov 15 15:47:01.803: INFO: ibm-keepalived-watcher-dqknf from kube-system started at 2023-11-15 13:13:21 +0000 UTC (1 container statuses recorded)
Nov 15 15:47:01.804: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 15 15:47:01.804: INFO: ibm-master-proxy-static-10.15.40.106 from kube-system started at 2023-11-15 13:13:08 +0000 UTC (2 container statuses recorded)
Nov 15 15:47:01.804: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 15 15:47:01.804: INFO: 	Container pause ready: true, restart count 0
Nov 15 15:47:01.804: INFO: ibm-storage-watcher-7d897847f4-k64t5 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 15:47:01.804: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Nov 15 15:47:01.805: INFO: ibmcloud-block-storage-driver-f97nm from kube-system started at 2023-11-15 13:13:30 +0000 UTC (1 container statuses recorded)
Nov 15 15:47:01.805: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 15 15:47:01.805: INFO: ibmcloud-block-storage-plugin-5bd59f7b48-2dxcj from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 15:47:01.805: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Nov 15 15:47:01.805: INFO: ingress-cluster-healthcheck-5985f966bb-jgjx8 from kube-system started at 2023-11-15 15:39:54 +0000 UTC (1 container statuses recorded)
Nov 15 15:47:01.805: INFO: 	Container ingress-cluster-healthcheck ready: true, restart count 0
Nov 15 15:47:01.806: INFO: konnectivity-agent-5brpz from kube-system started at 2023-11-15 13:22:38 +0000 UTC (1 container statuses recorded)
Nov 15 15:47:01.806: INFO: 	Container konnectivity-agent ready: true, restart count 0
Nov 15 15:47:01.807: INFO: kubernetes-dashboard-5ccdc9cbb8-wbmfz from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 15:47:01.807: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Nov 15 15:47:01.807: INFO: metrics-server-7cbd9c9b48-8tr74 from kube-system started at 2023-11-15 15:39:54 +0000 UTC (3 container statuses recorded)
Nov 15 15:47:01.807: INFO: 	Container config-watcher ready: true, restart count 0
Nov 15 15:47:01.808: INFO: 	Container metrics-server ready: true, restart count 0
Nov 15 15:47:01.808: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Nov 15 15:47:01.808: INFO: public-crclac150z0u38f277tnpg-alb1-6df9445bb6-qw2bt from kube-system started at 2023-11-15 15:39:54 +0000 UTC (1 container statuses recorded)
Nov 15 15:47:01.808: INFO: 	Container nginx-ingress ready: true, restart count 0
Nov 15 15:47:01.808: INFO: snapshot-controller-6db47fc545-5nnph from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 15:47:01.808: INFO: 	Container snapshot-controller ready: true, restart count 0
Nov 15 15:47:01.808: INFO: snapshot-controller-6db47fc545-h4q49 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 15:47:01.809: INFO: 	Container snapshot-controller ready: true, restart count 0
Nov 15 15:47:01.809: INFO: snapshot-controller-6db47fc545-khkb2 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 15:47:01.809: INFO: 	Container snapshot-controller ready: true, restart count 0
Nov 15 15:47:01.809: INFO: sonobuoy-systemd-logs-daemon-set-eabb9aa298c743f7-zxdnv from sonobuoy started at 2023-11-15 15:24:45 +0000 UTC (2 container statuses recorded)
Nov 15 15:47:01.809: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 15 15:47:01.809: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 15 15:47:01.810: INFO: 
Logging pods the apiserver thinks is on node 10.15.40.114 before test
Nov 15 15:47:01.854: INFO: ibm-cloud-provider-ip-163-109-74-98-5d59656977-wxlx6 from ibm-system started at 2023-11-15 13:18:32 +0000 UTC (1 container statuses recorded)
Nov 15 15:47:01.854: INFO: 	Container ibm-cloud-provider-ip-163-109-74-98 ready: true, restart count 0
Nov 15 15:47:01.854: INFO: calico-node-wqvj8 from kube-system started at 2023-11-15 13:13:28 +0000 UTC (1 container statuses recorded)
Nov 15 15:47:01.854: INFO: 	Container calico-node ready: true, restart count 0
Nov 15 15:47:01.855: INFO: calico-typha-5b5db87f55-4qv4c from kube-system started at 2023-11-15 13:13:53 +0000 UTC (1 container statuses recorded)
Nov 15 15:47:01.855: INFO: 	Container calico-typha ready: true, restart count 0
Nov 15 15:47:01.855: INFO: coredns-5845f98d4-6cld8 from kube-system started at 2023-11-15 13:23:15 +0000 UTC (1 container statuses recorded)
Nov 15 15:47:01.855: INFO: 	Container coredns ready: true, restart count 0
Nov 15 15:47:01.855: INFO: ibm-keepalived-watcher-xsslz from kube-system started at 2023-11-15 13:13:28 +0000 UTC (1 container statuses recorded)
Nov 15 15:47:01.855: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 15 15:47:01.855: INFO: ibm-master-proxy-static-10.15.40.114 from kube-system started at 2023-11-15 13:13:15 +0000 UTC (2 container statuses recorded)
Nov 15 15:47:01.855: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 15 15:47:01.855: INFO: 	Container pause ready: true, restart count 0
Nov 15 15:47:01.856: INFO: ibmcloud-block-storage-driver-4txx8 from kube-system started at 2023-11-15 13:13:37 +0000 UTC (1 container statuses recorded)
Nov 15 15:47:01.856: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 15 15:47:01.856: INFO: konnectivity-agent-zr4zl from kube-system started at 2023-11-15 13:22:41 +0000 UTC (1 container statuses recorded)
Nov 15 15:47:01.856: INFO: 	Container konnectivity-agent ready: true, restart count 0
Nov 15 15:47:01.856: INFO: metrics-server-7cbd9c9b48-cnskr from kube-system started at 2023-11-15 13:56:24 +0000 UTC (3 container statuses recorded)
Nov 15 15:47:01.856: INFO: 	Container config-watcher ready: true, restart count 0
Nov 15 15:47:01.856: INFO: 	Container metrics-server ready: true, restart count 0
Nov 15 15:47:01.856: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Nov 15 15:47:01.856: INFO: public-crclac150z0u38f277tnpg-alb1-6df9445bb6-jkpcd from kube-system started at 2023-11-15 13:23:30 +0000 UTC (1 container statuses recorded)
Nov 15 15:47:01.856: INFO: 	Container nginx-ingress ready: true, restart count 0
Nov 15 15:47:01.856: INFO: sonobuoy-e2e-job-3a51e32fa56b4156 from sonobuoy started at 2023-11-15 15:24:45 +0000 UTC (2 container statuses recorded)
Nov 15 15:47:01.856: INFO: 	Container e2e ready: true, restart count 0
Nov 15 15:47:01.856: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 15 15:47:01.856: INFO: sonobuoy-systemd-logs-daemon-set-eabb9aa298c743f7-sxg5b from sonobuoy started at 2023-11-15 15:24:45 +0000 UTC (2 container statuses recorded)
Nov 15 15:47:01.856: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 15 15:47:01.857: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 15 15:47:01.857: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-11-15 13:15:59 +0000 UTC (1 container statuses recorded)
Nov 15 15:47:01.857: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Nov 15 15:47:01.857: INFO: 
Logging pods the apiserver thinks is on node 10.15.40.115 before test
Nov 15 15:47:01.912: INFO: calico-node-gknnt from kube-system started at 2023-11-15 13:13:40 +0000 UTC (1 container statuses recorded)
Nov 15 15:47:01.912: INFO: 	Container calico-node ready: true, restart count 0
Nov 15 15:47:01.912: INFO: calico-typha-5b5db87f55-fchsg from kube-system started at 2023-11-15 15:40:02 +0000 UTC (1 container statuses recorded)
Nov 15 15:47:01.912: INFO: 	Container calico-typha ready: true, restart count 0
Nov 15 15:47:01.912: INFO: coredns-5845f98d4-rt4sq from kube-system started at 2023-11-15 15:39:54 +0000 UTC (1 container statuses recorded)
Nov 15 15:47:01.912: INFO: 	Container coredns ready: true, restart count 0
Nov 15 15:47:01.912: INFO: ibm-keepalived-watcher-75lpq from kube-system started at 2023-11-15 13:13:40 +0000 UTC (1 container statuses recorded)
Nov 15 15:47:01.912: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 15 15:47:01.912: INFO: ibm-master-proxy-static-10.15.40.115 from kube-system started at 2023-11-15 13:13:28 +0000 UTC (2 container statuses recorded)
Nov 15 15:47:01.912: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 15 15:47:01.912: INFO: 	Container pause ready: true, restart count 0
Nov 15 15:47:01.912: INFO: ibmcloud-block-storage-driver-gbqth from kube-system started at 2023-11-15 13:13:49 +0000 UTC (1 container statuses recorded)
Nov 15 15:47:01.912: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 15 15:47:01.912: INFO: konnectivity-agent-9pkld from kube-system started at 2023-11-15 13:22:45 +0000 UTC (1 container statuses recorded)
Nov 15 15:47:01.912: INFO: 	Container konnectivity-agent ready: true, restart count 0
Nov 15 15:47:01.912: INFO: sonobuoy from sonobuoy started at 2023-11-15 15:24:39 +0000 UTC (1 container statuses recorded)
Nov 15 15:47:01.912: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 15 15:47:01.912: INFO: sonobuoy-systemd-logs-daemon-set-eabb9aa298c743f7-hmfn7 from sonobuoy started at 2023-11-15 15:24:45 +0000 UTC (2 container statuses recorded)
Nov 15 15:47:01.912: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 15 15:47:01.912: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node 10.15.40.106 11/15/23 15:47:01.988
STEP: verifying the node has the label node 10.15.40.114 11/15/23 15:47:02.057
STEP: verifying the node has the label node 10.15.40.115 11/15/23 15:47:02.111
Nov 15 15:47:02.174: INFO: Pod ibm-cloud-provider-ip-163-109-74-98-5d59656977-bgllw requesting resource cpu=5m on Node 10.15.40.106
Nov 15 15:47:02.174: INFO: Pod ibm-cloud-provider-ip-163-109-74-98-5d59656977-wxlx6 requesting resource cpu=5m on Node 10.15.40.114
Nov 15 15:47:02.174: INFO: Pod calico-kube-controllers-7847f7647d-srqd4 requesting resource cpu=10m on Node 10.15.40.106
Nov 15 15:47:02.174: INFO: Pod calico-node-67csc requesting resource cpu=250m on Node 10.15.40.106
Nov 15 15:47:02.174: INFO: Pod calico-node-gknnt requesting resource cpu=250m on Node 10.15.40.115
Nov 15 15:47:02.174: INFO: Pod calico-node-wqvj8 requesting resource cpu=250m on Node 10.15.40.114
Nov 15 15:47:02.174: INFO: Pod calico-typha-5b5db87f55-4qv4c requesting resource cpu=250m on Node 10.15.40.114
Nov 15 15:47:02.175: INFO: Pod calico-typha-5b5db87f55-fchsg requesting resource cpu=250m on Node 10.15.40.115
Nov 15 15:47:02.175: INFO: Pod calico-typha-5b5db87f55-nwmh4 requesting resource cpu=250m on Node 10.15.40.106
Nov 15 15:47:02.175: INFO: Pod coredns-5845f98d4-2lnrf requesting resource cpu=100m on Node 10.15.40.106
Nov 15 15:47:02.175: INFO: Pod coredns-5845f98d4-6cld8 requesting resource cpu=100m on Node 10.15.40.114
Nov 15 15:47:02.175: INFO: Pod coredns-5845f98d4-rt4sq requesting resource cpu=100m on Node 10.15.40.115
Nov 15 15:47:02.175: INFO: Pod coredns-autoscaler-85f4bdddf6-qwxmw requesting resource cpu=1m on Node 10.15.40.106
Nov 15 15:47:02.175: INFO: Pod dashboard-metrics-scraper-7cf679fbdf-frv7t requesting resource cpu=15m on Node 10.15.40.106
Nov 15 15:47:02.175: INFO: Pod ibm-file-plugin-557f875d5f-jrfnv requesting resource cpu=50m on Node 10.15.40.106
Nov 15 15:47:02.175: INFO: Pod ibm-keepalived-watcher-75lpq requesting resource cpu=5m on Node 10.15.40.115
Nov 15 15:47:02.175: INFO: Pod ibm-keepalived-watcher-dqknf requesting resource cpu=5m on Node 10.15.40.106
Nov 15 15:47:02.175: INFO: Pod ibm-keepalived-watcher-xsslz requesting resource cpu=5m on Node 10.15.40.114
Nov 15 15:47:02.175: INFO: Pod ibm-master-proxy-static-10.15.40.106 requesting resource cpu=25m on Node 10.15.40.106
Nov 15 15:47:02.175: INFO: Pod ibm-master-proxy-static-10.15.40.114 requesting resource cpu=25m on Node 10.15.40.114
Nov 15 15:47:02.176: INFO: Pod ibm-master-proxy-static-10.15.40.115 requesting resource cpu=25m on Node 10.15.40.115
Nov 15 15:47:02.176: INFO: Pod ibm-storage-watcher-7d897847f4-k64t5 requesting resource cpu=50m on Node 10.15.40.106
Nov 15 15:47:02.176: INFO: Pod ibmcloud-block-storage-driver-4txx8 requesting resource cpu=50m on Node 10.15.40.114
Nov 15 15:47:02.176: INFO: Pod ibmcloud-block-storage-driver-f97nm requesting resource cpu=50m on Node 10.15.40.106
Nov 15 15:47:02.176: INFO: Pod ibmcloud-block-storage-driver-gbqth requesting resource cpu=50m on Node 10.15.40.115
Nov 15 15:47:02.176: INFO: Pod ibmcloud-block-storage-plugin-5bd59f7b48-2dxcj requesting resource cpu=50m on Node 10.15.40.106
Nov 15 15:47:02.176: INFO: Pod ingress-cluster-healthcheck-5985f966bb-jgjx8 requesting resource cpu=5m on Node 10.15.40.106
Nov 15 15:47:02.176: INFO: Pod konnectivity-agent-5brpz requesting resource cpu=10m on Node 10.15.40.106
Nov 15 15:47:02.176: INFO: Pod konnectivity-agent-9pkld requesting resource cpu=10m on Node 10.15.40.115
Nov 15 15:47:02.177: INFO: Pod konnectivity-agent-zr4zl requesting resource cpu=10m on Node 10.15.40.114
Nov 15 15:47:02.177: INFO: Pod kubernetes-dashboard-5ccdc9cbb8-wbmfz requesting resource cpu=50m on Node 10.15.40.106
Nov 15 15:47:02.177: INFO: Pod metrics-server-7cbd9c9b48-8tr74 requesting resource cpu=126m on Node 10.15.40.106
Nov 15 15:47:02.177: INFO: Pod metrics-server-7cbd9c9b48-cnskr requesting resource cpu=126m on Node 10.15.40.114
Nov 15 15:47:02.177: INFO: Pod public-crclac150z0u38f277tnpg-alb1-6df9445bb6-jkpcd requesting resource cpu=20m on Node 10.15.40.114
Nov 15 15:47:02.177: INFO: Pod public-crclac150z0u38f277tnpg-alb1-6df9445bb6-qw2bt requesting resource cpu=20m on Node 10.15.40.106
Nov 15 15:47:02.177: INFO: Pod snapshot-controller-6db47fc545-5nnph requesting resource cpu=10m on Node 10.15.40.106
Nov 15 15:47:02.177: INFO: Pod snapshot-controller-6db47fc545-h4q49 requesting resource cpu=10m on Node 10.15.40.106
Nov 15 15:47:02.177: INFO: Pod snapshot-controller-6db47fc545-khkb2 requesting resource cpu=10m on Node 10.15.40.106
Nov 15 15:47:02.178: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.15.40.115
Nov 15 15:47:02.178: INFO: Pod sonobuoy-e2e-job-3a51e32fa56b4156 requesting resource cpu=0m on Node 10.15.40.114
Nov 15 15:47:02.178: INFO: Pod sonobuoy-systemd-logs-daemon-set-eabb9aa298c743f7-hmfn7 requesting resource cpu=0m on Node 10.15.40.115
Nov 15 15:47:02.178: INFO: Pod sonobuoy-systemd-logs-daemon-set-eabb9aa298c743f7-sxg5b requesting resource cpu=0m on Node 10.15.40.114
Nov 15 15:47:02.179: INFO: Pod sonobuoy-systemd-logs-daemon-set-eabb9aa298c743f7-zxdnv requesting resource cpu=0m on Node 10.15.40.106
Nov 15 15:47:02.179: INFO: Pod test-k8s-e2e-pvg-master-verification requesting resource cpu=0m on Node 10.15.40.114
STEP: Starting Pods to consume most of the cluster CPU. 11/15/23 15:47:02.179
Nov 15 15:47:02.179: INFO: Creating a pod which consumes cpu=1965m on Node 10.15.40.106
Nov 15 15:47:02.224: INFO: Creating a pod which consumes cpu=2148m on Node 10.15.40.114
Nov 15 15:47:02.243: INFO: Creating a pod which consumes cpu=2254m on Node 10.15.40.115
Nov 15 15:47:02.265: INFO: Waiting up to 5m0s for pod "filler-pod-1abd25d1-2e6a-4621-9a12-151991f4b802" in namespace "sched-pred-3486" to be "running"
Nov 15 15:47:02.290: INFO: Pod "filler-pod-1abd25d1-2e6a-4621-9a12-151991f4b802": Phase="Pending", Reason="", readiness=false. Elapsed: 24.882149ms
Nov 15 15:47:04.306: INFO: Pod "filler-pod-1abd25d1-2e6a-4621-9a12-151991f4b802": Phase="Running", Reason="", readiness=true. Elapsed: 2.04103891s
Nov 15 15:47:04.306: INFO: Pod "filler-pod-1abd25d1-2e6a-4621-9a12-151991f4b802" satisfied condition "running"
Nov 15 15:47:04.306: INFO: Waiting up to 5m0s for pod "filler-pod-5e347677-bb9f-4b04-8fa6-99582031af1f" in namespace "sched-pred-3486" to be "running"
Nov 15 15:47:04.321: INFO: Pod "filler-pod-5e347677-bb9f-4b04-8fa6-99582031af1f": Phase="Running", Reason="", readiness=true. Elapsed: 14.98907ms
Nov 15 15:47:04.321: INFO: Pod "filler-pod-5e347677-bb9f-4b04-8fa6-99582031af1f" satisfied condition "running"
Nov 15 15:47:04.322: INFO: Waiting up to 5m0s for pod "filler-pod-71972d8e-3cd1-43eb-952c-b97dad0f9854" in namespace "sched-pred-3486" to be "running"
Nov 15 15:47:04.337: INFO: Pod "filler-pod-71972d8e-3cd1-43eb-952c-b97dad0f9854": Phase="Running", Reason="", readiness=true. Elapsed: 15.153772ms
Nov 15 15:47:04.337: INFO: Pod "filler-pod-71972d8e-3cd1-43eb-952c-b97dad0f9854" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 11/15/23 15:47:04.338
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1abd25d1-2e6a-4621-9a12-151991f4b802.1797d67e49937cec], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3486/filler-pod-1abd25d1-2e6a-4621-9a12-151991f4b802 to 10.15.40.106] 11/15/23 15:47:04.359
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1abd25d1-2e6a-4621-9a12-151991f4b802.1797d67e8de28a4b], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 11/15/23 15:47:04.359
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1abd25d1-2e6a-4621-9a12-151991f4b802.1797d67e8f89c0c4], Reason = [Created], Message = [Created container filler-pod-1abd25d1-2e6a-4621-9a12-151991f4b802] 11/15/23 15:47:04.359
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1abd25d1-2e6a-4621-9a12-151991f4b802.1797d67e998c615a], Reason = [Started], Message = [Started container filler-pod-1abd25d1-2e6a-4621-9a12-151991f4b802] 11/15/23 15:47:04.36
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5e347677-bb9f-4b04-8fa6-99582031af1f.1797d67e4a60ee2f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3486/filler-pod-5e347677-bb9f-4b04-8fa6-99582031af1f to 10.15.40.114] 11/15/23 15:47:04.36
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5e347677-bb9f-4b04-8fa6-99582031af1f.1797d67e8ea6f493], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 11/15/23 15:47:04.361
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5e347677-bb9f-4b04-8fa6-99582031af1f.1797d67e8fe7e882], Reason = [Created], Message = [Created container filler-pod-5e347677-bb9f-4b04-8fa6-99582031af1f] 11/15/23 15:47:04.361
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5e347677-bb9f-4b04-8fa6-99582031af1f.1797d67e9833c72a], Reason = [Started], Message = [Started container filler-pod-5e347677-bb9f-4b04-8fa6-99582031af1f] 11/15/23 15:47:04.361
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-71972d8e-3cd1-43eb-952c-b97dad0f9854.1797d67e4bd40235], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3486/filler-pod-71972d8e-3cd1-43eb-952c-b97dad0f9854 to 10.15.40.115] 11/15/23 15:47:04.361
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-71972d8e-3cd1-43eb-952c-b97dad0f9854.1797d67e9289576f], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 11/15/23 15:47:04.362
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-71972d8e-3cd1-43eb-952c-b97dad0f9854.1797d67e9404acb7], Reason = [Created], Message = [Created container filler-pod-71972d8e-3cd1-43eb-952c-b97dad0f9854] 11/15/23 15:47:04.362
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-71972d8e-3cd1-43eb-952c-b97dad0f9854.1797d67e9f3ac13e], Reason = [Started], Message = [Started container filler-pod-71972d8e-3cd1-43eb-952c-b97dad0f9854] 11/15/23 15:47:04.362
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1797d67ec9368a80], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] 11/15/23 15:47:04.417
STEP: removing the label node off the node 10.15.40.106 11/15/23 15:47:05.411
STEP: verifying the node doesn't have the label node 11/15/23 15:47:05.463
STEP: removing the label node off the node 10.15.40.114 11/15/23 15:47:05.481
STEP: verifying the node doesn't have the label node 11/15/23 15:47:05.539
STEP: removing the label node off the node 10.15.40.115 11/15/23 15:47:05.558
STEP: verifying the node doesn't have the label node 11/15/23 15:47:05.613
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 15:47:05.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-3486" for this suite. 11/15/23 15:47:05.659
------------------------------
• [4.106 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:47:01.58
    Nov 15 15:47:01.580: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename sched-pred 11/15/23 15:47:01.582
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:47:01.648
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:47:01.662
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Nov 15 15:47:01.678: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Nov 15 15:47:01.730: INFO: Waiting for terminating namespaces to be deleted...
    Nov 15 15:47:01.746: INFO: 
    Logging pods the apiserver thinks is on node 10.15.40.106 before test
    Nov 15 15:47:01.800: INFO: ibm-cloud-provider-ip-163-109-74-98-5d59656977-bgllw from ibm-system started at 2023-11-15 15:39:54 +0000 UTC (1 container statuses recorded)
    Nov 15 15:47:01.800: INFO: 	Container ibm-cloud-provider-ip-163-109-74-98 ready: true, restart count 0
    Nov 15 15:47:01.800: INFO: calico-kube-controllers-7847f7647d-srqd4 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 15:47:01.800: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Nov 15 15:47:01.801: INFO: calico-node-67csc from kube-system started at 2023-11-15 13:13:21 +0000 UTC (1 container statuses recorded)
    Nov 15 15:47:01.801: INFO: 	Container calico-node ready: true, restart count 0
    Nov 15 15:47:01.801: INFO: calico-typha-5b5db87f55-nwmh4 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 15:47:01.801: INFO: 	Container calico-typha ready: true, restart count 0
    Nov 15 15:47:01.802: INFO: coredns-5845f98d4-2lnrf from kube-system started at 2023-11-15 13:23:15 +0000 UTC (1 container statuses recorded)
    Nov 15 15:47:01.802: INFO: 	Container coredns ready: true, restart count 0
    Nov 15 15:47:01.802: INFO: coredns-autoscaler-85f4bdddf6-qwxmw from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 15:47:01.802: INFO: 	Container autoscaler ready: true, restart count 0
    Nov 15 15:47:01.802: INFO: dashboard-metrics-scraper-7cf679fbdf-frv7t from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 15:47:01.803: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Nov 15 15:47:01.803: INFO: ibm-file-plugin-557f875d5f-jrfnv from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 15:47:01.803: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    Nov 15 15:47:01.803: INFO: ibm-keepalived-watcher-dqknf from kube-system started at 2023-11-15 13:13:21 +0000 UTC (1 container statuses recorded)
    Nov 15 15:47:01.804: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Nov 15 15:47:01.804: INFO: ibm-master-proxy-static-10.15.40.106 from kube-system started at 2023-11-15 13:13:08 +0000 UTC (2 container statuses recorded)
    Nov 15 15:47:01.804: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Nov 15 15:47:01.804: INFO: 	Container pause ready: true, restart count 0
    Nov 15 15:47:01.804: INFO: ibm-storage-watcher-7d897847f4-k64t5 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 15:47:01.804: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    Nov 15 15:47:01.805: INFO: ibmcloud-block-storage-driver-f97nm from kube-system started at 2023-11-15 13:13:30 +0000 UTC (1 container statuses recorded)
    Nov 15 15:47:01.805: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Nov 15 15:47:01.805: INFO: ibmcloud-block-storage-plugin-5bd59f7b48-2dxcj from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 15:47:01.805: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    Nov 15 15:47:01.805: INFO: ingress-cluster-healthcheck-5985f966bb-jgjx8 from kube-system started at 2023-11-15 15:39:54 +0000 UTC (1 container statuses recorded)
    Nov 15 15:47:01.805: INFO: 	Container ingress-cluster-healthcheck ready: true, restart count 0
    Nov 15 15:47:01.806: INFO: konnectivity-agent-5brpz from kube-system started at 2023-11-15 13:22:38 +0000 UTC (1 container statuses recorded)
    Nov 15 15:47:01.806: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Nov 15 15:47:01.807: INFO: kubernetes-dashboard-5ccdc9cbb8-wbmfz from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 15:47:01.807: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Nov 15 15:47:01.807: INFO: metrics-server-7cbd9c9b48-8tr74 from kube-system started at 2023-11-15 15:39:54 +0000 UTC (3 container statuses recorded)
    Nov 15 15:47:01.807: INFO: 	Container config-watcher ready: true, restart count 0
    Nov 15 15:47:01.808: INFO: 	Container metrics-server ready: true, restart count 0
    Nov 15 15:47:01.808: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Nov 15 15:47:01.808: INFO: public-crclac150z0u38f277tnpg-alb1-6df9445bb6-qw2bt from kube-system started at 2023-11-15 15:39:54 +0000 UTC (1 container statuses recorded)
    Nov 15 15:47:01.808: INFO: 	Container nginx-ingress ready: true, restart count 0
    Nov 15 15:47:01.808: INFO: snapshot-controller-6db47fc545-5nnph from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 15:47:01.808: INFO: 	Container snapshot-controller ready: true, restart count 0
    Nov 15 15:47:01.808: INFO: snapshot-controller-6db47fc545-h4q49 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 15:47:01.809: INFO: 	Container snapshot-controller ready: true, restart count 0
    Nov 15 15:47:01.809: INFO: snapshot-controller-6db47fc545-khkb2 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 15:47:01.809: INFO: 	Container snapshot-controller ready: true, restart count 0
    Nov 15 15:47:01.809: INFO: sonobuoy-systemd-logs-daemon-set-eabb9aa298c743f7-zxdnv from sonobuoy started at 2023-11-15 15:24:45 +0000 UTC (2 container statuses recorded)
    Nov 15 15:47:01.809: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 15 15:47:01.809: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 15 15:47:01.810: INFO: 
    Logging pods the apiserver thinks is on node 10.15.40.114 before test
    Nov 15 15:47:01.854: INFO: ibm-cloud-provider-ip-163-109-74-98-5d59656977-wxlx6 from ibm-system started at 2023-11-15 13:18:32 +0000 UTC (1 container statuses recorded)
    Nov 15 15:47:01.854: INFO: 	Container ibm-cloud-provider-ip-163-109-74-98 ready: true, restart count 0
    Nov 15 15:47:01.854: INFO: calico-node-wqvj8 from kube-system started at 2023-11-15 13:13:28 +0000 UTC (1 container statuses recorded)
    Nov 15 15:47:01.854: INFO: 	Container calico-node ready: true, restart count 0
    Nov 15 15:47:01.855: INFO: calico-typha-5b5db87f55-4qv4c from kube-system started at 2023-11-15 13:13:53 +0000 UTC (1 container statuses recorded)
    Nov 15 15:47:01.855: INFO: 	Container calico-typha ready: true, restart count 0
    Nov 15 15:47:01.855: INFO: coredns-5845f98d4-6cld8 from kube-system started at 2023-11-15 13:23:15 +0000 UTC (1 container statuses recorded)
    Nov 15 15:47:01.855: INFO: 	Container coredns ready: true, restart count 0
    Nov 15 15:47:01.855: INFO: ibm-keepalived-watcher-xsslz from kube-system started at 2023-11-15 13:13:28 +0000 UTC (1 container statuses recorded)
    Nov 15 15:47:01.855: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Nov 15 15:47:01.855: INFO: ibm-master-proxy-static-10.15.40.114 from kube-system started at 2023-11-15 13:13:15 +0000 UTC (2 container statuses recorded)
    Nov 15 15:47:01.855: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Nov 15 15:47:01.855: INFO: 	Container pause ready: true, restart count 0
    Nov 15 15:47:01.856: INFO: ibmcloud-block-storage-driver-4txx8 from kube-system started at 2023-11-15 13:13:37 +0000 UTC (1 container statuses recorded)
    Nov 15 15:47:01.856: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Nov 15 15:47:01.856: INFO: konnectivity-agent-zr4zl from kube-system started at 2023-11-15 13:22:41 +0000 UTC (1 container statuses recorded)
    Nov 15 15:47:01.856: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Nov 15 15:47:01.856: INFO: metrics-server-7cbd9c9b48-cnskr from kube-system started at 2023-11-15 13:56:24 +0000 UTC (3 container statuses recorded)
    Nov 15 15:47:01.856: INFO: 	Container config-watcher ready: true, restart count 0
    Nov 15 15:47:01.856: INFO: 	Container metrics-server ready: true, restart count 0
    Nov 15 15:47:01.856: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Nov 15 15:47:01.856: INFO: public-crclac150z0u38f277tnpg-alb1-6df9445bb6-jkpcd from kube-system started at 2023-11-15 13:23:30 +0000 UTC (1 container statuses recorded)
    Nov 15 15:47:01.856: INFO: 	Container nginx-ingress ready: true, restart count 0
    Nov 15 15:47:01.856: INFO: sonobuoy-e2e-job-3a51e32fa56b4156 from sonobuoy started at 2023-11-15 15:24:45 +0000 UTC (2 container statuses recorded)
    Nov 15 15:47:01.856: INFO: 	Container e2e ready: true, restart count 0
    Nov 15 15:47:01.856: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 15 15:47:01.856: INFO: sonobuoy-systemd-logs-daemon-set-eabb9aa298c743f7-sxg5b from sonobuoy started at 2023-11-15 15:24:45 +0000 UTC (2 container statuses recorded)
    Nov 15 15:47:01.856: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 15 15:47:01.857: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 15 15:47:01.857: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-11-15 13:15:59 +0000 UTC (1 container statuses recorded)
    Nov 15 15:47:01.857: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
    Nov 15 15:47:01.857: INFO: 
    Logging pods the apiserver thinks is on node 10.15.40.115 before test
    Nov 15 15:47:01.912: INFO: calico-node-gknnt from kube-system started at 2023-11-15 13:13:40 +0000 UTC (1 container statuses recorded)
    Nov 15 15:47:01.912: INFO: 	Container calico-node ready: true, restart count 0
    Nov 15 15:47:01.912: INFO: calico-typha-5b5db87f55-fchsg from kube-system started at 2023-11-15 15:40:02 +0000 UTC (1 container statuses recorded)
    Nov 15 15:47:01.912: INFO: 	Container calico-typha ready: true, restart count 0
    Nov 15 15:47:01.912: INFO: coredns-5845f98d4-rt4sq from kube-system started at 2023-11-15 15:39:54 +0000 UTC (1 container statuses recorded)
    Nov 15 15:47:01.912: INFO: 	Container coredns ready: true, restart count 0
    Nov 15 15:47:01.912: INFO: ibm-keepalived-watcher-75lpq from kube-system started at 2023-11-15 13:13:40 +0000 UTC (1 container statuses recorded)
    Nov 15 15:47:01.912: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Nov 15 15:47:01.912: INFO: ibm-master-proxy-static-10.15.40.115 from kube-system started at 2023-11-15 13:13:28 +0000 UTC (2 container statuses recorded)
    Nov 15 15:47:01.912: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Nov 15 15:47:01.912: INFO: 	Container pause ready: true, restart count 0
    Nov 15 15:47:01.912: INFO: ibmcloud-block-storage-driver-gbqth from kube-system started at 2023-11-15 13:13:49 +0000 UTC (1 container statuses recorded)
    Nov 15 15:47:01.912: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Nov 15 15:47:01.912: INFO: konnectivity-agent-9pkld from kube-system started at 2023-11-15 13:22:45 +0000 UTC (1 container statuses recorded)
    Nov 15 15:47:01.912: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Nov 15 15:47:01.912: INFO: sonobuoy from sonobuoy started at 2023-11-15 15:24:39 +0000 UTC (1 container statuses recorded)
    Nov 15 15:47:01.912: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Nov 15 15:47:01.912: INFO: sonobuoy-systemd-logs-daemon-set-eabb9aa298c743f7-hmfn7 from sonobuoy started at 2023-11-15 15:24:45 +0000 UTC (2 container statuses recorded)
    Nov 15 15:47:01.912: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 15 15:47:01.912: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node 10.15.40.106 11/15/23 15:47:01.988
    STEP: verifying the node has the label node 10.15.40.114 11/15/23 15:47:02.057
    STEP: verifying the node has the label node 10.15.40.115 11/15/23 15:47:02.111
    Nov 15 15:47:02.174: INFO: Pod ibm-cloud-provider-ip-163-109-74-98-5d59656977-bgllw requesting resource cpu=5m on Node 10.15.40.106
    Nov 15 15:47:02.174: INFO: Pod ibm-cloud-provider-ip-163-109-74-98-5d59656977-wxlx6 requesting resource cpu=5m on Node 10.15.40.114
    Nov 15 15:47:02.174: INFO: Pod calico-kube-controllers-7847f7647d-srqd4 requesting resource cpu=10m on Node 10.15.40.106
    Nov 15 15:47:02.174: INFO: Pod calico-node-67csc requesting resource cpu=250m on Node 10.15.40.106
    Nov 15 15:47:02.174: INFO: Pod calico-node-gknnt requesting resource cpu=250m on Node 10.15.40.115
    Nov 15 15:47:02.174: INFO: Pod calico-node-wqvj8 requesting resource cpu=250m on Node 10.15.40.114
    Nov 15 15:47:02.174: INFO: Pod calico-typha-5b5db87f55-4qv4c requesting resource cpu=250m on Node 10.15.40.114
    Nov 15 15:47:02.175: INFO: Pod calico-typha-5b5db87f55-fchsg requesting resource cpu=250m on Node 10.15.40.115
    Nov 15 15:47:02.175: INFO: Pod calico-typha-5b5db87f55-nwmh4 requesting resource cpu=250m on Node 10.15.40.106
    Nov 15 15:47:02.175: INFO: Pod coredns-5845f98d4-2lnrf requesting resource cpu=100m on Node 10.15.40.106
    Nov 15 15:47:02.175: INFO: Pod coredns-5845f98d4-6cld8 requesting resource cpu=100m on Node 10.15.40.114
    Nov 15 15:47:02.175: INFO: Pod coredns-5845f98d4-rt4sq requesting resource cpu=100m on Node 10.15.40.115
    Nov 15 15:47:02.175: INFO: Pod coredns-autoscaler-85f4bdddf6-qwxmw requesting resource cpu=1m on Node 10.15.40.106
    Nov 15 15:47:02.175: INFO: Pod dashboard-metrics-scraper-7cf679fbdf-frv7t requesting resource cpu=15m on Node 10.15.40.106
    Nov 15 15:47:02.175: INFO: Pod ibm-file-plugin-557f875d5f-jrfnv requesting resource cpu=50m on Node 10.15.40.106
    Nov 15 15:47:02.175: INFO: Pod ibm-keepalived-watcher-75lpq requesting resource cpu=5m on Node 10.15.40.115
    Nov 15 15:47:02.175: INFO: Pod ibm-keepalived-watcher-dqknf requesting resource cpu=5m on Node 10.15.40.106
    Nov 15 15:47:02.175: INFO: Pod ibm-keepalived-watcher-xsslz requesting resource cpu=5m on Node 10.15.40.114
    Nov 15 15:47:02.175: INFO: Pod ibm-master-proxy-static-10.15.40.106 requesting resource cpu=25m on Node 10.15.40.106
    Nov 15 15:47:02.175: INFO: Pod ibm-master-proxy-static-10.15.40.114 requesting resource cpu=25m on Node 10.15.40.114
    Nov 15 15:47:02.176: INFO: Pod ibm-master-proxy-static-10.15.40.115 requesting resource cpu=25m on Node 10.15.40.115
    Nov 15 15:47:02.176: INFO: Pod ibm-storage-watcher-7d897847f4-k64t5 requesting resource cpu=50m on Node 10.15.40.106
    Nov 15 15:47:02.176: INFO: Pod ibmcloud-block-storage-driver-4txx8 requesting resource cpu=50m on Node 10.15.40.114
    Nov 15 15:47:02.176: INFO: Pod ibmcloud-block-storage-driver-f97nm requesting resource cpu=50m on Node 10.15.40.106
    Nov 15 15:47:02.176: INFO: Pod ibmcloud-block-storage-driver-gbqth requesting resource cpu=50m on Node 10.15.40.115
    Nov 15 15:47:02.176: INFO: Pod ibmcloud-block-storage-plugin-5bd59f7b48-2dxcj requesting resource cpu=50m on Node 10.15.40.106
    Nov 15 15:47:02.176: INFO: Pod ingress-cluster-healthcheck-5985f966bb-jgjx8 requesting resource cpu=5m on Node 10.15.40.106
    Nov 15 15:47:02.176: INFO: Pod konnectivity-agent-5brpz requesting resource cpu=10m on Node 10.15.40.106
    Nov 15 15:47:02.176: INFO: Pod konnectivity-agent-9pkld requesting resource cpu=10m on Node 10.15.40.115
    Nov 15 15:47:02.177: INFO: Pod konnectivity-agent-zr4zl requesting resource cpu=10m on Node 10.15.40.114
    Nov 15 15:47:02.177: INFO: Pod kubernetes-dashboard-5ccdc9cbb8-wbmfz requesting resource cpu=50m on Node 10.15.40.106
    Nov 15 15:47:02.177: INFO: Pod metrics-server-7cbd9c9b48-8tr74 requesting resource cpu=126m on Node 10.15.40.106
    Nov 15 15:47:02.177: INFO: Pod metrics-server-7cbd9c9b48-cnskr requesting resource cpu=126m on Node 10.15.40.114
    Nov 15 15:47:02.177: INFO: Pod public-crclac150z0u38f277tnpg-alb1-6df9445bb6-jkpcd requesting resource cpu=20m on Node 10.15.40.114
    Nov 15 15:47:02.177: INFO: Pod public-crclac150z0u38f277tnpg-alb1-6df9445bb6-qw2bt requesting resource cpu=20m on Node 10.15.40.106
    Nov 15 15:47:02.177: INFO: Pod snapshot-controller-6db47fc545-5nnph requesting resource cpu=10m on Node 10.15.40.106
    Nov 15 15:47:02.177: INFO: Pod snapshot-controller-6db47fc545-h4q49 requesting resource cpu=10m on Node 10.15.40.106
    Nov 15 15:47:02.177: INFO: Pod snapshot-controller-6db47fc545-khkb2 requesting resource cpu=10m on Node 10.15.40.106
    Nov 15 15:47:02.178: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.15.40.115
    Nov 15 15:47:02.178: INFO: Pod sonobuoy-e2e-job-3a51e32fa56b4156 requesting resource cpu=0m on Node 10.15.40.114
    Nov 15 15:47:02.178: INFO: Pod sonobuoy-systemd-logs-daemon-set-eabb9aa298c743f7-hmfn7 requesting resource cpu=0m on Node 10.15.40.115
    Nov 15 15:47:02.178: INFO: Pod sonobuoy-systemd-logs-daemon-set-eabb9aa298c743f7-sxg5b requesting resource cpu=0m on Node 10.15.40.114
    Nov 15 15:47:02.179: INFO: Pod sonobuoy-systemd-logs-daemon-set-eabb9aa298c743f7-zxdnv requesting resource cpu=0m on Node 10.15.40.106
    Nov 15 15:47:02.179: INFO: Pod test-k8s-e2e-pvg-master-verification requesting resource cpu=0m on Node 10.15.40.114
    STEP: Starting Pods to consume most of the cluster CPU. 11/15/23 15:47:02.179
    Nov 15 15:47:02.179: INFO: Creating a pod which consumes cpu=1965m on Node 10.15.40.106
    Nov 15 15:47:02.224: INFO: Creating a pod which consumes cpu=2148m on Node 10.15.40.114
    Nov 15 15:47:02.243: INFO: Creating a pod which consumes cpu=2254m on Node 10.15.40.115
    Nov 15 15:47:02.265: INFO: Waiting up to 5m0s for pod "filler-pod-1abd25d1-2e6a-4621-9a12-151991f4b802" in namespace "sched-pred-3486" to be "running"
    Nov 15 15:47:02.290: INFO: Pod "filler-pod-1abd25d1-2e6a-4621-9a12-151991f4b802": Phase="Pending", Reason="", readiness=false. Elapsed: 24.882149ms
    Nov 15 15:47:04.306: INFO: Pod "filler-pod-1abd25d1-2e6a-4621-9a12-151991f4b802": Phase="Running", Reason="", readiness=true. Elapsed: 2.04103891s
    Nov 15 15:47:04.306: INFO: Pod "filler-pod-1abd25d1-2e6a-4621-9a12-151991f4b802" satisfied condition "running"
    Nov 15 15:47:04.306: INFO: Waiting up to 5m0s for pod "filler-pod-5e347677-bb9f-4b04-8fa6-99582031af1f" in namespace "sched-pred-3486" to be "running"
    Nov 15 15:47:04.321: INFO: Pod "filler-pod-5e347677-bb9f-4b04-8fa6-99582031af1f": Phase="Running", Reason="", readiness=true. Elapsed: 14.98907ms
    Nov 15 15:47:04.321: INFO: Pod "filler-pod-5e347677-bb9f-4b04-8fa6-99582031af1f" satisfied condition "running"
    Nov 15 15:47:04.322: INFO: Waiting up to 5m0s for pod "filler-pod-71972d8e-3cd1-43eb-952c-b97dad0f9854" in namespace "sched-pred-3486" to be "running"
    Nov 15 15:47:04.337: INFO: Pod "filler-pod-71972d8e-3cd1-43eb-952c-b97dad0f9854": Phase="Running", Reason="", readiness=true. Elapsed: 15.153772ms
    Nov 15 15:47:04.337: INFO: Pod "filler-pod-71972d8e-3cd1-43eb-952c-b97dad0f9854" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 11/15/23 15:47:04.338
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-1abd25d1-2e6a-4621-9a12-151991f4b802.1797d67e49937cec], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3486/filler-pod-1abd25d1-2e6a-4621-9a12-151991f4b802 to 10.15.40.106] 11/15/23 15:47:04.359
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-1abd25d1-2e6a-4621-9a12-151991f4b802.1797d67e8de28a4b], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 11/15/23 15:47:04.359
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-1abd25d1-2e6a-4621-9a12-151991f4b802.1797d67e8f89c0c4], Reason = [Created], Message = [Created container filler-pod-1abd25d1-2e6a-4621-9a12-151991f4b802] 11/15/23 15:47:04.359
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-1abd25d1-2e6a-4621-9a12-151991f4b802.1797d67e998c615a], Reason = [Started], Message = [Started container filler-pod-1abd25d1-2e6a-4621-9a12-151991f4b802] 11/15/23 15:47:04.36
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5e347677-bb9f-4b04-8fa6-99582031af1f.1797d67e4a60ee2f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3486/filler-pod-5e347677-bb9f-4b04-8fa6-99582031af1f to 10.15.40.114] 11/15/23 15:47:04.36
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5e347677-bb9f-4b04-8fa6-99582031af1f.1797d67e8ea6f493], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 11/15/23 15:47:04.361
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5e347677-bb9f-4b04-8fa6-99582031af1f.1797d67e8fe7e882], Reason = [Created], Message = [Created container filler-pod-5e347677-bb9f-4b04-8fa6-99582031af1f] 11/15/23 15:47:04.361
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5e347677-bb9f-4b04-8fa6-99582031af1f.1797d67e9833c72a], Reason = [Started], Message = [Started container filler-pod-5e347677-bb9f-4b04-8fa6-99582031af1f] 11/15/23 15:47:04.361
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-71972d8e-3cd1-43eb-952c-b97dad0f9854.1797d67e4bd40235], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3486/filler-pod-71972d8e-3cd1-43eb-952c-b97dad0f9854 to 10.15.40.115] 11/15/23 15:47:04.361
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-71972d8e-3cd1-43eb-952c-b97dad0f9854.1797d67e9289576f], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 11/15/23 15:47:04.362
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-71972d8e-3cd1-43eb-952c-b97dad0f9854.1797d67e9404acb7], Reason = [Created], Message = [Created container filler-pod-71972d8e-3cd1-43eb-952c-b97dad0f9854] 11/15/23 15:47:04.362
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-71972d8e-3cd1-43eb-952c-b97dad0f9854.1797d67e9f3ac13e], Reason = [Started], Message = [Started container filler-pod-71972d8e-3cd1-43eb-952c-b97dad0f9854] 11/15/23 15:47:04.362
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.1797d67ec9368a80], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] 11/15/23 15:47:04.417
    STEP: removing the label node off the node 10.15.40.106 11/15/23 15:47:05.411
    STEP: verifying the node doesn't have the label node 11/15/23 15:47:05.463
    STEP: removing the label node off the node 10.15.40.114 11/15/23 15:47:05.481
    STEP: verifying the node doesn't have the label node 11/15/23 15:47:05.539
    STEP: removing the label node off the node 10.15.40.115 11/15/23 15:47:05.558
    STEP: verifying the node doesn't have the label node 11/15/23 15:47:05.613
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:47:05.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-3486" for this suite. 11/15/23 15:47:05.659
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:47:05.696
Nov 15 15:47:05.696: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename secrets 11/15/23 15:47:05.699
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:47:05.755
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:47:05.772
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-60c6a9f9-e80b-4e74-a97a-09977e75cffb 11/15/23 15:47:05.787
STEP: Creating a pod to test consume secrets 11/15/23 15:47:05.809
Nov 15 15:47:05.842: INFO: Waiting up to 5m0s for pod "pod-secrets-25cb9711-38c4-40d5-92fe-6c1c119eef14" in namespace "secrets-2586" to be "Succeeded or Failed"
Nov 15 15:47:05.857: INFO: Pod "pod-secrets-25cb9711-38c4-40d5-92fe-6c1c119eef14": Phase="Pending", Reason="", readiness=false. Elapsed: 14.898369ms
Nov 15 15:47:07.879: INFO: Pod "pod-secrets-25cb9711-38c4-40d5-92fe-6c1c119eef14": Phase="Running", Reason="", readiness=true. Elapsed: 2.036677699s
Nov 15 15:47:09.876: INFO: Pod "pod-secrets-25cb9711-38c4-40d5-92fe-6c1c119eef14": Phase="Running", Reason="", readiness=false. Elapsed: 4.033611311s
Nov 15 15:47:11.873: INFO: Pod "pod-secrets-25cb9711-38c4-40d5-92fe-6c1c119eef14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031134097s
STEP: Saw pod success 11/15/23 15:47:11.873
Nov 15 15:47:11.874: INFO: Pod "pod-secrets-25cb9711-38c4-40d5-92fe-6c1c119eef14" satisfied condition "Succeeded or Failed"
Nov 15 15:47:11.889: INFO: Trying to get logs from node 10.15.40.115 pod pod-secrets-25cb9711-38c4-40d5-92fe-6c1c119eef14 container secret-volume-test: <nil>
STEP: delete the pod 11/15/23 15:47:12.037
Nov 15 15:47:12.080: INFO: Waiting for pod pod-secrets-25cb9711-38c4-40d5-92fe-6c1c119eef14 to disappear
Nov 15 15:47:12.093: INFO: Pod pod-secrets-25cb9711-38c4-40d5-92fe-6c1c119eef14 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 15 15:47:12.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2586" for this suite. 11/15/23 15:47:12.125
------------------------------
• [SLOW TEST] [6.454 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:47:05.696
    Nov 15 15:47:05.696: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename secrets 11/15/23 15:47:05.699
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:47:05.755
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:47:05.772
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-60c6a9f9-e80b-4e74-a97a-09977e75cffb 11/15/23 15:47:05.787
    STEP: Creating a pod to test consume secrets 11/15/23 15:47:05.809
    Nov 15 15:47:05.842: INFO: Waiting up to 5m0s for pod "pod-secrets-25cb9711-38c4-40d5-92fe-6c1c119eef14" in namespace "secrets-2586" to be "Succeeded or Failed"
    Nov 15 15:47:05.857: INFO: Pod "pod-secrets-25cb9711-38c4-40d5-92fe-6c1c119eef14": Phase="Pending", Reason="", readiness=false. Elapsed: 14.898369ms
    Nov 15 15:47:07.879: INFO: Pod "pod-secrets-25cb9711-38c4-40d5-92fe-6c1c119eef14": Phase="Running", Reason="", readiness=true. Elapsed: 2.036677699s
    Nov 15 15:47:09.876: INFO: Pod "pod-secrets-25cb9711-38c4-40d5-92fe-6c1c119eef14": Phase="Running", Reason="", readiness=false. Elapsed: 4.033611311s
    Nov 15 15:47:11.873: INFO: Pod "pod-secrets-25cb9711-38c4-40d5-92fe-6c1c119eef14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031134097s
    STEP: Saw pod success 11/15/23 15:47:11.873
    Nov 15 15:47:11.874: INFO: Pod "pod-secrets-25cb9711-38c4-40d5-92fe-6c1c119eef14" satisfied condition "Succeeded or Failed"
    Nov 15 15:47:11.889: INFO: Trying to get logs from node 10.15.40.115 pod pod-secrets-25cb9711-38c4-40d5-92fe-6c1c119eef14 container secret-volume-test: <nil>
    STEP: delete the pod 11/15/23 15:47:12.037
    Nov 15 15:47:12.080: INFO: Waiting for pod pod-secrets-25cb9711-38c4-40d5-92fe-6c1c119eef14 to disappear
    Nov 15 15:47:12.093: INFO: Pod pod-secrets-25cb9711-38c4-40d5-92fe-6c1c119eef14 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:47:12.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2586" for this suite. 11/15/23 15:47:12.125
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:47:12.151
Nov 15 15:47:12.151: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename subpath 11/15/23 15:47:12.153
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:47:12.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:47:12.221
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 11/15/23 15:47:12.239
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-wpjh 11/15/23 15:47:12.277
STEP: Creating a pod to test atomic-volume-subpath 11/15/23 15:47:12.277
Nov 15 15:47:12.313: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-wpjh" in namespace "subpath-9824" to be "Succeeded or Failed"
Nov 15 15:47:12.328: INFO: Pod "pod-subpath-test-projected-wpjh": Phase="Pending", Reason="", readiness=false. Elapsed: 14.291834ms
Nov 15 15:47:14.344: INFO: Pod "pod-subpath-test-projected-wpjh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030945659s
Nov 15 15:47:16.345: INFO: Pod "pod-subpath-test-projected-wpjh": Phase="Running", Reason="", readiness=true. Elapsed: 4.031532632s
Nov 15 15:47:18.350: INFO: Pod "pod-subpath-test-projected-wpjh": Phase="Running", Reason="", readiness=true. Elapsed: 6.036954688s
Nov 15 15:47:20.344: INFO: Pod "pod-subpath-test-projected-wpjh": Phase="Running", Reason="", readiness=true. Elapsed: 8.031126937s
Nov 15 15:47:22.343: INFO: Pod "pod-subpath-test-projected-wpjh": Phase="Running", Reason="", readiness=true. Elapsed: 10.029624642s
Nov 15 15:47:24.346: INFO: Pod "pod-subpath-test-projected-wpjh": Phase="Running", Reason="", readiness=true. Elapsed: 12.032583553s
Nov 15 15:47:26.343: INFO: Pod "pod-subpath-test-projected-wpjh": Phase="Running", Reason="", readiness=true. Elapsed: 14.03002476s
Nov 15 15:47:28.346: INFO: Pod "pod-subpath-test-projected-wpjh": Phase="Running", Reason="", readiness=true. Elapsed: 16.033012144s
Nov 15 15:47:30.346: INFO: Pod "pod-subpath-test-projected-wpjh": Phase="Running", Reason="", readiness=true. Elapsed: 18.032859122s
Nov 15 15:47:32.344: INFO: Pod "pod-subpath-test-projected-wpjh": Phase="Running", Reason="", readiness=true. Elapsed: 20.030816703s
Nov 15 15:47:34.348: INFO: Pod "pod-subpath-test-projected-wpjh": Phase="Running", Reason="", readiness=true. Elapsed: 22.03449978s
Nov 15 15:47:36.344: INFO: Pod "pod-subpath-test-projected-wpjh": Phase="Running", Reason="", readiness=false. Elapsed: 24.030872899s
Nov 15 15:47:38.344: INFO: Pod "pod-subpath-test-projected-wpjh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.030872034s
STEP: Saw pod success 11/15/23 15:47:38.344
Nov 15 15:47:38.344: INFO: Pod "pod-subpath-test-projected-wpjh" satisfied condition "Succeeded or Failed"
Nov 15 15:47:38.359: INFO: Trying to get logs from node 10.15.40.115 pod pod-subpath-test-projected-wpjh container test-container-subpath-projected-wpjh: <nil>
STEP: delete the pod 11/15/23 15:47:38.433
Nov 15 15:47:38.481: INFO: Waiting for pod pod-subpath-test-projected-wpjh to disappear
Nov 15 15:47:38.497: INFO: Pod pod-subpath-test-projected-wpjh no longer exists
STEP: Deleting pod pod-subpath-test-projected-wpjh 11/15/23 15:47:38.498
Nov 15 15:47:38.498: INFO: Deleting pod "pod-subpath-test-projected-wpjh" in namespace "subpath-9824"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Nov 15 15:47:38.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-9824" for this suite. 11/15/23 15:47:38.557
------------------------------
• [SLOW TEST] [26.431 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:47:12.151
    Nov 15 15:47:12.151: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename subpath 11/15/23 15:47:12.153
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:47:12.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:47:12.221
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 11/15/23 15:47:12.239
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-wpjh 11/15/23 15:47:12.277
    STEP: Creating a pod to test atomic-volume-subpath 11/15/23 15:47:12.277
    Nov 15 15:47:12.313: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-wpjh" in namespace "subpath-9824" to be "Succeeded or Failed"
    Nov 15 15:47:12.328: INFO: Pod "pod-subpath-test-projected-wpjh": Phase="Pending", Reason="", readiness=false. Elapsed: 14.291834ms
    Nov 15 15:47:14.344: INFO: Pod "pod-subpath-test-projected-wpjh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030945659s
    Nov 15 15:47:16.345: INFO: Pod "pod-subpath-test-projected-wpjh": Phase="Running", Reason="", readiness=true. Elapsed: 4.031532632s
    Nov 15 15:47:18.350: INFO: Pod "pod-subpath-test-projected-wpjh": Phase="Running", Reason="", readiness=true. Elapsed: 6.036954688s
    Nov 15 15:47:20.344: INFO: Pod "pod-subpath-test-projected-wpjh": Phase="Running", Reason="", readiness=true. Elapsed: 8.031126937s
    Nov 15 15:47:22.343: INFO: Pod "pod-subpath-test-projected-wpjh": Phase="Running", Reason="", readiness=true. Elapsed: 10.029624642s
    Nov 15 15:47:24.346: INFO: Pod "pod-subpath-test-projected-wpjh": Phase="Running", Reason="", readiness=true. Elapsed: 12.032583553s
    Nov 15 15:47:26.343: INFO: Pod "pod-subpath-test-projected-wpjh": Phase="Running", Reason="", readiness=true. Elapsed: 14.03002476s
    Nov 15 15:47:28.346: INFO: Pod "pod-subpath-test-projected-wpjh": Phase="Running", Reason="", readiness=true. Elapsed: 16.033012144s
    Nov 15 15:47:30.346: INFO: Pod "pod-subpath-test-projected-wpjh": Phase="Running", Reason="", readiness=true. Elapsed: 18.032859122s
    Nov 15 15:47:32.344: INFO: Pod "pod-subpath-test-projected-wpjh": Phase="Running", Reason="", readiness=true. Elapsed: 20.030816703s
    Nov 15 15:47:34.348: INFO: Pod "pod-subpath-test-projected-wpjh": Phase="Running", Reason="", readiness=true. Elapsed: 22.03449978s
    Nov 15 15:47:36.344: INFO: Pod "pod-subpath-test-projected-wpjh": Phase="Running", Reason="", readiness=false. Elapsed: 24.030872899s
    Nov 15 15:47:38.344: INFO: Pod "pod-subpath-test-projected-wpjh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.030872034s
    STEP: Saw pod success 11/15/23 15:47:38.344
    Nov 15 15:47:38.344: INFO: Pod "pod-subpath-test-projected-wpjh" satisfied condition "Succeeded or Failed"
    Nov 15 15:47:38.359: INFO: Trying to get logs from node 10.15.40.115 pod pod-subpath-test-projected-wpjh container test-container-subpath-projected-wpjh: <nil>
    STEP: delete the pod 11/15/23 15:47:38.433
    Nov 15 15:47:38.481: INFO: Waiting for pod pod-subpath-test-projected-wpjh to disappear
    Nov 15 15:47:38.497: INFO: Pod pod-subpath-test-projected-wpjh no longer exists
    STEP: Deleting pod pod-subpath-test-projected-wpjh 11/15/23 15:47:38.498
    Nov 15 15:47:38.498: INFO: Deleting pod "pod-subpath-test-projected-wpjh" in namespace "subpath-9824"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:47:38.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-9824" for this suite. 11/15/23 15:47:38.557
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:47:38.597
Nov 15 15:47:38.597: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename sysctl 11/15/23 15:47:38.599
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:47:38.653
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:47:38.67
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 11/15/23 15:47:38.687
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Nov 15 15:47:38.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-6773" for this suite. 11/15/23 15:47:38.734
------------------------------
• [0.163 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:47:38.597
    Nov 15 15:47:38.597: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename sysctl 11/15/23 15:47:38.599
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:47:38.653
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:47:38.67
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 11/15/23 15:47:38.687
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:47:38.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-6773" for this suite. 11/15/23 15:47:38.734
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:47:38.766
Nov 15 15:47:38.767: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename discovery 11/15/23 15:47:38.768
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:47:38.817
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:47:38.832
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 11/15/23 15:47:38.858
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Nov 15 15:47:39.286: INFO: Checking APIGroup: apiregistration.k8s.io
Nov 15 15:47:39.293: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Nov 15 15:47:39.293: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Nov 15 15:47:39.293: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Nov 15 15:47:39.293: INFO: Checking APIGroup: apps
Nov 15 15:47:39.300: INFO: PreferredVersion.GroupVersion: apps/v1
Nov 15 15:47:39.300: INFO: Versions found [{apps/v1 v1}]
Nov 15 15:47:39.300: INFO: apps/v1 matches apps/v1
Nov 15 15:47:39.300: INFO: Checking APIGroup: events.k8s.io
Nov 15 15:47:39.307: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Nov 15 15:47:39.308: INFO: Versions found [{events.k8s.io/v1 v1}]
Nov 15 15:47:39.308: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Nov 15 15:47:39.308: INFO: Checking APIGroup: authentication.k8s.io
Nov 15 15:47:39.315: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Nov 15 15:47:39.315: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Nov 15 15:47:39.315: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Nov 15 15:47:39.315: INFO: Checking APIGroup: authorization.k8s.io
Nov 15 15:47:39.322: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Nov 15 15:47:39.322: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Nov 15 15:47:39.323: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Nov 15 15:47:39.323: INFO: Checking APIGroup: autoscaling
Nov 15 15:47:39.330: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Nov 15 15:47:39.330: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Nov 15 15:47:39.330: INFO: autoscaling/v2 matches autoscaling/v2
Nov 15 15:47:39.330: INFO: Checking APIGroup: batch
Nov 15 15:47:39.336: INFO: PreferredVersion.GroupVersion: batch/v1
Nov 15 15:47:39.337: INFO: Versions found [{batch/v1 v1}]
Nov 15 15:47:39.337: INFO: batch/v1 matches batch/v1
Nov 15 15:47:39.337: INFO: Checking APIGroup: certificates.k8s.io
Nov 15 15:47:39.345: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Nov 15 15:47:39.345: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Nov 15 15:47:39.345: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Nov 15 15:47:39.345: INFO: Checking APIGroup: networking.k8s.io
Nov 15 15:47:39.352: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Nov 15 15:47:39.352: INFO: Versions found [{networking.k8s.io/v1 v1}]
Nov 15 15:47:39.352: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Nov 15 15:47:39.352: INFO: Checking APIGroup: policy
Nov 15 15:47:39.358: INFO: PreferredVersion.GroupVersion: policy/v1
Nov 15 15:47:39.358: INFO: Versions found [{policy/v1 v1}]
Nov 15 15:47:39.359: INFO: policy/v1 matches policy/v1
Nov 15 15:47:39.359: INFO: Checking APIGroup: rbac.authorization.k8s.io
Nov 15 15:47:39.366: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Nov 15 15:47:39.366: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Nov 15 15:47:39.366: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Nov 15 15:47:39.366: INFO: Checking APIGroup: storage.k8s.io
Nov 15 15:47:39.374: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Nov 15 15:47:39.374: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Nov 15 15:47:39.374: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Nov 15 15:47:39.374: INFO: Checking APIGroup: admissionregistration.k8s.io
Nov 15 15:47:39.381: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Nov 15 15:47:39.381: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Nov 15 15:47:39.382: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Nov 15 15:47:39.382: INFO: Checking APIGroup: apiextensions.k8s.io
Nov 15 15:47:39.389: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Nov 15 15:47:39.389: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Nov 15 15:47:39.389: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Nov 15 15:47:39.389: INFO: Checking APIGroup: scheduling.k8s.io
Nov 15 15:47:39.396: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Nov 15 15:47:39.396: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Nov 15 15:47:39.396: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Nov 15 15:47:39.396: INFO: Checking APIGroup: coordination.k8s.io
Nov 15 15:47:39.403: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Nov 15 15:47:39.403: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Nov 15 15:47:39.403: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Nov 15 15:47:39.403: INFO: Checking APIGroup: node.k8s.io
Nov 15 15:47:39.410: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Nov 15 15:47:39.411: INFO: Versions found [{node.k8s.io/v1 v1}]
Nov 15 15:47:39.411: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Nov 15 15:47:39.411: INFO: Checking APIGroup: discovery.k8s.io
Nov 15 15:47:39.419: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Nov 15 15:47:39.419: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Nov 15 15:47:39.419: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Nov 15 15:47:39.420: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Nov 15 15:47:39.427: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Nov 15 15:47:39.427: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Nov 15 15:47:39.428: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Nov 15 15:47:39.429: INFO: Checking APIGroup: crd.projectcalico.org
Nov 15 15:47:39.436: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Nov 15 15:47:39.436: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Nov 15 15:47:39.436: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Nov 15 15:47:39.436: INFO: Checking APIGroup: snapshot.storage.k8s.io
Nov 15 15:47:39.444: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Nov 15 15:47:39.444: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
Nov 15 15:47:39.445: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Nov 15 15:47:39.445: INFO: Checking APIGroup: ibm.com
Nov 15 15:47:39.453: INFO: PreferredVersion.GroupVersion: ibm.com/v1alpha1
Nov 15 15:47:39.453: INFO: Versions found [{ibm.com/v1alpha1 v1alpha1}]
Nov 15 15:47:39.454: INFO: ibm.com/v1alpha1 matches ibm.com/v1alpha1
Nov 15 15:47:39.454: INFO: Checking APIGroup: metrics.k8s.io
Nov 15 15:47:39.461: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Nov 15 15:47:39.461: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Nov 15 15:47:39.461: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Nov 15 15:47:39.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-1092" for this suite. 11/15/23 15:47:39.487
------------------------------
• [0.746 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:47:38.766
    Nov 15 15:47:38.767: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename discovery 11/15/23 15:47:38.768
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:47:38.817
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:47:38.832
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 11/15/23 15:47:38.858
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Nov 15 15:47:39.286: INFO: Checking APIGroup: apiregistration.k8s.io
    Nov 15 15:47:39.293: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Nov 15 15:47:39.293: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Nov 15 15:47:39.293: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Nov 15 15:47:39.293: INFO: Checking APIGroup: apps
    Nov 15 15:47:39.300: INFO: PreferredVersion.GroupVersion: apps/v1
    Nov 15 15:47:39.300: INFO: Versions found [{apps/v1 v1}]
    Nov 15 15:47:39.300: INFO: apps/v1 matches apps/v1
    Nov 15 15:47:39.300: INFO: Checking APIGroup: events.k8s.io
    Nov 15 15:47:39.307: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Nov 15 15:47:39.308: INFO: Versions found [{events.k8s.io/v1 v1}]
    Nov 15 15:47:39.308: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Nov 15 15:47:39.308: INFO: Checking APIGroup: authentication.k8s.io
    Nov 15 15:47:39.315: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Nov 15 15:47:39.315: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Nov 15 15:47:39.315: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Nov 15 15:47:39.315: INFO: Checking APIGroup: authorization.k8s.io
    Nov 15 15:47:39.322: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Nov 15 15:47:39.322: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Nov 15 15:47:39.323: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Nov 15 15:47:39.323: INFO: Checking APIGroup: autoscaling
    Nov 15 15:47:39.330: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Nov 15 15:47:39.330: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Nov 15 15:47:39.330: INFO: autoscaling/v2 matches autoscaling/v2
    Nov 15 15:47:39.330: INFO: Checking APIGroup: batch
    Nov 15 15:47:39.336: INFO: PreferredVersion.GroupVersion: batch/v1
    Nov 15 15:47:39.337: INFO: Versions found [{batch/v1 v1}]
    Nov 15 15:47:39.337: INFO: batch/v1 matches batch/v1
    Nov 15 15:47:39.337: INFO: Checking APIGroup: certificates.k8s.io
    Nov 15 15:47:39.345: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Nov 15 15:47:39.345: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Nov 15 15:47:39.345: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Nov 15 15:47:39.345: INFO: Checking APIGroup: networking.k8s.io
    Nov 15 15:47:39.352: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Nov 15 15:47:39.352: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Nov 15 15:47:39.352: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Nov 15 15:47:39.352: INFO: Checking APIGroup: policy
    Nov 15 15:47:39.358: INFO: PreferredVersion.GroupVersion: policy/v1
    Nov 15 15:47:39.358: INFO: Versions found [{policy/v1 v1}]
    Nov 15 15:47:39.359: INFO: policy/v1 matches policy/v1
    Nov 15 15:47:39.359: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Nov 15 15:47:39.366: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Nov 15 15:47:39.366: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Nov 15 15:47:39.366: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Nov 15 15:47:39.366: INFO: Checking APIGroup: storage.k8s.io
    Nov 15 15:47:39.374: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Nov 15 15:47:39.374: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Nov 15 15:47:39.374: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Nov 15 15:47:39.374: INFO: Checking APIGroup: admissionregistration.k8s.io
    Nov 15 15:47:39.381: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Nov 15 15:47:39.381: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Nov 15 15:47:39.382: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Nov 15 15:47:39.382: INFO: Checking APIGroup: apiextensions.k8s.io
    Nov 15 15:47:39.389: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Nov 15 15:47:39.389: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Nov 15 15:47:39.389: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Nov 15 15:47:39.389: INFO: Checking APIGroup: scheduling.k8s.io
    Nov 15 15:47:39.396: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Nov 15 15:47:39.396: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Nov 15 15:47:39.396: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Nov 15 15:47:39.396: INFO: Checking APIGroup: coordination.k8s.io
    Nov 15 15:47:39.403: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Nov 15 15:47:39.403: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Nov 15 15:47:39.403: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Nov 15 15:47:39.403: INFO: Checking APIGroup: node.k8s.io
    Nov 15 15:47:39.410: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Nov 15 15:47:39.411: INFO: Versions found [{node.k8s.io/v1 v1}]
    Nov 15 15:47:39.411: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Nov 15 15:47:39.411: INFO: Checking APIGroup: discovery.k8s.io
    Nov 15 15:47:39.419: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Nov 15 15:47:39.419: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Nov 15 15:47:39.419: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Nov 15 15:47:39.420: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Nov 15 15:47:39.427: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Nov 15 15:47:39.427: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Nov 15 15:47:39.428: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Nov 15 15:47:39.429: INFO: Checking APIGroup: crd.projectcalico.org
    Nov 15 15:47:39.436: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Nov 15 15:47:39.436: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Nov 15 15:47:39.436: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Nov 15 15:47:39.436: INFO: Checking APIGroup: snapshot.storage.k8s.io
    Nov 15 15:47:39.444: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    Nov 15 15:47:39.444: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
    Nov 15 15:47:39.445: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    Nov 15 15:47:39.445: INFO: Checking APIGroup: ibm.com
    Nov 15 15:47:39.453: INFO: PreferredVersion.GroupVersion: ibm.com/v1alpha1
    Nov 15 15:47:39.453: INFO: Versions found [{ibm.com/v1alpha1 v1alpha1}]
    Nov 15 15:47:39.454: INFO: ibm.com/v1alpha1 matches ibm.com/v1alpha1
    Nov 15 15:47:39.454: INFO: Checking APIGroup: metrics.k8s.io
    Nov 15 15:47:39.461: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Nov 15 15:47:39.461: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Nov 15 15:47:39.461: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:47:39.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-1092" for this suite. 11/15/23 15:47:39.487
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:47:39.516
Nov 15 15:47:39.516: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename var-expansion 11/15/23 15:47:39.52
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:47:39.571
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:47:39.589
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 11/15/23 15:47:39.606
Nov 15 15:47:39.644: INFO: Waiting up to 5m0s for pod "var-expansion-472dd342-0292-4ca5-8836-671d6462e235" in namespace "var-expansion-8654" to be "Succeeded or Failed"
Nov 15 15:47:39.659: INFO: Pod "var-expansion-472dd342-0292-4ca5-8836-671d6462e235": Phase="Pending", Reason="", readiness=false. Elapsed: 14.014012ms
Nov 15 15:47:41.679: INFO: Pod "var-expansion-472dd342-0292-4ca5-8836-671d6462e235": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034147091s
Nov 15 15:47:43.675: INFO: Pod "var-expansion-472dd342-0292-4ca5-8836-671d6462e235": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029951244s
STEP: Saw pod success 11/15/23 15:47:43.675
Nov 15 15:47:43.676: INFO: Pod "var-expansion-472dd342-0292-4ca5-8836-671d6462e235" satisfied condition "Succeeded or Failed"
Nov 15 15:47:43.701: INFO: Trying to get logs from node 10.15.40.115 pod var-expansion-472dd342-0292-4ca5-8836-671d6462e235 container dapi-container: <nil>
STEP: delete the pod 11/15/23 15:47:43.751
Nov 15 15:47:43.791: INFO: Waiting for pod var-expansion-472dd342-0292-4ca5-8836-671d6462e235 to disappear
Nov 15 15:47:43.806: INFO: Pod var-expansion-472dd342-0292-4ca5-8836-671d6462e235 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Nov 15 15:47:43.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8654" for this suite. 11/15/23 15:47:43.832
------------------------------
• [4.342 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:47:39.516
    Nov 15 15:47:39.516: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename var-expansion 11/15/23 15:47:39.52
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:47:39.571
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:47:39.589
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 11/15/23 15:47:39.606
    Nov 15 15:47:39.644: INFO: Waiting up to 5m0s for pod "var-expansion-472dd342-0292-4ca5-8836-671d6462e235" in namespace "var-expansion-8654" to be "Succeeded or Failed"
    Nov 15 15:47:39.659: INFO: Pod "var-expansion-472dd342-0292-4ca5-8836-671d6462e235": Phase="Pending", Reason="", readiness=false. Elapsed: 14.014012ms
    Nov 15 15:47:41.679: INFO: Pod "var-expansion-472dd342-0292-4ca5-8836-671d6462e235": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034147091s
    Nov 15 15:47:43.675: INFO: Pod "var-expansion-472dd342-0292-4ca5-8836-671d6462e235": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029951244s
    STEP: Saw pod success 11/15/23 15:47:43.675
    Nov 15 15:47:43.676: INFO: Pod "var-expansion-472dd342-0292-4ca5-8836-671d6462e235" satisfied condition "Succeeded or Failed"
    Nov 15 15:47:43.701: INFO: Trying to get logs from node 10.15.40.115 pod var-expansion-472dd342-0292-4ca5-8836-671d6462e235 container dapi-container: <nil>
    STEP: delete the pod 11/15/23 15:47:43.751
    Nov 15 15:47:43.791: INFO: Waiting for pod var-expansion-472dd342-0292-4ca5-8836-671d6462e235 to disappear
    Nov 15 15:47:43.806: INFO: Pod var-expansion-472dd342-0292-4ca5-8836-671d6462e235 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:47:43.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8654" for this suite. 11/15/23 15:47:43.832
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:47:43.864
Nov 15 15:47:43.864: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename cronjob 11/15/23 15:47:43.868
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:47:43.919
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:47:43.937
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 11/15/23 15:47:43.956
STEP: Ensuring no jobs are scheduled 11/15/23 15:47:43.979
STEP: Ensuring no job exists by listing jobs explicitly 11/15/23 15:52:44.022
STEP: Removing cronjob 11/15/23 15:52:44.037
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Nov 15 15:52:44.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-4348" for this suite. 11/15/23 15:52:44.088
------------------------------
• [SLOW TEST] [300.250 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:47:43.864
    Nov 15 15:47:43.864: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename cronjob 11/15/23 15:47:43.868
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:47:43.919
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:47:43.937
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 11/15/23 15:47:43.956
    STEP: Ensuring no jobs are scheduled 11/15/23 15:47:43.979
    STEP: Ensuring no job exists by listing jobs explicitly 11/15/23 15:52:44.022
    STEP: Removing cronjob 11/15/23 15:52:44.037
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:52:44.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-4348" for this suite. 11/15/23 15:52:44.088
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:52:44.121
Nov 15 15:52:44.122: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename aggregator 11/15/23 15:52:44.125
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:52:44.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:52:44.197
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Nov 15 15:52:44.216: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 11/15/23 15:52:44.218
Nov 15 15:52:44.892: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Nov 15 15:52:47.074: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 15:52:49.095: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 15:52:51.100: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 15:52:53.097: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 15:52:55.096: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 15:52:57.095: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 15:52:59.096: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 15:53:01.095: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 15:53:03.100: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 15:53:05.095: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 15:53:07.096: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 15:53:09.098: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 15:53:11.093: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 15:53:13.485: INFO: Waited 364.894054ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 11/15/23 15:53:13.775
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 11/15/23 15:53:13.791
STEP: List APIServices 11/15/23 15:53:13.816
Nov 15 15:53:13.853: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Nov 15 15:53:14.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-2693" for this suite. 11/15/23 15:53:14.391
------------------------------
• [SLOW TEST] [30.295 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:52:44.121
    Nov 15 15:52:44.122: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename aggregator 11/15/23 15:52:44.125
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:52:44.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:52:44.197
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Nov 15 15:52:44.216: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 11/15/23 15:52:44.218
    Nov 15 15:52:44.892: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Nov 15 15:52:47.074: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 15:52:49.095: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 15:52:51.100: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 15:52:53.097: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 15:52:55.096: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 15:52:57.095: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 15:52:59.096: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 15:53:01.095: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 15:53:03.100: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 15:53:05.095: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 15:53:07.096: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 15:53:09.098: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 15:53:11.093: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 52, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 15:53:13.485: INFO: Waited 364.894054ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 11/15/23 15:53:13.775
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 11/15/23 15:53:13.791
    STEP: List APIServices 11/15/23 15:53:13.816
    Nov 15 15:53:13.853: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:53:14.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-2693" for this suite. 11/15/23 15:53:14.391
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:53:14.418
Nov 15 15:53:14.418: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename configmap 11/15/23 15:53:14.421
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:53:14.472
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:53:14.488
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-54cf710b-d653-4bf5-a915-5d2f402b46a8 11/15/23 15:53:14.504
STEP: Creating a pod to test consume configMaps 11/15/23 15:53:14.523
Nov 15 15:53:14.557: INFO: Waiting up to 5m0s for pod "pod-configmaps-aa884c8f-646b-4778-bb03-9c01e6eeadc0" in namespace "configmap-5608" to be "Succeeded or Failed"
Nov 15 15:53:14.572: INFO: Pod "pod-configmaps-aa884c8f-646b-4778-bb03-9c01e6eeadc0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.766109ms
Nov 15 15:53:16.588: INFO: Pod "pod-configmaps-aa884c8f-646b-4778-bb03-9c01e6eeadc0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030704245s
Nov 15 15:53:18.591: INFO: Pod "pod-configmaps-aa884c8f-646b-4778-bb03-9c01e6eeadc0": Phase="Running", Reason="", readiness=false. Elapsed: 4.034045581s
Nov 15 15:53:20.588: INFO: Pod "pod-configmaps-aa884c8f-646b-4778-bb03-9c01e6eeadc0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031009078s
STEP: Saw pod success 11/15/23 15:53:20.588
Nov 15 15:53:20.589: INFO: Pod "pod-configmaps-aa884c8f-646b-4778-bb03-9c01e6eeadc0" satisfied condition "Succeeded or Failed"
Nov 15 15:53:20.604: INFO: Trying to get logs from node 10.15.40.115 pod pod-configmaps-aa884c8f-646b-4778-bb03-9c01e6eeadc0 container agnhost-container: <nil>
STEP: delete the pod 11/15/23 15:53:20.749
Nov 15 15:53:20.789: INFO: Waiting for pod pod-configmaps-aa884c8f-646b-4778-bb03-9c01e6eeadc0 to disappear
Nov 15 15:53:20.804: INFO: Pod pod-configmaps-aa884c8f-646b-4778-bb03-9c01e6eeadc0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 15 15:53:20.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5608" for this suite. 11/15/23 15:53:20.828
------------------------------
• [SLOW TEST] [6.436 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:53:14.418
    Nov 15 15:53:14.418: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename configmap 11/15/23 15:53:14.421
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:53:14.472
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:53:14.488
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-54cf710b-d653-4bf5-a915-5d2f402b46a8 11/15/23 15:53:14.504
    STEP: Creating a pod to test consume configMaps 11/15/23 15:53:14.523
    Nov 15 15:53:14.557: INFO: Waiting up to 5m0s for pod "pod-configmaps-aa884c8f-646b-4778-bb03-9c01e6eeadc0" in namespace "configmap-5608" to be "Succeeded or Failed"
    Nov 15 15:53:14.572: INFO: Pod "pod-configmaps-aa884c8f-646b-4778-bb03-9c01e6eeadc0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.766109ms
    Nov 15 15:53:16.588: INFO: Pod "pod-configmaps-aa884c8f-646b-4778-bb03-9c01e6eeadc0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030704245s
    Nov 15 15:53:18.591: INFO: Pod "pod-configmaps-aa884c8f-646b-4778-bb03-9c01e6eeadc0": Phase="Running", Reason="", readiness=false. Elapsed: 4.034045581s
    Nov 15 15:53:20.588: INFO: Pod "pod-configmaps-aa884c8f-646b-4778-bb03-9c01e6eeadc0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031009078s
    STEP: Saw pod success 11/15/23 15:53:20.588
    Nov 15 15:53:20.589: INFO: Pod "pod-configmaps-aa884c8f-646b-4778-bb03-9c01e6eeadc0" satisfied condition "Succeeded or Failed"
    Nov 15 15:53:20.604: INFO: Trying to get logs from node 10.15.40.115 pod pod-configmaps-aa884c8f-646b-4778-bb03-9c01e6eeadc0 container agnhost-container: <nil>
    STEP: delete the pod 11/15/23 15:53:20.749
    Nov 15 15:53:20.789: INFO: Waiting for pod pod-configmaps-aa884c8f-646b-4778-bb03-9c01e6eeadc0 to disappear
    Nov 15 15:53:20.804: INFO: Pod pod-configmaps-aa884c8f-646b-4778-bb03-9c01e6eeadc0 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:53:20.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5608" for this suite. 11/15/23 15:53:20.828
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:53:20.854
Nov 15 15:53:20.855: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename job 11/15/23 15:53:20.857
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:53:20.909
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:53:20.925
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 11/15/23 15:53:20.942
STEP: Ensuring job reaches completions 11/15/23 15:53:20.961
STEP: Ensuring pods with index for job exist 11/15/23 15:53:32.977
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Nov 15 15:53:32.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-5049" for this suite. 11/15/23 15:53:33.023
------------------------------
• [SLOW TEST] [12.194 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:53:20.854
    Nov 15 15:53:20.855: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename job 11/15/23 15:53:20.857
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:53:20.909
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:53:20.925
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 11/15/23 15:53:20.942
    STEP: Ensuring job reaches completions 11/15/23 15:53:20.961
    STEP: Ensuring pods with index for job exist 11/15/23 15:53:32.977
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:53:32.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-5049" for this suite. 11/15/23 15:53:33.023
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:53:33.065
Nov 15 15:53:33.065: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename kubectl 11/15/23 15:53:33.067
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:53:33.12
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:53:33.136
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 11/15/23 15:53:33.154
Nov 15 15:53:33.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-1917 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Nov 15 15:53:33.326: INFO: stderr: ""
Nov 15 15:53:33.326: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 11/15/23 15:53:33.326
STEP: verifying the pod e2e-test-httpd-pod was created 11/15/23 15:53:38.381
Nov 15 15:53:38.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-1917 get pod e2e-test-httpd-pod -o json'
Nov 15 15:53:38.529: INFO: stderr: ""
Nov 15 15:53:38.529: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"106e65b669af9b0fae3fc940c4fa303a83aa5920b30252b9c1a793b729ea62e2\",\n            \"cni.projectcalico.org/podIP\": \"172.30.164.14/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.164.14/32\"\n        },\n        \"creationTimestamp\": \"2023-11-15T15:53:33Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1917\",\n        \"resourceVersion\": \"24665\",\n        \"uid\": \"66f5028b-c14c-4f32-b478-24b153b2822e\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-ppzqr\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.15.40.115\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-ppzqr\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-11-15T15:53:33Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-11-15T15:53:35Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-11-15T15:53:35Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-11-15T15:53:33Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://7de85f3a2ef14383a0b683c6972efed050ed82ea806bea15afdda1a4dbad77b8\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-11-15T15:53:34Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.15.40.115\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.164.14\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.164.14\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-11-15T15:53:33Z\"\n    }\n}\n"
STEP: replace the image in the pod 11/15/23 15:53:38.531
Nov 15 15:53:38.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-1917 replace -f -'
Nov 15 15:53:39.738: INFO: stderr: ""
Nov 15 15:53:39.738: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 11/15/23 15:53:39.738
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Nov 15 15:53:39.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-1917 delete pods e2e-test-httpd-pod'
Nov 15 15:53:41.386: INFO: stderr: ""
Nov 15 15:53:41.386: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 15 15:53:41.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1917" for this suite. 11/15/23 15:53:41.415
------------------------------
• [SLOW TEST] [8.376 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:53:33.065
    Nov 15 15:53:33.065: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename kubectl 11/15/23 15:53:33.067
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:53:33.12
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:53:33.136
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 11/15/23 15:53:33.154
    Nov 15 15:53:33.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-1917 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Nov 15 15:53:33.326: INFO: stderr: ""
    Nov 15 15:53:33.326: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 11/15/23 15:53:33.326
    STEP: verifying the pod e2e-test-httpd-pod was created 11/15/23 15:53:38.381
    Nov 15 15:53:38.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-1917 get pod e2e-test-httpd-pod -o json'
    Nov 15 15:53:38.529: INFO: stderr: ""
    Nov 15 15:53:38.529: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"106e65b669af9b0fae3fc940c4fa303a83aa5920b30252b9c1a793b729ea62e2\",\n            \"cni.projectcalico.org/podIP\": \"172.30.164.14/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.164.14/32\"\n        },\n        \"creationTimestamp\": \"2023-11-15T15:53:33Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1917\",\n        \"resourceVersion\": \"24665\",\n        \"uid\": \"66f5028b-c14c-4f32-b478-24b153b2822e\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-ppzqr\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.15.40.115\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-ppzqr\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-11-15T15:53:33Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-11-15T15:53:35Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-11-15T15:53:35Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-11-15T15:53:33Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://7de85f3a2ef14383a0b683c6972efed050ed82ea806bea15afdda1a4dbad77b8\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-11-15T15:53:34Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.15.40.115\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.164.14\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.164.14\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-11-15T15:53:33Z\"\n    }\n}\n"
    STEP: replace the image in the pod 11/15/23 15:53:38.531
    Nov 15 15:53:38.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-1917 replace -f -'
    Nov 15 15:53:39.738: INFO: stderr: ""
    Nov 15 15:53:39.738: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 11/15/23 15:53:39.738
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Nov 15 15:53:39.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-1917 delete pods e2e-test-httpd-pod'
    Nov 15 15:53:41.386: INFO: stderr: ""
    Nov 15 15:53:41.386: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:53:41.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1917" for this suite. 11/15/23 15:53:41.415
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:53:41.453
Nov 15 15:53:41.453: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename svcaccounts 11/15/23 15:53:41.455
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:53:41.5
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:53:41.517
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  11/15/23 15:53:41.534
Nov 15 15:53:41.590: INFO: Waiting up to 5m0s for pod "test-pod-51d00d2a-bb44-41ef-88e4-b2d669b79035" in namespace "svcaccounts-185" to be "Succeeded or Failed"
Nov 15 15:53:41.637: INFO: Pod "test-pod-51d00d2a-bb44-41ef-88e4-b2d669b79035": Phase="Pending", Reason="", readiness=false. Elapsed: 47.267368ms
Nov 15 15:53:43.655: INFO: Pod "test-pod-51d00d2a-bb44-41ef-88e4-b2d669b79035": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065177904s
Nov 15 15:53:45.654: INFO: Pod "test-pod-51d00d2a-bb44-41ef-88e4-b2d669b79035": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064430826s
STEP: Saw pod success 11/15/23 15:53:45.654
Nov 15 15:53:45.655: INFO: Pod "test-pod-51d00d2a-bb44-41ef-88e4-b2d669b79035" satisfied condition "Succeeded or Failed"
Nov 15 15:53:45.670: INFO: Trying to get logs from node 10.15.40.115 pod test-pod-51d00d2a-bb44-41ef-88e4-b2d669b79035 container agnhost-container: <nil>
STEP: delete the pod 11/15/23 15:53:45.715
Nov 15 15:53:45.751: INFO: Waiting for pod test-pod-51d00d2a-bb44-41ef-88e4-b2d669b79035 to disappear
Nov 15 15:53:45.765: INFO: Pod test-pod-51d00d2a-bb44-41ef-88e4-b2d669b79035 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Nov 15 15:53:45.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-185" for this suite. 11/15/23 15:53:45.791
------------------------------
• [4.362 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:53:41.453
    Nov 15 15:53:41.453: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename svcaccounts 11/15/23 15:53:41.455
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:53:41.5
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:53:41.517
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  11/15/23 15:53:41.534
    Nov 15 15:53:41.590: INFO: Waiting up to 5m0s for pod "test-pod-51d00d2a-bb44-41ef-88e4-b2d669b79035" in namespace "svcaccounts-185" to be "Succeeded or Failed"
    Nov 15 15:53:41.637: INFO: Pod "test-pod-51d00d2a-bb44-41ef-88e4-b2d669b79035": Phase="Pending", Reason="", readiness=false. Elapsed: 47.267368ms
    Nov 15 15:53:43.655: INFO: Pod "test-pod-51d00d2a-bb44-41ef-88e4-b2d669b79035": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065177904s
    Nov 15 15:53:45.654: INFO: Pod "test-pod-51d00d2a-bb44-41ef-88e4-b2d669b79035": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064430826s
    STEP: Saw pod success 11/15/23 15:53:45.654
    Nov 15 15:53:45.655: INFO: Pod "test-pod-51d00d2a-bb44-41ef-88e4-b2d669b79035" satisfied condition "Succeeded or Failed"
    Nov 15 15:53:45.670: INFO: Trying to get logs from node 10.15.40.115 pod test-pod-51d00d2a-bb44-41ef-88e4-b2d669b79035 container agnhost-container: <nil>
    STEP: delete the pod 11/15/23 15:53:45.715
    Nov 15 15:53:45.751: INFO: Waiting for pod test-pod-51d00d2a-bb44-41ef-88e4-b2d669b79035 to disappear
    Nov 15 15:53:45.765: INFO: Pod test-pod-51d00d2a-bb44-41ef-88e4-b2d669b79035 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:53:45.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-185" for this suite. 11/15/23 15:53:45.791
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:53:45.816
Nov 15 15:53:45.817: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename kubectl 11/15/23 15:53:45.82
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:53:45.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:53:45.886
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 11/15/23 15:53:45.901
Nov 15 15:53:45.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-3295 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Nov 15 15:53:46.047: INFO: stderr: ""
Nov 15 15:53:46.047: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 11/15/23 15:53:46.047
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Nov 15 15:53:46.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-3295 delete pods e2e-test-httpd-pod'
Nov 15 15:53:49.452: INFO: stderr: ""
Nov 15 15:53:49.452: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 15 15:53:49.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3295" for this suite. 11/15/23 15:53:49.48
------------------------------
• [3.690 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:53:45.816
    Nov 15 15:53:45.817: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename kubectl 11/15/23 15:53:45.82
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:53:45.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:53:45.886
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 11/15/23 15:53:45.901
    Nov 15 15:53:45.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-3295 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Nov 15 15:53:46.047: INFO: stderr: ""
    Nov 15 15:53:46.047: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 11/15/23 15:53:46.047
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Nov 15 15:53:46.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-3295 delete pods e2e-test-httpd-pod'
    Nov 15 15:53:49.452: INFO: stderr: ""
    Nov 15 15:53:49.452: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:53:49.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3295" for this suite. 11/15/23 15:53:49.48
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:53:49.508
Nov 15 15:53:49.508: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename kubectl 11/15/23 15:53:49.512
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:53:49.563
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:53:49.579
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Nov 15 15:53:49.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-2629 create -f -'
Nov 15 15:53:49.914: INFO: stderr: ""
Nov 15 15:53:49.914: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Nov 15 15:53:49.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-2629 create -f -'
Nov 15 15:53:51.070: INFO: stderr: ""
Nov 15 15:53:51.070: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 11/15/23 15:53:51.07
Nov 15 15:53:52.089: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 15 15:53:52.090: INFO: Found 0 / 1
Nov 15 15:53:53.088: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 15 15:53:53.088: INFO: Found 1 / 1
Nov 15 15:53:53.088: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov 15 15:53:53.106: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 15 15:53:53.106: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 15 15:53:53.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-2629 describe pod agnhost-primary-52957'
Nov 15 15:53:53.307: INFO: stderr: ""
Nov 15 15:53:53.307: INFO: stdout: "Name:             agnhost-primary-52957\nNamespace:        kubectl-2629\nPriority:         0\nService Account:  default\nNode:             10.15.40.115/10.15.40.115\nStart Time:       Wed, 15 Nov 2023 15:53:49 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 348fdc76bb7aff84206ebf90b554037654d39908671d3c3483f08260548a57dd\n                  cni.projectcalico.org/podIP: 172.30.164.20/32\n                  cni.projectcalico.org/podIPs: 172.30.164.20/32\nStatus:           Running\nIP:               172.30.164.20\nIPs:\n  IP:           172.30.164.20\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://eca27ad73b313166a22e94d0fbc13ff31ec66d219c7c8bb0faed69c6cd5e5f60\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 15 Nov 2023 15:53:51 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jw4cg (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-jw4cg:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 600s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 600s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  4s    default-scheduler  Successfully assigned kubectl-2629/agnhost-primary-52957 to 10.15.40.115\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Nov 15 15:53:53.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-2629 describe rc agnhost-primary'
Nov 15 15:53:53.513: INFO: stderr: ""
Nov 15 15:53:53.514: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-2629\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: agnhost-primary-52957\n"
Nov 15 15:53:53.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-2629 describe service agnhost-primary'
Nov 15 15:53:53.710: INFO: stderr: ""
Nov 15 15:53:53.711: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-2629\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.21.142.59\nIPs:               172.21.142.59\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.164.20:6379\nSession Affinity:  None\nEvents:            <none>\n"
Nov 15 15:53:53.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-2629 describe node 10.15.40.106'
Nov 15 15:53:54.066: INFO: stderr: ""
Nov 15 15:53:54.066: INFO: stdout: "Name:               10.15.40.106\nRoles:              <none>\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=br-sao\n                    failure-domain.beta.kubernetes.io/zone=sao05\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=163.109.70.60\n                    ibm-cloud.kubernetes.io/ha-worker=true\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.15.40.106\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=UBUNTU_20_64\n                    ibm-cloud.kubernetes.io/region=br-sao\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-clac150z0u38f277tnpg-kubee2epvgm-default-000002cd\n                    ibm-cloud.kubernetes.io/worker-pool-id=clac150z0u38f277tnpg-af0a4ae\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=1.26.9_1559\n                    ibm-cloud.kubernetes.io/zone=sao05\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.15.40.106\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    privateVLAN=3083672\n                    publicVLAN=3083670\n                    topology.kubernetes.io/region=br-sao\n                    topology.kubernetes.io/zone=sao05\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.15.40.106/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.191.64\nCreationTimestamp:  Wed, 15 Nov 2023 13:13:21 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.15.40.106\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 15 Nov 2023 15:53:46 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 15 Nov 2023 13:14:10 +0000   Wed, 15 Nov 2023 13:14:10 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 15 Nov 2023 15:51:00 +0000   Wed, 15 Nov 2023 13:13:21 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 15 Nov 2023 15:51:00 +0000   Wed, 15 Nov 2023 13:13:21 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 15 Nov 2023 15:51:00 +0000   Wed, 15 Nov 2023 13:13:21 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 15 Nov 2023 15:51:00 +0000   Wed, 15 Nov 2023 13:13:44 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.15.40.106\n  ExternalIP:  163.109.70.60\n  Hostname:    10.15.40.106\nCapacity:\n  cpu:                    4\n  ephemeral-storage:      102609848Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 15786Mi\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    3910m\n  ephemeral-storage:      93913280025\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 13084Mi\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 008412050fc94f1bac8343002d665472\n  System UUID:                c0f43bb8-5f2d-b86a-73c2-64ef045fb522\n  Boot ID:                    aad8231d-5b91-475b-81f8-c9f55a4fb1c5\n  Kernel Version:             5.4.0-166-generic\n  OS Image:                   Ubuntu 20.04.6 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.7.8\n  Kubelet Version:            v1.26.9+IKS\n  Kube-Proxy Version:         v1.26.9+IKS\nProviderID:                   ibm://fee034388aa6435883a1f720010ab3a2///clac150z0u38f277tnpg/kube-clac150z0u38f277tnpg-kubee2epvgm-default-000002cd\nNon-terminated Pods:          (22 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  ibm-system                  ibm-cloud-provider-ip-163-109-74-98-5d59656977-bgllw       5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         14m\n  kube-system                 calico-kube-controllers-7847f7647d-srqd4                   10m (0%)      0 (0%)      25Mi (0%)        3Gi (23%)      165m\n  kube-system                 calico-node-67csc                                          250m (6%)     0 (0%)      90Mi (0%)        0 (0%)         160m\n  kube-system                 calico-typha-5b5db87f55-nwmh4                              250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         165m\n  kube-system                 coredns-5845f98d4-2lnrf                                    100m (2%)     0 (0%)      70Mi (0%)        400Mi (3%)     150m\n  kube-system                 coredns-autoscaler-85f4bdddf6-qwxmw                        1m (0%)       0 (0%)      7Mi (0%)         0 (0%)         165m\n  kube-system                 dashboard-metrics-scraper-7cf679fbdf-frv7t                 15m (0%)      0 (0%)      10Mi (0%)        0 (0%)         162m\n  kube-system                 ibm-file-plugin-557f875d5f-jrfnv                           50m (1%)      500m (12%)  100Mi (0%)       200Mi (1%)     164m\n  kube-system                 ibm-keepalived-watcher-dqknf                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         160m\n  kube-system                 ibm-master-proxy-static-10.15.40.106                       25m (0%)      300m (7%)   32M (0%)         512M (3%)      159m\n  kube-system                 ibm-storage-watcher-7d897847f4-k64t5                       50m (1%)      500m (12%)  100Mi (0%)       200Mi (1%)     164m\n  kube-system                 ibmcloud-block-storage-driver-f97nm                        50m (1%)      500m (12%)  100Mi (0%)       300Mi (2%)     160m\n  kube-system                 ibmcloud-block-storage-plugin-5bd59f7b48-2dxcj             50m (1%)      500m (12%)  100Mi (0%)       300Mi (2%)     164m\n  kube-system                 ingress-cluster-healthcheck-5985f966bb-jgjx8               5m (0%)       0 (0%)      20Mi (0%)        0 (0%)         14m\n  kube-system                 konnectivity-agent-5brpz                                   10m (0%)      0 (0%)      10Mi (0%)        500Mi (3%)     151m\n  kube-system                 kubernetes-dashboard-5ccdc9cbb8-wbmfz                      50m (1%)      0 (0%)      100Mi (0%)       0 (0%)         162m\n  kube-system                 metrics-server-7cbd9c9b48-8tr74                            126m (3%)     266m (6%)   191Mi (1%)       536Mi (4%)     14m\n  kube-system                 public-crclac150z0u38f277tnpg-alb1-6df9445bb6-qw2bt        20m (0%)      0 (0%)      135Mi (1%)       0 (0%)         14m\n  kube-system                 snapshot-controller-6db47fc545-5nnph                       10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         165m\n  kube-system                 snapshot-controller-6db47fc545-h4q49                       10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         165m\n  kube-system                 snapshot-controller-6db47fc545-khkb2                       10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         165m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-eabb9aa298c743f7-zxdnv    0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests         Limits\n  --------               --------         ------\n  cpu                    1102m (28%)      2566m (65%)\n  memory                 1370642Ki (10%)  6140192Ki (45%)\n  ephemeral-storage      0 (0%)           0 (0%)\n  hugepages-1Gi          0 (0%)           0 (0%)\n  hugepages-2Mi          0 (0%)           0 (0%)\n  scheduling.k8s.io/foo  0                0\nEvents:                  <none>\n"
Nov 15 15:53:54.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-2629 describe namespace kubectl-2629'
Nov 15 15:53:54.257: INFO: stderr: ""
Nov 15 15:53:54.257: INFO: stdout: "Name:         kubectl-2629\nLabels:       e2e-framework=kubectl\n              e2e-run=3b21b822-8b44-408e-b236-bfbd8cf999aa\n              kubernetes.io/metadata.name=kubectl-2629\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 15 15:53:54.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2629" for this suite. 11/15/23 15:53:54.278
------------------------------
• [4.800 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:53:49.508
    Nov 15 15:53:49.508: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename kubectl 11/15/23 15:53:49.512
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:53:49.563
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:53:49.579
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Nov 15 15:53:49.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-2629 create -f -'
    Nov 15 15:53:49.914: INFO: stderr: ""
    Nov 15 15:53:49.914: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Nov 15 15:53:49.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-2629 create -f -'
    Nov 15 15:53:51.070: INFO: stderr: ""
    Nov 15 15:53:51.070: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 11/15/23 15:53:51.07
    Nov 15 15:53:52.089: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 15 15:53:52.090: INFO: Found 0 / 1
    Nov 15 15:53:53.088: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 15 15:53:53.088: INFO: Found 1 / 1
    Nov 15 15:53:53.088: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Nov 15 15:53:53.106: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 15 15:53:53.106: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Nov 15 15:53:53.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-2629 describe pod agnhost-primary-52957'
    Nov 15 15:53:53.307: INFO: stderr: ""
    Nov 15 15:53:53.307: INFO: stdout: "Name:             agnhost-primary-52957\nNamespace:        kubectl-2629\nPriority:         0\nService Account:  default\nNode:             10.15.40.115/10.15.40.115\nStart Time:       Wed, 15 Nov 2023 15:53:49 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 348fdc76bb7aff84206ebf90b554037654d39908671d3c3483f08260548a57dd\n                  cni.projectcalico.org/podIP: 172.30.164.20/32\n                  cni.projectcalico.org/podIPs: 172.30.164.20/32\nStatus:           Running\nIP:               172.30.164.20\nIPs:\n  IP:           172.30.164.20\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://eca27ad73b313166a22e94d0fbc13ff31ec66d219c7c8bb0faed69c6cd5e5f60\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 15 Nov 2023 15:53:51 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jw4cg (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-jw4cg:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 600s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 600s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  4s    default-scheduler  Successfully assigned kubectl-2629/agnhost-primary-52957 to 10.15.40.115\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
    Nov 15 15:53:53.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-2629 describe rc agnhost-primary'
    Nov 15 15:53:53.513: INFO: stderr: ""
    Nov 15 15:53:53.514: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-2629\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: agnhost-primary-52957\n"
    Nov 15 15:53:53.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-2629 describe service agnhost-primary'
    Nov 15 15:53:53.710: INFO: stderr: ""
    Nov 15 15:53:53.711: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-2629\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.21.142.59\nIPs:               172.21.142.59\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.164.20:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Nov 15 15:53:53.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-2629 describe node 10.15.40.106'
    Nov 15 15:53:54.066: INFO: stderr: ""
    Nov 15 15:53:54.066: INFO: stdout: "Name:               10.15.40.106\nRoles:              <none>\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=br-sao\n                    failure-domain.beta.kubernetes.io/zone=sao05\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=163.109.70.60\n                    ibm-cloud.kubernetes.io/ha-worker=true\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.15.40.106\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=UBUNTU_20_64\n                    ibm-cloud.kubernetes.io/region=br-sao\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-clac150z0u38f277tnpg-kubee2epvgm-default-000002cd\n                    ibm-cloud.kubernetes.io/worker-pool-id=clac150z0u38f277tnpg-af0a4ae\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=1.26.9_1559\n                    ibm-cloud.kubernetes.io/zone=sao05\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.15.40.106\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    privateVLAN=3083672\n                    publicVLAN=3083670\n                    topology.kubernetes.io/region=br-sao\n                    topology.kubernetes.io/zone=sao05\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.15.40.106/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.191.64\nCreationTimestamp:  Wed, 15 Nov 2023 13:13:21 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.15.40.106\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 15 Nov 2023 15:53:46 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 15 Nov 2023 13:14:10 +0000   Wed, 15 Nov 2023 13:14:10 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 15 Nov 2023 15:51:00 +0000   Wed, 15 Nov 2023 13:13:21 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 15 Nov 2023 15:51:00 +0000   Wed, 15 Nov 2023 13:13:21 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 15 Nov 2023 15:51:00 +0000   Wed, 15 Nov 2023 13:13:21 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 15 Nov 2023 15:51:00 +0000   Wed, 15 Nov 2023 13:13:44 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.15.40.106\n  ExternalIP:  163.109.70.60\n  Hostname:    10.15.40.106\nCapacity:\n  cpu:                    4\n  ephemeral-storage:      102609848Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 15786Mi\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    3910m\n  ephemeral-storage:      93913280025\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 13084Mi\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 008412050fc94f1bac8343002d665472\n  System UUID:                c0f43bb8-5f2d-b86a-73c2-64ef045fb522\n  Boot ID:                    aad8231d-5b91-475b-81f8-c9f55a4fb1c5\n  Kernel Version:             5.4.0-166-generic\n  OS Image:                   Ubuntu 20.04.6 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.7.8\n  Kubelet Version:            v1.26.9+IKS\n  Kube-Proxy Version:         v1.26.9+IKS\nProviderID:                   ibm://fee034388aa6435883a1f720010ab3a2///clac150z0u38f277tnpg/kube-clac150z0u38f277tnpg-kubee2epvgm-default-000002cd\nNon-terminated Pods:          (22 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  ibm-system                  ibm-cloud-provider-ip-163-109-74-98-5d59656977-bgllw       5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         14m\n  kube-system                 calico-kube-controllers-7847f7647d-srqd4                   10m (0%)      0 (0%)      25Mi (0%)        3Gi (23%)      165m\n  kube-system                 calico-node-67csc                                          250m (6%)     0 (0%)      90Mi (0%)        0 (0%)         160m\n  kube-system                 calico-typha-5b5db87f55-nwmh4                              250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         165m\n  kube-system                 coredns-5845f98d4-2lnrf                                    100m (2%)     0 (0%)      70Mi (0%)        400Mi (3%)     150m\n  kube-system                 coredns-autoscaler-85f4bdddf6-qwxmw                        1m (0%)       0 (0%)      7Mi (0%)         0 (0%)         165m\n  kube-system                 dashboard-metrics-scraper-7cf679fbdf-frv7t                 15m (0%)      0 (0%)      10Mi (0%)        0 (0%)         162m\n  kube-system                 ibm-file-plugin-557f875d5f-jrfnv                           50m (1%)      500m (12%)  100Mi (0%)       200Mi (1%)     164m\n  kube-system                 ibm-keepalived-watcher-dqknf                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         160m\n  kube-system                 ibm-master-proxy-static-10.15.40.106                       25m (0%)      300m (7%)   32M (0%)         512M (3%)      159m\n  kube-system                 ibm-storage-watcher-7d897847f4-k64t5                       50m (1%)      500m (12%)  100Mi (0%)       200Mi (1%)     164m\n  kube-system                 ibmcloud-block-storage-driver-f97nm                        50m (1%)      500m (12%)  100Mi (0%)       300Mi (2%)     160m\n  kube-system                 ibmcloud-block-storage-plugin-5bd59f7b48-2dxcj             50m (1%)      500m (12%)  100Mi (0%)       300Mi (2%)     164m\n  kube-system                 ingress-cluster-healthcheck-5985f966bb-jgjx8               5m (0%)       0 (0%)      20Mi (0%)        0 (0%)         14m\n  kube-system                 konnectivity-agent-5brpz                                   10m (0%)      0 (0%)      10Mi (0%)        500Mi (3%)     151m\n  kube-system                 kubernetes-dashboard-5ccdc9cbb8-wbmfz                      50m (1%)      0 (0%)      100Mi (0%)       0 (0%)         162m\n  kube-system                 metrics-server-7cbd9c9b48-8tr74                            126m (3%)     266m (6%)   191Mi (1%)       536Mi (4%)     14m\n  kube-system                 public-crclac150z0u38f277tnpg-alb1-6df9445bb6-qw2bt        20m (0%)      0 (0%)      135Mi (1%)       0 (0%)         14m\n  kube-system                 snapshot-controller-6db47fc545-5nnph                       10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         165m\n  kube-system                 snapshot-controller-6db47fc545-h4q49                       10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         165m\n  kube-system                 snapshot-controller-6db47fc545-khkb2                       10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         165m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-eabb9aa298c743f7-zxdnv    0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests         Limits\n  --------               --------         ------\n  cpu                    1102m (28%)      2566m (65%)\n  memory                 1370642Ki (10%)  6140192Ki (45%)\n  ephemeral-storage      0 (0%)           0 (0%)\n  hugepages-1Gi          0 (0%)           0 (0%)\n  hugepages-2Mi          0 (0%)           0 (0%)\n  scheduling.k8s.io/foo  0                0\nEvents:                  <none>\n"
    Nov 15 15:53:54.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-2629 describe namespace kubectl-2629'
    Nov 15 15:53:54.257: INFO: stderr: ""
    Nov 15 15:53:54.257: INFO: stdout: "Name:         kubectl-2629\nLabels:       e2e-framework=kubectl\n              e2e-run=3b21b822-8b44-408e-b236-bfbd8cf999aa\n              kubernetes.io/metadata.name=kubectl-2629\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:53:54.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2629" for this suite. 11/15/23 15:53:54.278
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:53:54.321
Nov 15 15:53:54.321: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename endpointslice 11/15/23 15:53:54.323
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:53:54.382
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:53:54.399
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Nov 15 15:53:54.463: INFO: Endpoints addresses: [172.20.0.1] , ports: [2040]
Nov 15 15:53:54.463: INFO: EndpointSlices addresses: [172.20.0.1] , ports: [2040]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Nov 15 15:53:54.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-4937" for this suite. 11/15/23 15:53:54.49
------------------------------
• [0.201 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:53:54.321
    Nov 15 15:53:54.321: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename endpointslice 11/15/23 15:53:54.323
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:53:54.382
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:53:54.399
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Nov 15 15:53:54.463: INFO: Endpoints addresses: [172.20.0.1] , ports: [2040]
    Nov 15 15:53:54.463: INFO: EndpointSlices addresses: [172.20.0.1] , ports: [2040]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:53:54.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-4937" for this suite. 11/15/23 15:53:54.49
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:53:54.528
Nov 15 15:53:54.529: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename replication-controller 11/15/23 15:53:54.532
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:53:54.587
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:53:54.609
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-f9b43676-d134-4f5a-9587-ae50f89d2151 11/15/23 15:53:54.625
Nov 15 15:53:54.663: INFO: Pod name my-hostname-basic-f9b43676-d134-4f5a-9587-ae50f89d2151: Found 0 pods out of 1
Nov 15 15:53:59.679: INFO: Pod name my-hostname-basic-f9b43676-d134-4f5a-9587-ae50f89d2151: Found 1 pods out of 1
Nov 15 15:53:59.679: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-f9b43676-d134-4f5a-9587-ae50f89d2151" are running
Nov 15 15:53:59.679: INFO: Waiting up to 5m0s for pod "my-hostname-basic-f9b43676-d134-4f5a-9587-ae50f89d2151-s4kcc" in namespace "replication-controller-2712" to be "running"
Nov 15 15:53:59.696: INFO: Pod "my-hostname-basic-f9b43676-d134-4f5a-9587-ae50f89d2151-s4kcc": Phase="Running", Reason="", readiness=true. Elapsed: 16.046439ms
Nov 15 15:53:59.696: INFO: Pod "my-hostname-basic-f9b43676-d134-4f5a-9587-ae50f89d2151-s4kcc" satisfied condition "running"
Nov 15 15:53:59.696: INFO: Pod "my-hostname-basic-f9b43676-d134-4f5a-9587-ae50f89d2151-s4kcc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-15 15:53:54 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-15 15:53:56 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-15 15:53:56 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-15 15:53:54 +0000 UTC Reason: Message:}])
Nov 15 15:53:59.696: INFO: Trying to dial the pod
Nov 15 15:54:04.808: INFO: Controller my-hostname-basic-f9b43676-d134-4f5a-9587-ae50f89d2151: Got expected result from replica 1 [my-hostname-basic-f9b43676-d134-4f5a-9587-ae50f89d2151-s4kcc]: "my-hostname-basic-f9b43676-d134-4f5a-9587-ae50f89d2151-s4kcc", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Nov 15 15:54:04.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-2712" for this suite. 11/15/23 15:54:04.834
------------------------------
• [SLOW TEST] [10.333 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:53:54.528
    Nov 15 15:53:54.529: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename replication-controller 11/15/23 15:53:54.532
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:53:54.587
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:53:54.609
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-f9b43676-d134-4f5a-9587-ae50f89d2151 11/15/23 15:53:54.625
    Nov 15 15:53:54.663: INFO: Pod name my-hostname-basic-f9b43676-d134-4f5a-9587-ae50f89d2151: Found 0 pods out of 1
    Nov 15 15:53:59.679: INFO: Pod name my-hostname-basic-f9b43676-d134-4f5a-9587-ae50f89d2151: Found 1 pods out of 1
    Nov 15 15:53:59.679: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-f9b43676-d134-4f5a-9587-ae50f89d2151" are running
    Nov 15 15:53:59.679: INFO: Waiting up to 5m0s for pod "my-hostname-basic-f9b43676-d134-4f5a-9587-ae50f89d2151-s4kcc" in namespace "replication-controller-2712" to be "running"
    Nov 15 15:53:59.696: INFO: Pod "my-hostname-basic-f9b43676-d134-4f5a-9587-ae50f89d2151-s4kcc": Phase="Running", Reason="", readiness=true. Elapsed: 16.046439ms
    Nov 15 15:53:59.696: INFO: Pod "my-hostname-basic-f9b43676-d134-4f5a-9587-ae50f89d2151-s4kcc" satisfied condition "running"
    Nov 15 15:53:59.696: INFO: Pod "my-hostname-basic-f9b43676-d134-4f5a-9587-ae50f89d2151-s4kcc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-15 15:53:54 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-15 15:53:56 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-15 15:53:56 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-15 15:53:54 +0000 UTC Reason: Message:}])
    Nov 15 15:53:59.696: INFO: Trying to dial the pod
    Nov 15 15:54:04.808: INFO: Controller my-hostname-basic-f9b43676-d134-4f5a-9587-ae50f89d2151: Got expected result from replica 1 [my-hostname-basic-f9b43676-d134-4f5a-9587-ae50f89d2151-s4kcc]: "my-hostname-basic-f9b43676-d134-4f5a-9587-ae50f89d2151-s4kcc", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:54:04.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-2712" for this suite. 11/15/23 15:54:04.834
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:54:04.864
Nov 15 15:54:04.864: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename secrets 11/15/23 15:54:04.866
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:54:04.94
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:54:04.957
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-e70626f2-fc9e-420a-a52c-72bbd3ca577c 11/15/23 15:54:04.973
STEP: Creating a pod to test consume secrets 11/15/23 15:54:04.997
Nov 15 15:54:05.028: INFO: Waiting up to 5m0s for pod "pod-secrets-9c451d67-b1eb-405b-8d78-e05cf1426eb8" in namespace "secrets-520" to be "Succeeded or Failed"
Nov 15 15:54:05.046: INFO: Pod "pod-secrets-9c451d67-b1eb-405b-8d78-e05cf1426eb8": Phase="Pending", Reason="", readiness=false. Elapsed: 18.228301ms
Nov 15 15:54:07.063: INFO: Pod "pod-secrets-9c451d67-b1eb-405b-8d78-e05cf1426eb8": Phase="Running", Reason="", readiness=true. Elapsed: 2.035123107s
Nov 15 15:54:09.093: INFO: Pod "pod-secrets-9c451d67-b1eb-405b-8d78-e05cf1426eb8": Phase="Running", Reason="", readiness=false. Elapsed: 4.065376792s
Nov 15 15:54:11.063: INFO: Pod "pod-secrets-9c451d67-b1eb-405b-8d78-e05cf1426eb8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034784806s
STEP: Saw pod success 11/15/23 15:54:11.063
Nov 15 15:54:11.064: INFO: Pod "pod-secrets-9c451d67-b1eb-405b-8d78-e05cf1426eb8" satisfied condition "Succeeded or Failed"
Nov 15 15:54:11.078: INFO: Trying to get logs from node 10.15.40.115 pod pod-secrets-9c451d67-b1eb-405b-8d78-e05cf1426eb8 container secret-env-test: <nil>
STEP: delete the pod 11/15/23 15:54:11.115
Nov 15 15:54:11.151: INFO: Waiting for pod pod-secrets-9c451d67-b1eb-405b-8d78-e05cf1426eb8 to disappear
Nov 15 15:54:11.165: INFO: Pod pod-secrets-9c451d67-b1eb-405b-8d78-e05cf1426eb8 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 15 15:54:11.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-520" for this suite. 11/15/23 15:54:11.191
------------------------------
• [SLOW TEST] [6.350 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:54:04.864
    Nov 15 15:54:04.864: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename secrets 11/15/23 15:54:04.866
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:54:04.94
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:54:04.957
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-e70626f2-fc9e-420a-a52c-72bbd3ca577c 11/15/23 15:54:04.973
    STEP: Creating a pod to test consume secrets 11/15/23 15:54:04.997
    Nov 15 15:54:05.028: INFO: Waiting up to 5m0s for pod "pod-secrets-9c451d67-b1eb-405b-8d78-e05cf1426eb8" in namespace "secrets-520" to be "Succeeded or Failed"
    Nov 15 15:54:05.046: INFO: Pod "pod-secrets-9c451d67-b1eb-405b-8d78-e05cf1426eb8": Phase="Pending", Reason="", readiness=false. Elapsed: 18.228301ms
    Nov 15 15:54:07.063: INFO: Pod "pod-secrets-9c451d67-b1eb-405b-8d78-e05cf1426eb8": Phase="Running", Reason="", readiness=true. Elapsed: 2.035123107s
    Nov 15 15:54:09.093: INFO: Pod "pod-secrets-9c451d67-b1eb-405b-8d78-e05cf1426eb8": Phase="Running", Reason="", readiness=false. Elapsed: 4.065376792s
    Nov 15 15:54:11.063: INFO: Pod "pod-secrets-9c451d67-b1eb-405b-8d78-e05cf1426eb8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034784806s
    STEP: Saw pod success 11/15/23 15:54:11.063
    Nov 15 15:54:11.064: INFO: Pod "pod-secrets-9c451d67-b1eb-405b-8d78-e05cf1426eb8" satisfied condition "Succeeded or Failed"
    Nov 15 15:54:11.078: INFO: Trying to get logs from node 10.15.40.115 pod pod-secrets-9c451d67-b1eb-405b-8d78-e05cf1426eb8 container secret-env-test: <nil>
    STEP: delete the pod 11/15/23 15:54:11.115
    Nov 15 15:54:11.151: INFO: Waiting for pod pod-secrets-9c451d67-b1eb-405b-8d78-e05cf1426eb8 to disappear
    Nov 15 15:54:11.165: INFO: Pod pod-secrets-9c451d67-b1eb-405b-8d78-e05cf1426eb8 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:54:11.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-520" for this suite. 11/15/23 15:54:11.191
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:54:11.225
Nov 15 15:54:11.225: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename resourcequota 11/15/23 15:54:11.227
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:54:11.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:54:11.294
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 11/15/23 15:54:11.309
STEP: Getting a ResourceQuota 11/15/23 15:54:11.329
STEP: Listing all ResourceQuotas with LabelSelector 11/15/23 15:54:11.347
STEP: Patching the ResourceQuota 11/15/23 15:54:11.363
STEP: Deleting a Collection of ResourceQuotas 11/15/23 15:54:11.387
STEP: Verifying the deleted ResourceQuota 11/15/23 15:54:11.422
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Nov 15 15:54:11.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1183" for this suite. 11/15/23 15:54:11.462
------------------------------
• [0.263 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:54:11.225
    Nov 15 15:54:11.225: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename resourcequota 11/15/23 15:54:11.227
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:54:11.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:54:11.294
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 11/15/23 15:54:11.309
    STEP: Getting a ResourceQuota 11/15/23 15:54:11.329
    STEP: Listing all ResourceQuotas with LabelSelector 11/15/23 15:54:11.347
    STEP: Patching the ResourceQuota 11/15/23 15:54:11.363
    STEP: Deleting a Collection of ResourceQuotas 11/15/23 15:54:11.387
    STEP: Verifying the deleted ResourceQuota 11/15/23 15:54:11.422
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:54:11.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1183" for this suite. 11/15/23 15:54:11.462
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:54:11.494
Nov 15 15:54:11.494: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename lease-test 11/15/23 15:54:11.496
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:54:11.544
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:54:11.559
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Nov 15 15:54:11.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-641" for this suite. 11/15/23 15:54:11.837
------------------------------
• [0.371 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:54:11.494
    Nov 15 15:54:11.494: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename lease-test 11/15/23 15:54:11.496
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:54:11.544
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:54:11.559
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:54:11.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-641" for this suite. 11/15/23 15:54:11.837
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:54:11.869
Nov 15 15:54:11.869: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename projected 11/15/23 15:54:11.872
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:54:11.92
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:54:11.936
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 11/15/23 15:54:11.953
Nov 15 15:54:11.993: INFO: Waiting up to 5m0s for pod "downwardapi-volume-740cf168-1c19-414c-8da3-de69da5105fe" in namespace "projected-1859" to be "Succeeded or Failed"
Nov 15 15:54:12.007: INFO: Pod "downwardapi-volume-740cf168-1c19-414c-8da3-de69da5105fe": Phase="Pending", Reason="", readiness=false. Elapsed: 14.284158ms
Nov 15 15:54:14.024: INFO: Pod "downwardapi-volume-740cf168-1c19-414c-8da3-de69da5105fe": Phase="Running", Reason="", readiness=true. Elapsed: 2.031111667s
Nov 15 15:54:16.031: INFO: Pod "downwardapi-volume-740cf168-1c19-414c-8da3-de69da5105fe": Phase="Running", Reason="", readiness=false. Elapsed: 4.038746954s
Nov 15 15:54:18.025: INFO: Pod "downwardapi-volume-740cf168-1c19-414c-8da3-de69da5105fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032247148s
STEP: Saw pod success 11/15/23 15:54:18.025
Nov 15 15:54:18.026: INFO: Pod "downwardapi-volume-740cf168-1c19-414c-8da3-de69da5105fe" satisfied condition "Succeeded or Failed"
Nov 15 15:54:18.041: INFO: Trying to get logs from node 10.15.40.115 pod downwardapi-volume-740cf168-1c19-414c-8da3-de69da5105fe container client-container: <nil>
STEP: delete the pod 11/15/23 15:54:18.079
Nov 15 15:54:18.116: INFO: Waiting for pod downwardapi-volume-740cf168-1c19-414c-8da3-de69da5105fe to disappear
Nov 15 15:54:18.130: INFO: Pod downwardapi-volume-740cf168-1c19-414c-8da3-de69da5105fe no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Nov 15 15:54:18.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1859" for this suite. 11/15/23 15:54:18.154
------------------------------
• [SLOW TEST] [6.310 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:54:11.869
    Nov 15 15:54:11.869: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename projected 11/15/23 15:54:11.872
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:54:11.92
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:54:11.936
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 11/15/23 15:54:11.953
    Nov 15 15:54:11.993: INFO: Waiting up to 5m0s for pod "downwardapi-volume-740cf168-1c19-414c-8da3-de69da5105fe" in namespace "projected-1859" to be "Succeeded or Failed"
    Nov 15 15:54:12.007: INFO: Pod "downwardapi-volume-740cf168-1c19-414c-8da3-de69da5105fe": Phase="Pending", Reason="", readiness=false. Elapsed: 14.284158ms
    Nov 15 15:54:14.024: INFO: Pod "downwardapi-volume-740cf168-1c19-414c-8da3-de69da5105fe": Phase="Running", Reason="", readiness=true. Elapsed: 2.031111667s
    Nov 15 15:54:16.031: INFO: Pod "downwardapi-volume-740cf168-1c19-414c-8da3-de69da5105fe": Phase="Running", Reason="", readiness=false. Elapsed: 4.038746954s
    Nov 15 15:54:18.025: INFO: Pod "downwardapi-volume-740cf168-1c19-414c-8da3-de69da5105fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032247148s
    STEP: Saw pod success 11/15/23 15:54:18.025
    Nov 15 15:54:18.026: INFO: Pod "downwardapi-volume-740cf168-1c19-414c-8da3-de69da5105fe" satisfied condition "Succeeded or Failed"
    Nov 15 15:54:18.041: INFO: Trying to get logs from node 10.15.40.115 pod downwardapi-volume-740cf168-1c19-414c-8da3-de69da5105fe container client-container: <nil>
    STEP: delete the pod 11/15/23 15:54:18.079
    Nov 15 15:54:18.116: INFO: Waiting for pod downwardapi-volume-740cf168-1c19-414c-8da3-de69da5105fe to disappear
    Nov 15 15:54:18.130: INFO: Pod downwardapi-volume-740cf168-1c19-414c-8da3-de69da5105fe no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:54:18.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1859" for this suite. 11/15/23 15:54:18.154
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:54:18.194
Nov 15 15:54:18.194: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename services 11/15/23 15:54:18.196
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:54:18.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:54:18.255
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-865 11/15/23 15:54:18.271
STEP: creating service affinity-nodeport in namespace services-865 11/15/23 15:54:18.272
STEP: creating replication controller affinity-nodeport in namespace services-865 11/15/23 15:54:18.345
I1115 15:54:18.363209      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-865, replica count: 3
I1115 15:54:21.414011      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 15 15:54:21.470: INFO: Creating new exec pod
Nov 15 15:54:21.502: INFO: Waiting up to 5m0s for pod "execpod-affinityn5vt8" in namespace "services-865" to be "running"
Nov 15 15:54:21.517: INFO: Pod "execpod-affinityn5vt8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.350564ms
Nov 15 15:54:23.533: INFO: Pod "execpod-affinityn5vt8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031072961s
Nov 15 15:54:25.533: INFO: Pod "execpod-affinityn5vt8": Phase="Running", Reason="", readiness=true. Elapsed: 4.030386608s
Nov 15 15:54:25.533: INFO: Pod "execpod-affinityn5vt8" satisfied condition "running"
Nov 15 15:54:26.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-865 exec execpod-affinityn5vt8 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Nov 15 15:54:27.072: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Nov 15 15:54:27.072: INFO: stdout: ""
Nov 15 15:54:27.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-865 exec execpod-affinityn5vt8 -- /bin/sh -x -c nc -v -z -w 2 172.21.189.182 80'
Nov 15 15:54:27.500: INFO: stderr: "+ nc -v -z -w 2 172.21.189.182 80\nConnection to 172.21.189.182 80 port [tcp/http] succeeded!\n"
Nov 15 15:54:27.500: INFO: stdout: ""
Nov 15 15:54:27.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-865 exec execpod-affinityn5vt8 -- /bin/sh -x -c nc -v -z -w 2 10.15.40.106 31988'
Nov 15 15:54:27.998: INFO: stderr: "+ nc -v -z -w 2 10.15.40.106 31988\nConnection to 10.15.40.106 31988 port [tcp/*] succeeded!\n"
Nov 15 15:54:27.998: INFO: stdout: ""
Nov 15 15:54:27.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-865 exec execpod-affinityn5vt8 -- /bin/sh -x -c nc -v -z -w 2 10.15.40.115 31988'
Nov 15 15:54:28.422: INFO: stderr: "+ nc -v -z -w 2 10.15.40.115 31988\nConnection to 10.15.40.115 31988 port [tcp/*] succeeded!\n"
Nov 15 15:54:28.422: INFO: stdout: ""
Nov 15 15:54:28.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-865 exec execpod-affinityn5vt8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.15.40.106:31988/ ; done'
Nov 15 15:54:29.135: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31988/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31988/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31988/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31988/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31988/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31988/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31988/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31988/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31988/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31988/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31988/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31988/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31988/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31988/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31988/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31988/\n"
Nov 15 15:54:29.135: INFO: stdout: "\naffinity-nodeport-4pzvs\naffinity-nodeport-4pzvs\naffinity-nodeport-4pzvs\naffinity-nodeport-4pzvs\naffinity-nodeport-4pzvs\naffinity-nodeport-4pzvs\naffinity-nodeport-4pzvs\naffinity-nodeport-4pzvs\naffinity-nodeport-4pzvs\naffinity-nodeport-4pzvs\naffinity-nodeport-4pzvs\naffinity-nodeport-4pzvs\naffinity-nodeport-4pzvs\naffinity-nodeport-4pzvs\naffinity-nodeport-4pzvs\naffinity-nodeport-4pzvs"
Nov 15 15:54:29.136: INFO: Received response from host: affinity-nodeport-4pzvs
Nov 15 15:54:29.136: INFO: Received response from host: affinity-nodeport-4pzvs
Nov 15 15:54:29.136: INFO: Received response from host: affinity-nodeport-4pzvs
Nov 15 15:54:29.136: INFO: Received response from host: affinity-nodeport-4pzvs
Nov 15 15:54:29.136: INFO: Received response from host: affinity-nodeport-4pzvs
Nov 15 15:54:29.136: INFO: Received response from host: affinity-nodeport-4pzvs
Nov 15 15:54:29.136: INFO: Received response from host: affinity-nodeport-4pzvs
Nov 15 15:54:29.136: INFO: Received response from host: affinity-nodeport-4pzvs
Nov 15 15:54:29.136: INFO: Received response from host: affinity-nodeport-4pzvs
Nov 15 15:54:29.136: INFO: Received response from host: affinity-nodeport-4pzvs
Nov 15 15:54:29.136: INFO: Received response from host: affinity-nodeport-4pzvs
Nov 15 15:54:29.136: INFO: Received response from host: affinity-nodeport-4pzvs
Nov 15 15:54:29.136: INFO: Received response from host: affinity-nodeport-4pzvs
Nov 15 15:54:29.136: INFO: Received response from host: affinity-nodeport-4pzvs
Nov 15 15:54:29.136: INFO: Received response from host: affinity-nodeport-4pzvs
Nov 15 15:54:29.136: INFO: Received response from host: affinity-nodeport-4pzvs
Nov 15 15:54:29.136: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-865, will wait for the garbage collector to delete the pods 11/15/23 15:54:29.185
Nov 15 15:54:29.282: INFO: Deleting ReplicationController affinity-nodeport took: 27.50693ms
Nov 15 15:54:29.483: INFO: Terminating ReplicationController affinity-nodeport pods took: 200.806944ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 15 15:54:32.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-865" for this suite. 11/15/23 15:54:32.19
------------------------------
• [SLOW TEST] [14.046 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:54:18.194
    Nov 15 15:54:18.194: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename services 11/15/23 15:54:18.196
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:54:18.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:54:18.255
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-865 11/15/23 15:54:18.271
    STEP: creating service affinity-nodeport in namespace services-865 11/15/23 15:54:18.272
    STEP: creating replication controller affinity-nodeport in namespace services-865 11/15/23 15:54:18.345
    I1115 15:54:18.363209      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-865, replica count: 3
    I1115 15:54:21.414011      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 15 15:54:21.470: INFO: Creating new exec pod
    Nov 15 15:54:21.502: INFO: Waiting up to 5m0s for pod "execpod-affinityn5vt8" in namespace "services-865" to be "running"
    Nov 15 15:54:21.517: INFO: Pod "execpod-affinityn5vt8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.350564ms
    Nov 15 15:54:23.533: INFO: Pod "execpod-affinityn5vt8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031072961s
    Nov 15 15:54:25.533: INFO: Pod "execpod-affinityn5vt8": Phase="Running", Reason="", readiness=true. Elapsed: 4.030386608s
    Nov 15 15:54:25.533: INFO: Pod "execpod-affinityn5vt8" satisfied condition "running"
    Nov 15 15:54:26.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-865 exec execpod-affinityn5vt8 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Nov 15 15:54:27.072: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Nov 15 15:54:27.072: INFO: stdout: ""
    Nov 15 15:54:27.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-865 exec execpod-affinityn5vt8 -- /bin/sh -x -c nc -v -z -w 2 172.21.189.182 80'
    Nov 15 15:54:27.500: INFO: stderr: "+ nc -v -z -w 2 172.21.189.182 80\nConnection to 172.21.189.182 80 port [tcp/http] succeeded!\n"
    Nov 15 15:54:27.500: INFO: stdout: ""
    Nov 15 15:54:27.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-865 exec execpod-affinityn5vt8 -- /bin/sh -x -c nc -v -z -w 2 10.15.40.106 31988'
    Nov 15 15:54:27.998: INFO: stderr: "+ nc -v -z -w 2 10.15.40.106 31988\nConnection to 10.15.40.106 31988 port [tcp/*] succeeded!\n"
    Nov 15 15:54:27.998: INFO: stdout: ""
    Nov 15 15:54:27.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-865 exec execpod-affinityn5vt8 -- /bin/sh -x -c nc -v -z -w 2 10.15.40.115 31988'
    Nov 15 15:54:28.422: INFO: stderr: "+ nc -v -z -w 2 10.15.40.115 31988\nConnection to 10.15.40.115 31988 port [tcp/*] succeeded!\n"
    Nov 15 15:54:28.422: INFO: stdout: ""
    Nov 15 15:54:28.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-865 exec execpod-affinityn5vt8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.15.40.106:31988/ ; done'
    Nov 15 15:54:29.135: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31988/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31988/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31988/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31988/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31988/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31988/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31988/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31988/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31988/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31988/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31988/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31988/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31988/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31988/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31988/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31988/\n"
    Nov 15 15:54:29.135: INFO: stdout: "\naffinity-nodeport-4pzvs\naffinity-nodeport-4pzvs\naffinity-nodeport-4pzvs\naffinity-nodeport-4pzvs\naffinity-nodeport-4pzvs\naffinity-nodeport-4pzvs\naffinity-nodeport-4pzvs\naffinity-nodeport-4pzvs\naffinity-nodeport-4pzvs\naffinity-nodeport-4pzvs\naffinity-nodeport-4pzvs\naffinity-nodeport-4pzvs\naffinity-nodeport-4pzvs\naffinity-nodeport-4pzvs\naffinity-nodeport-4pzvs\naffinity-nodeport-4pzvs"
    Nov 15 15:54:29.136: INFO: Received response from host: affinity-nodeport-4pzvs
    Nov 15 15:54:29.136: INFO: Received response from host: affinity-nodeport-4pzvs
    Nov 15 15:54:29.136: INFO: Received response from host: affinity-nodeport-4pzvs
    Nov 15 15:54:29.136: INFO: Received response from host: affinity-nodeport-4pzvs
    Nov 15 15:54:29.136: INFO: Received response from host: affinity-nodeport-4pzvs
    Nov 15 15:54:29.136: INFO: Received response from host: affinity-nodeport-4pzvs
    Nov 15 15:54:29.136: INFO: Received response from host: affinity-nodeport-4pzvs
    Nov 15 15:54:29.136: INFO: Received response from host: affinity-nodeport-4pzvs
    Nov 15 15:54:29.136: INFO: Received response from host: affinity-nodeport-4pzvs
    Nov 15 15:54:29.136: INFO: Received response from host: affinity-nodeport-4pzvs
    Nov 15 15:54:29.136: INFO: Received response from host: affinity-nodeport-4pzvs
    Nov 15 15:54:29.136: INFO: Received response from host: affinity-nodeport-4pzvs
    Nov 15 15:54:29.136: INFO: Received response from host: affinity-nodeport-4pzvs
    Nov 15 15:54:29.136: INFO: Received response from host: affinity-nodeport-4pzvs
    Nov 15 15:54:29.136: INFO: Received response from host: affinity-nodeport-4pzvs
    Nov 15 15:54:29.136: INFO: Received response from host: affinity-nodeport-4pzvs
    Nov 15 15:54:29.136: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-865, will wait for the garbage collector to delete the pods 11/15/23 15:54:29.185
    Nov 15 15:54:29.282: INFO: Deleting ReplicationController affinity-nodeport took: 27.50693ms
    Nov 15 15:54:29.483: INFO: Terminating ReplicationController affinity-nodeport pods took: 200.806944ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:54:32.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-865" for this suite. 11/15/23 15:54:32.19
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:54:32.244
Nov 15 15:54:32.244: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename emptydir 11/15/23 15:54:32.248
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:54:32.307
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:54:32.322
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 11/15/23 15:54:32.339
Nov 15 15:54:32.379: INFO: Waiting up to 5m0s for pod "pod-c7005c67-0abc-45dc-8cde-75967575d149" in namespace "emptydir-8949" to be "Succeeded or Failed"
Nov 15 15:54:32.397: INFO: Pod "pod-c7005c67-0abc-45dc-8cde-75967575d149": Phase="Pending", Reason="", readiness=false. Elapsed: 18.117184ms
Nov 15 15:54:34.414: INFO: Pod "pod-c7005c67-0abc-45dc-8cde-75967575d149": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034660386s
Nov 15 15:54:36.414: INFO: Pod "pod-c7005c67-0abc-45dc-8cde-75967575d149": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035349244s
Nov 15 15:54:38.416: INFO: Pod "pod-c7005c67-0abc-45dc-8cde-75967575d149": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037365342s
STEP: Saw pod success 11/15/23 15:54:38.417
Nov 15 15:54:38.417: INFO: Pod "pod-c7005c67-0abc-45dc-8cde-75967575d149" satisfied condition "Succeeded or Failed"
Nov 15 15:54:38.432: INFO: Trying to get logs from node 10.15.40.115 pod pod-c7005c67-0abc-45dc-8cde-75967575d149 container test-container: <nil>
STEP: delete the pod 11/15/23 15:54:38.471
Nov 15 15:54:38.518: INFO: Waiting for pod pod-c7005c67-0abc-45dc-8cde-75967575d149 to disappear
Nov 15 15:54:38.533: INFO: Pod pod-c7005c67-0abc-45dc-8cde-75967575d149 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 15 15:54:38.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8949" for this suite. 11/15/23 15:54:38.556
------------------------------
• [SLOW TEST] [6.341 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:54:32.244
    Nov 15 15:54:32.244: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename emptydir 11/15/23 15:54:32.248
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:54:32.307
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:54:32.322
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 11/15/23 15:54:32.339
    Nov 15 15:54:32.379: INFO: Waiting up to 5m0s for pod "pod-c7005c67-0abc-45dc-8cde-75967575d149" in namespace "emptydir-8949" to be "Succeeded or Failed"
    Nov 15 15:54:32.397: INFO: Pod "pod-c7005c67-0abc-45dc-8cde-75967575d149": Phase="Pending", Reason="", readiness=false. Elapsed: 18.117184ms
    Nov 15 15:54:34.414: INFO: Pod "pod-c7005c67-0abc-45dc-8cde-75967575d149": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034660386s
    Nov 15 15:54:36.414: INFO: Pod "pod-c7005c67-0abc-45dc-8cde-75967575d149": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035349244s
    Nov 15 15:54:38.416: INFO: Pod "pod-c7005c67-0abc-45dc-8cde-75967575d149": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037365342s
    STEP: Saw pod success 11/15/23 15:54:38.417
    Nov 15 15:54:38.417: INFO: Pod "pod-c7005c67-0abc-45dc-8cde-75967575d149" satisfied condition "Succeeded or Failed"
    Nov 15 15:54:38.432: INFO: Trying to get logs from node 10.15.40.115 pod pod-c7005c67-0abc-45dc-8cde-75967575d149 container test-container: <nil>
    STEP: delete the pod 11/15/23 15:54:38.471
    Nov 15 15:54:38.518: INFO: Waiting for pod pod-c7005c67-0abc-45dc-8cde-75967575d149 to disappear
    Nov 15 15:54:38.533: INFO: Pod pod-c7005c67-0abc-45dc-8cde-75967575d149 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:54:38.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8949" for this suite. 11/15/23 15:54:38.556
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:54:38.588
Nov 15 15:54:38.588: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename webhook 11/15/23 15:54:38.591
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:54:38.703
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:54:38.719
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/15/23 15:54:38.784
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 15:54:39.012
STEP: Deploying the webhook pod 11/15/23 15:54:39.042
STEP: Wait for the deployment to be ready 11/15/23 15:54:39.088
Nov 15 15:54:39.127: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 15 15:54:41.181: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 54, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 54, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 54, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 54, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 11/15/23 15:54:43.201
STEP: Verifying the service has paired with the endpoint 11/15/23 15:54:43.242
Nov 15 15:54:44.243: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 11/15/23 15:54:44.259
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 11/15/23 15:54:44.268
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 11/15/23 15:54:44.268
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 11/15/23 15:54:44.269
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 11/15/23 15:54:44.277
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 11/15/23 15:54:44.277
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 11/15/23 15:54:44.285
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 15:54:44.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8252" for this suite. 11/15/23 15:54:44.494
STEP: Destroying namespace "webhook-8252-markers" for this suite. 11/15/23 15:54:44.523
------------------------------
• [SLOW TEST] [5.960 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:54:38.588
    Nov 15 15:54:38.588: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename webhook 11/15/23 15:54:38.591
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:54:38.703
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:54:38.719
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/15/23 15:54:38.784
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 15:54:39.012
    STEP: Deploying the webhook pod 11/15/23 15:54:39.042
    STEP: Wait for the deployment to be ready 11/15/23 15:54:39.088
    Nov 15 15:54:39.127: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Nov 15 15:54:41.181: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 54, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 54, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 54, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 54, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 11/15/23 15:54:43.201
    STEP: Verifying the service has paired with the endpoint 11/15/23 15:54:43.242
    Nov 15 15:54:44.243: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 11/15/23 15:54:44.259
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 11/15/23 15:54:44.268
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 11/15/23 15:54:44.268
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 11/15/23 15:54:44.269
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 11/15/23 15:54:44.277
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 11/15/23 15:54:44.277
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 11/15/23 15:54:44.285
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:54:44.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8252" for this suite. 11/15/23 15:54:44.494
    STEP: Destroying namespace "webhook-8252-markers" for this suite. 11/15/23 15:54:44.523
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:54:44.553
Nov 15 15:54:44.553: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename replication-controller 11/15/23 15:54:44.555
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:54:44.603
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:54:44.616
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 11/15/23 15:54:44.656
STEP: waiting for RC to be added 11/15/23 15:54:44.678
STEP: waiting for available Replicas 11/15/23 15:54:44.68
STEP: patching ReplicationController 11/15/23 15:54:46.866
STEP: waiting for RC to be modified 11/15/23 15:54:46.898
STEP: patching ReplicationController status 11/15/23 15:54:46.898
STEP: waiting for RC to be modified 11/15/23 15:54:46.924
STEP: waiting for available Replicas 11/15/23 15:54:46.925
STEP: fetching ReplicationController status 11/15/23 15:54:46.934
STEP: patching ReplicationController scale 11/15/23 15:54:46.952
STEP: waiting for RC to be modified 11/15/23 15:54:46.977
STEP: waiting for ReplicationController's scale to be the max amount 11/15/23 15:54:46.978
STEP: fetching ReplicationController; ensuring that it's patched 11/15/23 15:54:53.155
STEP: updating ReplicationController status 11/15/23 15:54:53.176
STEP: waiting for RC to be modified 11/15/23 15:54:53.199
STEP: listing all ReplicationControllers 11/15/23 15:54:53.2
STEP: checking that ReplicationController has expected values 11/15/23 15:54:53.218
STEP: deleting ReplicationControllers by collection 11/15/23 15:54:53.218
STEP: waiting for ReplicationController to have a DELETED watchEvent 11/15/23 15:54:53.265
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Nov 15 15:54:53.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-3717" for this suite. 11/15/23 15:54:53.451
------------------------------
• [SLOW TEST] [8.924 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:54:44.553
    Nov 15 15:54:44.553: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename replication-controller 11/15/23 15:54:44.555
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:54:44.603
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:54:44.616
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 11/15/23 15:54:44.656
    STEP: waiting for RC to be added 11/15/23 15:54:44.678
    STEP: waiting for available Replicas 11/15/23 15:54:44.68
    STEP: patching ReplicationController 11/15/23 15:54:46.866
    STEP: waiting for RC to be modified 11/15/23 15:54:46.898
    STEP: patching ReplicationController status 11/15/23 15:54:46.898
    STEP: waiting for RC to be modified 11/15/23 15:54:46.924
    STEP: waiting for available Replicas 11/15/23 15:54:46.925
    STEP: fetching ReplicationController status 11/15/23 15:54:46.934
    STEP: patching ReplicationController scale 11/15/23 15:54:46.952
    STEP: waiting for RC to be modified 11/15/23 15:54:46.977
    STEP: waiting for ReplicationController's scale to be the max amount 11/15/23 15:54:46.978
    STEP: fetching ReplicationController; ensuring that it's patched 11/15/23 15:54:53.155
    STEP: updating ReplicationController status 11/15/23 15:54:53.176
    STEP: waiting for RC to be modified 11/15/23 15:54:53.199
    STEP: listing all ReplicationControllers 11/15/23 15:54:53.2
    STEP: checking that ReplicationController has expected values 11/15/23 15:54:53.218
    STEP: deleting ReplicationControllers by collection 11/15/23 15:54:53.218
    STEP: waiting for ReplicationController to have a DELETED watchEvent 11/15/23 15:54:53.265
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:54:53.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-3717" for this suite. 11/15/23 15:54:53.451
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:54:53.48
Nov 15 15:54:53.481: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename pod-network-test 11/15/23 15:54:53.484
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:54:53.536
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:54:53.553
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-690 11/15/23 15:54:53.573
STEP: creating a selector 11/15/23 15:54:53.574
STEP: Creating the service pods in kubernetes 11/15/23 15:54:53.574
Nov 15 15:54:53.575: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 15 15:54:53.694: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-690" to be "running and ready"
Nov 15 15:54:53.708: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.095387ms
Nov 15 15:54:53.708: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 15:54:55.726: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.032244022s
Nov 15 15:54:55.727: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 15:54:57.727: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.032743343s
Nov 15 15:54:57.727: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 15:54:59.731: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.037062862s
Nov 15 15:54:59.731: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 15:55:01.791: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.096590487s
Nov 15 15:55:01.791: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 15:55:03.731: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.036696775s
Nov 15 15:55:03.731: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 15:55:05.726: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.031482397s
Nov 15 15:55:05.726: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Nov 15 15:55:05.726: INFO: Pod "netserver-0" satisfied condition "running and ready"
Nov 15 15:55:05.740: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-690" to be "running and ready"
Nov 15 15:55:05.756: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 16.449573ms
Nov 15 15:55:05.756: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Nov 15 15:55:05.757: INFO: Pod "netserver-1" satisfied condition "running and ready"
Nov 15 15:55:05.774: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-690" to be "running and ready"
Nov 15 15:55:05.789: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 14.72894ms
Nov 15 15:55:05.789: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Nov 15 15:55:07.806: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 2.031960757s
Nov 15 15:55:07.806: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Nov 15 15:55:09.808: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 4.033713897s
Nov 15 15:55:09.808: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Nov 15 15:55:11.811: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 6.037227529s
Nov 15 15:55:11.811: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Nov 15 15:55:13.808: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 8.0343672s
Nov 15 15:55:13.809: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Nov 15 15:55:15.805: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 10.031467864s
Nov 15 15:55:15.806: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Nov 15 15:55:15.806: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 11/15/23 15:55:15.822
Nov 15 15:55:15.862: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-690" to be "running"
Nov 15 15:55:15.877: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.725292ms
Nov 15 15:55:17.893: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031368012s
Nov 15 15:55:19.896: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.033684293s
Nov 15 15:55:19.896: INFO: Pod "test-container-pod" satisfied condition "running"
Nov 15 15:55:19.920: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-690" to be "running"
Nov 15 15:55:19.935: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 15.353524ms
Nov 15 15:55:19.935: INFO: Pod "host-test-container-pod" satisfied condition "running"
Nov 15 15:55:19.951: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Nov 15 15:55:19.952: INFO: Going to poll 172.30.191.107 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Nov 15 15:55:19.967: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.191.107 8081 | grep -v '^\s*$'] Namespace:pod-network-test-690 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 15:55:19.967: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
Nov 15 15:55:19.968: INFO: ExecWithOptions: Clientset creation
Nov 15 15:55:19.969: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-690/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.191.107+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Nov 15 15:55:21.285: INFO: Found all 1 expected endpoints: [netserver-0]
Nov 15 15:55:21.285: INFO: Going to poll 172.30.205.237 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Nov 15 15:55:21.302: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.205.237 8081 | grep -v '^\s*$'] Namespace:pod-network-test-690 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 15:55:21.302: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
Nov 15 15:55:21.303: INFO: ExecWithOptions: Clientset creation
Nov 15 15:55:21.303: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-690/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.205.237+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Nov 15 15:55:22.635: INFO: Found all 1 expected endpoints: [netserver-1]
Nov 15 15:55:22.636: INFO: Going to poll 172.30.164.31 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Nov 15 15:55:22.651: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.164.31 8081 | grep -v '^\s*$'] Namespace:pod-network-test-690 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 15:55:22.652: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
Nov 15 15:55:22.653: INFO: ExecWithOptions: Clientset creation
Nov 15 15:55:22.653: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-690/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.164.31+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Nov 15 15:55:23.972: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Nov 15 15:55:23.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-690" for this suite. 11/15/23 15:55:24.008
------------------------------
• [SLOW TEST] [30.552 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:54:53.48
    Nov 15 15:54:53.481: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename pod-network-test 11/15/23 15:54:53.484
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:54:53.536
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:54:53.553
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-690 11/15/23 15:54:53.573
    STEP: creating a selector 11/15/23 15:54:53.574
    STEP: Creating the service pods in kubernetes 11/15/23 15:54:53.574
    Nov 15 15:54:53.575: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Nov 15 15:54:53.694: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-690" to be "running and ready"
    Nov 15 15:54:53.708: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.095387ms
    Nov 15 15:54:53.708: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 15:54:55.726: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.032244022s
    Nov 15 15:54:55.727: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 15:54:57.727: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.032743343s
    Nov 15 15:54:57.727: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 15:54:59.731: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.037062862s
    Nov 15 15:54:59.731: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 15:55:01.791: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.096590487s
    Nov 15 15:55:01.791: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 15:55:03.731: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.036696775s
    Nov 15 15:55:03.731: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 15:55:05.726: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.031482397s
    Nov 15 15:55:05.726: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Nov 15 15:55:05.726: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Nov 15 15:55:05.740: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-690" to be "running and ready"
    Nov 15 15:55:05.756: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 16.449573ms
    Nov 15 15:55:05.756: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Nov 15 15:55:05.757: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Nov 15 15:55:05.774: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-690" to be "running and ready"
    Nov 15 15:55:05.789: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 14.72894ms
    Nov 15 15:55:05.789: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Nov 15 15:55:07.806: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 2.031960757s
    Nov 15 15:55:07.806: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Nov 15 15:55:09.808: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 4.033713897s
    Nov 15 15:55:09.808: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Nov 15 15:55:11.811: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 6.037227529s
    Nov 15 15:55:11.811: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Nov 15 15:55:13.808: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 8.0343672s
    Nov 15 15:55:13.809: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Nov 15 15:55:15.805: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 10.031467864s
    Nov 15 15:55:15.806: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Nov 15 15:55:15.806: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 11/15/23 15:55:15.822
    Nov 15 15:55:15.862: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-690" to be "running"
    Nov 15 15:55:15.877: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.725292ms
    Nov 15 15:55:17.893: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031368012s
    Nov 15 15:55:19.896: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.033684293s
    Nov 15 15:55:19.896: INFO: Pod "test-container-pod" satisfied condition "running"
    Nov 15 15:55:19.920: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-690" to be "running"
    Nov 15 15:55:19.935: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 15.353524ms
    Nov 15 15:55:19.935: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Nov 15 15:55:19.951: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Nov 15 15:55:19.952: INFO: Going to poll 172.30.191.107 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Nov 15 15:55:19.967: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.191.107 8081 | grep -v '^\s*$'] Namespace:pod-network-test-690 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 15:55:19.967: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    Nov 15 15:55:19.968: INFO: ExecWithOptions: Clientset creation
    Nov 15 15:55:19.969: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-690/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.191.107+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Nov 15 15:55:21.285: INFO: Found all 1 expected endpoints: [netserver-0]
    Nov 15 15:55:21.285: INFO: Going to poll 172.30.205.237 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Nov 15 15:55:21.302: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.205.237 8081 | grep -v '^\s*$'] Namespace:pod-network-test-690 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 15:55:21.302: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    Nov 15 15:55:21.303: INFO: ExecWithOptions: Clientset creation
    Nov 15 15:55:21.303: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-690/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.205.237+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Nov 15 15:55:22.635: INFO: Found all 1 expected endpoints: [netserver-1]
    Nov 15 15:55:22.636: INFO: Going to poll 172.30.164.31 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Nov 15 15:55:22.651: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.164.31 8081 | grep -v '^\s*$'] Namespace:pod-network-test-690 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 15:55:22.652: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    Nov 15 15:55:22.653: INFO: ExecWithOptions: Clientset creation
    Nov 15 15:55:22.653: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-690/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.164.31+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Nov 15 15:55:23.972: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:55:23.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-690" for this suite. 11/15/23 15:55:24.008
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:55:24.04
Nov 15 15:55:24.040: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename statefulset 11/15/23 15:55:24.042
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:55:24.099
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:55:24.114
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9554 11/15/23 15:55:24.132
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-9554 11/15/23 15:55:24.151
Nov 15 15:55:24.189: INFO: Found 0 stateful pods, waiting for 1
Nov 15 15:55:34.206: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 11/15/23 15:55:34.24
STEP: updating a scale subresource 11/15/23 15:55:34.255
STEP: verifying the statefulset Spec.Replicas was modified 11/15/23 15:55:34.275
STEP: Patch a scale subresource 11/15/23 15:55:34.291
STEP: verifying the statefulset Spec.Replicas was modified 11/15/23 15:55:34.312
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Nov 15 15:55:34.327: INFO: Deleting all statefulset in ns statefulset-9554
Nov 15 15:55:34.343: INFO: Scaling statefulset ss to 0
Nov 15 15:55:54.446: INFO: Waiting for statefulset status.replicas updated to 0
Nov 15 15:55:54.466: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Nov 15 15:55:54.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9554" for this suite. 11/15/23 15:55:54.585
------------------------------
• [SLOW TEST] [30.572 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:55:24.04
    Nov 15 15:55:24.040: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename statefulset 11/15/23 15:55:24.042
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:55:24.099
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:55:24.114
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9554 11/15/23 15:55:24.132
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-9554 11/15/23 15:55:24.151
    Nov 15 15:55:24.189: INFO: Found 0 stateful pods, waiting for 1
    Nov 15 15:55:34.206: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 11/15/23 15:55:34.24
    STEP: updating a scale subresource 11/15/23 15:55:34.255
    STEP: verifying the statefulset Spec.Replicas was modified 11/15/23 15:55:34.275
    STEP: Patch a scale subresource 11/15/23 15:55:34.291
    STEP: verifying the statefulset Spec.Replicas was modified 11/15/23 15:55:34.312
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Nov 15 15:55:34.327: INFO: Deleting all statefulset in ns statefulset-9554
    Nov 15 15:55:34.343: INFO: Scaling statefulset ss to 0
    Nov 15 15:55:54.446: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 15 15:55:54.466: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:55:54.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9554" for this suite. 11/15/23 15:55:54.585
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:55:54.623
Nov 15 15:55:54.624: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename container-probe 11/15/23 15:55:54.625
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:55:54.68
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:55:54.695
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Nov 15 15:55:54.749: INFO: Waiting up to 5m0s for pod "test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832" in namespace "container-probe-7639" to be "running and ready"
Nov 15 15:55:54.763: INFO: Pod "test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832": Phase="Pending", Reason="", readiness=false. Elapsed: 14.679745ms
Nov 15 15:55:54.764: INFO: The phase of Pod test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 15:55:56.782: INFO: Pod "test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832": Phase="Running", Reason="", readiness=false. Elapsed: 2.032913811s
Nov 15 15:55:56.782: INFO: The phase of Pod test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832 is Running (Ready = false)
Nov 15 15:55:58.780: INFO: Pod "test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832": Phase="Running", Reason="", readiness=false. Elapsed: 4.030902746s
Nov 15 15:55:58.780: INFO: The phase of Pod test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832 is Running (Ready = false)
Nov 15 15:56:00.780: INFO: Pod "test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832": Phase="Running", Reason="", readiness=false. Elapsed: 6.031581399s
Nov 15 15:56:00.780: INFO: The phase of Pod test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832 is Running (Ready = false)
Nov 15 15:56:02.782: INFO: Pod "test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832": Phase="Running", Reason="", readiness=false. Elapsed: 8.033319546s
Nov 15 15:56:02.782: INFO: The phase of Pod test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832 is Running (Ready = false)
Nov 15 15:56:04.785: INFO: Pod "test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832": Phase="Running", Reason="", readiness=false. Elapsed: 10.036390573s
Nov 15 15:56:04.785: INFO: The phase of Pod test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832 is Running (Ready = false)
Nov 15 15:56:06.783: INFO: Pod "test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832": Phase="Running", Reason="", readiness=false. Elapsed: 12.034332261s
Nov 15 15:56:06.783: INFO: The phase of Pod test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832 is Running (Ready = false)
Nov 15 15:56:08.780: INFO: Pod "test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832": Phase="Running", Reason="", readiness=false. Elapsed: 14.03135561s
Nov 15 15:56:08.780: INFO: The phase of Pod test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832 is Running (Ready = false)
Nov 15 15:56:10.780: INFO: Pod "test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832": Phase="Running", Reason="", readiness=false. Elapsed: 16.031096876s
Nov 15 15:56:10.780: INFO: The phase of Pod test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832 is Running (Ready = false)
Nov 15 15:56:12.786: INFO: Pod "test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832": Phase="Running", Reason="", readiness=false. Elapsed: 18.037604495s
Nov 15 15:56:12.786: INFO: The phase of Pod test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832 is Running (Ready = false)
Nov 15 15:56:14.802: INFO: Pod "test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832": Phase="Running", Reason="", readiness=false. Elapsed: 20.053077914s
Nov 15 15:56:14.802: INFO: The phase of Pod test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832 is Running (Ready = false)
Nov 15 15:56:16.787: INFO: Pod "test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832": Phase="Running", Reason="", readiness=true. Elapsed: 22.038245573s
Nov 15 15:56:16.787: INFO: The phase of Pod test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832 is Running (Ready = true)
Nov 15 15:56:16.787: INFO: Pod "test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832" satisfied condition "running and ready"
Nov 15 15:56:16.803: INFO: Container started at 2023-11-15 15:55:56 +0000 UTC, pod became ready at 2023-11-15 15:56:15 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Nov 15 15:56:16.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-7639" for this suite. 11/15/23 15:56:16.833
------------------------------
• [SLOW TEST] [22.236 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:55:54.623
    Nov 15 15:55:54.624: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename container-probe 11/15/23 15:55:54.625
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:55:54.68
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:55:54.695
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Nov 15 15:55:54.749: INFO: Waiting up to 5m0s for pod "test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832" in namespace "container-probe-7639" to be "running and ready"
    Nov 15 15:55:54.763: INFO: Pod "test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832": Phase="Pending", Reason="", readiness=false. Elapsed: 14.679745ms
    Nov 15 15:55:54.764: INFO: The phase of Pod test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 15:55:56.782: INFO: Pod "test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832": Phase="Running", Reason="", readiness=false. Elapsed: 2.032913811s
    Nov 15 15:55:56.782: INFO: The phase of Pod test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832 is Running (Ready = false)
    Nov 15 15:55:58.780: INFO: Pod "test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832": Phase="Running", Reason="", readiness=false. Elapsed: 4.030902746s
    Nov 15 15:55:58.780: INFO: The phase of Pod test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832 is Running (Ready = false)
    Nov 15 15:56:00.780: INFO: Pod "test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832": Phase="Running", Reason="", readiness=false. Elapsed: 6.031581399s
    Nov 15 15:56:00.780: INFO: The phase of Pod test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832 is Running (Ready = false)
    Nov 15 15:56:02.782: INFO: Pod "test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832": Phase="Running", Reason="", readiness=false. Elapsed: 8.033319546s
    Nov 15 15:56:02.782: INFO: The phase of Pod test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832 is Running (Ready = false)
    Nov 15 15:56:04.785: INFO: Pod "test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832": Phase="Running", Reason="", readiness=false. Elapsed: 10.036390573s
    Nov 15 15:56:04.785: INFO: The phase of Pod test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832 is Running (Ready = false)
    Nov 15 15:56:06.783: INFO: Pod "test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832": Phase="Running", Reason="", readiness=false. Elapsed: 12.034332261s
    Nov 15 15:56:06.783: INFO: The phase of Pod test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832 is Running (Ready = false)
    Nov 15 15:56:08.780: INFO: Pod "test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832": Phase="Running", Reason="", readiness=false. Elapsed: 14.03135561s
    Nov 15 15:56:08.780: INFO: The phase of Pod test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832 is Running (Ready = false)
    Nov 15 15:56:10.780: INFO: Pod "test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832": Phase="Running", Reason="", readiness=false. Elapsed: 16.031096876s
    Nov 15 15:56:10.780: INFO: The phase of Pod test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832 is Running (Ready = false)
    Nov 15 15:56:12.786: INFO: Pod "test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832": Phase="Running", Reason="", readiness=false. Elapsed: 18.037604495s
    Nov 15 15:56:12.786: INFO: The phase of Pod test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832 is Running (Ready = false)
    Nov 15 15:56:14.802: INFO: Pod "test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832": Phase="Running", Reason="", readiness=false. Elapsed: 20.053077914s
    Nov 15 15:56:14.802: INFO: The phase of Pod test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832 is Running (Ready = false)
    Nov 15 15:56:16.787: INFO: Pod "test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832": Phase="Running", Reason="", readiness=true. Elapsed: 22.038245573s
    Nov 15 15:56:16.787: INFO: The phase of Pod test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832 is Running (Ready = true)
    Nov 15 15:56:16.787: INFO: Pod "test-webserver-e1273c1f-2f1b-4c5e-8c2f-31c07cf01832" satisfied condition "running and ready"
    Nov 15 15:56:16.803: INFO: Container started at 2023-11-15 15:55:56 +0000 UTC, pod became ready at 2023-11-15 15:56:15 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:56:16.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-7639" for this suite. 11/15/23 15:56:16.833
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:56:16.864
Nov 15 15:56:16.864: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename projected 11/15/23 15:56:16.867
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:56:16.919
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:56:16.964
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-4ec61dda-cd39-4a29-808a-0dc5260f0aec 11/15/23 15:56:16.978
STEP: Creating a pod to test consume secrets 11/15/23 15:56:16.992
Nov 15 15:56:17.021: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6ec8ee89-860d-4a28-af30-8fb381ea8845" in namespace "projected-572" to be "Succeeded or Failed"
Nov 15 15:56:17.037: INFO: Pod "pod-projected-secrets-6ec8ee89-860d-4a28-af30-8fb381ea8845": Phase="Pending", Reason="", readiness=false. Elapsed: 16.838975ms
Nov 15 15:56:19.059: INFO: Pod "pod-projected-secrets-6ec8ee89-860d-4a28-af30-8fb381ea8845": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038255465s
Nov 15 15:56:21.056: INFO: Pod "pod-projected-secrets-6ec8ee89-860d-4a28-af30-8fb381ea8845": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03510421s
Nov 15 15:56:23.055: INFO: Pod "pod-projected-secrets-6ec8ee89-860d-4a28-af30-8fb381ea8845": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034752207s
STEP: Saw pod success 11/15/23 15:56:23.055
Nov 15 15:56:23.056: INFO: Pod "pod-projected-secrets-6ec8ee89-860d-4a28-af30-8fb381ea8845" satisfied condition "Succeeded or Failed"
Nov 15 15:56:23.075: INFO: Trying to get logs from node 10.15.40.115 pod pod-projected-secrets-6ec8ee89-860d-4a28-af30-8fb381ea8845 container projected-secret-volume-test: <nil>
STEP: delete the pod 11/15/23 15:56:23.191
Nov 15 15:56:23.253: INFO: Waiting for pod pod-projected-secrets-6ec8ee89-860d-4a28-af30-8fb381ea8845 to disappear
Nov 15 15:56:23.269: INFO: Pod pod-projected-secrets-6ec8ee89-860d-4a28-af30-8fb381ea8845 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Nov 15 15:56:23.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-572" for this suite. 11/15/23 15:56:23.287
------------------------------
• [SLOW TEST] [6.444 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:56:16.864
    Nov 15 15:56:16.864: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename projected 11/15/23 15:56:16.867
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:56:16.919
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:56:16.964
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-4ec61dda-cd39-4a29-808a-0dc5260f0aec 11/15/23 15:56:16.978
    STEP: Creating a pod to test consume secrets 11/15/23 15:56:16.992
    Nov 15 15:56:17.021: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6ec8ee89-860d-4a28-af30-8fb381ea8845" in namespace "projected-572" to be "Succeeded or Failed"
    Nov 15 15:56:17.037: INFO: Pod "pod-projected-secrets-6ec8ee89-860d-4a28-af30-8fb381ea8845": Phase="Pending", Reason="", readiness=false. Elapsed: 16.838975ms
    Nov 15 15:56:19.059: INFO: Pod "pod-projected-secrets-6ec8ee89-860d-4a28-af30-8fb381ea8845": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038255465s
    Nov 15 15:56:21.056: INFO: Pod "pod-projected-secrets-6ec8ee89-860d-4a28-af30-8fb381ea8845": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03510421s
    Nov 15 15:56:23.055: INFO: Pod "pod-projected-secrets-6ec8ee89-860d-4a28-af30-8fb381ea8845": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034752207s
    STEP: Saw pod success 11/15/23 15:56:23.055
    Nov 15 15:56:23.056: INFO: Pod "pod-projected-secrets-6ec8ee89-860d-4a28-af30-8fb381ea8845" satisfied condition "Succeeded or Failed"
    Nov 15 15:56:23.075: INFO: Trying to get logs from node 10.15.40.115 pod pod-projected-secrets-6ec8ee89-860d-4a28-af30-8fb381ea8845 container projected-secret-volume-test: <nil>
    STEP: delete the pod 11/15/23 15:56:23.191
    Nov 15 15:56:23.253: INFO: Waiting for pod pod-projected-secrets-6ec8ee89-860d-4a28-af30-8fb381ea8845 to disappear
    Nov 15 15:56:23.269: INFO: Pod pod-projected-secrets-6ec8ee89-860d-4a28-af30-8fb381ea8845 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:56:23.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-572" for this suite. 11/15/23 15:56:23.287
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:56:23.312
Nov 15 15:56:23.312: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename pods 11/15/23 15:56:23.315
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:56:23.353
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:56:23.367
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Nov 15 15:56:23.381: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: creating the pod 11/15/23 15:56:23.383
STEP: submitting the pod to kubernetes 11/15/23 15:56:23.383
Nov 15 15:56:23.411: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-c0900654-e127-4a0b-bff3-729f07053438" in namespace "pods-1492" to be "running and ready"
Nov 15 15:56:23.431: INFO: Pod "pod-logs-websocket-c0900654-e127-4a0b-bff3-729f07053438": Phase="Pending", Reason="", readiness=false. Elapsed: 19.881815ms
Nov 15 15:56:23.431: INFO: The phase of Pod pod-logs-websocket-c0900654-e127-4a0b-bff3-729f07053438 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 15:56:25.448: INFO: Pod "pod-logs-websocket-c0900654-e127-4a0b-bff3-729f07053438": Phase="Running", Reason="", readiness=true. Elapsed: 2.036919981s
Nov 15 15:56:25.448: INFO: The phase of Pod pod-logs-websocket-c0900654-e127-4a0b-bff3-729f07053438 is Running (Ready = true)
Nov 15 15:56:25.448: INFO: Pod "pod-logs-websocket-c0900654-e127-4a0b-bff3-729f07053438" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Nov 15 15:56:25.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1492" for this suite. 11/15/23 15:56:25.666
------------------------------
• [2.374 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:56:23.312
    Nov 15 15:56:23.312: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename pods 11/15/23 15:56:23.315
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:56:23.353
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:56:23.367
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Nov 15 15:56:23.381: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: creating the pod 11/15/23 15:56:23.383
    STEP: submitting the pod to kubernetes 11/15/23 15:56:23.383
    Nov 15 15:56:23.411: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-c0900654-e127-4a0b-bff3-729f07053438" in namespace "pods-1492" to be "running and ready"
    Nov 15 15:56:23.431: INFO: Pod "pod-logs-websocket-c0900654-e127-4a0b-bff3-729f07053438": Phase="Pending", Reason="", readiness=false. Elapsed: 19.881815ms
    Nov 15 15:56:23.431: INFO: The phase of Pod pod-logs-websocket-c0900654-e127-4a0b-bff3-729f07053438 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 15:56:25.448: INFO: Pod "pod-logs-websocket-c0900654-e127-4a0b-bff3-729f07053438": Phase="Running", Reason="", readiness=true. Elapsed: 2.036919981s
    Nov 15 15:56:25.448: INFO: The phase of Pod pod-logs-websocket-c0900654-e127-4a0b-bff3-729f07053438 is Running (Ready = true)
    Nov 15 15:56:25.448: INFO: Pod "pod-logs-websocket-c0900654-e127-4a0b-bff3-729f07053438" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:56:25.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1492" for this suite. 11/15/23 15:56:25.666
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:56:25.691
Nov 15 15:56:25.691: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename emptydir 11/15/23 15:56:25.693
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:56:25.736
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:56:25.749
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 11/15/23 15:56:25.763
Nov 15 15:56:25.792: INFO: Waiting up to 5m0s for pod "pod-8f563905-c483-4b88-83cf-9da48ced763a" in namespace "emptydir-9002" to be "Succeeded or Failed"
Nov 15 15:56:25.809: INFO: Pod "pod-8f563905-c483-4b88-83cf-9da48ced763a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.785233ms
Nov 15 15:56:27.832: INFO: Pod "pod-8f563905-c483-4b88-83cf-9da48ced763a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039639044s
Nov 15 15:56:29.830: INFO: Pod "pod-8f563905-c483-4b88-83cf-9da48ced763a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037354477s
STEP: Saw pod success 11/15/23 15:56:29.83
Nov 15 15:56:29.831: INFO: Pod "pod-8f563905-c483-4b88-83cf-9da48ced763a" satisfied condition "Succeeded or Failed"
Nov 15 15:56:29.849: INFO: Trying to get logs from node 10.15.40.115 pod pod-8f563905-c483-4b88-83cf-9da48ced763a container test-container: <nil>
STEP: delete the pod 11/15/23 15:56:29.889
Nov 15 15:56:29.960: INFO: Waiting for pod pod-8f563905-c483-4b88-83cf-9da48ced763a to disappear
Nov 15 15:56:29.981: INFO: Pod pod-8f563905-c483-4b88-83cf-9da48ced763a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 15 15:56:29.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9002" for this suite. 11/15/23 15:56:30
------------------------------
• [4.330 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:56:25.691
    Nov 15 15:56:25.691: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename emptydir 11/15/23 15:56:25.693
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:56:25.736
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:56:25.749
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 11/15/23 15:56:25.763
    Nov 15 15:56:25.792: INFO: Waiting up to 5m0s for pod "pod-8f563905-c483-4b88-83cf-9da48ced763a" in namespace "emptydir-9002" to be "Succeeded or Failed"
    Nov 15 15:56:25.809: INFO: Pod "pod-8f563905-c483-4b88-83cf-9da48ced763a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.785233ms
    Nov 15 15:56:27.832: INFO: Pod "pod-8f563905-c483-4b88-83cf-9da48ced763a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039639044s
    Nov 15 15:56:29.830: INFO: Pod "pod-8f563905-c483-4b88-83cf-9da48ced763a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037354477s
    STEP: Saw pod success 11/15/23 15:56:29.83
    Nov 15 15:56:29.831: INFO: Pod "pod-8f563905-c483-4b88-83cf-9da48ced763a" satisfied condition "Succeeded or Failed"
    Nov 15 15:56:29.849: INFO: Trying to get logs from node 10.15.40.115 pod pod-8f563905-c483-4b88-83cf-9da48ced763a container test-container: <nil>
    STEP: delete the pod 11/15/23 15:56:29.889
    Nov 15 15:56:29.960: INFO: Waiting for pod pod-8f563905-c483-4b88-83cf-9da48ced763a to disappear
    Nov 15 15:56:29.981: INFO: Pod pod-8f563905-c483-4b88-83cf-9da48ced763a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:56:29.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9002" for this suite. 11/15/23 15:56:30
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:56:30.022
Nov 15 15:56:30.022: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename configmap 11/15/23 15:56:30.025
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:56:30.065
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:56:30.079
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 15 15:56:30.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5530" for this suite. 11/15/23 15:56:30.321
------------------------------
• [0.321 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:56:30.022
    Nov 15 15:56:30.022: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename configmap 11/15/23 15:56:30.025
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:56:30.065
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:56:30.079
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:56:30.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5530" for this suite. 11/15/23 15:56:30.321
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:56:30.349
Nov 15 15:56:30.349: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename projected 11/15/23 15:56:30.351
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:56:30.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:56:30.403
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 11/15/23 15:56:30.417
Nov 15 15:56:30.446: INFO: Waiting up to 5m0s for pod "downwardapi-volume-59ea6419-7622-4317-89bc-058763c0b710" in namespace "projected-8329" to be "Succeeded or Failed"
Nov 15 15:56:30.464: INFO: Pod "downwardapi-volume-59ea6419-7622-4317-89bc-058763c0b710": Phase="Pending", Reason="", readiness=false. Elapsed: 17.111104ms
Nov 15 15:56:32.481: INFO: Pod "downwardapi-volume-59ea6419-7622-4317-89bc-058763c0b710": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034738451s
Nov 15 15:56:34.488: INFO: Pod "downwardapi-volume-59ea6419-7622-4317-89bc-058763c0b710": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041492364s
Nov 15 15:56:36.483: INFO: Pod "downwardapi-volume-59ea6419-7622-4317-89bc-058763c0b710": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03618268s
STEP: Saw pod success 11/15/23 15:56:36.483
Nov 15 15:56:36.483: INFO: Pod "downwardapi-volume-59ea6419-7622-4317-89bc-058763c0b710" satisfied condition "Succeeded or Failed"
Nov 15 15:56:36.501: INFO: Trying to get logs from node 10.15.40.115 pod downwardapi-volume-59ea6419-7622-4317-89bc-058763c0b710 container client-container: <nil>
STEP: delete the pod 11/15/23 15:56:36.538
Nov 15 15:56:36.583: INFO: Waiting for pod downwardapi-volume-59ea6419-7622-4317-89bc-058763c0b710 to disappear
Nov 15 15:56:36.600: INFO: Pod downwardapi-volume-59ea6419-7622-4317-89bc-058763c0b710 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Nov 15 15:56:36.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8329" for this suite. 11/15/23 15:56:36.619
------------------------------
• [SLOW TEST] [6.290 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:56:30.349
    Nov 15 15:56:30.349: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename projected 11/15/23 15:56:30.351
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:56:30.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:56:30.403
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 11/15/23 15:56:30.417
    Nov 15 15:56:30.446: INFO: Waiting up to 5m0s for pod "downwardapi-volume-59ea6419-7622-4317-89bc-058763c0b710" in namespace "projected-8329" to be "Succeeded or Failed"
    Nov 15 15:56:30.464: INFO: Pod "downwardapi-volume-59ea6419-7622-4317-89bc-058763c0b710": Phase="Pending", Reason="", readiness=false. Elapsed: 17.111104ms
    Nov 15 15:56:32.481: INFO: Pod "downwardapi-volume-59ea6419-7622-4317-89bc-058763c0b710": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034738451s
    Nov 15 15:56:34.488: INFO: Pod "downwardapi-volume-59ea6419-7622-4317-89bc-058763c0b710": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041492364s
    Nov 15 15:56:36.483: INFO: Pod "downwardapi-volume-59ea6419-7622-4317-89bc-058763c0b710": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03618268s
    STEP: Saw pod success 11/15/23 15:56:36.483
    Nov 15 15:56:36.483: INFO: Pod "downwardapi-volume-59ea6419-7622-4317-89bc-058763c0b710" satisfied condition "Succeeded or Failed"
    Nov 15 15:56:36.501: INFO: Trying to get logs from node 10.15.40.115 pod downwardapi-volume-59ea6419-7622-4317-89bc-058763c0b710 container client-container: <nil>
    STEP: delete the pod 11/15/23 15:56:36.538
    Nov 15 15:56:36.583: INFO: Waiting for pod downwardapi-volume-59ea6419-7622-4317-89bc-058763c0b710 to disappear
    Nov 15 15:56:36.600: INFO: Pod downwardapi-volume-59ea6419-7622-4317-89bc-058763c0b710 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:56:36.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8329" for this suite. 11/15/23 15:56:36.619
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:56:36.646
Nov 15 15:56:36.646: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename endpointslice 11/15/23 15:56:36.648
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:56:36.691
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:56:36.708
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Nov 15 15:56:40.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-5615" for this suite. 11/15/23 15:56:40.958
------------------------------
• [4.351 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:56:36.646
    Nov 15 15:56:36.646: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename endpointslice 11/15/23 15:56:36.648
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:56:36.691
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:56:36.708
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:56:40.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-5615" for this suite. 11/15/23 15:56:40.958
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:56:41.026
Nov 15 15:56:41.026: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename services 11/15/23 15:56:41.027
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:56:41.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:56:41.088
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-6165 11/15/23 15:56:41.116
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6165 to expose endpoints map[] 11/15/23 15:56:41.159
Nov 15 15:56:41.205: INFO: successfully validated that service multi-endpoint-test in namespace services-6165 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-6165 11/15/23 15:56:41.205
Nov 15 15:56:41.240: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-6165" to be "running and ready"
Nov 15 15:56:41.285: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 44.314761ms
Nov 15 15:56:41.285: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 15:56:43.304: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063112753s
Nov 15 15:56:43.304: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 15:56:45.304: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.063190153s
Nov 15 15:56:45.304: INFO: The phase of Pod pod1 is Running (Ready = true)
Nov 15 15:56:45.304: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6165 to expose endpoints map[pod1:[100]] 11/15/23 15:56:45.322
Nov 15 15:56:45.382: INFO: successfully validated that service multi-endpoint-test in namespace services-6165 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-6165 11/15/23 15:56:45.383
Nov 15 15:56:45.410: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-6165" to be "running and ready"
Nov 15 15:56:45.427: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.487024ms
Nov 15 15:56:45.427: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 15:56:47.456: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.046042585s
Nov 15 15:56:47.456: INFO: The phase of Pod pod2 is Running (Ready = true)
Nov 15 15:56:47.456: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6165 to expose endpoints map[pod1:[100] pod2:[101]] 11/15/23 15:56:47.473
Nov 15 15:56:47.542: INFO: successfully validated that service multi-endpoint-test in namespace services-6165 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 11/15/23 15:56:47.542
Nov 15 15:56:47.543: INFO: Creating new exec pod
Nov 15 15:56:47.564: INFO: Waiting up to 5m0s for pod "execpod7rk58" in namespace "services-6165" to be "running"
Nov 15 15:56:47.580: INFO: Pod "execpod7rk58": Phase="Pending", Reason="", readiness=false. Elapsed: 16.157203ms
Nov 15 15:56:49.602: INFO: Pod "execpod7rk58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038011893s
Nov 15 15:56:51.600: INFO: Pod "execpod7rk58": Phase="Running", Reason="", readiness=true. Elapsed: 4.035328508s
Nov 15 15:56:51.600: INFO: Pod "execpod7rk58" satisfied condition "running"
Nov 15 15:56:52.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-6165 exec execpod7rk58 -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Nov 15 15:56:53.077: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Nov 15 15:56:53.077: INFO: stdout: ""
Nov 15 15:56:53.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-6165 exec execpod7rk58 -- /bin/sh -x -c nc -v -z -w 2 172.21.147.9 80'
Nov 15 15:56:53.562: INFO: stderr: "+ nc -v -z -w 2 172.21.147.9 80\nConnection to 172.21.147.9 80 port [tcp/http] succeeded!\n"
Nov 15 15:56:53.562: INFO: stdout: ""
Nov 15 15:56:53.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-6165 exec execpod7rk58 -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Nov 15 15:56:53.985: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Nov 15 15:56:53.985: INFO: stdout: ""
Nov 15 15:56:53.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-6165 exec execpod7rk58 -- /bin/sh -x -c nc -v -z -w 2 172.21.147.9 81'
Nov 15 15:56:54.458: INFO: stderr: "+ nc -v -z -w 2 172.21.147.9 81\nConnection to 172.21.147.9 81 port [tcp/*] succeeded!\n"
Nov 15 15:56:54.458: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-6165 11/15/23 15:56:54.458
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6165 to expose endpoints map[pod2:[101]] 11/15/23 15:56:54.519
Nov 15 15:56:54.590: INFO: successfully validated that service multi-endpoint-test in namespace services-6165 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-6165 11/15/23 15:56:54.59
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6165 to expose endpoints map[] 11/15/23 15:56:54.634
Nov 15 15:56:54.669: INFO: successfully validated that service multi-endpoint-test in namespace services-6165 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 15 15:56:54.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6165" for this suite. 11/15/23 15:56:54.741
------------------------------
• [SLOW TEST] [13.733 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:56:41.026
    Nov 15 15:56:41.026: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename services 11/15/23 15:56:41.027
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:56:41.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:56:41.088
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-6165 11/15/23 15:56:41.116
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6165 to expose endpoints map[] 11/15/23 15:56:41.159
    Nov 15 15:56:41.205: INFO: successfully validated that service multi-endpoint-test in namespace services-6165 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-6165 11/15/23 15:56:41.205
    Nov 15 15:56:41.240: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-6165" to be "running and ready"
    Nov 15 15:56:41.285: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 44.314761ms
    Nov 15 15:56:41.285: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 15:56:43.304: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063112753s
    Nov 15 15:56:43.304: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 15:56:45.304: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.063190153s
    Nov 15 15:56:45.304: INFO: The phase of Pod pod1 is Running (Ready = true)
    Nov 15 15:56:45.304: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6165 to expose endpoints map[pod1:[100]] 11/15/23 15:56:45.322
    Nov 15 15:56:45.382: INFO: successfully validated that service multi-endpoint-test in namespace services-6165 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-6165 11/15/23 15:56:45.383
    Nov 15 15:56:45.410: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-6165" to be "running and ready"
    Nov 15 15:56:45.427: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.487024ms
    Nov 15 15:56:45.427: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 15:56:47.456: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.046042585s
    Nov 15 15:56:47.456: INFO: The phase of Pod pod2 is Running (Ready = true)
    Nov 15 15:56:47.456: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6165 to expose endpoints map[pod1:[100] pod2:[101]] 11/15/23 15:56:47.473
    Nov 15 15:56:47.542: INFO: successfully validated that service multi-endpoint-test in namespace services-6165 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 11/15/23 15:56:47.542
    Nov 15 15:56:47.543: INFO: Creating new exec pod
    Nov 15 15:56:47.564: INFO: Waiting up to 5m0s for pod "execpod7rk58" in namespace "services-6165" to be "running"
    Nov 15 15:56:47.580: INFO: Pod "execpod7rk58": Phase="Pending", Reason="", readiness=false. Elapsed: 16.157203ms
    Nov 15 15:56:49.602: INFO: Pod "execpod7rk58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038011893s
    Nov 15 15:56:51.600: INFO: Pod "execpod7rk58": Phase="Running", Reason="", readiness=true. Elapsed: 4.035328508s
    Nov 15 15:56:51.600: INFO: Pod "execpod7rk58" satisfied condition "running"
    Nov 15 15:56:52.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-6165 exec execpod7rk58 -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Nov 15 15:56:53.077: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Nov 15 15:56:53.077: INFO: stdout: ""
    Nov 15 15:56:53.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-6165 exec execpod7rk58 -- /bin/sh -x -c nc -v -z -w 2 172.21.147.9 80'
    Nov 15 15:56:53.562: INFO: stderr: "+ nc -v -z -w 2 172.21.147.9 80\nConnection to 172.21.147.9 80 port [tcp/http] succeeded!\n"
    Nov 15 15:56:53.562: INFO: stdout: ""
    Nov 15 15:56:53.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-6165 exec execpod7rk58 -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Nov 15 15:56:53.985: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Nov 15 15:56:53.985: INFO: stdout: ""
    Nov 15 15:56:53.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-6165 exec execpod7rk58 -- /bin/sh -x -c nc -v -z -w 2 172.21.147.9 81'
    Nov 15 15:56:54.458: INFO: stderr: "+ nc -v -z -w 2 172.21.147.9 81\nConnection to 172.21.147.9 81 port [tcp/*] succeeded!\n"
    Nov 15 15:56:54.458: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-6165 11/15/23 15:56:54.458
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6165 to expose endpoints map[pod2:[101]] 11/15/23 15:56:54.519
    Nov 15 15:56:54.590: INFO: successfully validated that service multi-endpoint-test in namespace services-6165 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-6165 11/15/23 15:56:54.59
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6165 to expose endpoints map[] 11/15/23 15:56:54.634
    Nov 15 15:56:54.669: INFO: successfully validated that service multi-endpoint-test in namespace services-6165 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:56:54.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6165" for this suite. 11/15/23 15:56:54.741
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:56:54.764
Nov 15 15:56:54.764: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename daemonsets 11/15/23 15:56:54.766
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:56:54.809
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:56:54.824
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443
Nov 15 15:56:54.923: INFO: Create a RollingUpdate DaemonSet
Nov 15 15:56:54.940: INFO: Check that daemon pods launch on every node of the cluster
Nov 15 15:56:54.980: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 15:56:54.980: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
Nov 15 15:56:56.031: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 15:56:56.031: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
Nov 15 15:56:57.033: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 15 15:56:57.033: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
Nov 15 15:56:58.017: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 15 15:56:58.017: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Nov 15 15:56:58.017: INFO: Update the DaemonSet to trigger a rollout
Nov 15 15:56:58.049: INFO: Updating DaemonSet daemon-set
Nov 15 15:57:01.130: INFO: Roll back the DaemonSet before rollout is complete
Nov 15 15:57:01.161: INFO: Updating DaemonSet daemon-set
Nov 15 15:57:01.161: INFO: Make sure DaemonSet rollback is complete
Nov 15 15:57:01.186: INFO: Wrong image for pod: daemon-set-8v7sn. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Nov 15 15:57:01.187: INFO: Pod daemon-set-8v7sn is not available
Nov 15 15:57:06.221: INFO: Pod daemon-set-r4q48 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 11/15/23 15:57:06.287
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9935, will wait for the garbage collector to delete the pods 11/15/23 15:57:06.287
Nov 15 15:57:06.369: INFO: Deleting DaemonSet.extensions daemon-set took: 18.716354ms
Nov 15 15:57:06.570: INFO: Terminating DaemonSet.extensions daemon-set pods took: 201.320901ms
Nov 15 15:57:08.287: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 15:57:08.287: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Nov 15 15:57:08.299: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26202"},"items":null}

Nov 15 15:57:08.315: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26202"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 15:57:08.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-9935" for this suite. 11/15/23 15:57:08.388
------------------------------
• [SLOW TEST] [13.643 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:56:54.764
    Nov 15 15:56:54.764: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename daemonsets 11/15/23 15:56:54.766
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:56:54.809
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:56:54.824
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:443
    Nov 15 15:56:54.923: INFO: Create a RollingUpdate DaemonSet
    Nov 15 15:56:54.940: INFO: Check that daemon pods launch on every node of the cluster
    Nov 15 15:56:54.980: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 15:56:54.980: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
    Nov 15 15:56:56.031: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 15:56:56.031: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
    Nov 15 15:56:57.033: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Nov 15 15:56:57.033: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
    Nov 15 15:56:58.017: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Nov 15 15:56:58.017: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Nov 15 15:56:58.017: INFO: Update the DaemonSet to trigger a rollout
    Nov 15 15:56:58.049: INFO: Updating DaemonSet daemon-set
    Nov 15 15:57:01.130: INFO: Roll back the DaemonSet before rollout is complete
    Nov 15 15:57:01.161: INFO: Updating DaemonSet daemon-set
    Nov 15 15:57:01.161: INFO: Make sure DaemonSet rollback is complete
    Nov 15 15:57:01.186: INFO: Wrong image for pod: daemon-set-8v7sn. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Nov 15 15:57:01.187: INFO: Pod daemon-set-8v7sn is not available
    Nov 15 15:57:06.221: INFO: Pod daemon-set-r4q48 is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 11/15/23 15:57:06.287
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9935, will wait for the garbage collector to delete the pods 11/15/23 15:57:06.287
    Nov 15 15:57:06.369: INFO: Deleting DaemonSet.extensions daemon-set took: 18.716354ms
    Nov 15 15:57:06.570: INFO: Terminating DaemonSet.extensions daemon-set pods took: 201.320901ms
    Nov 15 15:57:08.287: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 15:57:08.287: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Nov 15 15:57:08.299: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26202"},"items":null}

    Nov 15 15:57:08.315: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26202"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:57:08.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-9935" for this suite. 11/15/23 15:57:08.388
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:57:08.409
Nov 15 15:57:08.409: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename proxy 11/15/23 15:57:08.41
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:57:08.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:57:08.464
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 11/15/23 15:57:08.522
STEP: creating replication controller proxy-service-7qvk4 in namespace proxy-1388 11/15/23 15:57:08.523
I1115 15:57:08.541012      22 runners.go:193] Created replication controller with name: proxy-service-7qvk4, namespace: proxy-1388, replica count: 1
I1115 15:57:09.592098      22 runners.go:193] proxy-service-7qvk4 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1115 15:57:10.592695      22 runners.go:193] proxy-service-7qvk4 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1115 15:57:11.593052      22 runners.go:193] proxy-service-7qvk4 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 15 15:57:11.609: INFO: setup took 3.131523829s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 11/15/23 15:57:11.609
Nov 15 15:57:11.702: INFO: (0) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 88.570155ms)
Nov 15 15:57:11.702: INFO: (0) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 88.773135ms)
Nov 15 15:57:11.708: INFO: (0) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 95.155501ms)
Nov 15 15:57:11.710: INFO: (0) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 97.066805ms)
Nov 15 15:57:11.710: INFO: (0) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 98.417436ms)
Nov 15 15:57:11.710: INFO: (0) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 97.532698ms)
Nov 15 15:57:11.710: INFO: (0) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 100.842515ms)
Nov 15 15:57:11.712: INFO: (0) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 99.494454ms)
Nov 15 15:57:11.713: INFO: (0) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 102.644073ms)
Nov 15 15:57:11.714: INFO: (0) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 100.906576ms)
Nov 15 15:57:11.717: INFO: (0) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 106.602155ms)
Nov 15 15:57:11.717: INFO: (0) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 104.022833ms)
Nov 15 15:57:11.717: INFO: (0) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 104.076415ms)
Nov 15 15:57:11.719: INFO: (0) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 108.193232ms)
Nov 15 15:57:11.721: INFO: (0) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 108.675284ms)
Nov 15 15:57:11.721: INFO: (0) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 107.546699ms)
Nov 15 15:57:11.762: INFO: (1) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 40.306424ms)
Nov 15 15:57:11.768: INFO: (1) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 46.452047ms)
Nov 15 15:57:11.776: INFO: (1) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 53.997766ms)
Nov 15 15:57:11.776: INFO: (1) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 54.70207ms)
Nov 15 15:57:11.777: INFO: (1) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 54.533372ms)
Nov 15 15:57:11.777: INFO: (1) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 54.965977ms)
Nov 15 15:57:11.777: INFO: (1) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 55.307498ms)
Nov 15 15:57:11.777: INFO: (1) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 55.038675ms)
Nov 15 15:57:11.777: INFO: (1) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 54.681246ms)
Nov 15 15:57:11.777: INFO: (1) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 54.642289ms)
Nov 15 15:57:11.777: INFO: (1) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 55.095119ms)
Nov 15 15:57:11.777: INFO: (1) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 55.124649ms)
Nov 15 15:57:11.777: INFO: (1) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 55.295384ms)
Nov 15 15:57:11.779: INFO: (1) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 57.034562ms)
Nov 15 15:57:11.781: INFO: (1) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 58.204514ms)
Nov 15 15:57:11.790: INFO: (1) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 67.952049ms)
Nov 15 15:57:11.817: INFO: (2) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 25.895612ms)
Nov 15 15:57:11.822: INFO: (2) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 31.372858ms)
Nov 15 15:57:11.822: INFO: (2) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 31.746959ms)
Nov 15 15:57:11.822: INFO: (2) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 31.143218ms)
Nov 15 15:57:11.823: INFO: (2) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 31.539752ms)
Nov 15 15:57:11.824: INFO: (2) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 32.620539ms)
Nov 15 15:57:11.843: INFO: (2) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 52.562716ms)
Nov 15 15:57:11.843: INFO: (2) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 51.530382ms)
Nov 15 15:57:11.843: INFO: (2) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 51.666602ms)
Nov 15 15:57:11.844: INFO: (2) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 52.574481ms)
Nov 15 15:57:11.844: INFO: (2) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 52.429613ms)
Nov 15 15:57:11.844: INFO: (2) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 52.319701ms)
Nov 15 15:57:11.844: INFO: (2) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 52.579389ms)
Nov 15 15:57:11.844: INFO: (2) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 52.181456ms)
Nov 15 15:57:11.852: INFO: (2) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 60.313166ms)
Nov 15 15:57:11.858: INFO: (2) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 66.560506ms)
Nov 15 15:57:11.888: INFO: (3) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 28.777162ms)
Nov 15 15:57:11.891: INFO: (3) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 33.200528ms)
Nov 15 15:57:11.895: INFO: (3) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 36.615413ms)
Nov 15 15:57:11.895: INFO: (3) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 35.360314ms)
Nov 15 15:57:11.896: INFO: (3) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 37.174139ms)
Nov 15 15:57:11.896: INFO: (3) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 36.521239ms)
Nov 15 15:57:11.898: INFO: (3) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 37.770517ms)
Nov 15 15:57:11.898: INFO: (3) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 37.385507ms)
Nov 15 15:57:11.898: INFO: (3) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 39.959117ms)
Nov 15 15:57:11.898: INFO: (3) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 38.263501ms)
Nov 15 15:57:11.898: INFO: (3) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 38.180867ms)
Nov 15 15:57:11.898: INFO: (3) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 39.285689ms)
Nov 15 15:57:11.898: INFO: (3) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 38.484997ms)
Nov 15 15:57:11.896: INFO: (3) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 36.622631ms)
Nov 15 15:57:11.909: INFO: (3) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 50.787002ms)
Nov 15 15:57:11.915: INFO: (3) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 56.870783ms)
Nov 15 15:57:11.938: INFO: (4) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 22.743663ms)
Nov 15 15:57:11.952: INFO: (4) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 35.359754ms)
Nov 15 15:57:11.952: INFO: (4) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 36.031401ms)
Nov 15 15:57:11.952: INFO: (4) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 35.473629ms)
Nov 15 15:57:11.955: INFO: (4) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 39.062407ms)
Nov 15 15:57:11.955: INFO: (4) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 38.621823ms)
Nov 15 15:57:11.955: INFO: (4) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 39.043256ms)
Nov 15 15:57:11.955: INFO: (4) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 38.860774ms)
Nov 15 15:57:11.956: INFO: (4) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 40.065798ms)
Nov 15 15:57:11.956: INFO: (4) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 40.312152ms)
Nov 15 15:57:11.956: INFO: (4) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 39.707557ms)
Nov 15 15:57:11.956: INFO: (4) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 39.478699ms)
Nov 15 15:57:11.960: INFO: (4) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 43.655607ms)
Nov 15 15:57:11.960: INFO: (4) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 43.953086ms)
Nov 15 15:57:11.960: INFO: (4) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 43.621897ms)
Nov 15 15:57:11.960: INFO: (4) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 44.426843ms)
Nov 15 15:57:11.982: INFO: (5) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 21.96726ms)
Nov 15 15:57:11.993: INFO: (5) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 32.783402ms)
Nov 15 15:57:11.994: INFO: (5) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 32.619451ms)
Nov 15 15:57:11.994: INFO: (5) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 32.927906ms)
Nov 15 15:57:11.994: INFO: (5) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 33.226784ms)
Nov 15 15:57:11.994: INFO: (5) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 33.012855ms)
Nov 15 15:57:11.994: INFO: (5) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 33.942234ms)
Nov 15 15:57:11.994: INFO: (5) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 33.528368ms)
Nov 15 15:57:11.995: INFO: (5) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 34.109259ms)
Nov 15 15:57:11.995: INFO: (5) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 34.043463ms)
Nov 15 15:57:11.995: INFO: (5) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 34.790175ms)
Nov 15 15:57:11.997: INFO: (5) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 36.054752ms)
Nov 15 15:57:11.998: INFO: (5) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 36.576244ms)
Nov 15 15:57:12.003: INFO: (5) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 42.597648ms)
Nov 15 15:57:12.004: INFO: (5) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 42.386081ms)
Nov 15 15:57:12.004: INFO: (5) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 42.081869ms)
Nov 15 15:57:12.026: INFO: (6) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 21.955546ms)
Nov 15 15:57:12.031: INFO: (6) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 27.425852ms)
Nov 15 15:57:12.033: INFO: (6) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 28.692863ms)
Nov 15 15:57:12.033: INFO: (6) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 27.998893ms)
Nov 15 15:57:12.056: INFO: (6) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 51.756827ms)
Nov 15 15:57:12.056: INFO: (6) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 51.928755ms)
Nov 15 15:57:12.056: INFO: (6) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 51.46696ms)
Nov 15 15:57:12.056: INFO: (6) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 51.223821ms)
Nov 15 15:57:12.056: INFO: (6) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 50.824711ms)
Nov 15 15:57:12.056: INFO: (6) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 50.981661ms)
Nov 15 15:57:12.056: INFO: (6) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 52.211133ms)
Nov 15 15:57:12.063: INFO: (6) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 57.780193ms)
Nov 15 15:57:12.063: INFO: (6) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 58.313356ms)
Nov 15 15:57:12.063: INFO: (6) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 57.825024ms)
Nov 15 15:57:12.063: INFO: (6) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 57.72866ms)
Nov 15 15:57:12.063: INFO: (6) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 57.911706ms)
Nov 15 15:57:12.083: INFO: (7) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 19.74414ms)
Nov 15 15:57:12.092: INFO: (7) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 27.18963ms)
Nov 15 15:57:12.092: INFO: (7) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 28.719378ms)
Nov 15 15:57:12.094: INFO: (7) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 30.43905ms)
Nov 15 15:57:12.094: INFO: (7) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 29.919846ms)
Nov 15 15:57:12.097: INFO: (7) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 33.102626ms)
Nov 15 15:57:12.098: INFO: (7) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 33.573733ms)
Nov 15 15:57:12.098: INFO: (7) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 33.318793ms)
Nov 15 15:57:12.100: INFO: (7) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 35.221242ms)
Nov 15 15:57:12.102: INFO: (7) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 36.77103ms)
Nov 15 15:57:12.102: INFO: (7) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 36.860727ms)
Nov 15 15:57:12.102: INFO: (7) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 36.696065ms)
Nov 15 15:57:12.102: INFO: (7) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 37.555367ms)
Nov 15 15:57:12.104: INFO: (7) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 39.077168ms)
Nov 15 15:57:12.107: INFO: (7) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 43.955617ms)
Nov 15 15:57:12.109: INFO: (7) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 44.693179ms)
Nov 15 15:57:12.134: INFO: (8) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 24.663352ms)
Nov 15 15:57:12.138: INFO: (8) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 28.530157ms)
Nov 15 15:57:12.141: INFO: (8) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 31.80492ms)
Nov 15 15:57:12.141: INFO: (8) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 31.091637ms)
Nov 15 15:57:12.143: INFO: (8) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 34.02885ms)
Nov 15 15:57:12.146: INFO: (8) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 37.241484ms)
Nov 15 15:57:12.147: INFO: (8) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 37.566102ms)
Nov 15 15:57:12.148: INFO: (8) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 37.812946ms)
Nov 15 15:57:12.148: INFO: (8) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 38.189204ms)
Nov 15 15:57:12.148: INFO: (8) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 38.755082ms)
Nov 15 15:57:12.148: INFO: (8) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 38.262541ms)
Nov 15 15:57:12.148: INFO: (8) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 39.553985ms)
Nov 15 15:57:12.149: INFO: (8) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 39.207854ms)
Nov 15 15:57:12.156: INFO: (8) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 46.341506ms)
Nov 15 15:57:12.156: INFO: (8) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 46.664294ms)
Nov 15 15:57:12.162: INFO: (8) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 52.238427ms)
Nov 15 15:57:12.185: INFO: (9) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 22.296353ms)
Nov 15 15:57:12.186: INFO: (9) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 22.618802ms)
Nov 15 15:57:12.198: INFO: (9) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 34.415601ms)
Nov 15 15:57:12.206: INFO: (9) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 43.1069ms)
Nov 15 15:57:12.207: INFO: (9) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 43.276338ms)
Nov 15 15:57:12.207: INFO: (9) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 43.536043ms)
Nov 15 15:57:12.207: INFO: (9) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 44.16483ms)
Nov 15 15:57:12.207: INFO: (9) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 43.185158ms)
Nov 15 15:57:12.219: INFO: (9) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 54.845155ms)
Nov 15 15:57:12.219: INFO: (9) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 56.674978ms)
Nov 15 15:57:12.220: INFO: (9) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 57.19098ms)
Nov 15 15:57:12.220: INFO: (9) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 56.010003ms)
Nov 15 15:57:12.220: INFO: (9) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 57.755366ms)
Nov 15 15:57:12.220: INFO: (9) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 56.781629ms)
Nov 15 15:57:12.220: INFO: (9) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 55.854148ms)
Nov 15 15:57:12.238: INFO: (9) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 74.977695ms)
Nov 15 15:57:12.268: INFO: (10) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 28.37668ms)
Nov 15 15:57:12.279: INFO: (10) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 40.032364ms)
Nov 15 15:57:12.279: INFO: (10) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 38.974554ms)
Nov 15 15:57:12.292: INFO: (10) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 51.999755ms)
Nov 15 15:57:12.293: INFO: (10) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 52.173915ms)
Nov 15 15:57:12.294: INFO: (10) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 54.167386ms)
Nov 15 15:57:12.294: INFO: (10) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 54.079949ms)
Nov 15 15:57:12.301: INFO: (10) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 61.917552ms)
Nov 15 15:57:12.301: INFO: (10) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 60.965061ms)
Nov 15 15:57:12.301: INFO: (10) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 60.440183ms)
Nov 15 15:57:12.308: INFO: (10) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 68.27905ms)
Nov 15 15:57:12.309: INFO: (10) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 69.579541ms)
Nov 15 15:57:12.309: INFO: (10) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 68.814204ms)
Nov 15 15:57:12.309: INFO: (10) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 69.30176ms)
Nov 15 15:57:12.309: INFO: (10) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 69.059033ms)
Nov 15 15:57:12.309: INFO: (10) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 68.276406ms)
Nov 15 15:57:12.337: INFO: (11) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 26.201088ms)
Nov 15 15:57:12.344: INFO: (11) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 32.30875ms)
Nov 15 15:57:12.344: INFO: (11) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 32.100713ms)
Nov 15 15:57:12.345: INFO: (11) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 33.151757ms)
Nov 15 15:57:12.345: INFO: (11) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 32.624697ms)
Nov 15 15:57:12.345: INFO: (11) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 33.686037ms)
Nov 15 15:57:12.346: INFO: (11) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 34.784632ms)
Nov 15 15:57:12.348: INFO: (11) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 35.200581ms)
Nov 15 15:57:12.348: INFO: (11) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 35.574929ms)
Nov 15 15:57:12.349: INFO: (11) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 35.249845ms)
Nov 15 15:57:12.349: INFO: (11) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 35.603894ms)
Nov 15 15:57:12.349: INFO: (11) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 36.345516ms)
Nov 15 15:57:12.349: INFO: (11) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 36.027358ms)
Nov 15 15:57:12.357: INFO: (11) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 44.610343ms)
Nov 15 15:57:12.357: INFO: (11) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 45.62388ms)
Nov 15 15:57:12.358: INFO: (11) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 44.465387ms)
Nov 15 15:57:12.389: INFO: (12) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 29.761872ms)
Nov 15 15:57:12.394: INFO: (12) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 34.270532ms)
Nov 15 15:57:12.395: INFO: (12) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 35.543295ms)
Nov 15 15:57:12.395: INFO: (12) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 35.245424ms)
Nov 15 15:57:12.395: INFO: (12) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 35.272266ms)
Nov 15 15:57:12.400: INFO: (12) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 40.850719ms)
Nov 15 15:57:12.400: INFO: (12) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 40.34501ms)
Nov 15 15:57:12.401: INFO: (12) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 42.653029ms)
Nov 15 15:57:12.413: INFO: (12) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 54.410548ms)
Nov 15 15:57:12.413: INFO: (12) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 54.929379ms)
Nov 15 15:57:12.414: INFO: (12) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 54.905192ms)
Nov 15 15:57:12.414: INFO: (12) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 55.396969ms)
Nov 15 15:57:12.415: INFO: (12) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 56.089047ms)
Nov 15 15:57:12.421: INFO: (12) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 61.619127ms)
Nov 15 15:57:12.421: INFO: (12) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 60.576074ms)
Nov 15 15:57:12.423: INFO: (12) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 63.843558ms)
Nov 15 15:57:12.446: INFO: (13) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 22.552536ms)
Nov 15 15:57:12.451: INFO: (13) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 27.271668ms)
Nov 15 15:57:12.453: INFO: (13) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 28.327006ms)
Nov 15 15:57:12.464: INFO: (13) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 39.814487ms)
Nov 15 15:57:12.464: INFO: (13) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 39.784114ms)
Nov 15 15:57:12.465: INFO: (13) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 39.936576ms)
Nov 15 15:57:12.465: INFO: (13) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 40.257435ms)
Nov 15 15:57:12.467: INFO: (13) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 42.83685ms)
Nov 15 15:57:12.467: INFO: (13) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 42.255341ms)
Nov 15 15:57:12.467: INFO: (13) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 43.134896ms)
Nov 15 15:57:12.467: INFO: (13) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 42.467062ms)
Nov 15 15:57:12.470: INFO: (13) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 45.879348ms)
Nov 15 15:57:12.470: INFO: (13) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 46.176577ms)
Nov 15 15:57:12.473: INFO: (13) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 49.41081ms)
Nov 15 15:57:12.474: INFO: (13) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 50.153458ms)
Nov 15 15:57:12.474: INFO: (13) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 50.34124ms)
Nov 15 15:57:12.503: INFO: (14) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 27.178439ms)
Nov 15 15:57:12.509: INFO: (14) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 31.404398ms)
Nov 15 15:57:12.509: INFO: (14) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 31.416517ms)
Nov 15 15:57:12.509: INFO: (14) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 31.578779ms)
Nov 15 15:57:12.509: INFO: (14) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 33.001643ms)
Nov 15 15:57:12.509: INFO: (14) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 33.151976ms)
Nov 15 15:57:12.510: INFO: (14) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 32.095125ms)
Nov 15 15:57:12.511: INFO: (14) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 34.336826ms)
Nov 15 15:57:12.511: INFO: (14) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 34.275621ms)
Nov 15 15:57:12.511: INFO: (14) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 34.739082ms)
Nov 15 15:57:12.512: INFO: (14) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 34.821339ms)
Nov 15 15:57:12.514: INFO: (14) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 35.943772ms)
Nov 15 15:57:12.514: INFO: (14) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 37.522447ms)
Nov 15 15:57:12.516: INFO: (14) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 38.865752ms)
Nov 15 15:57:12.526: INFO: (14) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 49.59924ms)
Nov 15 15:57:12.527: INFO: (14) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 49.368777ms)
Nov 15 15:57:12.554: INFO: (15) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 25.908874ms)
Nov 15 15:57:12.557: INFO: (15) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 28.800238ms)
Nov 15 15:57:12.557: INFO: (15) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 28.976132ms)
Nov 15 15:57:12.557: INFO: (15) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 28.701505ms)
Nov 15 15:57:12.558: INFO: (15) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 28.847793ms)
Nov 15 15:57:12.558: INFO: (15) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 30.32769ms)
Nov 15 15:57:12.566: INFO: (15) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 37.840823ms)
Nov 15 15:57:12.567: INFO: (15) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 38.438969ms)
Nov 15 15:57:12.568: INFO: (15) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 40.620233ms)
Nov 15 15:57:12.568: INFO: (15) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 39.182059ms)
Nov 15 15:57:12.568: INFO: (15) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 39.631772ms)
Nov 15 15:57:12.569: INFO: (15) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 40.888ms)
Nov 15 15:57:12.569: INFO: (15) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 40.292435ms)
Nov 15 15:57:12.569: INFO: (15) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 40.571364ms)
Nov 15 15:57:12.578: INFO: (15) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 49.272693ms)
Nov 15 15:57:12.578: INFO: (15) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 49.243793ms)
Nov 15 15:57:12.598: INFO: (16) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 19.937029ms)
Nov 15 15:57:12.606: INFO: (16) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 26.479811ms)
Nov 15 15:57:12.606: INFO: (16) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 27.553017ms)
Nov 15 15:57:12.607: INFO: (16) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 28.317642ms)
Nov 15 15:57:12.610: INFO: (16) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 31.814183ms)
Nov 15 15:57:12.611: INFO: (16) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 31.998894ms)
Nov 15 15:57:12.612: INFO: (16) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 32.598485ms)
Nov 15 15:57:12.612: INFO: (16) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 33.585015ms)
Nov 15 15:57:12.612: INFO: (16) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 32.999084ms)
Nov 15 15:57:12.612: INFO: (16) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 33.302691ms)
Nov 15 15:57:12.612: INFO: (16) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 33.104043ms)
Nov 15 15:57:12.613: INFO: (16) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 34.288123ms)
Nov 15 15:57:12.613: INFO: (16) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 34.754135ms)
Nov 15 15:57:12.627: INFO: (16) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 47.814015ms)
Nov 15 15:57:12.627: INFO: (16) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 48.611992ms)
Nov 15 15:57:12.627: INFO: (16) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 48.472819ms)
Nov 15 15:57:12.655: INFO: (17) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 28.132586ms)
Nov 15 15:57:12.656: INFO: (17) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 27.354499ms)
Nov 15 15:57:12.656: INFO: (17) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 27.446016ms)
Nov 15 15:57:12.656: INFO: (17) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 27.871034ms)
Nov 15 15:57:12.658: INFO: (17) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 29.80753ms)
Nov 15 15:57:12.661: INFO: (17) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 33.49096ms)
Nov 15 15:57:12.661: INFO: (17) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 33.471175ms)
Nov 15 15:57:12.662: INFO: (17) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 33.39517ms)
Nov 15 15:57:12.666: INFO: (17) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 37.889864ms)
Nov 15 15:57:12.667: INFO: (17) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 38.980183ms)
Nov 15 15:57:12.667: INFO: (17) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 39.032513ms)
Nov 15 15:57:12.667: INFO: (17) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 39.470704ms)
Nov 15 15:57:12.667: INFO: (17) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 39.546046ms)
Nov 15 15:57:12.668: INFO: (17) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 39.992638ms)
Nov 15 15:57:12.679: INFO: (17) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 50.775383ms)
Nov 15 15:57:12.679: INFO: (17) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 51.978939ms)
Nov 15 15:57:12.705: INFO: (18) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 25.683147ms)
Nov 15 15:57:12.707: INFO: (18) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 27.133341ms)
Nov 15 15:57:12.709: INFO: (18) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 28.823918ms)
Nov 15 15:57:12.711: INFO: (18) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 30.703567ms)
Nov 15 15:57:12.712: INFO: (18) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 31.414111ms)
Nov 15 15:57:12.712: INFO: (18) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 31.335436ms)
Nov 15 15:57:12.713: INFO: (18) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 31.842434ms)
Nov 15 15:57:12.713: INFO: (18) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 31.460723ms)
Nov 15 15:57:12.714: INFO: (18) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 32.228295ms)
Nov 15 15:57:12.716: INFO: (18) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 34.104814ms)
Nov 15 15:57:12.716: INFO: (18) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 36.872637ms)
Nov 15 15:57:12.717: INFO: (18) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 36.103382ms)
Nov 15 15:57:12.717: INFO: (18) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 36.408315ms)
Nov 15 15:57:12.719: INFO: (18) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 37.753985ms)
Nov 15 15:57:12.729: INFO: (18) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 46.656856ms)
Nov 15 15:57:12.729: INFO: (18) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 46.615637ms)
Nov 15 15:57:12.762: INFO: (19) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 32.388936ms)
Nov 15 15:57:12.776: INFO: (19) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 45.524841ms)
Nov 15 15:57:12.777: INFO: (19) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 48.690767ms)
Nov 15 15:57:12.778: INFO: (19) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 47.84621ms)
Nov 15 15:57:12.779: INFO: (19) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 48.283991ms)
Nov 15 15:57:12.779: INFO: (19) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 49.728046ms)
Nov 15 15:57:12.779: INFO: (19) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 49.042304ms)
Nov 15 15:57:12.779: INFO: (19) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 48.362172ms)
Nov 15 15:57:12.779: INFO: (19) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 48.528464ms)
Nov 15 15:57:12.780: INFO: (19) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 48.998869ms)
Nov 15 15:57:12.780: INFO: (19) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 49.146504ms)
Nov 15 15:57:12.781: INFO: (19) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 51.175305ms)
Nov 15 15:57:12.782: INFO: (19) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 51.807146ms)
Nov 15 15:57:12.789: INFO: (19) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 59.146298ms)
Nov 15 15:57:12.790: INFO: (19) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 60.033656ms)
Nov 15 15:57:12.790: INFO: (19) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 59.495583ms)
STEP: deleting ReplicationController proxy-service-7qvk4 in namespace proxy-1388, will wait for the garbage collector to delete the pods 11/15/23 15:57:12.791
Nov 15 15:57:12.871: INFO: Deleting ReplicationController proxy-service-7qvk4 took: 17.583779ms
Nov 15 15:57:12.972: INFO: Terminating ReplicationController proxy-service-7qvk4 pods took: 100.883595ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Nov 15 15:57:14.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-1388" for this suite. 11/15/23 15:57:14.893
------------------------------
• [SLOW TEST] [6.505 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:57:08.409
    Nov 15 15:57:08.409: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename proxy 11/15/23 15:57:08.41
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:57:08.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:57:08.464
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 11/15/23 15:57:08.522
    STEP: creating replication controller proxy-service-7qvk4 in namespace proxy-1388 11/15/23 15:57:08.523
    I1115 15:57:08.541012      22 runners.go:193] Created replication controller with name: proxy-service-7qvk4, namespace: proxy-1388, replica count: 1
    I1115 15:57:09.592098      22 runners.go:193] proxy-service-7qvk4 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I1115 15:57:10.592695      22 runners.go:193] proxy-service-7qvk4 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I1115 15:57:11.593052      22 runners.go:193] proxy-service-7qvk4 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 15 15:57:11.609: INFO: setup took 3.131523829s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 11/15/23 15:57:11.609
    Nov 15 15:57:11.702: INFO: (0) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 88.570155ms)
    Nov 15 15:57:11.702: INFO: (0) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 88.773135ms)
    Nov 15 15:57:11.708: INFO: (0) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 95.155501ms)
    Nov 15 15:57:11.710: INFO: (0) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 97.066805ms)
    Nov 15 15:57:11.710: INFO: (0) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 98.417436ms)
    Nov 15 15:57:11.710: INFO: (0) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 97.532698ms)
    Nov 15 15:57:11.710: INFO: (0) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 100.842515ms)
    Nov 15 15:57:11.712: INFO: (0) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 99.494454ms)
    Nov 15 15:57:11.713: INFO: (0) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 102.644073ms)
    Nov 15 15:57:11.714: INFO: (0) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 100.906576ms)
    Nov 15 15:57:11.717: INFO: (0) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 106.602155ms)
    Nov 15 15:57:11.717: INFO: (0) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 104.022833ms)
    Nov 15 15:57:11.717: INFO: (0) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 104.076415ms)
    Nov 15 15:57:11.719: INFO: (0) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 108.193232ms)
    Nov 15 15:57:11.721: INFO: (0) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 108.675284ms)
    Nov 15 15:57:11.721: INFO: (0) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 107.546699ms)
    Nov 15 15:57:11.762: INFO: (1) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 40.306424ms)
    Nov 15 15:57:11.768: INFO: (1) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 46.452047ms)
    Nov 15 15:57:11.776: INFO: (1) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 53.997766ms)
    Nov 15 15:57:11.776: INFO: (1) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 54.70207ms)
    Nov 15 15:57:11.777: INFO: (1) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 54.533372ms)
    Nov 15 15:57:11.777: INFO: (1) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 54.965977ms)
    Nov 15 15:57:11.777: INFO: (1) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 55.307498ms)
    Nov 15 15:57:11.777: INFO: (1) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 55.038675ms)
    Nov 15 15:57:11.777: INFO: (1) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 54.681246ms)
    Nov 15 15:57:11.777: INFO: (1) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 54.642289ms)
    Nov 15 15:57:11.777: INFO: (1) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 55.095119ms)
    Nov 15 15:57:11.777: INFO: (1) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 55.124649ms)
    Nov 15 15:57:11.777: INFO: (1) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 55.295384ms)
    Nov 15 15:57:11.779: INFO: (1) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 57.034562ms)
    Nov 15 15:57:11.781: INFO: (1) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 58.204514ms)
    Nov 15 15:57:11.790: INFO: (1) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 67.952049ms)
    Nov 15 15:57:11.817: INFO: (2) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 25.895612ms)
    Nov 15 15:57:11.822: INFO: (2) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 31.372858ms)
    Nov 15 15:57:11.822: INFO: (2) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 31.746959ms)
    Nov 15 15:57:11.822: INFO: (2) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 31.143218ms)
    Nov 15 15:57:11.823: INFO: (2) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 31.539752ms)
    Nov 15 15:57:11.824: INFO: (2) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 32.620539ms)
    Nov 15 15:57:11.843: INFO: (2) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 52.562716ms)
    Nov 15 15:57:11.843: INFO: (2) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 51.530382ms)
    Nov 15 15:57:11.843: INFO: (2) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 51.666602ms)
    Nov 15 15:57:11.844: INFO: (2) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 52.574481ms)
    Nov 15 15:57:11.844: INFO: (2) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 52.429613ms)
    Nov 15 15:57:11.844: INFO: (2) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 52.319701ms)
    Nov 15 15:57:11.844: INFO: (2) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 52.579389ms)
    Nov 15 15:57:11.844: INFO: (2) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 52.181456ms)
    Nov 15 15:57:11.852: INFO: (2) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 60.313166ms)
    Nov 15 15:57:11.858: INFO: (2) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 66.560506ms)
    Nov 15 15:57:11.888: INFO: (3) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 28.777162ms)
    Nov 15 15:57:11.891: INFO: (3) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 33.200528ms)
    Nov 15 15:57:11.895: INFO: (3) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 36.615413ms)
    Nov 15 15:57:11.895: INFO: (3) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 35.360314ms)
    Nov 15 15:57:11.896: INFO: (3) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 37.174139ms)
    Nov 15 15:57:11.896: INFO: (3) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 36.521239ms)
    Nov 15 15:57:11.898: INFO: (3) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 37.770517ms)
    Nov 15 15:57:11.898: INFO: (3) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 37.385507ms)
    Nov 15 15:57:11.898: INFO: (3) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 39.959117ms)
    Nov 15 15:57:11.898: INFO: (3) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 38.263501ms)
    Nov 15 15:57:11.898: INFO: (3) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 38.180867ms)
    Nov 15 15:57:11.898: INFO: (3) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 39.285689ms)
    Nov 15 15:57:11.898: INFO: (3) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 38.484997ms)
    Nov 15 15:57:11.896: INFO: (3) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 36.622631ms)
    Nov 15 15:57:11.909: INFO: (3) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 50.787002ms)
    Nov 15 15:57:11.915: INFO: (3) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 56.870783ms)
    Nov 15 15:57:11.938: INFO: (4) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 22.743663ms)
    Nov 15 15:57:11.952: INFO: (4) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 35.359754ms)
    Nov 15 15:57:11.952: INFO: (4) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 36.031401ms)
    Nov 15 15:57:11.952: INFO: (4) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 35.473629ms)
    Nov 15 15:57:11.955: INFO: (4) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 39.062407ms)
    Nov 15 15:57:11.955: INFO: (4) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 38.621823ms)
    Nov 15 15:57:11.955: INFO: (4) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 39.043256ms)
    Nov 15 15:57:11.955: INFO: (4) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 38.860774ms)
    Nov 15 15:57:11.956: INFO: (4) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 40.065798ms)
    Nov 15 15:57:11.956: INFO: (4) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 40.312152ms)
    Nov 15 15:57:11.956: INFO: (4) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 39.707557ms)
    Nov 15 15:57:11.956: INFO: (4) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 39.478699ms)
    Nov 15 15:57:11.960: INFO: (4) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 43.655607ms)
    Nov 15 15:57:11.960: INFO: (4) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 43.953086ms)
    Nov 15 15:57:11.960: INFO: (4) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 43.621897ms)
    Nov 15 15:57:11.960: INFO: (4) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 44.426843ms)
    Nov 15 15:57:11.982: INFO: (5) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 21.96726ms)
    Nov 15 15:57:11.993: INFO: (5) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 32.783402ms)
    Nov 15 15:57:11.994: INFO: (5) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 32.619451ms)
    Nov 15 15:57:11.994: INFO: (5) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 32.927906ms)
    Nov 15 15:57:11.994: INFO: (5) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 33.226784ms)
    Nov 15 15:57:11.994: INFO: (5) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 33.012855ms)
    Nov 15 15:57:11.994: INFO: (5) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 33.942234ms)
    Nov 15 15:57:11.994: INFO: (5) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 33.528368ms)
    Nov 15 15:57:11.995: INFO: (5) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 34.109259ms)
    Nov 15 15:57:11.995: INFO: (5) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 34.043463ms)
    Nov 15 15:57:11.995: INFO: (5) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 34.790175ms)
    Nov 15 15:57:11.997: INFO: (5) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 36.054752ms)
    Nov 15 15:57:11.998: INFO: (5) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 36.576244ms)
    Nov 15 15:57:12.003: INFO: (5) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 42.597648ms)
    Nov 15 15:57:12.004: INFO: (5) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 42.386081ms)
    Nov 15 15:57:12.004: INFO: (5) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 42.081869ms)
    Nov 15 15:57:12.026: INFO: (6) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 21.955546ms)
    Nov 15 15:57:12.031: INFO: (6) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 27.425852ms)
    Nov 15 15:57:12.033: INFO: (6) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 28.692863ms)
    Nov 15 15:57:12.033: INFO: (6) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 27.998893ms)
    Nov 15 15:57:12.056: INFO: (6) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 51.756827ms)
    Nov 15 15:57:12.056: INFO: (6) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 51.928755ms)
    Nov 15 15:57:12.056: INFO: (6) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 51.46696ms)
    Nov 15 15:57:12.056: INFO: (6) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 51.223821ms)
    Nov 15 15:57:12.056: INFO: (6) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 50.824711ms)
    Nov 15 15:57:12.056: INFO: (6) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 50.981661ms)
    Nov 15 15:57:12.056: INFO: (6) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 52.211133ms)
    Nov 15 15:57:12.063: INFO: (6) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 57.780193ms)
    Nov 15 15:57:12.063: INFO: (6) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 58.313356ms)
    Nov 15 15:57:12.063: INFO: (6) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 57.825024ms)
    Nov 15 15:57:12.063: INFO: (6) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 57.72866ms)
    Nov 15 15:57:12.063: INFO: (6) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 57.911706ms)
    Nov 15 15:57:12.083: INFO: (7) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 19.74414ms)
    Nov 15 15:57:12.092: INFO: (7) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 27.18963ms)
    Nov 15 15:57:12.092: INFO: (7) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 28.719378ms)
    Nov 15 15:57:12.094: INFO: (7) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 30.43905ms)
    Nov 15 15:57:12.094: INFO: (7) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 29.919846ms)
    Nov 15 15:57:12.097: INFO: (7) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 33.102626ms)
    Nov 15 15:57:12.098: INFO: (7) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 33.573733ms)
    Nov 15 15:57:12.098: INFO: (7) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 33.318793ms)
    Nov 15 15:57:12.100: INFO: (7) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 35.221242ms)
    Nov 15 15:57:12.102: INFO: (7) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 36.77103ms)
    Nov 15 15:57:12.102: INFO: (7) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 36.860727ms)
    Nov 15 15:57:12.102: INFO: (7) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 36.696065ms)
    Nov 15 15:57:12.102: INFO: (7) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 37.555367ms)
    Nov 15 15:57:12.104: INFO: (7) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 39.077168ms)
    Nov 15 15:57:12.107: INFO: (7) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 43.955617ms)
    Nov 15 15:57:12.109: INFO: (7) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 44.693179ms)
    Nov 15 15:57:12.134: INFO: (8) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 24.663352ms)
    Nov 15 15:57:12.138: INFO: (8) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 28.530157ms)
    Nov 15 15:57:12.141: INFO: (8) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 31.80492ms)
    Nov 15 15:57:12.141: INFO: (8) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 31.091637ms)
    Nov 15 15:57:12.143: INFO: (8) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 34.02885ms)
    Nov 15 15:57:12.146: INFO: (8) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 37.241484ms)
    Nov 15 15:57:12.147: INFO: (8) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 37.566102ms)
    Nov 15 15:57:12.148: INFO: (8) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 37.812946ms)
    Nov 15 15:57:12.148: INFO: (8) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 38.189204ms)
    Nov 15 15:57:12.148: INFO: (8) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 38.755082ms)
    Nov 15 15:57:12.148: INFO: (8) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 38.262541ms)
    Nov 15 15:57:12.148: INFO: (8) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 39.553985ms)
    Nov 15 15:57:12.149: INFO: (8) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 39.207854ms)
    Nov 15 15:57:12.156: INFO: (8) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 46.341506ms)
    Nov 15 15:57:12.156: INFO: (8) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 46.664294ms)
    Nov 15 15:57:12.162: INFO: (8) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 52.238427ms)
    Nov 15 15:57:12.185: INFO: (9) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 22.296353ms)
    Nov 15 15:57:12.186: INFO: (9) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 22.618802ms)
    Nov 15 15:57:12.198: INFO: (9) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 34.415601ms)
    Nov 15 15:57:12.206: INFO: (9) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 43.1069ms)
    Nov 15 15:57:12.207: INFO: (9) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 43.276338ms)
    Nov 15 15:57:12.207: INFO: (9) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 43.536043ms)
    Nov 15 15:57:12.207: INFO: (9) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 44.16483ms)
    Nov 15 15:57:12.207: INFO: (9) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 43.185158ms)
    Nov 15 15:57:12.219: INFO: (9) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 54.845155ms)
    Nov 15 15:57:12.219: INFO: (9) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 56.674978ms)
    Nov 15 15:57:12.220: INFO: (9) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 57.19098ms)
    Nov 15 15:57:12.220: INFO: (9) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 56.010003ms)
    Nov 15 15:57:12.220: INFO: (9) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 57.755366ms)
    Nov 15 15:57:12.220: INFO: (9) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 56.781629ms)
    Nov 15 15:57:12.220: INFO: (9) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 55.854148ms)
    Nov 15 15:57:12.238: INFO: (9) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 74.977695ms)
    Nov 15 15:57:12.268: INFO: (10) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 28.37668ms)
    Nov 15 15:57:12.279: INFO: (10) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 40.032364ms)
    Nov 15 15:57:12.279: INFO: (10) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 38.974554ms)
    Nov 15 15:57:12.292: INFO: (10) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 51.999755ms)
    Nov 15 15:57:12.293: INFO: (10) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 52.173915ms)
    Nov 15 15:57:12.294: INFO: (10) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 54.167386ms)
    Nov 15 15:57:12.294: INFO: (10) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 54.079949ms)
    Nov 15 15:57:12.301: INFO: (10) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 61.917552ms)
    Nov 15 15:57:12.301: INFO: (10) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 60.965061ms)
    Nov 15 15:57:12.301: INFO: (10) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 60.440183ms)
    Nov 15 15:57:12.308: INFO: (10) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 68.27905ms)
    Nov 15 15:57:12.309: INFO: (10) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 69.579541ms)
    Nov 15 15:57:12.309: INFO: (10) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 68.814204ms)
    Nov 15 15:57:12.309: INFO: (10) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 69.30176ms)
    Nov 15 15:57:12.309: INFO: (10) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 69.059033ms)
    Nov 15 15:57:12.309: INFO: (10) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 68.276406ms)
    Nov 15 15:57:12.337: INFO: (11) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 26.201088ms)
    Nov 15 15:57:12.344: INFO: (11) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 32.30875ms)
    Nov 15 15:57:12.344: INFO: (11) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 32.100713ms)
    Nov 15 15:57:12.345: INFO: (11) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 33.151757ms)
    Nov 15 15:57:12.345: INFO: (11) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 32.624697ms)
    Nov 15 15:57:12.345: INFO: (11) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 33.686037ms)
    Nov 15 15:57:12.346: INFO: (11) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 34.784632ms)
    Nov 15 15:57:12.348: INFO: (11) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 35.200581ms)
    Nov 15 15:57:12.348: INFO: (11) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 35.574929ms)
    Nov 15 15:57:12.349: INFO: (11) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 35.249845ms)
    Nov 15 15:57:12.349: INFO: (11) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 35.603894ms)
    Nov 15 15:57:12.349: INFO: (11) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 36.345516ms)
    Nov 15 15:57:12.349: INFO: (11) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 36.027358ms)
    Nov 15 15:57:12.357: INFO: (11) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 44.610343ms)
    Nov 15 15:57:12.357: INFO: (11) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 45.62388ms)
    Nov 15 15:57:12.358: INFO: (11) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 44.465387ms)
    Nov 15 15:57:12.389: INFO: (12) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 29.761872ms)
    Nov 15 15:57:12.394: INFO: (12) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 34.270532ms)
    Nov 15 15:57:12.395: INFO: (12) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 35.543295ms)
    Nov 15 15:57:12.395: INFO: (12) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 35.245424ms)
    Nov 15 15:57:12.395: INFO: (12) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 35.272266ms)
    Nov 15 15:57:12.400: INFO: (12) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 40.850719ms)
    Nov 15 15:57:12.400: INFO: (12) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 40.34501ms)
    Nov 15 15:57:12.401: INFO: (12) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 42.653029ms)
    Nov 15 15:57:12.413: INFO: (12) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 54.410548ms)
    Nov 15 15:57:12.413: INFO: (12) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 54.929379ms)
    Nov 15 15:57:12.414: INFO: (12) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 54.905192ms)
    Nov 15 15:57:12.414: INFO: (12) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 55.396969ms)
    Nov 15 15:57:12.415: INFO: (12) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 56.089047ms)
    Nov 15 15:57:12.421: INFO: (12) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 61.619127ms)
    Nov 15 15:57:12.421: INFO: (12) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 60.576074ms)
    Nov 15 15:57:12.423: INFO: (12) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 63.843558ms)
    Nov 15 15:57:12.446: INFO: (13) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 22.552536ms)
    Nov 15 15:57:12.451: INFO: (13) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 27.271668ms)
    Nov 15 15:57:12.453: INFO: (13) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 28.327006ms)
    Nov 15 15:57:12.464: INFO: (13) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 39.814487ms)
    Nov 15 15:57:12.464: INFO: (13) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 39.784114ms)
    Nov 15 15:57:12.465: INFO: (13) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 39.936576ms)
    Nov 15 15:57:12.465: INFO: (13) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 40.257435ms)
    Nov 15 15:57:12.467: INFO: (13) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 42.83685ms)
    Nov 15 15:57:12.467: INFO: (13) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 42.255341ms)
    Nov 15 15:57:12.467: INFO: (13) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 43.134896ms)
    Nov 15 15:57:12.467: INFO: (13) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 42.467062ms)
    Nov 15 15:57:12.470: INFO: (13) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 45.879348ms)
    Nov 15 15:57:12.470: INFO: (13) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 46.176577ms)
    Nov 15 15:57:12.473: INFO: (13) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 49.41081ms)
    Nov 15 15:57:12.474: INFO: (13) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 50.153458ms)
    Nov 15 15:57:12.474: INFO: (13) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 50.34124ms)
    Nov 15 15:57:12.503: INFO: (14) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 27.178439ms)
    Nov 15 15:57:12.509: INFO: (14) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 31.404398ms)
    Nov 15 15:57:12.509: INFO: (14) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 31.416517ms)
    Nov 15 15:57:12.509: INFO: (14) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 31.578779ms)
    Nov 15 15:57:12.509: INFO: (14) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 33.001643ms)
    Nov 15 15:57:12.509: INFO: (14) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 33.151976ms)
    Nov 15 15:57:12.510: INFO: (14) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 32.095125ms)
    Nov 15 15:57:12.511: INFO: (14) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 34.336826ms)
    Nov 15 15:57:12.511: INFO: (14) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 34.275621ms)
    Nov 15 15:57:12.511: INFO: (14) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 34.739082ms)
    Nov 15 15:57:12.512: INFO: (14) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 34.821339ms)
    Nov 15 15:57:12.514: INFO: (14) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 35.943772ms)
    Nov 15 15:57:12.514: INFO: (14) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 37.522447ms)
    Nov 15 15:57:12.516: INFO: (14) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 38.865752ms)
    Nov 15 15:57:12.526: INFO: (14) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 49.59924ms)
    Nov 15 15:57:12.527: INFO: (14) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 49.368777ms)
    Nov 15 15:57:12.554: INFO: (15) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 25.908874ms)
    Nov 15 15:57:12.557: INFO: (15) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 28.800238ms)
    Nov 15 15:57:12.557: INFO: (15) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 28.976132ms)
    Nov 15 15:57:12.557: INFO: (15) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 28.701505ms)
    Nov 15 15:57:12.558: INFO: (15) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 28.847793ms)
    Nov 15 15:57:12.558: INFO: (15) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 30.32769ms)
    Nov 15 15:57:12.566: INFO: (15) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 37.840823ms)
    Nov 15 15:57:12.567: INFO: (15) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 38.438969ms)
    Nov 15 15:57:12.568: INFO: (15) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 40.620233ms)
    Nov 15 15:57:12.568: INFO: (15) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 39.182059ms)
    Nov 15 15:57:12.568: INFO: (15) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 39.631772ms)
    Nov 15 15:57:12.569: INFO: (15) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 40.888ms)
    Nov 15 15:57:12.569: INFO: (15) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 40.292435ms)
    Nov 15 15:57:12.569: INFO: (15) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 40.571364ms)
    Nov 15 15:57:12.578: INFO: (15) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 49.272693ms)
    Nov 15 15:57:12.578: INFO: (15) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 49.243793ms)
    Nov 15 15:57:12.598: INFO: (16) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 19.937029ms)
    Nov 15 15:57:12.606: INFO: (16) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 26.479811ms)
    Nov 15 15:57:12.606: INFO: (16) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 27.553017ms)
    Nov 15 15:57:12.607: INFO: (16) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 28.317642ms)
    Nov 15 15:57:12.610: INFO: (16) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 31.814183ms)
    Nov 15 15:57:12.611: INFO: (16) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 31.998894ms)
    Nov 15 15:57:12.612: INFO: (16) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 32.598485ms)
    Nov 15 15:57:12.612: INFO: (16) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 33.585015ms)
    Nov 15 15:57:12.612: INFO: (16) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 32.999084ms)
    Nov 15 15:57:12.612: INFO: (16) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 33.302691ms)
    Nov 15 15:57:12.612: INFO: (16) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 33.104043ms)
    Nov 15 15:57:12.613: INFO: (16) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 34.288123ms)
    Nov 15 15:57:12.613: INFO: (16) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 34.754135ms)
    Nov 15 15:57:12.627: INFO: (16) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 47.814015ms)
    Nov 15 15:57:12.627: INFO: (16) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 48.611992ms)
    Nov 15 15:57:12.627: INFO: (16) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 48.472819ms)
    Nov 15 15:57:12.655: INFO: (17) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 28.132586ms)
    Nov 15 15:57:12.656: INFO: (17) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 27.354499ms)
    Nov 15 15:57:12.656: INFO: (17) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 27.446016ms)
    Nov 15 15:57:12.656: INFO: (17) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 27.871034ms)
    Nov 15 15:57:12.658: INFO: (17) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 29.80753ms)
    Nov 15 15:57:12.661: INFO: (17) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 33.49096ms)
    Nov 15 15:57:12.661: INFO: (17) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 33.471175ms)
    Nov 15 15:57:12.662: INFO: (17) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 33.39517ms)
    Nov 15 15:57:12.666: INFO: (17) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 37.889864ms)
    Nov 15 15:57:12.667: INFO: (17) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 38.980183ms)
    Nov 15 15:57:12.667: INFO: (17) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 39.032513ms)
    Nov 15 15:57:12.667: INFO: (17) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 39.470704ms)
    Nov 15 15:57:12.667: INFO: (17) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 39.546046ms)
    Nov 15 15:57:12.668: INFO: (17) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 39.992638ms)
    Nov 15 15:57:12.679: INFO: (17) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 50.775383ms)
    Nov 15 15:57:12.679: INFO: (17) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 51.978939ms)
    Nov 15 15:57:12.705: INFO: (18) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 25.683147ms)
    Nov 15 15:57:12.707: INFO: (18) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 27.133341ms)
    Nov 15 15:57:12.709: INFO: (18) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 28.823918ms)
    Nov 15 15:57:12.711: INFO: (18) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 30.703567ms)
    Nov 15 15:57:12.712: INFO: (18) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 31.414111ms)
    Nov 15 15:57:12.712: INFO: (18) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 31.335436ms)
    Nov 15 15:57:12.713: INFO: (18) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 31.842434ms)
    Nov 15 15:57:12.713: INFO: (18) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 31.460723ms)
    Nov 15 15:57:12.714: INFO: (18) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 32.228295ms)
    Nov 15 15:57:12.716: INFO: (18) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 34.104814ms)
    Nov 15 15:57:12.716: INFO: (18) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 36.872637ms)
    Nov 15 15:57:12.717: INFO: (18) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 36.103382ms)
    Nov 15 15:57:12.717: INFO: (18) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 36.408315ms)
    Nov 15 15:57:12.719: INFO: (18) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 37.753985ms)
    Nov 15 15:57:12.729: INFO: (18) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 46.656856ms)
    Nov 15 15:57:12.729: INFO: (18) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 46.615637ms)
    Nov 15 15:57:12.762: INFO: (19) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:443/proxy/tlsrewritem... (200; 32.388936ms)
    Nov 15 15:57:12.776: INFO: (19) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 45.524841ms)
    Nov 15 15:57:12.777: INFO: (19) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname2/proxy/: tls qux (200; 48.690767ms)
    Nov 15 15:57:12.778: INFO: (19) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z/proxy/rewriteme">test</a> (200; 47.84621ms)
    Nov 15 15:57:12.779: INFO: (19) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 48.283991ms)
    Nov 15 15:57:12.779: INFO: (19) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname1/proxy/: foo (200; 49.728046ms)
    Nov 15 15:57:12.779: INFO: (19) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:462/proxy/: tls qux (200; 49.042304ms)
    Nov 15 15:57:12.779: INFO: (19) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">test<... (200; 48.362172ms)
    Nov 15 15:57:12.779: INFO: (19) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/: <a href="/api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:1080/proxy/rewriteme">... (200; 48.528464ms)
    Nov 15 15:57:12.780: INFO: (19) /api/v1/namespaces/proxy-1388/services/https:proxy-service-7qvk4:tlsportname1/proxy/: tls baz (200; 48.998869ms)
    Nov 15 15:57:12.780: INFO: (19) /api/v1/namespaces/proxy-1388/pods/https:proxy-service-7qvk4-tbw2z:460/proxy/: tls baz (200; 49.146504ms)
    Nov 15 15:57:12.781: INFO: (19) /api/v1/namespaces/proxy-1388/services/proxy-service-7qvk4:portname2/proxy/: bar (200; 51.175305ms)
    Nov 15 15:57:12.782: INFO: (19) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname2/proxy/: bar (200; 51.807146ms)
    Nov 15 15:57:12.789: INFO: (19) /api/v1/namespaces/proxy-1388/pods/proxy-service-7qvk4-tbw2z:160/proxy/: foo (200; 59.146298ms)
    Nov 15 15:57:12.790: INFO: (19) /api/v1/namespaces/proxy-1388/services/http:proxy-service-7qvk4:portname1/proxy/: foo (200; 60.033656ms)
    Nov 15 15:57:12.790: INFO: (19) /api/v1/namespaces/proxy-1388/pods/http:proxy-service-7qvk4-tbw2z:162/proxy/: bar (200; 59.495583ms)
    STEP: deleting ReplicationController proxy-service-7qvk4 in namespace proxy-1388, will wait for the garbage collector to delete the pods 11/15/23 15:57:12.791
    Nov 15 15:57:12.871: INFO: Deleting ReplicationController proxy-service-7qvk4 took: 17.583779ms
    Nov 15 15:57:12.972: INFO: Terminating ReplicationController proxy-service-7qvk4 pods took: 100.883595ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:57:14.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-1388" for this suite. 11/15/23 15:57:14.893
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:57:14.934
Nov 15 15:57:14.934: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename services 11/15/23 15:57:14.936
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:57:14.976
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:57:14.989
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 11/15/23 15:57:15.007
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 15 15:57:15.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8347" for this suite. 11/15/23 15:57:15.04
------------------------------
• [0.124 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:57:14.934
    Nov 15 15:57:14.934: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename services 11/15/23 15:57:14.936
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:57:14.976
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:57:14.989
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 11/15/23 15:57:15.007
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:57:15.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8347" for this suite. 11/15/23 15:57:15.04
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:57:15.072
Nov 15 15:57:15.072: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename gc 11/15/23 15:57:15.074
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:57:15.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:57:15.13
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 11/15/23 15:57:15.163
STEP: delete the rc 11/15/23 15:57:20.195
STEP: wait for the rc to be deleted 11/15/23 15:57:20.216
STEP: Gathering metrics 11/15/23 15:57:21.245
W1115 15:57:21.296304      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Nov 15 15:57:21.296: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Nov 15 15:57:21.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6997" for this suite. 11/15/23 15:57:21.312
------------------------------
• [SLOW TEST] [6.268 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:57:15.072
    Nov 15 15:57:15.072: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename gc 11/15/23 15:57:15.074
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:57:15.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:57:15.13
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 11/15/23 15:57:15.163
    STEP: delete the rc 11/15/23 15:57:20.195
    STEP: wait for the rc to be deleted 11/15/23 15:57:20.216
    STEP: Gathering metrics 11/15/23 15:57:21.245
    W1115 15:57:21.296304      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Nov 15 15:57:21.296: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:57:21.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6997" for this suite. 11/15/23 15:57:21.312
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:57:21.34
Nov 15 15:57:21.340: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename deployment 11/15/23 15:57:21.341
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:57:21.387
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:57:21.4
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Nov 15 15:57:21.413: INFO: Creating simple deployment test-new-deployment
Nov 15 15:57:21.455: INFO: deployment "test-new-deployment" doesn't have the required revision set
Nov 15 15:57:23.538: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 57, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 57, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 57, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 57, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 15:57:25.556: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 57, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 57, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 57, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 57, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 15:57:27.554: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 57, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 57, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 57, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 57, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 15:57:29.552: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 57, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 57, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 57, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 57, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource 11/15/23 15:57:31.562
STEP: updating a scale subresource 11/15/23 15:57:31.573
STEP: verifying the deployment Spec.Replicas was modified 11/15/23 15:57:31.592
STEP: Patch a scale subresource 11/15/23 15:57:31.603
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Nov 15 15:57:31.675: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-5497  c8ed52fa-ef95-442d-9ea4-2376ebb9a251 27815 3 2023-11-15 15:57:21 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-11-15 15:57:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 15:57:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d95558 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-11-15 15:57:30 +0000 UTC,LastTransitionTime:2023-11-15 15:57:21 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-11-15 15:57:31 +0000 UTC,LastTransitionTime:2023-11-15 15:57:31 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 15 15:57:31.686: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-5497  f5c5e010-b486-4236-831a-14f0fb6cd629 27818 3 2023-11-15 15:57:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment c8ed52fa-ef95-442d-9ea4-2376ebb9a251 0xc00511a587 0xc00511a588}] [] [{kube-controller-manager Update apps/v1 2023-11-15 15:57:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c8ed52fa-ef95-442d-9ea4-2376ebb9a251\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 15:57:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00511a618 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 15 15:57:31.707: INFO: Pod "test-new-deployment-7f5969cbc7-dpv9c" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-dpv9c test-new-deployment-7f5969cbc7- deployment-5497  7dd3479e-4ecc-4188-9114-968950249427 27811 0 2023-11-15 15:57:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 f5c5e010-b486-4236-831a-14f0fb6cd629 0xc004d95907 0xc004d95908}] [] [{kube-controller-manager Update v1 2023-11-15 15:57:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c5e010-b486-4236-831a-14f0fb6cd629\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8t5gs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8t5gs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 15:57:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 15:57:31.708: INFO: Pod "test-new-deployment-7f5969cbc7-hkm8d" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-hkm8d test-new-deployment-7f5969cbc7- deployment-5497  9807c25f-eaa9-46e1-ab87-78bc7809c880 27820 0 2023-11-15 15:57:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 f5c5e010-b486-4236-831a-14f0fb6cd629 0xc004d95a70 0xc004d95a71}] [] [{kube-controller-manager Update v1 2023-11-15 15:57:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c5e010-b486-4236-831a-14f0fb6cd629\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6k52c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6k52c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 15:57:31.710: INFO: Pod "test-new-deployment-7f5969cbc7-tbrcj" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-tbrcj test-new-deployment-7f5969cbc7- deployment-5497  3ffe2d0c-80b1-4101-b5ae-bbebbf0f89c1 27747 0 2023-11-15 15:57:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:3ead30b4c284dac9838b72cfbc53509ec9b19dbf4949c99d6106da279edfb399 cni.projectcalico.org/podIP:172.30.164.18/32 cni.projectcalico.org/podIPs:172.30.164.18/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 f5c5e010-b486-4236-831a-14f0fb6cd629 0xc004d95bc7 0xc004d95bc8}] [] [{kube-controller-manager Update v1 2023-11-15 15:57:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c5e010-b486-4236-831a-14f0fb6cd629\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 15:57:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 15:57:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.164.18\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8c7sz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8c7sz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 15:57:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 15:57:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 15:57:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 15:57:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:172.30.164.18,StartTime:2023-11-15 15:57:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 15:57:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7c84cf8128725a5b82aa4a8553b09f599d7a133b146555f3069f4f78ed289a50,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.164.18,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Nov 15 15:57:31.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5497" for this suite. 11/15/23 15:57:31.728
------------------------------
• [SLOW TEST] [10.410 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:57:21.34
    Nov 15 15:57:21.340: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename deployment 11/15/23 15:57:21.341
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:57:21.387
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:57:21.4
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Nov 15 15:57:21.413: INFO: Creating simple deployment test-new-deployment
    Nov 15 15:57:21.455: INFO: deployment "test-new-deployment" doesn't have the required revision set
    Nov 15 15:57:23.538: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 57, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 57, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 57, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 57, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 15:57:25.556: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 57, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 57, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 57, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 57, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 15:57:27.554: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 57, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 57, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 57, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 57, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 15:57:29.552: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 15, 57, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 57, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 57, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 57, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: getting scale subresource 11/15/23 15:57:31.562
    STEP: updating a scale subresource 11/15/23 15:57:31.573
    STEP: verifying the deployment Spec.Replicas was modified 11/15/23 15:57:31.592
    STEP: Patch a scale subresource 11/15/23 15:57:31.603
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Nov 15 15:57:31.675: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-5497  c8ed52fa-ef95-442d-9ea4-2376ebb9a251 27815 3 2023-11-15 15:57:21 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-11-15 15:57:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 15:57:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d95558 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-11-15 15:57:30 +0000 UTC,LastTransitionTime:2023-11-15 15:57:21 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-11-15 15:57:31 +0000 UTC,LastTransitionTime:2023-11-15 15:57:31 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Nov 15 15:57:31.686: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-5497  f5c5e010-b486-4236-831a-14f0fb6cd629 27818 3 2023-11-15 15:57:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment c8ed52fa-ef95-442d-9ea4-2376ebb9a251 0xc00511a587 0xc00511a588}] [] [{kube-controller-manager Update apps/v1 2023-11-15 15:57:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c8ed52fa-ef95-442d-9ea4-2376ebb9a251\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 15:57:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00511a618 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Nov 15 15:57:31.707: INFO: Pod "test-new-deployment-7f5969cbc7-dpv9c" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-dpv9c test-new-deployment-7f5969cbc7- deployment-5497  7dd3479e-4ecc-4188-9114-968950249427 27811 0 2023-11-15 15:57:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 f5c5e010-b486-4236-831a-14f0fb6cd629 0xc004d95907 0xc004d95908}] [] [{kube-controller-manager Update v1 2023-11-15 15:57:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c5e010-b486-4236-831a-14f0fb6cd629\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8t5gs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8t5gs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 15:57:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 15:57:31.708: INFO: Pod "test-new-deployment-7f5969cbc7-hkm8d" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-hkm8d test-new-deployment-7f5969cbc7- deployment-5497  9807c25f-eaa9-46e1-ab87-78bc7809c880 27820 0 2023-11-15 15:57:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 f5c5e010-b486-4236-831a-14f0fb6cd629 0xc004d95a70 0xc004d95a71}] [] [{kube-controller-manager Update v1 2023-11-15 15:57:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c5e010-b486-4236-831a-14f0fb6cd629\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6k52c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6k52c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 15:57:31.710: INFO: Pod "test-new-deployment-7f5969cbc7-tbrcj" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-tbrcj test-new-deployment-7f5969cbc7- deployment-5497  3ffe2d0c-80b1-4101-b5ae-bbebbf0f89c1 27747 0 2023-11-15 15:57:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:3ead30b4c284dac9838b72cfbc53509ec9b19dbf4949c99d6106da279edfb399 cni.projectcalico.org/podIP:172.30.164.18/32 cni.projectcalico.org/podIPs:172.30.164.18/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 f5c5e010-b486-4236-831a-14f0fb6cd629 0xc004d95bc7 0xc004d95bc8}] [] [{kube-controller-manager Update v1 2023-11-15 15:57:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c5e010-b486-4236-831a-14f0fb6cd629\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 15:57:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 15:57:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.164.18\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8c7sz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8c7sz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 15:57:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 15:57:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 15:57:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 15:57:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:172.30.164.18,StartTime:2023-11-15 15:57:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 15:57:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7c84cf8128725a5b82aa4a8553b09f599d7a133b146555f3069f4f78ed289a50,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.164.18,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:57:31.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5497" for this suite. 11/15/23 15:57:31.728
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:57:31.756
Nov 15 15:57:31.756: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename emptydir-wrapper 11/15/23 15:57:31.758
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:57:31.812
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:57:31.824
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Nov 15 15:57:31.907: INFO: Waiting up to 5m0s for pod "pod-secrets-527f8f02-87e9-4f46-b646-62df46a4679e" in namespace "emptydir-wrapper-1527" to be "running and ready"
Nov 15 15:57:31.929: INFO: Pod "pod-secrets-527f8f02-87e9-4f46-b646-62df46a4679e": Phase="Pending", Reason="", readiness=false. Elapsed: 22.043227ms
Nov 15 15:57:31.929: INFO: The phase of Pod pod-secrets-527f8f02-87e9-4f46-b646-62df46a4679e is Pending, waiting for it to be Running (with Ready = true)
Nov 15 15:57:33.953: INFO: Pod "pod-secrets-527f8f02-87e9-4f46-b646-62df46a4679e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046193073s
Nov 15 15:57:33.954: INFO: The phase of Pod pod-secrets-527f8f02-87e9-4f46-b646-62df46a4679e is Pending, waiting for it to be Running (with Ready = true)
Nov 15 15:57:35.946: INFO: Pod "pod-secrets-527f8f02-87e9-4f46-b646-62df46a4679e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038817651s
Nov 15 15:57:35.946: INFO: The phase of Pod pod-secrets-527f8f02-87e9-4f46-b646-62df46a4679e is Pending, waiting for it to be Running (with Ready = true)
Nov 15 15:57:37.954: INFO: Pod "pod-secrets-527f8f02-87e9-4f46-b646-62df46a4679e": Phase="Running", Reason="", readiness=true. Elapsed: 6.046232854s
Nov 15 15:57:37.954: INFO: The phase of Pod pod-secrets-527f8f02-87e9-4f46-b646-62df46a4679e is Running (Ready = true)
Nov 15 15:57:37.954: INFO: Pod "pod-secrets-527f8f02-87e9-4f46-b646-62df46a4679e" satisfied condition "running and ready"
STEP: Cleaning up the secret 11/15/23 15:57:37.972
STEP: Cleaning up the configmap 11/15/23 15:57:37.998
STEP: Cleaning up the pod 11/15/23 15:57:38.034
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Nov 15 15:57:38.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-1527" for this suite. 11/15/23 15:57:38.123
------------------------------
• [SLOW TEST] [6.390 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:57:31.756
    Nov 15 15:57:31.756: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename emptydir-wrapper 11/15/23 15:57:31.758
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:57:31.812
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:57:31.824
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Nov 15 15:57:31.907: INFO: Waiting up to 5m0s for pod "pod-secrets-527f8f02-87e9-4f46-b646-62df46a4679e" in namespace "emptydir-wrapper-1527" to be "running and ready"
    Nov 15 15:57:31.929: INFO: Pod "pod-secrets-527f8f02-87e9-4f46-b646-62df46a4679e": Phase="Pending", Reason="", readiness=false. Elapsed: 22.043227ms
    Nov 15 15:57:31.929: INFO: The phase of Pod pod-secrets-527f8f02-87e9-4f46-b646-62df46a4679e is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 15:57:33.953: INFO: Pod "pod-secrets-527f8f02-87e9-4f46-b646-62df46a4679e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046193073s
    Nov 15 15:57:33.954: INFO: The phase of Pod pod-secrets-527f8f02-87e9-4f46-b646-62df46a4679e is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 15:57:35.946: INFO: Pod "pod-secrets-527f8f02-87e9-4f46-b646-62df46a4679e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038817651s
    Nov 15 15:57:35.946: INFO: The phase of Pod pod-secrets-527f8f02-87e9-4f46-b646-62df46a4679e is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 15:57:37.954: INFO: Pod "pod-secrets-527f8f02-87e9-4f46-b646-62df46a4679e": Phase="Running", Reason="", readiness=true. Elapsed: 6.046232854s
    Nov 15 15:57:37.954: INFO: The phase of Pod pod-secrets-527f8f02-87e9-4f46-b646-62df46a4679e is Running (Ready = true)
    Nov 15 15:57:37.954: INFO: Pod "pod-secrets-527f8f02-87e9-4f46-b646-62df46a4679e" satisfied condition "running and ready"
    STEP: Cleaning up the secret 11/15/23 15:57:37.972
    STEP: Cleaning up the configmap 11/15/23 15:57:37.998
    STEP: Cleaning up the pod 11/15/23 15:57:38.034
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:57:38.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-1527" for this suite. 11/15/23 15:57:38.123
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:57:38.148
Nov 15 15:57:38.148: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename svc-latency 11/15/23 15:57:38.151
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:57:38.189
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:57:38.202
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Nov 15 15:57:38.216: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5478 11/15/23 15:57:38.217
I1115 15:57:38.230545      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5478, replica count: 1
I1115 15:57:39.281344      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1115 15:57:40.282526      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 15 15:57:40.432: INFO: Created: latency-svc-5hnxr
Nov 15 15:57:40.441: INFO: Got endpoints: latency-svc-5hnxr [58.140798ms]
Nov 15 15:57:40.482: INFO: Created: latency-svc-4b9t4
Nov 15 15:57:40.495: INFO: Got endpoints: latency-svc-4b9t4 [52.232107ms]
Nov 15 15:57:40.509: INFO: Created: latency-svc-cdwpx
Nov 15 15:57:40.518: INFO: Got endpoints: latency-svc-cdwpx [75.044911ms]
Nov 15 15:57:40.534: INFO: Created: latency-svc-jrkdr
Nov 15 15:57:40.542: INFO: Got endpoints: latency-svc-jrkdr [99.117884ms]
Nov 15 15:57:40.563: INFO: Created: latency-svc-jf84n
Nov 15 15:57:40.573: INFO: Got endpoints: latency-svc-jf84n [129.829179ms]
Nov 15 15:57:40.591: INFO: Created: latency-svc-8v5s5
Nov 15 15:57:40.603: INFO: Got endpoints: latency-svc-8v5s5 [160.157772ms]
Nov 15 15:57:40.620: INFO: Created: latency-svc-h9sln
Nov 15 15:57:40.627: INFO: Got endpoints: latency-svc-h9sln [184.620873ms]
Nov 15 15:57:40.648: INFO: Created: latency-svc-26zdd
Nov 15 15:57:40.657: INFO: Got endpoints: latency-svc-26zdd [213.652495ms]
Nov 15 15:57:40.670: INFO: Created: latency-svc-b92dv
Nov 15 15:57:40.681: INFO: Got endpoints: latency-svc-b92dv [237.427561ms]
Nov 15 15:57:40.695: INFO: Created: latency-svc-fn5f7
Nov 15 15:57:40.706: INFO: Got endpoints: latency-svc-fn5f7 [262.871665ms]
Nov 15 15:57:40.720: INFO: Created: latency-svc-gjjl6
Nov 15 15:57:40.731: INFO: Got endpoints: latency-svc-gjjl6 [287.187728ms]
Nov 15 15:57:40.754: INFO: Created: latency-svc-lhbf5
Nov 15 15:57:40.763: INFO: Got endpoints: latency-svc-lhbf5 [320.337919ms]
Nov 15 15:57:40.773: INFO: Created: latency-svc-s4pfd
Nov 15 15:57:40.780: INFO: Got endpoints: latency-svc-s4pfd [336.688608ms]
Nov 15 15:57:40.798: INFO: Created: latency-svc-9kksd
Nov 15 15:57:40.808: INFO: Got endpoints: latency-svc-9kksd [364.785401ms]
Nov 15 15:57:40.823: INFO: Created: latency-svc-sfpjm
Nov 15 15:57:40.838: INFO: Got endpoints: latency-svc-sfpjm [394.110174ms]
Nov 15 15:57:40.860: INFO: Created: latency-svc-ncwtv
Nov 15 15:57:40.873: INFO: Got endpoints: latency-svc-ncwtv [429.116324ms]
Nov 15 15:57:40.885: INFO: Created: latency-svc-4vh2l
Nov 15 15:57:40.896: INFO: Got endpoints: latency-svc-4vh2l [401.130503ms]
Nov 15 15:57:40.912: INFO: Created: latency-svc-q4s7w
Nov 15 15:57:40.923: INFO: Got endpoints: latency-svc-q4s7w [405.321023ms]
Nov 15 15:57:40.939: INFO: Created: latency-svc-pcrjn
Nov 15 15:57:40.946: INFO: Got endpoints: latency-svc-pcrjn [404.057623ms]
Nov 15 15:57:40.972: INFO: Created: latency-svc-jjhb2
Nov 15 15:57:40.978: INFO: Got endpoints: latency-svc-jjhb2 [405.158014ms]
Nov 15 15:57:40.996: INFO: Created: latency-svc-cdr45
Nov 15 15:57:41.004: INFO: Got endpoints: latency-svc-cdr45 [401.006939ms]
Nov 15 15:57:41.029: INFO: Created: latency-svc-dvmcc
Nov 15 15:57:41.037: INFO: Got endpoints: latency-svc-dvmcc [409.25214ms]
Nov 15 15:57:41.051: INFO: Created: latency-svc-fnj2t
Nov 15 15:57:41.059: INFO: Got endpoints: latency-svc-fnj2t [402.172353ms]
Nov 15 15:57:41.080: INFO: Created: latency-svc-6cjr5
Nov 15 15:57:41.088: INFO: Got endpoints: latency-svc-6cjr5 [407.792866ms]
Nov 15 15:57:41.106: INFO: Created: latency-svc-6nzh8
Nov 15 15:57:41.117: INFO: Got endpoints: latency-svc-6nzh8 [410.447191ms]
Nov 15 15:57:41.161: INFO: Created: latency-svc-84jl8
Nov 15 15:57:41.171: INFO: Got endpoints: latency-svc-84jl8 [440.391325ms]
Nov 15 15:57:41.204: INFO: Created: latency-svc-dqctr
Nov 15 15:57:41.210: INFO: Got endpoints: latency-svc-dqctr [446.687375ms]
Nov 15 15:57:41.227: INFO: Created: latency-svc-jh9zt
Nov 15 15:57:41.238: INFO: Got endpoints: latency-svc-jh9zt [457.70716ms]
Nov 15 15:57:41.270: INFO: Created: latency-svc-lpz7k
Nov 15 15:57:41.281: INFO: Got endpoints: latency-svc-lpz7k [473.065735ms]
Nov 15 15:57:41.284: INFO: Created: latency-svc-92qlk
Nov 15 15:57:41.293: INFO: Got endpoints: latency-svc-92qlk [454.829656ms]
Nov 15 15:57:41.314: INFO: Created: latency-svc-slqph
Nov 15 15:57:41.326: INFO: Got endpoints: latency-svc-slqph [453.234316ms]
Nov 15 15:57:41.337: INFO: Created: latency-svc-626nf
Nov 15 15:57:41.349: INFO: Got endpoints: latency-svc-626nf [452.623006ms]
Nov 15 15:57:41.371: INFO: Created: latency-svc-kd6dz
Nov 15 15:57:41.387: INFO: Got endpoints: latency-svc-kd6dz [463.764719ms]
Nov 15 15:57:41.404: INFO: Created: latency-svc-rkbr2
Nov 15 15:57:41.412: INFO: Got endpoints: latency-svc-rkbr2 [465.514623ms]
Nov 15 15:57:41.799: INFO: Created: latency-svc-496j6
Nov 15 15:57:41.800: INFO: Created: latency-svc-wqsqs
Nov 15 15:57:41.800: INFO: Created: latency-svc-r6m9b
Nov 15 15:57:41.800: INFO: Created: latency-svc-frfzt
Nov 15 15:57:41.805: INFO: Created: latency-svc-7sk6z
Nov 15 15:57:41.810: INFO: Created: latency-svc-nn4bp
Nov 15 15:57:41.810: INFO: Created: latency-svc-d8vm6
Nov 15 15:57:41.810: INFO: Created: latency-svc-pkbzk
Nov 15 15:57:41.811: INFO: Created: latency-svc-jjhfn
Nov 15 15:57:41.811: INFO: Created: latency-svc-zhhr6
Nov 15 15:57:41.811: INFO: Created: latency-svc-m69ln
Nov 15 15:57:41.812: INFO: Created: latency-svc-7hcjp
Nov 15 15:57:41.811: INFO: Got endpoints: latency-svc-wqsqs [751.867726ms]
Nov 15 15:57:41.812: INFO: Created: latency-svc-r869p
Nov 15 15:57:41.812: INFO: Created: latency-svc-zfksm
Nov 15 15:57:41.812: INFO: Got endpoints: latency-svc-zhhr6 [695.25465ms]
Nov 15 15:57:41.812: INFO: Created: latency-svc-v28g2
Nov 15 15:57:41.814: INFO: Got endpoints: latency-svc-r6m9b [777.330631ms]
Nov 15 15:57:41.816: INFO: Got endpoints: latency-svc-496j6 [403.03845ms]
Nov 15 15:57:41.816: INFO: Got endpoints: latency-svc-jjhfn [428.611248ms]
Nov 15 15:57:41.820: INFO: Got endpoints: latency-svc-v28g2 [582.229571ms]
Nov 15 15:57:41.825: INFO: Got endpoints: latency-svc-7sk6z [498.99004ms]
Nov 15 15:57:41.826: INFO: Got endpoints: latency-svc-frfzt [654.184169ms]
Nov 15 15:57:41.831: INFO: Got endpoints: latency-svc-zfksm [549.512652ms]
Nov 15 15:57:41.832: INFO: Got endpoints: latency-svc-m69ln [743.178317ms]
Nov 15 15:57:41.833: INFO: Got endpoints: latency-svc-pkbzk [828.39202ms]
Nov 15 15:57:41.841: INFO: Got endpoints: latency-svc-r869p [862.146052ms]
Nov 15 15:57:41.841: INFO: Got endpoints: latency-svc-7hcjp [630.966412ms]
Nov 15 15:57:41.843: INFO: Got endpoints: latency-svc-d8vm6 [549.765509ms]
Nov 15 15:57:41.844: INFO: Got endpoints: latency-svc-nn4bp [494.476535ms]
Nov 15 15:57:41.863: INFO: Created: latency-svc-zngzm
Nov 15 15:57:41.872: INFO: Got endpoints: latency-svc-zngzm [59.948661ms]
Nov 15 15:57:41.898: INFO: Created: latency-svc-sfbz2
Nov 15 15:57:41.902: INFO: Got endpoints: latency-svc-sfbz2 [89.223834ms]
Nov 15 15:57:41.914: INFO: Created: latency-svc-6b69f
Nov 15 15:57:41.923: INFO: Got endpoints: latency-svc-6b69f [108.748091ms]
Nov 15 15:57:41.941: INFO: Created: latency-svc-8ncnq
Nov 15 15:57:41.949: INFO: Got endpoints: latency-svc-8ncnq [133.531766ms]
Nov 15 15:57:41.971: INFO: Created: latency-svc-hg6n6
Nov 15 15:57:41.981: INFO: Got endpoints: latency-svc-hg6n6 [164.732792ms]
Nov 15 15:57:41.990: INFO: Created: latency-svc-hlh7d
Nov 15 15:57:42.000: INFO: Got endpoints: latency-svc-hlh7d [178.931136ms]
Nov 15 15:57:42.017: INFO: Created: latency-svc-bbq7g
Nov 15 15:57:42.028: INFO: Got endpoints: latency-svc-bbq7g [202.582287ms]
Nov 15 15:57:42.041: INFO: Created: latency-svc-pjb9d
Nov 15 15:57:42.058: INFO: Got endpoints: latency-svc-pjb9d [231.934046ms]
Nov 15 15:57:42.066: INFO: Created: latency-svc-f4lbd
Nov 15 15:57:42.074: INFO: Got endpoints: latency-svc-f4lbd [243.202599ms]
Nov 15 15:57:42.093: INFO: Created: latency-svc-qvnh7
Nov 15 15:57:42.104: INFO: Got endpoints: latency-svc-qvnh7 [272.441665ms]
Nov 15 15:57:42.117: INFO: Created: latency-svc-s8dkv
Nov 15 15:57:42.129: INFO: Got endpoints: latency-svc-s8dkv [295.653539ms]
Nov 15 15:57:42.143: INFO: Created: latency-svc-l7ds9
Nov 15 15:57:42.153: INFO: Got endpoints: latency-svc-l7ds9 [311.953827ms]
Nov 15 15:57:42.176: INFO: Created: latency-svc-dvkrj
Nov 15 15:57:42.186: INFO: Got endpoints: latency-svc-dvkrj [345.600983ms]
Nov 15 15:57:42.203: INFO: Created: latency-svc-fp9l7
Nov 15 15:57:42.213: INFO: Got endpoints: latency-svc-fp9l7 [370.908231ms]
Nov 15 15:57:42.231: INFO: Created: latency-svc-8w95r
Nov 15 15:57:42.240: INFO: Got endpoints: latency-svc-8w95r [395.833981ms]
Nov 15 15:57:42.268: INFO: Created: latency-svc-vnv6q
Nov 15 15:57:42.275: INFO: Got endpoints: latency-svc-vnv6q [402.948264ms]
Nov 15 15:57:42.294: INFO: Created: latency-svc-qg8l2
Nov 15 15:57:42.305: INFO: Got endpoints: latency-svc-qg8l2 [402.910767ms]
Nov 15 15:57:42.317: INFO: Created: latency-svc-9b8dc
Nov 15 15:57:42.326: INFO: Got endpoints: latency-svc-9b8dc [403.203272ms]
Nov 15 15:57:42.347: INFO: Created: latency-svc-pxwwd
Nov 15 15:57:42.358: INFO: Got endpoints: latency-svc-pxwwd [408.597559ms]
Nov 15 15:57:42.373: INFO: Created: latency-svc-m2vsm
Nov 15 15:57:42.383: INFO: Got endpoints: latency-svc-m2vsm [402.195467ms]
Nov 15 15:57:42.745: INFO: Created: latency-svc-fqrlj
Nov 15 15:57:42.747: INFO: Created: latency-svc-jw85b
Nov 15 15:57:42.759: INFO: Created: latency-svc-hsq99
Nov 15 15:57:42.760: INFO: Created: latency-svc-kxsmj
Nov 15 15:57:42.760: INFO: Created: latency-svc-tfgvr
Nov 15 15:57:42.760: INFO: Created: latency-svc-rw6ql
Nov 15 15:57:42.760: INFO: Created: latency-svc-xbzgw
Nov 15 15:57:42.760: INFO: Created: latency-svc-nj4s2
Nov 15 15:57:42.761: INFO: Created: latency-svc-7jw6p
Nov 15 15:57:42.761: INFO: Created: latency-svc-kwlcm
Nov 15 15:57:42.761: INFO: Created: latency-svc-flw8p
Nov 15 15:57:42.762: INFO: Created: latency-svc-ss27h
Nov 15 15:57:42.762: INFO: Created: latency-svc-fvxjg
Nov 15 15:57:42.763: INFO: Created: latency-svc-8wr72
Nov 15 15:57:42.764: INFO: Got endpoints: latency-svc-jw85b [578.05833ms]
Nov 15 15:57:42.764: INFO: Got endpoints: latency-svc-fqrlj [489.349335ms]
Nov 15 15:57:42.766: INFO: Created: latency-svc-tkq2d
Nov 15 15:57:42.767: INFO: Got endpoints: latency-svc-kxsmj [767.360367ms]
Nov 15 15:57:42.767: INFO: Got endpoints: latency-svc-tfgvr [383.954786ms]
Nov 15 15:57:42.767: INFO: Got endpoints: latency-svc-flw8p [662.97981ms]
Nov 15 15:57:42.768: INFO: Got endpoints: latency-svc-ss27h [526.803102ms]
Nov 15 15:57:42.768: INFO: Got endpoints: latency-svc-xbzgw [554.191621ms]
Nov 15 15:57:42.770: INFO: Got endpoints: latency-svc-hsq99 [712.185567ms]
Nov 15 15:57:42.770: INFO: Got endpoints: latency-svc-kwlcm [695.763603ms]
Nov 15 15:57:42.772: INFO: Got endpoints: latency-svc-rw6ql [413.401354ms]
Nov 15 15:57:42.777: INFO: Got endpoints: latency-svc-nj4s2 [748.796307ms]
Nov 15 15:57:42.781: INFO: Got endpoints: latency-svc-7jw6p [454.498519ms]
Nov 15 15:57:42.782: INFO: Got endpoints: latency-svc-fvxjg [653.054547ms]
Nov 15 15:57:42.782: INFO: Got endpoints: latency-svc-8wr72 [476.750663ms]
Nov 15 15:57:42.782: INFO: Got endpoints: latency-svc-tkq2d [628.654728ms]
Nov 15 15:57:42.806: INFO: Created: latency-svc-jvsw6
Nov 15 15:57:42.814: INFO: Got endpoints: latency-svc-jvsw6 [50.048843ms]
Nov 15 15:57:42.832: INFO: Created: latency-svc-gmpm9
Nov 15 15:57:42.843: INFO: Got endpoints: latency-svc-gmpm9 [75.987824ms]
Nov 15 15:57:42.860: INFO: Created: latency-svc-6m5pz
Nov 15 15:57:42.869: INFO: Got endpoints: latency-svc-6m5pz [101.196826ms]
Nov 15 15:57:42.881: INFO: Created: latency-svc-gsl6m
Nov 15 15:57:42.893: INFO: Got endpoints: latency-svc-gsl6m [125.676084ms]
Nov 15 15:57:42.918: INFO: Created: latency-svc-djfqq
Nov 15 15:57:42.927: INFO: Got endpoints: latency-svc-djfqq [159.630676ms]
Nov 15 15:57:42.943: INFO: Created: latency-svc-zqhln
Nov 15 15:57:42.954: INFO: Got endpoints: latency-svc-zqhln [189.01029ms]
Nov 15 15:57:43.317: INFO: Created: latency-svc-8kp6k
Nov 15 15:57:43.317: INFO: Created: latency-svc-66vlp
Nov 15 15:57:43.326: INFO: Created: latency-svc-h6vpb
Nov 15 15:57:43.326: INFO: Created: latency-svc-xkwfc
Nov 15 15:57:43.331: INFO: Created: latency-svc-zmlr5
Nov 15 15:57:43.332: INFO: Created: latency-svc-8rqk2
Nov 15 15:57:43.332: INFO: Created: latency-svc-ht8cz
Nov 15 15:57:43.332: INFO: Created: latency-svc-llzmx
Nov 15 15:57:43.332: INFO: Created: latency-svc-8pmdv
Nov 15 15:57:43.332: INFO: Created: latency-svc-p5m8b
Nov 15 15:57:43.333: INFO: Created: latency-svc-bngj9
Nov 15 15:57:43.333: INFO: Created: latency-svc-4tx76
Nov 15 15:57:43.333: INFO: Created: latency-svc-jslxp
Nov 15 15:57:43.333: INFO: Created: latency-svc-9mlgt
Nov 15 15:57:43.333: INFO: Created: latency-svc-l87v4
Nov 15 15:57:43.336: INFO: Got endpoints: latency-svc-8rqk2 [381.737434ms]
Nov 15 15:57:43.336: INFO: Got endpoints: latency-svc-8kp6k [559.035558ms]
Nov 15 15:57:43.337: INFO: Got endpoints: latency-svc-4tx76 [567.202567ms]
Nov 15 15:57:43.337: INFO: Got endpoints: latency-svc-h6vpb [567.194325ms]
Nov 15 15:57:43.338: INFO: Got endpoints: latency-svc-66vlp [565.851791ms]
Nov 15 15:57:43.339: INFO: Got endpoints: latency-svc-zmlr5 [557.66171ms]
Nov 15 15:57:43.345: INFO: Got endpoints: latency-svc-ht8cz [417.49733ms]
Nov 15 15:57:43.346: INFO: Got endpoints: latency-svc-p5m8b [563.500549ms]
Nov 15 15:57:43.390: INFO: Got endpoints: latency-svc-8pmdv [607.275848ms]
Nov 15 15:57:43.390: INFO: Got endpoints: latency-svc-jslxp [496.102378ms]
Nov 15 15:57:43.390: INFO: Got endpoints: latency-svc-bngj9 [622.166524ms]
Nov 15 15:57:43.390: INFO: Got endpoints: latency-svc-llzmx [575.649051ms]
Nov 15 15:57:43.390: INFO: Got endpoints: latency-svc-9mlgt [607.937056ms]
Nov 15 15:57:43.395: INFO: Got endpoints: latency-svc-l87v4 [552.27276ms]
Nov 15 15:57:43.397: INFO: Got endpoints: latency-svc-xkwfc [528.070674ms]
Nov 15 15:57:43.410: INFO: Created: latency-svc-ctzdz
Nov 15 15:57:43.417: INFO: Got endpoints: latency-svc-ctzdz [81.621973ms]
Nov 15 15:57:43.788: INFO: Created: latency-svc-27klt
Nov 15 15:57:43.793: INFO: Created: latency-svc-9m8js
Nov 15 15:57:43.800: INFO: Created: latency-svc-2t46t
Nov 15 15:57:43.802: INFO: Created: latency-svc-czrsg
Nov 15 15:57:43.803: INFO: Created: latency-svc-9wbds
Nov 15 15:57:43.803: INFO: Created: latency-svc-4h67q
Nov 15 15:57:43.803: INFO: Created: latency-svc-jn7lf
Nov 15 15:57:43.803: INFO: Created: latency-svc-rbf5r
Nov 15 15:57:43.804: INFO: Created: latency-svc-cc99d
Nov 15 15:57:43.804: INFO: Created: latency-svc-rrrqc
Nov 15 15:57:43.804: INFO: Created: latency-svc-r2cqw
Nov 15 15:57:43.803: INFO: Created: latency-svc-t4qsg
Nov 15 15:57:43.804: INFO: Created: latency-svc-8kb27
Nov 15 15:57:43.804: INFO: Created: latency-svc-l22jc
Nov 15 15:57:43.805: INFO: Created: latency-svc-6h795
Nov 15 15:57:43.808: INFO: Got endpoints: latency-svc-27klt [470.797388ms]
Nov 15 15:57:43.809: INFO: Got endpoints: latency-svc-l22jc [462.802968ms]
Nov 15 15:57:43.809: INFO: Got endpoints: latency-svc-2t46t [472.835477ms]
Nov 15 15:57:43.810: INFO: Got endpoints: latency-svc-9wbds [419.624269ms]
Nov 15 15:57:43.812: INFO: Got endpoints: latency-svc-9m8js [467.033505ms]
Nov 15 15:57:43.818: INFO: Got endpoints: latency-svc-6h795 [479.127334ms]
Nov 15 15:57:43.818: INFO: Got endpoints: latency-svc-czrsg [428.266233ms]
Nov 15 15:57:43.818: INFO: Got endpoints: latency-svc-jn7lf [428.801857ms]
Nov 15 15:57:43.820: INFO: Got endpoints: latency-svc-t4qsg [482.468777ms]
Nov 15 15:57:43.820: INFO: Got endpoints: latency-svc-cc99d [430.505646ms]
Nov 15 15:57:43.824: INFO: Got endpoints: latency-svc-r2cqw [427.526993ms]
Nov 15 15:57:43.829: INFO: Got endpoints: latency-svc-8kb27 [491.468847ms]
Nov 15 15:57:43.830: INFO: Got endpoints: latency-svc-4h67q [434.22016ms]
Nov 15 15:57:43.830: INFO: Got endpoints: latency-svc-rbf5r [439.693352ms]
Nov 15 15:57:43.835: INFO: Got endpoints: latency-svc-rrrqc [417.946837ms]
Nov 15 15:57:43.852: INFO: Created: latency-svc-hsdxw
Nov 15 15:57:43.861: INFO: Got endpoints: latency-svc-hsdxw [53.27268ms]
Nov 15 15:57:43.880: INFO: Created: latency-svc-qfl7x
Nov 15 15:57:43.890: INFO: Got endpoints: latency-svc-qfl7x [80.731399ms]
Nov 15 15:57:43.907: INFO: Created: latency-svc-mndq4
Nov 15 15:57:43.917: INFO: Got endpoints: latency-svc-mndq4 [107.689083ms]
Nov 15 15:57:43.937: INFO: Created: latency-svc-v2dhn
Nov 15 15:57:43.948: INFO: Got endpoints: latency-svc-v2dhn [138.467174ms]
Nov 15 15:57:43.975: INFO: Created: latency-svc-r9fm8
Nov 15 15:57:43.994: INFO: Got endpoints: latency-svc-r9fm8 [182.841201ms]
Nov 15 15:57:44.012: INFO: Created: latency-svc-rds57
Nov 15 15:57:44.022: INFO: Got endpoints: latency-svc-rds57 [204.387186ms]
Nov 15 15:57:44.042: INFO: Created: latency-svc-d6sn6
Nov 15 15:57:44.052: INFO: Got endpoints: latency-svc-d6sn6 [233.508211ms]
Nov 15 15:57:44.060: INFO: Created: latency-svc-6s8t2
Nov 15 15:57:44.069: INFO: Got endpoints: latency-svc-6s8t2 [250.707537ms]
Nov 15 15:57:44.087: INFO: Created: latency-svc-nnwtb
Nov 15 15:57:44.097: INFO: Got endpoints: latency-svc-nnwtb [277.502144ms]
Nov 15 15:57:44.116: INFO: Created: latency-svc-f4lxc
Nov 15 15:57:44.126: INFO: Got endpoints: latency-svc-f4lxc [305.994606ms]
Nov 15 15:57:44.146: INFO: Created: latency-svc-mxwx9
Nov 15 15:57:44.152: INFO: Got endpoints: latency-svc-mxwx9 [326.999014ms]
Nov 15 15:57:44.166: INFO: Created: latency-svc-8tbdf
Nov 15 15:57:44.173: INFO: Got endpoints: latency-svc-8tbdf [342.720206ms]
Nov 15 15:57:44.196: INFO: Created: latency-svc-khtrr
Nov 15 15:57:44.205: INFO: Got endpoints: latency-svc-khtrr [376.035431ms]
Nov 15 15:57:44.220: INFO: Created: latency-svc-7n49q
Nov 15 15:57:44.232: INFO: Got endpoints: latency-svc-7n49q [401.846832ms]
Nov 15 15:57:44.248: INFO: Created: latency-svc-kbtzh
Nov 15 15:57:44.259: INFO: Got endpoints: latency-svc-kbtzh [423.984598ms]
Nov 15 15:57:44.271: INFO: Created: latency-svc-r9ndf
Nov 15 15:57:44.281: INFO: Got endpoints: latency-svc-r9ndf [419.444398ms]
Nov 15 15:57:44.302: INFO: Created: latency-svc-tgk6d
Nov 15 15:57:44.306: INFO: Got endpoints: latency-svc-tgk6d [415.242461ms]
Nov 15 15:57:44.323: INFO: Created: latency-svc-q2fqz
Nov 15 15:57:44.335: INFO: Got endpoints: latency-svc-q2fqz [417.703922ms]
Nov 15 15:57:44.342: INFO: Created: latency-svc-nfd4c
Nov 15 15:57:44.352: INFO: Got endpoints: latency-svc-nfd4c [403.330562ms]
Nov 15 15:57:44.370: INFO: Created: latency-svc-cwbhz
Nov 15 15:57:44.375: INFO: Got endpoints: latency-svc-cwbhz [380.492463ms]
Nov 15 15:57:44.394: INFO: Created: latency-svc-nm548
Nov 15 15:57:44.405: INFO: Got endpoints: latency-svc-nm548 [382.610365ms]
Nov 15 15:57:44.423: INFO: Created: latency-svc-5zs8m
Nov 15 15:57:44.431: INFO: Got endpoints: latency-svc-5zs8m [379.175847ms]
Nov 15 15:57:44.450: INFO: Created: latency-svc-g5rk6
Nov 15 15:57:44.461: INFO: Got endpoints: latency-svc-g5rk6 [392.204506ms]
Nov 15 15:57:44.476: INFO: Created: latency-svc-2jlr5
Nov 15 15:57:44.486: INFO: Got endpoints: latency-svc-2jlr5 [388.818497ms]
Nov 15 15:57:44.498: INFO: Created: latency-svc-vnrrq
Nov 15 15:57:44.517: INFO: Got endpoints: latency-svc-vnrrq [390.53263ms]
Nov 15 15:57:44.525: INFO: Created: latency-svc-szfwh
Nov 15 15:57:44.537: INFO: Got endpoints: latency-svc-szfwh [385.065643ms]
Nov 15 15:57:44.561: INFO: Created: latency-svc-c572s
Nov 15 15:57:44.568: INFO: Got endpoints: latency-svc-c572s [395.303069ms]
Nov 15 15:57:44.587: INFO: Created: latency-svc-9s44f
Nov 15 15:57:44.599: INFO: Got endpoints: latency-svc-9s44f [393.526154ms]
Nov 15 15:57:44.611: INFO: Created: latency-svc-hw45q
Nov 15 15:57:44.620: INFO: Got endpoints: latency-svc-hw45q [386.209139ms]
Nov 15 15:57:44.636: INFO: Created: latency-svc-mspbz
Nov 15 15:57:44.645: INFO: Got endpoints: latency-svc-mspbz [385.241676ms]
Nov 15 15:57:44.660: INFO: Created: latency-svc-4mppw
Nov 15 15:57:44.669: INFO: Got endpoints: latency-svc-4mppw [388.095854ms]
Nov 15 15:57:44.688: INFO: Created: latency-svc-8n8dn
Nov 15 15:57:44.704: INFO: Got endpoints: latency-svc-8n8dn [398.337265ms]
Nov 15 15:57:44.715: INFO: Created: latency-svc-77xsn
Nov 15 15:57:44.727: INFO: Got endpoints: latency-svc-77xsn [392.800258ms]
Nov 15 15:57:44.741: INFO: Created: latency-svc-psh9k
Nov 15 15:57:44.751: INFO: Got endpoints: latency-svc-psh9k [398.953092ms]
Nov 15 15:57:44.765: INFO: Created: latency-svc-5phwr
Nov 15 15:57:44.774: INFO: Got endpoints: latency-svc-5phwr [398.802508ms]
Nov 15 15:57:45.172: INFO: Created: latency-svc-zq9n2
Nov 15 15:57:45.179: INFO: Created: latency-svc-x55xf
Nov 15 15:57:45.179: INFO: Created: latency-svc-hkq58
Nov 15 15:57:45.179: INFO: Created: latency-svc-xxp7w
Nov 15 15:57:45.180: INFO: Created: latency-svc-b5tf6
Nov 15 15:57:45.180: INFO: Created: latency-svc-jx9lg
Nov 15 15:57:45.180: INFO: Created: latency-svc-kxz8h
Nov 15 15:57:45.180: INFO: Created: latency-svc-2p2fl
Nov 15 15:57:45.180: INFO: Created: latency-svc-nfc2b
Nov 15 15:57:45.182: INFO: Got endpoints: latency-svc-b5tf6 [665.249812ms]
Nov 15 15:57:45.184: INFO: Created: latency-svc-hrrxq
Nov 15 15:57:45.185: INFO: Created: latency-svc-xzv4h
Nov 15 15:57:45.185: INFO: Created: latency-svc-h47dz
Nov 15 15:57:45.185: INFO: Created: latency-svc-6ss9q
Nov 15 15:57:45.185: INFO: Created: latency-svc-v4wdl
Nov 15 15:57:45.186: INFO: Created: latency-svc-962zq
Nov 15 15:57:45.187: INFO: Got endpoints: latency-svc-nfc2b [781.270043ms]
Nov 15 15:57:45.187: INFO: Got endpoints: latency-svc-x55xf [650.218158ms]
Nov 15 15:57:45.187: INFO: Got endpoints: latency-svc-zq9n2 [588.262842ms]
Nov 15 15:57:45.187: INFO: Got endpoints: latency-svc-jx9lg [755.850546ms]
Nov 15 15:57:45.198: INFO: Got endpoints: latency-svc-kxz8h [424.128296ms]
Nov 15 15:57:45.198: INFO: Got endpoints: latency-svc-hkq58 [711.854585ms]
Nov 15 15:57:45.206: INFO: Got endpoints: latency-svc-xxp7w [745.102351ms]
Nov 15 15:57:45.207: INFO: Got endpoints: latency-svc-2p2fl [587.271509ms]
Nov 15 15:57:45.209: INFO: Got endpoints: latency-svc-h47dz [639.270828ms]
Nov 15 15:57:45.215: INFO: Got endpoints: latency-svc-v4wdl [487.220939ms]
Nov 15 15:57:45.215: INFO: Got endpoints: latency-svc-xzv4h [463.770555ms]
Nov 15 15:57:45.215: INFO: Got endpoints: latency-svc-962zq [510.545495ms]
Nov 15 15:57:45.215: INFO: Got endpoints: latency-svc-hrrxq [569.621918ms]
Nov 15 15:57:45.218: INFO: Got endpoints: latency-svc-6ss9q [548.325155ms]
Nov 15 15:57:45.236: INFO: Created: latency-svc-rkw84
Nov 15 15:57:45.255: INFO: Got endpoints: latency-svc-rkw84 [68.366503ms]
Nov 15 15:57:45.271: INFO: Created: latency-svc-bcnt6
Nov 15 15:57:45.280: INFO: Got endpoints: latency-svc-bcnt6 [93.61939ms]
Nov 15 15:57:45.292: INFO: Created: latency-svc-x5shl
Nov 15 15:57:45.303: INFO: Got endpoints: latency-svc-x5shl [115.735209ms]
Nov 15 15:57:45.317: INFO: Created: latency-svc-n7ntl
Nov 15 15:57:45.335: INFO: Got endpoints: latency-svc-n7ntl [147.367676ms]
Nov 15 15:57:45.340: INFO: Created: latency-svc-m7v9r
Nov 15 15:57:45.351: INFO: Got endpoints: latency-svc-m7v9r [169.147303ms]
Nov 15 15:57:45.367: INFO: Created: latency-svc-8k7dq
Nov 15 15:57:45.381: INFO: Got endpoints: latency-svc-8k7dq [182.740645ms]
Nov 15 15:57:45.401: INFO: Created: latency-svc-f2lrx
Nov 15 15:57:45.412: INFO: Got endpoints: latency-svc-f2lrx [213.508103ms]
Nov 15 15:57:45.450: INFO: Created: latency-svc-h7m55
Nov 15 15:57:45.458: INFO: Got endpoints: latency-svc-h7m55 [251.29864ms]
Nov 15 15:57:45.475: INFO: Created: latency-svc-t666r
Nov 15 15:57:45.486: INFO: Got endpoints: latency-svc-t666r [278.72664ms]
Nov 15 15:57:45.510: INFO: Created: latency-svc-qrhzg
Nov 15 15:57:45.513: INFO: Got endpoints: latency-svc-qrhzg [304.222805ms]
Nov 15 15:57:45.531: INFO: Created: latency-svc-wqx8f
Nov 15 15:57:45.539: INFO: Got endpoints: latency-svc-wqx8f [324.652804ms]
Nov 15 15:57:45.573: INFO: Created: latency-svc-q5jcl
Nov 15 15:57:45.580: INFO: Got endpoints: latency-svc-q5jcl [365.194012ms]
Nov 15 15:57:45.593: INFO: Created: latency-svc-ngljx
Nov 15 15:57:45.604: INFO: Got endpoints: latency-svc-ngljx [389.293536ms]
Nov 15 15:57:45.620: INFO: Created: latency-svc-q78m2
Nov 15 15:57:45.632: INFO: Got endpoints: latency-svc-q78m2 [416.76265ms]
Nov 15 15:57:45.645: INFO: Created: latency-svc-z4pgt
Nov 15 15:57:45.655: INFO: Got endpoints: latency-svc-z4pgt [437.514484ms]
Nov 15 15:57:45.678: INFO: Created: latency-svc-8gwvd
Nov 15 15:57:45.688: INFO: Got endpoints: latency-svc-8gwvd [432.166524ms]
Nov 15 15:57:45.702: INFO: Created: latency-svc-2s25w
Nov 15 15:57:45.712: INFO: Got endpoints: latency-svc-2s25w [431.34043ms]
Nov 15 15:57:45.733: INFO: Created: latency-svc-w9gj8
Nov 15 15:57:45.745: INFO: Got endpoints: latency-svc-w9gj8 [441.714027ms]
Nov 15 15:57:45.759: INFO: Created: latency-svc-l65hl
Nov 15 15:57:45.769: INFO: Got endpoints: latency-svc-l65hl [433.706883ms]
Nov 15 15:57:45.785: INFO: Created: latency-svc-rklsj
Nov 15 15:57:45.802: INFO: Got endpoints: latency-svc-rklsj [450.837833ms]
Nov 15 15:57:45.820: INFO: Created: latency-svc-w5f4r
Nov 15 15:57:45.824: INFO: Got endpoints: latency-svc-w5f4r [443.350124ms]
Nov 15 15:57:45.841: INFO: Created: latency-svc-kn56g
Nov 15 15:57:45.852: INFO: Got endpoints: latency-svc-kn56g [439.695862ms]
Nov 15 15:57:45.868: INFO: Created: latency-svc-7tkhq
Nov 15 15:57:45.878: INFO: Got endpoints: latency-svc-7tkhq [420.218563ms]
Nov 15 15:57:45.893: INFO: Created: latency-svc-x6dbs
Nov 15 15:57:45.906: INFO: Got endpoints: latency-svc-x6dbs [420.277486ms]
Nov 15 15:57:45.918: INFO: Created: latency-svc-5f4g8
Nov 15 15:57:45.927: INFO: Got endpoints: latency-svc-5f4g8 [413.205131ms]
Nov 15 15:57:45.943: INFO: Created: latency-svc-82jn7
Nov 15 15:57:46.000: INFO: Got endpoints: latency-svc-82jn7 [460.770486ms]
Nov 15 15:57:46.039: INFO: Created: latency-svc-vlvbs
Nov 15 15:57:46.047: INFO: Got endpoints: latency-svc-vlvbs [466.932073ms]
Nov 15 15:57:46.051: INFO: Created: latency-svc-pmdlh
Nov 15 15:57:46.070: INFO: Got endpoints: latency-svc-pmdlh [466.012703ms]
Nov 15 15:57:46.073: INFO: Created: latency-svc-vwrfm
Nov 15 15:57:46.087: INFO: Got endpoints: latency-svc-vwrfm [455.645265ms]
Nov 15 15:57:46.105: INFO: Created: latency-svc-64klh
Nov 15 15:57:46.116: INFO: Got endpoints: latency-svc-64klh [461.085151ms]
Nov 15 15:57:46.117: INFO: Latencies: [50.048843ms 52.232107ms 53.27268ms 59.948661ms 68.366503ms 75.044911ms 75.987824ms 80.731399ms 81.621973ms 89.223834ms 93.61939ms 99.117884ms 101.196826ms 107.689083ms 108.748091ms 115.735209ms 125.676084ms 129.829179ms 133.531766ms 138.467174ms 147.367676ms 159.630676ms 160.157772ms 164.732792ms 169.147303ms 178.931136ms 182.740645ms 182.841201ms 184.620873ms 189.01029ms 202.582287ms 204.387186ms 213.508103ms 213.652495ms 231.934046ms 233.508211ms 237.427561ms 243.202599ms 250.707537ms 251.29864ms 262.871665ms 272.441665ms 277.502144ms 278.72664ms 287.187728ms 295.653539ms 304.222805ms 305.994606ms 311.953827ms 320.337919ms 324.652804ms 326.999014ms 336.688608ms 342.720206ms 345.600983ms 364.785401ms 365.194012ms 370.908231ms 376.035431ms 379.175847ms 380.492463ms 381.737434ms 382.610365ms 383.954786ms 385.065643ms 385.241676ms 386.209139ms 388.095854ms 388.818497ms 389.293536ms 390.53263ms 392.204506ms 392.800258ms 393.526154ms 394.110174ms 395.303069ms 395.833981ms 398.337265ms 398.802508ms 398.953092ms 401.006939ms 401.130503ms 401.846832ms 402.172353ms 402.195467ms 402.910767ms 402.948264ms 403.03845ms 403.203272ms 403.330562ms 404.057623ms 405.158014ms 405.321023ms 407.792866ms 408.597559ms 409.25214ms 410.447191ms 413.205131ms 413.401354ms 415.242461ms 416.76265ms 417.49733ms 417.703922ms 417.946837ms 419.444398ms 419.624269ms 420.218563ms 420.277486ms 423.984598ms 424.128296ms 427.526993ms 428.266233ms 428.611248ms 428.801857ms 429.116324ms 430.505646ms 431.34043ms 432.166524ms 433.706883ms 434.22016ms 437.514484ms 439.693352ms 439.695862ms 440.391325ms 441.714027ms 443.350124ms 446.687375ms 450.837833ms 452.623006ms 453.234316ms 454.498519ms 454.829656ms 455.645265ms 457.70716ms 460.770486ms 461.085151ms 462.802968ms 463.764719ms 463.770555ms 465.514623ms 466.012703ms 466.932073ms 467.033505ms 470.797388ms 472.835477ms 473.065735ms 476.750663ms 479.127334ms 482.468777ms 487.220939ms 489.349335ms 491.468847ms 494.476535ms 496.102378ms 498.99004ms 510.545495ms 526.803102ms 528.070674ms 548.325155ms 549.512652ms 549.765509ms 552.27276ms 554.191621ms 557.66171ms 559.035558ms 563.500549ms 565.851791ms 567.194325ms 567.202567ms 569.621918ms 575.649051ms 578.05833ms 582.229571ms 587.271509ms 588.262842ms 607.275848ms 607.937056ms 622.166524ms 628.654728ms 630.966412ms 639.270828ms 650.218158ms 653.054547ms 654.184169ms 662.97981ms 665.249812ms 695.25465ms 695.763603ms 711.854585ms 712.185567ms 743.178317ms 745.102351ms 748.796307ms 751.867726ms 755.850546ms 767.360367ms 777.330631ms 781.270043ms 828.39202ms 862.146052ms]
Nov 15 15:57:46.117: INFO: 50 %ile: 416.76265ms
Nov 15 15:57:46.118: INFO: 90 %ile: 639.270828ms
Nov 15 15:57:46.118: INFO: 99 %ile: 828.39202ms
Nov 15 15:57:46.118: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Nov 15 15:57:46.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-5478" for this suite. 11/15/23 15:57:46.143
------------------------------
• [SLOW TEST] [8.017 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:57:38.148
    Nov 15 15:57:38.148: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename svc-latency 11/15/23 15:57:38.151
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:57:38.189
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:57:38.202
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Nov 15 15:57:38.216: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-5478 11/15/23 15:57:38.217
    I1115 15:57:38.230545      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5478, replica count: 1
    I1115 15:57:39.281344      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I1115 15:57:40.282526      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 15 15:57:40.432: INFO: Created: latency-svc-5hnxr
    Nov 15 15:57:40.441: INFO: Got endpoints: latency-svc-5hnxr [58.140798ms]
    Nov 15 15:57:40.482: INFO: Created: latency-svc-4b9t4
    Nov 15 15:57:40.495: INFO: Got endpoints: latency-svc-4b9t4 [52.232107ms]
    Nov 15 15:57:40.509: INFO: Created: latency-svc-cdwpx
    Nov 15 15:57:40.518: INFO: Got endpoints: latency-svc-cdwpx [75.044911ms]
    Nov 15 15:57:40.534: INFO: Created: latency-svc-jrkdr
    Nov 15 15:57:40.542: INFO: Got endpoints: latency-svc-jrkdr [99.117884ms]
    Nov 15 15:57:40.563: INFO: Created: latency-svc-jf84n
    Nov 15 15:57:40.573: INFO: Got endpoints: latency-svc-jf84n [129.829179ms]
    Nov 15 15:57:40.591: INFO: Created: latency-svc-8v5s5
    Nov 15 15:57:40.603: INFO: Got endpoints: latency-svc-8v5s5 [160.157772ms]
    Nov 15 15:57:40.620: INFO: Created: latency-svc-h9sln
    Nov 15 15:57:40.627: INFO: Got endpoints: latency-svc-h9sln [184.620873ms]
    Nov 15 15:57:40.648: INFO: Created: latency-svc-26zdd
    Nov 15 15:57:40.657: INFO: Got endpoints: latency-svc-26zdd [213.652495ms]
    Nov 15 15:57:40.670: INFO: Created: latency-svc-b92dv
    Nov 15 15:57:40.681: INFO: Got endpoints: latency-svc-b92dv [237.427561ms]
    Nov 15 15:57:40.695: INFO: Created: latency-svc-fn5f7
    Nov 15 15:57:40.706: INFO: Got endpoints: latency-svc-fn5f7 [262.871665ms]
    Nov 15 15:57:40.720: INFO: Created: latency-svc-gjjl6
    Nov 15 15:57:40.731: INFO: Got endpoints: latency-svc-gjjl6 [287.187728ms]
    Nov 15 15:57:40.754: INFO: Created: latency-svc-lhbf5
    Nov 15 15:57:40.763: INFO: Got endpoints: latency-svc-lhbf5 [320.337919ms]
    Nov 15 15:57:40.773: INFO: Created: latency-svc-s4pfd
    Nov 15 15:57:40.780: INFO: Got endpoints: latency-svc-s4pfd [336.688608ms]
    Nov 15 15:57:40.798: INFO: Created: latency-svc-9kksd
    Nov 15 15:57:40.808: INFO: Got endpoints: latency-svc-9kksd [364.785401ms]
    Nov 15 15:57:40.823: INFO: Created: latency-svc-sfpjm
    Nov 15 15:57:40.838: INFO: Got endpoints: latency-svc-sfpjm [394.110174ms]
    Nov 15 15:57:40.860: INFO: Created: latency-svc-ncwtv
    Nov 15 15:57:40.873: INFO: Got endpoints: latency-svc-ncwtv [429.116324ms]
    Nov 15 15:57:40.885: INFO: Created: latency-svc-4vh2l
    Nov 15 15:57:40.896: INFO: Got endpoints: latency-svc-4vh2l [401.130503ms]
    Nov 15 15:57:40.912: INFO: Created: latency-svc-q4s7w
    Nov 15 15:57:40.923: INFO: Got endpoints: latency-svc-q4s7w [405.321023ms]
    Nov 15 15:57:40.939: INFO: Created: latency-svc-pcrjn
    Nov 15 15:57:40.946: INFO: Got endpoints: latency-svc-pcrjn [404.057623ms]
    Nov 15 15:57:40.972: INFO: Created: latency-svc-jjhb2
    Nov 15 15:57:40.978: INFO: Got endpoints: latency-svc-jjhb2 [405.158014ms]
    Nov 15 15:57:40.996: INFO: Created: latency-svc-cdr45
    Nov 15 15:57:41.004: INFO: Got endpoints: latency-svc-cdr45 [401.006939ms]
    Nov 15 15:57:41.029: INFO: Created: latency-svc-dvmcc
    Nov 15 15:57:41.037: INFO: Got endpoints: latency-svc-dvmcc [409.25214ms]
    Nov 15 15:57:41.051: INFO: Created: latency-svc-fnj2t
    Nov 15 15:57:41.059: INFO: Got endpoints: latency-svc-fnj2t [402.172353ms]
    Nov 15 15:57:41.080: INFO: Created: latency-svc-6cjr5
    Nov 15 15:57:41.088: INFO: Got endpoints: latency-svc-6cjr5 [407.792866ms]
    Nov 15 15:57:41.106: INFO: Created: latency-svc-6nzh8
    Nov 15 15:57:41.117: INFO: Got endpoints: latency-svc-6nzh8 [410.447191ms]
    Nov 15 15:57:41.161: INFO: Created: latency-svc-84jl8
    Nov 15 15:57:41.171: INFO: Got endpoints: latency-svc-84jl8 [440.391325ms]
    Nov 15 15:57:41.204: INFO: Created: latency-svc-dqctr
    Nov 15 15:57:41.210: INFO: Got endpoints: latency-svc-dqctr [446.687375ms]
    Nov 15 15:57:41.227: INFO: Created: latency-svc-jh9zt
    Nov 15 15:57:41.238: INFO: Got endpoints: latency-svc-jh9zt [457.70716ms]
    Nov 15 15:57:41.270: INFO: Created: latency-svc-lpz7k
    Nov 15 15:57:41.281: INFO: Got endpoints: latency-svc-lpz7k [473.065735ms]
    Nov 15 15:57:41.284: INFO: Created: latency-svc-92qlk
    Nov 15 15:57:41.293: INFO: Got endpoints: latency-svc-92qlk [454.829656ms]
    Nov 15 15:57:41.314: INFO: Created: latency-svc-slqph
    Nov 15 15:57:41.326: INFO: Got endpoints: latency-svc-slqph [453.234316ms]
    Nov 15 15:57:41.337: INFO: Created: latency-svc-626nf
    Nov 15 15:57:41.349: INFO: Got endpoints: latency-svc-626nf [452.623006ms]
    Nov 15 15:57:41.371: INFO: Created: latency-svc-kd6dz
    Nov 15 15:57:41.387: INFO: Got endpoints: latency-svc-kd6dz [463.764719ms]
    Nov 15 15:57:41.404: INFO: Created: latency-svc-rkbr2
    Nov 15 15:57:41.412: INFO: Got endpoints: latency-svc-rkbr2 [465.514623ms]
    Nov 15 15:57:41.799: INFO: Created: latency-svc-496j6
    Nov 15 15:57:41.800: INFO: Created: latency-svc-wqsqs
    Nov 15 15:57:41.800: INFO: Created: latency-svc-r6m9b
    Nov 15 15:57:41.800: INFO: Created: latency-svc-frfzt
    Nov 15 15:57:41.805: INFO: Created: latency-svc-7sk6z
    Nov 15 15:57:41.810: INFO: Created: latency-svc-nn4bp
    Nov 15 15:57:41.810: INFO: Created: latency-svc-d8vm6
    Nov 15 15:57:41.810: INFO: Created: latency-svc-pkbzk
    Nov 15 15:57:41.811: INFO: Created: latency-svc-jjhfn
    Nov 15 15:57:41.811: INFO: Created: latency-svc-zhhr6
    Nov 15 15:57:41.811: INFO: Created: latency-svc-m69ln
    Nov 15 15:57:41.812: INFO: Created: latency-svc-7hcjp
    Nov 15 15:57:41.811: INFO: Got endpoints: latency-svc-wqsqs [751.867726ms]
    Nov 15 15:57:41.812: INFO: Created: latency-svc-r869p
    Nov 15 15:57:41.812: INFO: Created: latency-svc-zfksm
    Nov 15 15:57:41.812: INFO: Got endpoints: latency-svc-zhhr6 [695.25465ms]
    Nov 15 15:57:41.812: INFO: Created: latency-svc-v28g2
    Nov 15 15:57:41.814: INFO: Got endpoints: latency-svc-r6m9b [777.330631ms]
    Nov 15 15:57:41.816: INFO: Got endpoints: latency-svc-496j6 [403.03845ms]
    Nov 15 15:57:41.816: INFO: Got endpoints: latency-svc-jjhfn [428.611248ms]
    Nov 15 15:57:41.820: INFO: Got endpoints: latency-svc-v28g2 [582.229571ms]
    Nov 15 15:57:41.825: INFO: Got endpoints: latency-svc-7sk6z [498.99004ms]
    Nov 15 15:57:41.826: INFO: Got endpoints: latency-svc-frfzt [654.184169ms]
    Nov 15 15:57:41.831: INFO: Got endpoints: latency-svc-zfksm [549.512652ms]
    Nov 15 15:57:41.832: INFO: Got endpoints: latency-svc-m69ln [743.178317ms]
    Nov 15 15:57:41.833: INFO: Got endpoints: latency-svc-pkbzk [828.39202ms]
    Nov 15 15:57:41.841: INFO: Got endpoints: latency-svc-r869p [862.146052ms]
    Nov 15 15:57:41.841: INFO: Got endpoints: latency-svc-7hcjp [630.966412ms]
    Nov 15 15:57:41.843: INFO: Got endpoints: latency-svc-d8vm6 [549.765509ms]
    Nov 15 15:57:41.844: INFO: Got endpoints: latency-svc-nn4bp [494.476535ms]
    Nov 15 15:57:41.863: INFO: Created: latency-svc-zngzm
    Nov 15 15:57:41.872: INFO: Got endpoints: latency-svc-zngzm [59.948661ms]
    Nov 15 15:57:41.898: INFO: Created: latency-svc-sfbz2
    Nov 15 15:57:41.902: INFO: Got endpoints: latency-svc-sfbz2 [89.223834ms]
    Nov 15 15:57:41.914: INFO: Created: latency-svc-6b69f
    Nov 15 15:57:41.923: INFO: Got endpoints: latency-svc-6b69f [108.748091ms]
    Nov 15 15:57:41.941: INFO: Created: latency-svc-8ncnq
    Nov 15 15:57:41.949: INFO: Got endpoints: latency-svc-8ncnq [133.531766ms]
    Nov 15 15:57:41.971: INFO: Created: latency-svc-hg6n6
    Nov 15 15:57:41.981: INFO: Got endpoints: latency-svc-hg6n6 [164.732792ms]
    Nov 15 15:57:41.990: INFO: Created: latency-svc-hlh7d
    Nov 15 15:57:42.000: INFO: Got endpoints: latency-svc-hlh7d [178.931136ms]
    Nov 15 15:57:42.017: INFO: Created: latency-svc-bbq7g
    Nov 15 15:57:42.028: INFO: Got endpoints: latency-svc-bbq7g [202.582287ms]
    Nov 15 15:57:42.041: INFO: Created: latency-svc-pjb9d
    Nov 15 15:57:42.058: INFO: Got endpoints: latency-svc-pjb9d [231.934046ms]
    Nov 15 15:57:42.066: INFO: Created: latency-svc-f4lbd
    Nov 15 15:57:42.074: INFO: Got endpoints: latency-svc-f4lbd [243.202599ms]
    Nov 15 15:57:42.093: INFO: Created: latency-svc-qvnh7
    Nov 15 15:57:42.104: INFO: Got endpoints: latency-svc-qvnh7 [272.441665ms]
    Nov 15 15:57:42.117: INFO: Created: latency-svc-s8dkv
    Nov 15 15:57:42.129: INFO: Got endpoints: latency-svc-s8dkv [295.653539ms]
    Nov 15 15:57:42.143: INFO: Created: latency-svc-l7ds9
    Nov 15 15:57:42.153: INFO: Got endpoints: latency-svc-l7ds9 [311.953827ms]
    Nov 15 15:57:42.176: INFO: Created: latency-svc-dvkrj
    Nov 15 15:57:42.186: INFO: Got endpoints: latency-svc-dvkrj [345.600983ms]
    Nov 15 15:57:42.203: INFO: Created: latency-svc-fp9l7
    Nov 15 15:57:42.213: INFO: Got endpoints: latency-svc-fp9l7 [370.908231ms]
    Nov 15 15:57:42.231: INFO: Created: latency-svc-8w95r
    Nov 15 15:57:42.240: INFO: Got endpoints: latency-svc-8w95r [395.833981ms]
    Nov 15 15:57:42.268: INFO: Created: latency-svc-vnv6q
    Nov 15 15:57:42.275: INFO: Got endpoints: latency-svc-vnv6q [402.948264ms]
    Nov 15 15:57:42.294: INFO: Created: latency-svc-qg8l2
    Nov 15 15:57:42.305: INFO: Got endpoints: latency-svc-qg8l2 [402.910767ms]
    Nov 15 15:57:42.317: INFO: Created: latency-svc-9b8dc
    Nov 15 15:57:42.326: INFO: Got endpoints: latency-svc-9b8dc [403.203272ms]
    Nov 15 15:57:42.347: INFO: Created: latency-svc-pxwwd
    Nov 15 15:57:42.358: INFO: Got endpoints: latency-svc-pxwwd [408.597559ms]
    Nov 15 15:57:42.373: INFO: Created: latency-svc-m2vsm
    Nov 15 15:57:42.383: INFO: Got endpoints: latency-svc-m2vsm [402.195467ms]
    Nov 15 15:57:42.745: INFO: Created: latency-svc-fqrlj
    Nov 15 15:57:42.747: INFO: Created: latency-svc-jw85b
    Nov 15 15:57:42.759: INFO: Created: latency-svc-hsq99
    Nov 15 15:57:42.760: INFO: Created: latency-svc-kxsmj
    Nov 15 15:57:42.760: INFO: Created: latency-svc-tfgvr
    Nov 15 15:57:42.760: INFO: Created: latency-svc-rw6ql
    Nov 15 15:57:42.760: INFO: Created: latency-svc-xbzgw
    Nov 15 15:57:42.760: INFO: Created: latency-svc-nj4s2
    Nov 15 15:57:42.761: INFO: Created: latency-svc-7jw6p
    Nov 15 15:57:42.761: INFO: Created: latency-svc-kwlcm
    Nov 15 15:57:42.761: INFO: Created: latency-svc-flw8p
    Nov 15 15:57:42.762: INFO: Created: latency-svc-ss27h
    Nov 15 15:57:42.762: INFO: Created: latency-svc-fvxjg
    Nov 15 15:57:42.763: INFO: Created: latency-svc-8wr72
    Nov 15 15:57:42.764: INFO: Got endpoints: latency-svc-jw85b [578.05833ms]
    Nov 15 15:57:42.764: INFO: Got endpoints: latency-svc-fqrlj [489.349335ms]
    Nov 15 15:57:42.766: INFO: Created: latency-svc-tkq2d
    Nov 15 15:57:42.767: INFO: Got endpoints: latency-svc-kxsmj [767.360367ms]
    Nov 15 15:57:42.767: INFO: Got endpoints: latency-svc-tfgvr [383.954786ms]
    Nov 15 15:57:42.767: INFO: Got endpoints: latency-svc-flw8p [662.97981ms]
    Nov 15 15:57:42.768: INFO: Got endpoints: latency-svc-ss27h [526.803102ms]
    Nov 15 15:57:42.768: INFO: Got endpoints: latency-svc-xbzgw [554.191621ms]
    Nov 15 15:57:42.770: INFO: Got endpoints: latency-svc-hsq99 [712.185567ms]
    Nov 15 15:57:42.770: INFO: Got endpoints: latency-svc-kwlcm [695.763603ms]
    Nov 15 15:57:42.772: INFO: Got endpoints: latency-svc-rw6ql [413.401354ms]
    Nov 15 15:57:42.777: INFO: Got endpoints: latency-svc-nj4s2 [748.796307ms]
    Nov 15 15:57:42.781: INFO: Got endpoints: latency-svc-7jw6p [454.498519ms]
    Nov 15 15:57:42.782: INFO: Got endpoints: latency-svc-fvxjg [653.054547ms]
    Nov 15 15:57:42.782: INFO: Got endpoints: latency-svc-8wr72 [476.750663ms]
    Nov 15 15:57:42.782: INFO: Got endpoints: latency-svc-tkq2d [628.654728ms]
    Nov 15 15:57:42.806: INFO: Created: latency-svc-jvsw6
    Nov 15 15:57:42.814: INFO: Got endpoints: latency-svc-jvsw6 [50.048843ms]
    Nov 15 15:57:42.832: INFO: Created: latency-svc-gmpm9
    Nov 15 15:57:42.843: INFO: Got endpoints: latency-svc-gmpm9 [75.987824ms]
    Nov 15 15:57:42.860: INFO: Created: latency-svc-6m5pz
    Nov 15 15:57:42.869: INFO: Got endpoints: latency-svc-6m5pz [101.196826ms]
    Nov 15 15:57:42.881: INFO: Created: latency-svc-gsl6m
    Nov 15 15:57:42.893: INFO: Got endpoints: latency-svc-gsl6m [125.676084ms]
    Nov 15 15:57:42.918: INFO: Created: latency-svc-djfqq
    Nov 15 15:57:42.927: INFO: Got endpoints: latency-svc-djfqq [159.630676ms]
    Nov 15 15:57:42.943: INFO: Created: latency-svc-zqhln
    Nov 15 15:57:42.954: INFO: Got endpoints: latency-svc-zqhln [189.01029ms]
    Nov 15 15:57:43.317: INFO: Created: latency-svc-8kp6k
    Nov 15 15:57:43.317: INFO: Created: latency-svc-66vlp
    Nov 15 15:57:43.326: INFO: Created: latency-svc-h6vpb
    Nov 15 15:57:43.326: INFO: Created: latency-svc-xkwfc
    Nov 15 15:57:43.331: INFO: Created: latency-svc-zmlr5
    Nov 15 15:57:43.332: INFO: Created: latency-svc-8rqk2
    Nov 15 15:57:43.332: INFO: Created: latency-svc-ht8cz
    Nov 15 15:57:43.332: INFO: Created: latency-svc-llzmx
    Nov 15 15:57:43.332: INFO: Created: latency-svc-8pmdv
    Nov 15 15:57:43.332: INFO: Created: latency-svc-p5m8b
    Nov 15 15:57:43.333: INFO: Created: latency-svc-bngj9
    Nov 15 15:57:43.333: INFO: Created: latency-svc-4tx76
    Nov 15 15:57:43.333: INFO: Created: latency-svc-jslxp
    Nov 15 15:57:43.333: INFO: Created: latency-svc-9mlgt
    Nov 15 15:57:43.333: INFO: Created: latency-svc-l87v4
    Nov 15 15:57:43.336: INFO: Got endpoints: latency-svc-8rqk2 [381.737434ms]
    Nov 15 15:57:43.336: INFO: Got endpoints: latency-svc-8kp6k [559.035558ms]
    Nov 15 15:57:43.337: INFO: Got endpoints: latency-svc-4tx76 [567.202567ms]
    Nov 15 15:57:43.337: INFO: Got endpoints: latency-svc-h6vpb [567.194325ms]
    Nov 15 15:57:43.338: INFO: Got endpoints: latency-svc-66vlp [565.851791ms]
    Nov 15 15:57:43.339: INFO: Got endpoints: latency-svc-zmlr5 [557.66171ms]
    Nov 15 15:57:43.345: INFO: Got endpoints: latency-svc-ht8cz [417.49733ms]
    Nov 15 15:57:43.346: INFO: Got endpoints: latency-svc-p5m8b [563.500549ms]
    Nov 15 15:57:43.390: INFO: Got endpoints: latency-svc-8pmdv [607.275848ms]
    Nov 15 15:57:43.390: INFO: Got endpoints: latency-svc-jslxp [496.102378ms]
    Nov 15 15:57:43.390: INFO: Got endpoints: latency-svc-bngj9 [622.166524ms]
    Nov 15 15:57:43.390: INFO: Got endpoints: latency-svc-llzmx [575.649051ms]
    Nov 15 15:57:43.390: INFO: Got endpoints: latency-svc-9mlgt [607.937056ms]
    Nov 15 15:57:43.395: INFO: Got endpoints: latency-svc-l87v4 [552.27276ms]
    Nov 15 15:57:43.397: INFO: Got endpoints: latency-svc-xkwfc [528.070674ms]
    Nov 15 15:57:43.410: INFO: Created: latency-svc-ctzdz
    Nov 15 15:57:43.417: INFO: Got endpoints: latency-svc-ctzdz [81.621973ms]
    Nov 15 15:57:43.788: INFO: Created: latency-svc-27klt
    Nov 15 15:57:43.793: INFO: Created: latency-svc-9m8js
    Nov 15 15:57:43.800: INFO: Created: latency-svc-2t46t
    Nov 15 15:57:43.802: INFO: Created: latency-svc-czrsg
    Nov 15 15:57:43.803: INFO: Created: latency-svc-9wbds
    Nov 15 15:57:43.803: INFO: Created: latency-svc-4h67q
    Nov 15 15:57:43.803: INFO: Created: latency-svc-jn7lf
    Nov 15 15:57:43.803: INFO: Created: latency-svc-rbf5r
    Nov 15 15:57:43.804: INFO: Created: latency-svc-cc99d
    Nov 15 15:57:43.804: INFO: Created: latency-svc-rrrqc
    Nov 15 15:57:43.804: INFO: Created: latency-svc-r2cqw
    Nov 15 15:57:43.803: INFO: Created: latency-svc-t4qsg
    Nov 15 15:57:43.804: INFO: Created: latency-svc-8kb27
    Nov 15 15:57:43.804: INFO: Created: latency-svc-l22jc
    Nov 15 15:57:43.805: INFO: Created: latency-svc-6h795
    Nov 15 15:57:43.808: INFO: Got endpoints: latency-svc-27klt [470.797388ms]
    Nov 15 15:57:43.809: INFO: Got endpoints: latency-svc-l22jc [462.802968ms]
    Nov 15 15:57:43.809: INFO: Got endpoints: latency-svc-2t46t [472.835477ms]
    Nov 15 15:57:43.810: INFO: Got endpoints: latency-svc-9wbds [419.624269ms]
    Nov 15 15:57:43.812: INFO: Got endpoints: latency-svc-9m8js [467.033505ms]
    Nov 15 15:57:43.818: INFO: Got endpoints: latency-svc-6h795 [479.127334ms]
    Nov 15 15:57:43.818: INFO: Got endpoints: latency-svc-czrsg [428.266233ms]
    Nov 15 15:57:43.818: INFO: Got endpoints: latency-svc-jn7lf [428.801857ms]
    Nov 15 15:57:43.820: INFO: Got endpoints: latency-svc-t4qsg [482.468777ms]
    Nov 15 15:57:43.820: INFO: Got endpoints: latency-svc-cc99d [430.505646ms]
    Nov 15 15:57:43.824: INFO: Got endpoints: latency-svc-r2cqw [427.526993ms]
    Nov 15 15:57:43.829: INFO: Got endpoints: latency-svc-8kb27 [491.468847ms]
    Nov 15 15:57:43.830: INFO: Got endpoints: latency-svc-4h67q [434.22016ms]
    Nov 15 15:57:43.830: INFO: Got endpoints: latency-svc-rbf5r [439.693352ms]
    Nov 15 15:57:43.835: INFO: Got endpoints: latency-svc-rrrqc [417.946837ms]
    Nov 15 15:57:43.852: INFO: Created: latency-svc-hsdxw
    Nov 15 15:57:43.861: INFO: Got endpoints: latency-svc-hsdxw [53.27268ms]
    Nov 15 15:57:43.880: INFO: Created: latency-svc-qfl7x
    Nov 15 15:57:43.890: INFO: Got endpoints: latency-svc-qfl7x [80.731399ms]
    Nov 15 15:57:43.907: INFO: Created: latency-svc-mndq4
    Nov 15 15:57:43.917: INFO: Got endpoints: latency-svc-mndq4 [107.689083ms]
    Nov 15 15:57:43.937: INFO: Created: latency-svc-v2dhn
    Nov 15 15:57:43.948: INFO: Got endpoints: latency-svc-v2dhn [138.467174ms]
    Nov 15 15:57:43.975: INFO: Created: latency-svc-r9fm8
    Nov 15 15:57:43.994: INFO: Got endpoints: latency-svc-r9fm8 [182.841201ms]
    Nov 15 15:57:44.012: INFO: Created: latency-svc-rds57
    Nov 15 15:57:44.022: INFO: Got endpoints: latency-svc-rds57 [204.387186ms]
    Nov 15 15:57:44.042: INFO: Created: latency-svc-d6sn6
    Nov 15 15:57:44.052: INFO: Got endpoints: latency-svc-d6sn6 [233.508211ms]
    Nov 15 15:57:44.060: INFO: Created: latency-svc-6s8t2
    Nov 15 15:57:44.069: INFO: Got endpoints: latency-svc-6s8t2 [250.707537ms]
    Nov 15 15:57:44.087: INFO: Created: latency-svc-nnwtb
    Nov 15 15:57:44.097: INFO: Got endpoints: latency-svc-nnwtb [277.502144ms]
    Nov 15 15:57:44.116: INFO: Created: latency-svc-f4lxc
    Nov 15 15:57:44.126: INFO: Got endpoints: latency-svc-f4lxc [305.994606ms]
    Nov 15 15:57:44.146: INFO: Created: latency-svc-mxwx9
    Nov 15 15:57:44.152: INFO: Got endpoints: latency-svc-mxwx9 [326.999014ms]
    Nov 15 15:57:44.166: INFO: Created: latency-svc-8tbdf
    Nov 15 15:57:44.173: INFO: Got endpoints: latency-svc-8tbdf [342.720206ms]
    Nov 15 15:57:44.196: INFO: Created: latency-svc-khtrr
    Nov 15 15:57:44.205: INFO: Got endpoints: latency-svc-khtrr [376.035431ms]
    Nov 15 15:57:44.220: INFO: Created: latency-svc-7n49q
    Nov 15 15:57:44.232: INFO: Got endpoints: latency-svc-7n49q [401.846832ms]
    Nov 15 15:57:44.248: INFO: Created: latency-svc-kbtzh
    Nov 15 15:57:44.259: INFO: Got endpoints: latency-svc-kbtzh [423.984598ms]
    Nov 15 15:57:44.271: INFO: Created: latency-svc-r9ndf
    Nov 15 15:57:44.281: INFO: Got endpoints: latency-svc-r9ndf [419.444398ms]
    Nov 15 15:57:44.302: INFO: Created: latency-svc-tgk6d
    Nov 15 15:57:44.306: INFO: Got endpoints: latency-svc-tgk6d [415.242461ms]
    Nov 15 15:57:44.323: INFO: Created: latency-svc-q2fqz
    Nov 15 15:57:44.335: INFO: Got endpoints: latency-svc-q2fqz [417.703922ms]
    Nov 15 15:57:44.342: INFO: Created: latency-svc-nfd4c
    Nov 15 15:57:44.352: INFO: Got endpoints: latency-svc-nfd4c [403.330562ms]
    Nov 15 15:57:44.370: INFO: Created: latency-svc-cwbhz
    Nov 15 15:57:44.375: INFO: Got endpoints: latency-svc-cwbhz [380.492463ms]
    Nov 15 15:57:44.394: INFO: Created: latency-svc-nm548
    Nov 15 15:57:44.405: INFO: Got endpoints: latency-svc-nm548 [382.610365ms]
    Nov 15 15:57:44.423: INFO: Created: latency-svc-5zs8m
    Nov 15 15:57:44.431: INFO: Got endpoints: latency-svc-5zs8m [379.175847ms]
    Nov 15 15:57:44.450: INFO: Created: latency-svc-g5rk6
    Nov 15 15:57:44.461: INFO: Got endpoints: latency-svc-g5rk6 [392.204506ms]
    Nov 15 15:57:44.476: INFO: Created: latency-svc-2jlr5
    Nov 15 15:57:44.486: INFO: Got endpoints: latency-svc-2jlr5 [388.818497ms]
    Nov 15 15:57:44.498: INFO: Created: latency-svc-vnrrq
    Nov 15 15:57:44.517: INFO: Got endpoints: latency-svc-vnrrq [390.53263ms]
    Nov 15 15:57:44.525: INFO: Created: latency-svc-szfwh
    Nov 15 15:57:44.537: INFO: Got endpoints: latency-svc-szfwh [385.065643ms]
    Nov 15 15:57:44.561: INFO: Created: latency-svc-c572s
    Nov 15 15:57:44.568: INFO: Got endpoints: latency-svc-c572s [395.303069ms]
    Nov 15 15:57:44.587: INFO: Created: latency-svc-9s44f
    Nov 15 15:57:44.599: INFO: Got endpoints: latency-svc-9s44f [393.526154ms]
    Nov 15 15:57:44.611: INFO: Created: latency-svc-hw45q
    Nov 15 15:57:44.620: INFO: Got endpoints: latency-svc-hw45q [386.209139ms]
    Nov 15 15:57:44.636: INFO: Created: latency-svc-mspbz
    Nov 15 15:57:44.645: INFO: Got endpoints: latency-svc-mspbz [385.241676ms]
    Nov 15 15:57:44.660: INFO: Created: latency-svc-4mppw
    Nov 15 15:57:44.669: INFO: Got endpoints: latency-svc-4mppw [388.095854ms]
    Nov 15 15:57:44.688: INFO: Created: latency-svc-8n8dn
    Nov 15 15:57:44.704: INFO: Got endpoints: latency-svc-8n8dn [398.337265ms]
    Nov 15 15:57:44.715: INFO: Created: latency-svc-77xsn
    Nov 15 15:57:44.727: INFO: Got endpoints: latency-svc-77xsn [392.800258ms]
    Nov 15 15:57:44.741: INFO: Created: latency-svc-psh9k
    Nov 15 15:57:44.751: INFO: Got endpoints: latency-svc-psh9k [398.953092ms]
    Nov 15 15:57:44.765: INFO: Created: latency-svc-5phwr
    Nov 15 15:57:44.774: INFO: Got endpoints: latency-svc-5phwr [398.802508ms]
    Nov 15 15:57:45.172: INFO: Created: latency-svc-zq9n2
    Nov 15 15:57:45.179: INFO: Created: latency-svc-x55xf
    Nov 15 15:57:45.179: INFO: Created: latency-svc-hkq58
    Nov 15 15:57:45.179: INFO: Created: latency-svc-xxp7w
    Nov 15 15:57:45.180: INFO: Created: latency-svc-b5tf6
    Nov 15 15:57:45.180: INFO: Created: latency-svc-jx9lg
    Nov 15 15:57:45.180: INFO: Created: latency-svc-kxz8h
    Nov 15 15:57:45.180: INFO: Created: latency-svc-2p2fl
    Nov 15 15:57:45.180: INFO: Created: latency-svc-nfc2b
    Nov 15 15:57:45.182: INFO: Got endpoints: latency-svc-b5tf6 [665.249812ms]
    Nov 15 15:57:45.184: INFO: Created: latency-svc-hrrxq
    Nov 15 15:57:45.185: INFO: Created: latency-svc-xzv4h
    Nov 15 15:57:45.185: INFO: Created: latency-svc-h47dz
    Nov 15 15:57:45.185: INFO: Created: latency-svc-6ss9q
    Nov 15 15:57:45.185: INFO: Created: latency-svc-v4wdl
    Nov 15 15:57:45.186: INFO: Created: latency-svc-962zq
    Nov 15 15:57:45.187: INFO: Got endpoints: latency-svc-nfc2b [781.270043ms]
    Nov 15 15:57:45.187: INFO: Got endpoints: latency-svc-x55xf [650.218158ms]
    Nov 15 15:57:45.187: INFO: Got endpoints: latency-svc-zq9n2 [588.262842ms]
    Nov 15 15:57:45.187: INFO: Got endpoints: latency-svc-jx9lg [755.850546ms]
    Nov 15 15:57:45.198: INFO: Got endpoints: latency-svc-kxz8h [424.128296ms]
    Nov 15 15:57:45.198: INFO: Got endpoints: latency-svc-hkq58 [711.854585ms]
    Nov 15 15:57:45.206: INFO: Got endpoints: latency-svc-xxp7w [745.102351ms]
    Nov 15 15:57:45.207: INFO: Got endpoints: latency-svc-2p2fl [587.271509ms]
    Nov 15 15:57:45.209: INFO: Got endpoints: latency-svc-h47dz [639.270828ms]
    Nov 15 15:57:45.215: INFO: Got endpoints: latency-svc-v4wdl [487.220939ms]
    Nov 15 15:57:45.215: INFO: Got endpoints: latency-svc-xzv4h [463.770555ms]
    Nov 15 15:57:45.215: INFO: Got endpoints: latency-svc-962zq [510.545495ms]
    Nov 15 15:57:45.215: INFO: Got endpoints: latency-svc-hrrxq [569.621918ms]
    Nov 15 15:57:45.218: INFO: Got endpoints: latency-svc-6ss9q [548.325155ms]
    Nov 15 15:57:45.236: INFO: Created: latency-svc-rkw84
    Nov 15 15:57:45.255: INFO: Got endpoints: latency-svc-rkw84 [68.366503ms]
    Nov 15 15:57:45.271: INFO: Created: latency-svc-bcnt6
    Nov 15 15:57:45.280: INFO: Got endpoints: latency-svc-bcnt6 [93.61939ms]
    Nov 15 15:57:45.292: INFO: Created: latency-svc-x5shl
    Nov 15 15:57:45.303: INFO: Got endpoints: latency-svc-x5shl [115.735209ms]
    Nov 15 15:57:45.317: INFO: Created: latency-svc-n7ntl
    Nov 15 15:57:45.335: INFO: Got endpoints: latency-svc-n7ntl [147.367676ms]
    Nov 15 15:57:45.340: INFO: Created: latency-svc-m7v9r
    Nov 15 15:57:45.351: INFO: Got endpoints: latency-svc-m7v9r [169.147303ms]
    Nov 15 15:57:45.367: INFO: Created: latency-svc-8k7dq
    Nov 15 15:57:45.381: INFO: Got endpoints: latency-svc-8k7dq [182.740645ms]
    Nov 15 15:57:45.401: INFO: Created: latency-svc-f2lrx
    Nov 15 15:57:45.412: INFO: Got endpoints: latency-svc-f2lrx [213.508103ms]
    Nov 15 15:57:45.450: INFO: Created: latency-svc-h7m55
    Nov 15 15:57:45.458: INFO: Got endpoints: latency-svc-h7m55 [251.29864ms]
    Nov 15 15:57:45.475: INFO: Created: latency-svc-t666r
    Nov 15 15:57:45.486: INFO: Got endpoints: latency-svc-t666r [278.72664ms]
    Nov 15 15:57:45.510: INFO: Created: latency-svc-qrhzg
    Nov 15 15:57:45.513: INFO: Got endpoints: latency-svc-qrhzg [304.222805ms]
    Nov 15 15:57:45.531: INFO: Created: latency-svc-wqx8f
    Nov 15 15:57:45.539: INFO: Got endpoints: latency-svc-wqx8f [324.652804ms]
    Nov 15 15:57:45.573: INFO: Created: latency-svc-q5jcl
    Nov 15 15:57:45.580: INFO: Got endpoints: latency-svc-q5jcl [365.194012ms]
    Nov 15 15:57:45.593: INFO: Created: latency-svc-ngljx
    Nov 15 15:57:45.604: INFO: Got endpoints: latency-svc-ngljx [389.293536ms]
    Nov 15 15:57:45.620: INFO: Created: latency-svc-q78m2
    Nov 15 15:57:45.632: INFO: Got endpoints: latency-svc-q78m2 [416.76265ms]
    Nov 15 15:57:45.645: INFO: Created: latency-svc-z4pgt
    Nov 15 15:57:45.655: INFO: Got endpoints: latency-svc-z4pgt [437.514484ms]
    Nov 15 15:57:45.678: INFO: Created: latency-svc-8gwvd
    Nov 15 15:57:45.688: INFO: Got endpoints: latency-svc-8gwvd [432.166524ms]
    Nov 15 15:57:45.702: INFO: Created: latency-svc-2s25w
    Nov 15 15:57:45.712: INFO: Got endpoints: latency-svc-2s25w [431.34043ms]
    Nov 15 15:57:45.733: INFO: Created: latency-svc-w9gj8
    Nov 15 15:57:45.745: INFO: Got endpoints: latency-svc-w9gj8 [441.714027ms]
    Nov 15 15:57:45.759: INFO: Created: latency-svc-l65hl
    Nov 15 15:57:45.769: INFO: Got endpoints: latency-svc-l65hl [433.706883ms]
    Nov 15 15:57:45.785: INFO: Created: latency-svc-rklsj
    Nov 15 15:57:45.802: INFO: Got endpoints: latency-svc-rklsj [450.837833ms]
    Nov 15 15:57:45.820: INFO: Created: latency-svc-w5f4r
    Nov 15 15:57:45.824: INFO: Got endpoints: latency-svc-w5f4r [443.350124ms]
    Nov 15 15:57:45.841: INFO: Created: latency-svc-kn56g
    Nov 15 15:57:45.852: INFO: Got endpoints: latency-svc-kn56g [439.695862ms]
    Nov 15 15:57:45.868: INFO: Created: latency-svc-7tkhq
    Nov 15 15:57:45.878: INFO: Got endpoints: latency-svc-7tkhq [420.218563ms]
    Nov 15 15:57:45.893: INFO: Created: latency-svc-x6dbs
    Nov 15 15:57:45.906: INFO: Got endpoints: latency-svc-x6dbs [420.277486ms]
    Nov 15 15:57:45.918: INFO: Created: latency-svc-5f4g8
    Nov 15 15:57:45.927: INFO: Got endpoints: latency-svc-5f4g8 [413.205131ms]
    Nov 15 15:57:45.943: INFO: Created: latency-svc-82jn7
    Nov 15 15:57:46.000: INFO: Got endpoints: latency-svc-82jn7 [460.770486ms]
    Nov 15 15:57:46.039: INFO: Created: latency-svc-vlvbs
    Nov 15 15:57:46.047: INFO: Got endpoints: latency-svc-vlvbs [466.932073ms]
    Nov 15 15:57:46.051: INFO: Created: latency-svc-pmdlh
    Nov 15 15:57:46.070: INFO: Got endpoints: latency-svc-pmdlh [466.012703ms]
    Nov 15 15:57:46.073: INFO: Created: latency-svc-vwrfm
    Nov 15 15:57:46.087: INFO: Got endpoints: latency-svc-vwrfm [455.645265ms]
    Nov 15 15:57:46.105: INFO: Created: latency-svc-64klh
    Nov 15 15:57:46.116: INFO: Got endpoints: latency-svc-64klh [461.085151ms]
    Nov 15 15:57:46.117: INFO: Latencies: [50.048843ms 52.232107ms 53.27268ms 59.948661ms 68.366503ms 75.044911ms 75.987824ms 80.731399ms 81.621973ms 89.223834ms 93.61939ms 99.117884ms 101.196826ms 107.689083ms 108.748091ms 115.735209ms 125.676084ms 129.829179ms 133.531766ms 138.467174ms 147.367676ms 159.630676ms 160.157772ms 164.732792ms 169.147303ms 178.931136ms 182.740645ms 182.841201ms 184.620873ms 189.01029ms 202.582287ms 204.387186ms 213.508103ms 213.652495ms 231.934046ms 233.508211ms 237.427561ms 243.202599ms 250.707537ms 251.29864ms 262.871665ms 272.441665ms 277.502144ms 278.72664ms 287.187728ms 295.653539ms 304.222805ms 305.994606ms 311.953827ms 320.337919ms 324.652804ms 326.999014ms 336.688608ms 342.720206ms 345.600983ms 364.785401ms 365.194012ms 370.908231ms 376.035431ms 379.175847ms 380.492463ms 381.737434ms 382.610365ms 383.954786ms 385.065643ms 385.241676ms 386.209139ms 388.095854ms 388.818497ms 389.293536ms 390.53263ms 392.204506ms 392.800258ms 393.526154ms 394.110174ms 395.303069ms 395.833981ms 398.337265ms 398.802508ms 398.953092ms 401.006939ms 401.130503ms 401.846832ms 402.172353ms 402.195467ms 402.910767ms 402.948264ms 403.03845ms 403.203272ms 403.330562ms 404.057623ms 405.158014ms 405.321023ms 407.792866ms 408.597559ms 409.25214ms 410.447191ms 413.205131ms 413.401354ms 415.242461ms 416.76265ms 417.49733ms 417.703922ms 417.946837ms 419.444398ms 419.624269ms 420.218563ms 420.277486ms 423.984598ms 424.128296ms 427.526993ms 428.266233ms 428.611248ms 428.801857ms 429.116324ms 430.505646ms 431.34043ms 432.166524ms 433.706883ms 434.22016ms 437.514484ms 439.693352ms 439.695862ms 440.391325ms 441.714027ms 443.350124ms 446.687375ms 450.837833ms 452.623006ms 453.234316ms 454.498519ms 454.829656ms 455.645265ms 457.70716ms 460.770486ms 461.085151ms 462.802968ms 463.764719ms 463.770555ms 465.514623ms 466.012703ms 466.932073ms 467.033505ms 470.797388ms 472.835477ms 473.065735ms 476.750663ms 479.127334ms 482.468777ms 487.220939ms 489.349335ms 491.468847ms 494.476535ms 496.102378ms 498.99004ms 510.545495ms 526.803102ms 528.070674ms 548.325155ms 549.512652ms 549.765509ms 552.27276ms 554.191621ms 557.66171ms 559.035558ms 563.500549ms 565.851791ms 567.194325ms 567.202567ms 569.621918ms 575.649051ms 578.05833ms 582.229571ms 587.271509ms 588.262842ms 607.275848ms 607.937056ms 622.166524ms 628.654728ms 630.966412ms 639.270828ms 650.218158ms 653.054547ms 654.184169ms 662.97981ms 665.249812ms 695.25465ms 695.763603ms 711.854585ms 712.185567ms 743.178317ms 745.102351ms 748.796307ms 751.867726ms 755.850546ms 767.360367ms 777.330631ms 781.270043ms 828.39202ms 862.146052ms]
    Nov 15 15:57:46.117: INFO: 50 %ile: 416.76265ms
    Nov 15 15:57:46.118: INFO: 90 %ile: 639.270828ms
    Nov 15 15:57:46.118: INFO: 99 %ile: 828.39202ms
    Nov 15 15:57:46.118: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:57:46.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-5478" for this suite. 11/15/23 15:57:46.143
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:57:46.169
Nov 15 15:57:46.170: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename deployment 11/15/23 15:57:46.171
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:57:46.225
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:57:46.237
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Nov 15 15:57:46.295: INFO: Pod name rollover-pod: Found 0 pods out of 1
Nov 15 15:57:51.313: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 11/15/23 15:57:51.313
Nov 15 15:57:51.313: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Nov 15 15:57:53.327: INFO: Creating deployment "test-rollover-deployment"
Nov 15 15:57:53.377: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Nov 15 15:57:55.410: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Nov 15 15:57:55.437: INFO: Ensure that both replica sets have 1 created replica
Nov 15 15:57:55.463: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Nov 15 15:57:55.494: INFO: Updating deployment test-rollover-deployment
Nov 15 15:57:55.494: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Nov 15 15:57:57.523: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Nov 15 15:57:57.550: INFO: Make sure deployment "test-rollover-deployment" is complete
Nov 15 15:57:57.573: INFO: all replica sets need to contain the pod-template-hash label
Nov 15 15:57:57.573: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 57, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 57, 53, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 57, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 57, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 15:57:59.599: INFO: all replica sets need to contain the pod-template-hash label
Nov 15 15:57:59.599: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 57, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 57, 53, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 57, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 57, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 15:58:01.607: INFO: all replica sets need to contain the pod-template-hash label
Nov 15 15:58:01.608: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 57, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 57, 53, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 57, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 57, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 15:58:03.599: INFO: all replica sets need to contain the pod-template-hash label
Nov 15 15:58:03.599: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 57, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 57, 53, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 57, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 57, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 15:58:05.605: INFO: all replica sets need to contain the pod-template-hash label
Nov 15 15:58:05.605: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 57, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 57, 53, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 57, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 57, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 15:58:07.603: INFO: 
Nov 15 15:58:07.603: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Nov 15 15:58:07.641: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-330  a22da8ce-1057-4d0b-a4ab-123da2291242 29769 2 2023-11-15 15:57:53 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-11-15 15:57:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 15:58:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00370b878 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-11-15 15:57:53 +0000 UTC,LastTransitionTime:2023-11-15 15:57:53 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-11-15 15:58:07 +0000 UTC,LastTransitionTime:2023-11-15 15:57:53 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 15 15:58:07.656: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-330  d32b7d23-2381-4ea1-8cd3-392fb69911df 29755 2 2023-11-15 15:57:55 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment a22da8ce-1057-4d0b-a4ab-123da2291242 0xc00034cac7 0xc00034cac8}] [] [{kube-controller-manager Update apps/v1 2023-11-15 15:57:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a22da8ce-1057-4d0b-a4ab-123da2291242\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 15:58:07 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00034cc48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 15 15:58:07.656: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Nov 15 15:58:07.657: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-330  d05e9bac-83a9-49ca-8875-ced0fefa1798 29768 2 2023-11-15 15:57:46 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment a22da8ce-1057-4d0b-a4ab-123da2291242 0xc00034c8a7 0xc00034c8a8}] [] [{e2e.test Update apps/v1 2023-11-15 15:57:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 15:58:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a22da8ce-1057-4d0b-a4ab-123da2291242\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-11-15 15:58:07 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00034ca58 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 15 15:58:07.658: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-330  b640720e-74f5-4aff-94dd-86498dddf834 29102 2 2023-11-15 15:57:53 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment a22da8ce-1057-4d0b-a4ab-123da2291242 0xc00034ccf7 0xc00034ccf8}] [] [{kube-controller-manager Update apps/v1 2023-11-15 15:57:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a22da8ce-1057-4d0b-a4ab-123da2291242\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 15:57:55 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00034ce38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 15 15:58:07.679: INFO: Pod "test-rollover-deployment-6c6df9974f-pf8nj" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-pf8nj test-rollover-deployment-6c6df9974f- deployment-330  571b9551-3316-4ca4-ba27-93db0983c80e 29230 0 2023-11-15 15:57:55 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:e55088b8f4058c70555a6d242cbd14fbfd9b588160d566426a1641be42889fba cni.projectcalico.org/podIP:172.30.164.20/32 cni.projectcalico.org/podIPs:172.30.164.20/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f d32b7d23-2381-4ea1-8cd3-392fb69911df 0xc00370bc67 0xc00370bc68}] [] [{kube-controller-manager Update v1 2023-11-15 15:57:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d32b7d23-2381-4ea1-8cd3-392fb69911df\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 15:57:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 15:57:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.164.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cbn5q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cbn5q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 15:57:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 15:57:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 15:57:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 15:57:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:172.30.164.20,StartTime:2023-11-15 15:57:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 15:57:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://e17db86b0ec6b49ec336413d5c2a08d7dc2d825583567299572601b8d5ccf99c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.164.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Nov 15 15:58:07.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-330" for this suite. 11/15/23 15:58:07.697
------------------------------
• [SLOW TEST] [21.549 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:57:46.169
    Nov 15 15:57:46.170: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename deployment 11/15/23 15:57:46.171
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:57:46.225
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:57:46.237
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Nov 15 15:57:46.295: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Nov 15 15:57:51.313: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 11/15/23 15:57:51.313
    Nov 15 15:57:51.313: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Nov 15 15:57:53.327: INFO: Creating deployment "test-rollover-deployment"
    Nov 15 15:57:53.377: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Nov 15 15:57:55.410: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Nov 15 15:57:55.437: INFO: Ensure that both replica sets have 1 created replica
    Nov 15 15:57:55.463: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Nov 15 15:57:55.494: INFO: Updating deployment test-rollover-deployment
    Nov 15 15:57:55.494: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Nov 15 15:57:57.523: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Nov 15 15:57:57.550: INFO: Make sure deployment "test-rollover-deployment" is complete
    Nov 15 15:57:57.573: INFO: all replica sets need to contain the pod-template-hash label
    Nov 15 15:57:57.573: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 57, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 57, 53, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 57, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 57, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 15:57:59.599: INFO: all replica sets need to contain the pod-template-hash label
    Nov 15 15:57:59.599: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 57, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 57, 53, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 57, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 57, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 15:58:01.607: INFO: all replica sets need to contain the pod-template-hash label
    Nov 15 15:58:01.608: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 57, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 57, 53, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 57, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 57, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 15:58:03.599: INFO: all replica sets need to contain the pod-template-hash label
    Nov 15 15:58:03.599: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 57, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 57, 53, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 57, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 57, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 15:58:05.605: INFO: all replica sets need to contain the pod-template-hash label
    Nov 15 15:58:05.605: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 57, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 57, 53, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 15, 57, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 15, 57, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 15:58:07.603: INFO: 
    Nov 15 15:58:07.603: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Nov 15 15:58:07.641: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-330  a22da8ce-1057-4d0b-a4ab-123da2291242 29769 2 2023-11-15 15:57:53 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-11-15 15:57:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 15:58:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00370b878 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-11-15 15:57:53 +0000 UTC,LastTransitionTime:2023-11-15 15:57:53 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-11-15 15:58:07 +0000 UTC,LastTransitionTime:2023-11-15 15:57:53 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Nov 15 15:58:07.656: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-330  d32b7d23-2381-4ea1-8cd3-392fb69911df 29755 2 2023-11-15 15:57:55 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment a22da8ce-1057-4d0b-a4ab-123da2291242 0xc00034cac7 0xc00034cac8}] [] [{kube-controller-manager Update apps/v1 2023-11-15 15:57:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a22da8ce-1057-4d0b-a4ab-123da2291242\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 15:58:07 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00034cc48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Nov 15 15:58:07.656: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Nov 15 15:58:07.657: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-330  d05e9bac-83a9-49ca-8875-ced0fefa1798 29768 2 2023-11-15 15:57:46 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment a22da8ce-1057-4d0b-a4ab-123da2291242 0xc00034c8a7 0xc00034c8a8}] [] [{e2e.test Update apps/v1 2023-11-15 15:57:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 15:58:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a22da8ce-1057-4d0b-a4ab-123da2291242\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-11-15 15:58:07 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00034ca58 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Nov 15 15:58:07.658: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-330  b640720e-74f5-4aff-94dd-86498dddf834 29102 2 2023-11-15 15:57:53 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment a22da8ce-1057-4d0b-a4ab-123da2291242 0xc00034ccf7 0xc00034ccf8}] [] [{kube-controller-manager Update apps/v1 2023-11-15 15:57:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a22da8ce-1057-4d0b-a4ab-123da2291242\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 15:57:55 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00034ce38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Nov 15 15:58:07.679: INFO: Pod "test-rollover-deployment-6c6df9974f-pf8nj" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-pf8nj test-rollover-deployment-6c6df9974f- deployment-330  571b9551-3316-4ca4-ba27-93db0983c80e 29230 0 2023-11-15 15:57:55 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:e55088b8f4058c70555a6d242cbd14fbfd9b588160d566426a1641be42889fba cni.projectcalico.org/podIP:172.30.164.20/32 cni.projectcalico.org/podIPs:172.30.164.20/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f d32b7d23-2381-4ea1-8cd3-392fb69911df 0xc00370bc67 0xc00370bc68}] [] [{kube-controller-manager Update v1 2023-11-15 15:57:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d32b7d23-2381-4ea1-8cd3-392fb69911df\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 15:57:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 15:57:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.164.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cbn5q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cbn5q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 15:57:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 15:57:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 15:57:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 15:57:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:172.30.164.20,StartTime:2023-11-15 15:57:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 15:57:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://e17db86b0ec6b49ec336413d5c2a08d7dc2d825583567299572601b8d5ccf99c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.164.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:58:07.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-330" for this suite. 11/15/23 15:58:07.697
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:58:07.742
Nov 15 15:58:07.742: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename statefulset 11/15/23 15:58:07.743
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:58:07.789
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:58:07.802
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4588 11/15/23 15:58:07.818
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-4588 11/15/23 15:58:07.834
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4588 11/15/23 15:58:07.858
Nov 15 15:58:07.879: INFO: Found 0 stateful pods, waiting for 1
Nov 15 15:58:17.904: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 11/15/23 15:58:17.904
Nov 15 15:58:17.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-4588 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 15 15:58:18.371: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 15 15:58:18.371: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 15 15:58:18.371: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 15 15:58:18.389: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Nov 15 15:58:28.412: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 15 15:58:28.412: INFO: Waiting for statefulset status.replicas updated to 0
Nov 15 15:58:28.487: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Nov 15 15:58:28.487: INFO: ss-0  10.15.40.115  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:07 +0000 UTC  }]
Nov 15 15:58:28.487: INFO: 
Nov 15 15:58:28.487: INFO: StatefulSet ss has not reached scale 3, at 1
Nov 15 15:58:29.507: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.980579989s
Nov 15 15:58:30.530: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.961469051s
Nov 15 15:58:31.554: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.936493804s
Nov 15 15:58:32.573: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.914719321s
Nov 15 15:58:33.592: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.896060188s
Nov 15 15:58:34.615: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.876301211s
Nov 15 15:58:35.635: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.852744341s
Nov 15 15:58:36.656: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.833591665s
Nov 15 15:58:37.675: INFO: Verifying statefulset ss doesn't scale past 3 for another 812.247272ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4588 11/15/23 15:58:38.676
Nov 15 15:58:38.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-4588 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 15 15:58:39.179: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 15 15:58:39.179: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 15 15:58:39.179: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 15 15:58:39.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-4588 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 15 15:58:39.649: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Nov 15 15:58:39.649: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 15 15:58:39.649: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 15 15:58:39.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-4588 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 15 15:58:40.102: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Nov 15 15:58:40.102: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 15 15:58:40.102: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 15 15:58:40.122: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 15:58:40.122: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 15:58:40.122: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 11/15/23 15:58:40.122
Nov 15 15:58:40.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-4588 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 15 15:58:40.611: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 15 15:58:40.611: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 15 15:58:40.611: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 15 15:58:40.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-4588 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 15 15:58:41.081: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 15 15:58:41.081: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 15 15:58:41.081: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 15 15:58:41.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-4588 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 15 15:58:41.541: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 15 15:58:41.541: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 15 15:58:41.541: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 15 15:58:41.541: INFO: Waiting for statefulset status.replicas updated to 0
Nov 15 15:58:41.557: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Nov 15 15:58:51.601: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 15 15:58:51.601: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov 15 15:58:51.601: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov 15 15:58:51.658: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Nov 15 15:58:51.658: INFO: ss-0  10.15.40.115  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:07 +0000 UTC  }]
Nov 15 15:58:51.658: INFO: ss-1  10.15.40.106  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:28 +0000 UTC  }]
Nov 15 15:58:51.658: INFO: ss-2  10.15.40.114  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:28 +0000 UTC  }]
Nov 15 15:58:51.658: INFO: 
Nov 15 15:58:51.658: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 15 15:58:52.677: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Nov 15 15:58:52.677: INFO: ss-0  10.15.40.115  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:07 +0000 UTC  }]
Nov 15 15:58:52.678: INFO: ss-1  10.15.40.106  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:28 +0000 UTC  }]
Nov 15 15:58:52.678: INFO: ss-2  10.15.40.114  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:28 +0000 UTC  }]
Nov 15 15:58:52.678: INFO: 
Nov 15 15:58:52.678: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 15 15:58:53.697: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.961791793s
Nov 15 15:58:54.752: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.941582303s
Nov 15 15:58:55.770: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.887001381s
Nov 15 15:58:56.786: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.86948479s
Nov 15 15:58:57.802: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.853726281s
Nov 15 15:58:58.819: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.83697725s
Nov 15 15:58:59.835: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.820557006s
Nov 15 15:59:00.852: INFO: Verifying statefulset ss doesn't scale past 0 for another 803.626567ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4588 11/15/23 15:59:01.852
Nov 15 15:59:01.868: INFO: Scaling statefulset ss to 0
Nov 15 15:59:01.933: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Nov 15 15:59:01.953: INFO: Deleting all statefulset in ns statefulset-4588
Nov 15 15:59:01.969: INFO: Scaling statefulset ss to 0
Nov 15 15:59:02.052: INFO: Waiting for statefulset status.replicas updated to 0
Nov 15 15:59:02.087: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Nov 15 15:59:02.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4588" for this suite. 11/15/23 15:59:02.194
------------------------------
• [SLOW TEST] [54.486 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:58:07.742
    Nov 15 15:58:07.742: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename statefulset 11/15/23 15:58:07.743
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:58:07.789
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:58:07.802
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4588 11/15/23 15:58:07.818
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-4588 11/15/23 15:58:07.834
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4588 11/15/23 15:58:07.858
    Nov 15 15:58:07.879: INFO: Found 0 stateful pods, waiting for 1
    Nov 15 15:58:17.904: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 11/15/23 15:58:17.904
    Nov 15 15:58:17.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-4588 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 15 15:58:18.371: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 15 15:58:18.371: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 15 15:58:18.371: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Nov 15 15:58:18.389: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Nov 15 15:58:28.412: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Nov 15 15:58:28.412: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 15 15:58:28.487: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
    Nov 15 15:58:28.487: INFO: ss-0  10.15.40.115  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:07 +0000 UTC  }]
    Nov 15 15:58:28.487: INFO: 
    Nov 15 15:58:28.487: INFO: StatefulSet ss has not reached scale 3, at 1
    Nov 15 15:58:29.507: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.980579989s
    Nov 15 15:58:30.530: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.961469051s
    Nov 15 15:58:31.554: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.936493804s
    Nov 15 15:58:32.573: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.914719321s
    Nov 15 15:58:33.592: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.896060188s
    Nov 15 15:58:34.615: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.876301211s
    Nov 15 15:58:35.635: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.852744341s
    Nov 15 15:58:36.656: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.833591665s
    Nov 15 15:58:37.675: INFO: Verifying statefulset ss doesn't scale past 3 for another 812.247272ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4588 11/15/23 15:58:38.676
    Nov 15 15:58:38.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-4588 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Nov 15 15:58:39.179: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Nov 15 15:58:39.179: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Nov 15 15:58:39.179: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Nov 15 15:58:39.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-4588 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Nov 15 15:58:39.649: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Nov 15 15:58:39.649: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Nov 15 15:58:39.649: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Nov 15 15:58:39.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-4588 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Nov 15 15:58:40.102: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Nov 15 15:58:40.102: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Nov 15 15:58:40.102: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Nov 15 15:58:40.122: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Nov 15 15:58:40.122: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Nov 15 15:58:40.122: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 11/15/23 15:58:40.122
    Nov 15 15:58:40.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-4588 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 15 15:58:40.611: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 15 15:58:40.611: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 15 15:58:40.611: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Nov 15 15:58:40.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-4588 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 15 15:58:41.081: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 15 15:58:41.081: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 15 15:58:41.081: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Nov 15 15:58:41.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-4588 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 15 15:58:41.541: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 15 15:58:41.541: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 15 15:58:41.541: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Nov 15 15:58:41.541: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 15 15:58:41.557: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Nov 15 15:58:51.601: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Nov 15 15:58:51.601: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Nov 15 15:58:51.601: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Nov 15 15:58:51.658: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
    Nov 15 15:58:51.658: INFO: ss-0  10.15.40.115  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:07 +0000 UTC  }]
    Nov 15 15:58:51.658: INFO: ss-1  10.15.40.106  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:28 +0000 UTC  }]
    Nov 15 15:58:51.658: INFO: ss-2  10.15.40.114  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:28 +0000 UTC  }]
    Nov 15 15:58:51.658: INFO: 
    Nov 15 15:58:51.658: INFO: StatefulSet ss has not reached scale 0, at 3
    Nov 15 15:58:52.677: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
    Nov 15 15:58:52.677: INFO: ss-0  10.15.40.115  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:07 +0000 UTC  }]
    Nov 15 15:58:52.678: INFO: ss-1  10.15.40.106  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:28 +0000 UTC  }]
    Nov 15 15:58:52.678: INFO: ss-2  10.15.40.114  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 15:58:28 +0000 UTC  }]
    Nov 15 15:58:52.678: INFO: 
    Nov 15 15:58:52.678: INFO: StatefulSet ss has not reached scale 0, at 3
    Nov 15 15:58:53.697: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.961791793s
    Nov 15 15:58:54.752: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.941582303s
    Nov 15 15:58:55.770: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.887001381s
    Nov 15 15:58:56.786: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.86948479s
    Nov 15 15:58:57.802: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.853726281s
    Nov 15 15:58:58.819: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.83697725s
    Nov 15 15:58:59.835: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.820557006s
    Nov 15 15:59:00.852: INFO: Verifying statefulset ss doesn't scale past 0 for another 803.626567ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4588 11/15/23 15:59:01.852
    Nov 15 15:59:01.868: INFO: Scaling statefulset ss to 0
    Nov 15 15:59:01.933: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Nov 15 15:59:01.953: INFO: Deleting all statefulset in ns statefulset-4588
    Nov 15 15:59:01.969: INFO: Scaling statefulset ss to 0
    Nov 15 15:59:02.052: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 15 15:59:02.087: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:59:02.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4588" for this suite. 11/15/23 15:59:02.194
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:59:02.234
Nov 15 15:59:02.234: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename services 11/15/23 15:59:02.236
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:59:02.293
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:59:02.31
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2432 11/15/23 15:59:02.328
STEP: changing the ExternalName service to type=ClusterIP 11/15/23 15:59:02.364
STEP: creating replication controller externalname-service in namespace services-2432 11/15/23 15:59:02.434
I1115 15:59:02.455808      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2432, replica count: 2
I1115 15:59:05.507966      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 15 15:59:05.508: INFO: Creating new exec pod
Nov 15 15:59:05.545: INFO: Waiting up to 5m0s for pod "execpodftlfq" in namespace "services-2432" to be "running"
Nov 15 15:59:05.563: INFO: Pod "execpodftlfq": Phase="Pending", Reason="", readiness=false. Elapsed: 17.955725ms
Nov 15 15:59:07.580: INFO: Pod "execpodftlfq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035286269s
Nov 15 15:59:09.581: INFO: Pod "execpodftlfq": Phase="Running", Reason="", readiness=true. Elapsed: 4.03651944s
Nov 15 15:59:09.581: INFO: Pod "execpodftlfq" satisfied condition "running"
Nov 15 15:59:10.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-2432 exec execpodftlfq -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Nov 15 15:59:11.038: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Nov 15 15:59:11.038: INFO: stdout: ""
Nov 15 15:59:11.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-2432 exec execpodftlfq -- /bin/sh -x -c nc -v -z -w 2 172.21.210.72 80'
Nov 15 15:59:11.487: INFO: stderr: "+ nc -v -z -w 2 172.21.210.72 80\nConnection to 172.21.210.72 80 port [tcp/http] succeeded!\n"
Nov 15 15:59:11.487: INFO: stdout: ""
Nov 15 15:59:11.487: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 15 15:59:11.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2432" for this suite. 11/15/23 15:59:11.58
------------------------------
• [SLOW TEST] [9.370 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:59:02.234
    Nov 15 15:59:02.234: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename services 11/15/23 15:59:02.236
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:59:02.293
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:59:02.31
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-2432 11/15/23 15:59:02.328
    STEP: changing the ExternalName service to type=ClusterIP 11/15/23 15:59:02.364
    STEP: creating replication controller externalname-service in namespace services-2432 11/15/23 15:59:02.434
    I1115 15:59:02.455808      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2432, replica count: 2
    I1115 15:59:05.507966      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 15 15:59:05.508: INFO: Creating new exec pod
    Nov 15 15:59:05.545: INFO: Waiting up to 5m0s for pod "execpodftlfq" in namespace "services-2432" to be "running"
    Nov 15 15:59:05.563: INFO: Pod "execpodftlfq": Phase="Pending", Reason="", readiness=false. Elapsed: 17.955725ms
    Nov 15 15:59:07.580: INFO: Pod "execpodftlfq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035286269s
    Nov 15 15:59:09.581: INFO: Pod "execpodftlfq": Phase="Running", Reason="", readiness=true. Elapsed: 4.03651944s
    Nov 15 15:59:09.581: INFO: Pod "execpodftlfq" satisfied condition "running"
    Nov 15 15:59:10.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-2432 exec execpodftlfq -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Nov 15 15:59:11.038: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Nov 15 15:59:11.038: INFO: stdout: ""
    Nov 15 15:59:11.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-2432 exec execpodftlfq -- /bin/sh -x -c nc -v -z -w 2 172.21.210.72 80'
    Nov 15 15:59:11.487: INFO: stderr: "+ nc -v -z -w 2 172.21.210.72 80\nConnection to 172.21.210.72 80 port [tcp/http] succeeded!\n"
    Nov 15 15:59:11.487: INFO: stdout: ""
    Nov 15 15:59:11.487: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:59:11.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2432" for this suite. 11/15/23 15:59:11.58
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:59:11.606
Nov 15 15:59:11.607: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename crd-publish-openapi 11/15/23 15:59:11.611
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:59:11.709
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:59:11.727
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 11/15/23 15:59:11.746
Nov 15 15:59:11.747: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: mark a version not serverd 11/15/23 15:59:17.69
STEP: check the unserved version gets removed 11/15/23 15:59:17.776
STEP: check the other version is not changed 11/15/23 15:59:20.132
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 15:59:24.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3721" for this suite. 11/15/23 15:59:25.029
------------------------------
• [SLOW TEST] [13.442 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:59:11.606
    Nov 15 15:59:11.607: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename crd-publish-openapi 11/15/23 15:59:11.611
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:59:11.709
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:59:11.727
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 11/15/23 15:59:11.746
    Nov 15 15:59:11.747: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: mark a version not serverd 11/15/23 15:59:17.69
    STEP: check the unserved version gets removed 11/15/23 15:59:17.776
    STEP: check the other version is not changed 11/15/23 15:59:20.132
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:59:24.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3721" for this suite. 11/15/23 15:59:25.029
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:59:25.065
Nov 15 15:59:25.066: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename custom-resource-definition 11/15/23 15:59:25.067
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:59:25.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:59:25.124
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Nov 15 15:59:25.141: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 15:59:32.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-5848" for this suite. 11/15/23 15:59:32.143
------------------------------
• [SLOW TEST] [7.099 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:59:25.065
    Nov 15 15:59:25.066: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename custom-resource-definition 11/15/23 15:59:25.067
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:59:25.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:59:25.124
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Nov 15 15:59:25.141: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:59:32.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-5848" for this suite. 11/15/23 15:59:32.143
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:59:32.17
Nov 15 15:59:32.170: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename emptydir 11/15/23 15:59:32.174
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:59:32.22
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:59:32.233
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 11/15/23 15:59:32.248
Nov 15 15:59:32.284: INFO: Waiting up to 5m0s for pod "pod-fef0829b-e124-4b14-bce7-68eee42cc9b5" in namespace "emptydir-4643" to be "Succeeded or Failed"
Nov 15 15:59:32.303: INFO: Pod "pod-fef0829b-e124-4b14-bce7-68eee42cc9b5": Phase="Pending", Reason="", readiness=false. Elapsed: 19.420662ms
Nov 15 15:59:34.322: INFO: Pod "pod-fef0829b-e124-4b14-bce7-68eee42cc9b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038582892s
Nov 15 15:59:36.323: INFO: Pod "pod-fef0829b-e124-4b14-bce7-68eee42cc9b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0393198s
STEP: Saw pod success 11/15/23 15:59:36.323
Nov 15 15:59:36.323: INFO: Pod "pod-fef0829b-e124-4b14-bce7-68eee42cc9b5" satisfied condition "Succeeded or Failed"
Nov 15 15:59:36.339: INFO: Trying to get logs from node 10.15.40.115 pod pod-fef0829b-e124-4b14-bce7-68eee42cc9b5 container test-container: <nil>
STEP: delete the pod 11/15/23 15:59:36.481
Nov 15 15:59:36.537: INFO: Waiting for pod pod-fef0829b-e124-4b14-bce7-68eee42cc9b5 to disappear
Nov 15 15:59:36.555: INFO: Pod pod-fef0829b-e124-4b14-bce7-68eee42cc9b5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 15 15:59:36.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4643" for this suite. 11/15/23 15:59:36.572
------------------------------
• [4.422 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:59:32.17
    Nov 15 15:59:32.170: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename emptydir 11/15/23 15:59:32.174
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:59:32.22
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:59:32.233
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 11/15/23 15:59:32.248
    Nov 15 15:59:32.284: INFO: Waiting up to 5m0s for pod "pod-fef0829b-e124-4b14-bce7-68eee42cc9b5" in namespace "emptydir-4643" to be "Succeeded or Failed"
    Nov 15 15:59:32.303: INFO: Pod "pod-fef0829b-e124-4b14-bce7-68eee42cc9b5": Phase="Pending", Reason="", readiness=false. Elapsed: 19.420662ms
    Nov 15 15:59:34.322: INFO: Pod "pod-fef0829b-e124-4b14-bce7-68eee42cc9b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038582892s
    Nov 15 15:59:36.323: INFO: Pod "pod-fef0829b-e124-4b14-bce7-68eee42cc9b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0393198s
    STEP: Saw pod success 11/15/23 15:59:36.323
    Nov 15 15:59:36.323: INFO: Pod "pod-fef0829b-e124-4b14-bce7-68eee42cc9b5" satisfied condition "Succeeded or Failed"
    Nov 15 15:59:36.339: INFO: Trying to get logs from node 10.15.40.115 pod pod-fef0829b-e124-4b14-bce7-68eee42cc9b5 container test-container: <nil>
    STEP: delete the pod 11/15/23 15:59:36.481
    Nov 15 15:59:36.537: INFO: Waiting for pod pod-fef0829b-e124-4b14-bce7-68eee42cc9b5 to disappear
    Nov 15 15:59:36.555: INFO: Pod pod-fef0829b-e124-4b14-bce7-68eee42cc9b5 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:59:36.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4643" for this suite. 11/15/23 15:59:36.572
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:59:36.608
Nov 15 15:59:36.608: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename runtimeclass 11/15/23 15:59:36.61
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:59:36.654
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:59:36.667
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Nov 15 15:59:36.724: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9210 to be scheduled
Nov 15 15:59:36.743: INFO: 1 pods are not scheduled: [runtimeclass-9210/test-runtimeclass-runtimeclass-9210-preconfigured-handler-sfnql(5c48bda5-58ff-4c98-8ca9-746c6d195249)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Nov 15 15:59:38.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-9210" for this suite. 11/15/23 15:59:38.802
------------------------------
• [2.213 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:59:36.608
    Nov 15 15:59:36.608: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename runtimeclass 11/15/23 15:59:36.61
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:59:36.654
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:59:36.667
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Nov 15 15:59:36.724: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9210 to be scheduled
    Nov 15 15:59:36.743: INFO: 1 pods are not scheduled: [runtimeclass-9210/test-runtimeclass-runtimeclass-9210-preconfigured-handler-sfnql(5c48bda5-58ff-4c98-8ca9-746c6d195249)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:59:38.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-9210" for this suite. 11/15/23 15:59:38.802
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:59:38.824
Nov 15 15:59:38.824: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename kubectl 11/15/23 15:59:38.83
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:59:38.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:59:38.881
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 11/15/23 15:59:38.893
Nov 15 15:59:38.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6779 create -f -'
Nov 15 15:59:39.930: INFO: stderr: ""
Nov 15 15:59:39.931: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 11/15/23 15:59:39.931
Nov 15 15:59:39.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6779 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 15 15:59:40.071: INFO: stderr: ""
Nov 15 15:59:40.072: INFO: stdout: "update-demo-nautilus-9skwc update-demo-nautilus-mbqm2 "
Nov 15 15:59:40.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6779 get pods update-demo-nautilus-9skwc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 15 15:59:40.206: INFO: stderr: ""
Nov 15 15:59:40.206: INFO: stdout: ""
Nov 15 15:59:40.206: INFO: update-demo-nautilus-9skwc is created but not running
Nov 15 15:59:45.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6779 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 15 15:59:45.390: INFO: stderr: ""
Nov 15 15:59:45.390: INFO: stdout: "update-demo-nautilus-9skwc update-demo-nautilus-mbqm2 "
Nov 15 15:59:45.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6779 get pods update-demo-nautilus-9skwc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 15 15:59:45.523: INFO: stderr: ""
Nov 15 15:59:45.523: INFO: stdout: ""
Nov 15 15:59:45.523: INFO: update-demo-nautilus-9skwc is created but not running
Nov 15 15:59:50.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6779 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 15 15:59:50.678: INFO: stderr: ""
Nov 15 15:59:50.678: INFO: stdout: "update-demo-nautilus-9skwc update-demo-nautilus-mbqm2 "
Nov 15 15:59:50.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6779 get pods update-demo-nautilus-9skwc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 15 15:59:50.804: INFO: stderr: ""
Nov 15 15:59:50.804: INFO: stdout: "true"
Nov 15 15:59:50.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6779 get pods update-demo-nautilus-9skwc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 15 15:59:50.943: INFO: stderr: ""
Nov 15 15:59:50.943: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Nov 15 15:59:50.943: INFO: validating pod update-demo-nautilus-9skwc
Nov 15 15:59:51.025: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 15 15:59:51.025: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 15 15:59:51.025: INFO: update-demo-nautilus-9skwc is verified up and running
Nov 15 15:59:51.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6779 get pods update-demo-nautilus-mbqm2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 15 15:59:51.170: INFO: stderr: ""
Nov 15 15:59:51.170: INFO: stdout: "true"
Nov 15 15:59:51.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6779 get pods update-demo-nautilus-mbqm2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 15 15:59:51.329: INFO: stderr: ""
Nov 15 15:59:51.329: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Nov 15 15:59:51.329: INFO: validating pod update-demo-nautilus-mbqm2
Nov 15 15:59:51.384: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 15 15:59:51.384: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 15 15:59:51.384: INFO: update-demo-nautilus-mbqm2 is verified up and running
STEP: using delete to clean up resources 11/15/23 15:59:51.384
Nov 15 15:59:51.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6779 delete --grace-period=0 --force -f -'
Nov 15 15:59:51.567: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 15 15:59:51.567: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov 15 15:59:51.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6779 get rc,svc -l name=update-demo --no-headers'
Nov 15 15:59:51.789: INFO: stderr: "No resources found in kubectl-6779 namespace.\n"
Nov 15 15:59:51.789: INFO: stdout: ""
Nov 15 15:59:51.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6779 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 15 15:59:51.959: INFO: stderr: ""
Nov 15 15:59:51.959: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 15 15:59:51.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6779" for this suite. 11/15/23 15:59:51.979
------------------------------
• [SLOW TEST] [13.176 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:59:38.824
    Nov 15 15:59:38.824: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename kubectl 11/15/23 15:59:38.83
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:59:38.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:59:38.881
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 11/15/23 15:59:38.893
    Nov 15 15:59:38.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6779 create -f -'
    Nov 15 15:59:39.930: INFO: stderr: ""
    Nov 15 15:59:39.931: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 11/15/23 15:59:39.931
    Nov 15 15:59:39.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6779 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Nov 15 15:59:40.071: INFO: stderr: ""
    Nov 15 15:59:40.072: INFO: stdout: "update-demo-nautilus-9skwc update-demo-nautilus-mbqm2 "
    Nov 15 15:59:40.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6779 get pods update-demo-nautilus-9skwc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 15 15:59:40.206: INFO: stderr: ""
    Nov 15 15:59:40.206: INFO: stdout: ""
    Nov 15 15:59:40.206: INFO: update-demo-nautilus-9skwc is created but not running
    Nov 15 15:59:45.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6779 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Nov 15 15:59:45.390: INFO: stderr: ""
    Nov 15 15:59:45.390: INFO: stdout: "update-demo-nautilus-9skwc update-demo-nautilus-mbqm2 "
    Nov 15 15:59:45.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6779 get pods update-demo-nautilus-9skwc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 15 15:59:45.523: INFO: stderr: ""
    Nov 15 15:59:45.523: INFO: stdout: ""
    Nov 15 15:59:45.523: INFO: update-demo-nautilus-9skwc is created but not running
    Nov 15 15:59:50.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6779 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Nov 15 15:59:50.678: INFO: stderr: ""
    Nov 15 15:59:50.678: INFO: stdout: "update-demo-nautilus-9skwc update-demo-nautilus-mbqm2 "
    Nov 15 15:59:50.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6779 get pods update-demo-nautilus-9skwc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 15 15:59:50.804: INFO: stderr: ""
    Nov 15 15:59:50.804: INFO: stdout: "true"
    Nov 15 15:59:50.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6779 get pods update-demo-nautilus-9skwc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Nov 15 15:59:50.943: INFO: stderr: ""
    Nov 15 15:59:50.943: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Nov 15 15:59:50.943: INFO: validating pod update-demo-nautilus-9skwc
    Nov 15 15:59:51.025: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Nov 15 15:59:51.025: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Nov 15 15:59:51.025: INFO: update-demo-nautilus-9skwc is verified up and running
    Nov 15 15:59:51.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6779 get pods update-demo-nautilus-mbqm2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 15 15:59:51.170: INFO: stderr: ""
    Nov 15 15:59:51.170: INFO: stdout: "true"
    Nov 15 15:59:51.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6779 get pods update-demo-nautilus-mbqm2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Nov 15 15:59:51.329: INFO: stderr: ""
    Nov 15 15:59:51.329: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Nov 15 15:59:51.329: INFO: validating pod update-demo-nautilus-mbqm2
    Nov 15 15:59:51.384: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Nov 15 15:59:51.384: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Nov 15 15:59:51.384: INFO: update-demo-nautilus-mbqm2 is verified up and running
    STEP: using delete to clean up resources 11/15/23 15:59:51.384
    Nov 15 15:59:51.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6779 delete --grace-period=0 --force -f -'
    Nov 15 15:59:51.567: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Nov 15 15:59:51.567: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Nov 15 15:59:51.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6779 get rc,svc -l name=update-demo --no-headers'
    Nov 15 15:59:51.789: INFO: stderr: "No resources found in kubectl-6779 namespace.\n"
    Nov 15 15:59:51.789: INFO: stdout: ""
    Nov 15 15:59:51.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-6779 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Nov 15 15:59:51.959: INFO: stderr: ""
    Nov 15 15:59:51.959: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:59:51.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6779" for this suite. 11/15/23 15:59:51.979
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:59:52.002
Nov 15 15:59:52.003: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename emptydir 11/15/23 15:59:52.005
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:59:52.052
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:59:52.063
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 11/15/23 15:59:52.075
Nov 15 15:59:52.101: INFO: Waiting up to 5m0s for pod "pod-bfdcbbe9-9a35-4d89-9542-ebe5409b0435" in namespace "emptydir-2426" to be "Succeeded or Failed"
Nov 15 15:59:52.117: INFO: Pod "pod-bfdcbbe9-9a35-4d89-9542-ebe5409b0435": Phase="Pending", Reason="", readiness=false. Elapsed: 16.001162ms
Nov 15 15:59:54.135: INFO: Pod "pod-bfdcbbe9-9a35-4d89-9542-ebe5409b0435": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033650225s
Nov 15 15:59:56.142: INFO: Pod "pod-bfdcbbe9-9a35-4d89-9542-ebe5409b0435": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040630397s
STEP: Saw pod success 11/15/23 15:59:56.142
Nov 15 15:59:56.143: INFO: Pod "pod-bfdcbbe9-9a35-4d89-9542-ebe5409b0435" satisfied condition "Succeeded or Failed"
Nov 15 15:59:56.161: INFO: Trying to get logs from node 10.15.40.115 pod pod-bfdcbbe9-9a35-4d89-9542-ebe5409b0435 container test-container: <nil>
STEP: delete the pod 11/15/23 15:59:56.2
Nov 15 15:59:56.255: INFO: Waiting for pod pod-bfdcbbe9-9a35-4d89-9542-ebe5409b0435 to disappear
Nov 15 15:59:56.273: INFO: Pod pod-bfdcbbe9-9a35-4d89-9542-ebe5409b0435 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 15 15:59:56.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2426" for this suite. 11/15/23 15:59:56.292
------------------------------
• [4.311 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:59:52.002
    Nov 15 15:59:52.003: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename emptydir 11/15/23 15:59:52.005
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:59:52.052
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:59:52.063
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 11/15/23 15:59:52.075
    Nov 15 15:59:52.101: INFO: Waiting up to 5m0s for pod "pod-bfdcbbe9-9a35-4d89-9542-ebe5409b0435" in namespace "emptydir-2426" to be "Succeeded or Failed"
    Nov 15 15:59:52.117: INFO: Pod "pod-bfdcbbe9-9a35-4d89-9542-ebe5409b0435": Phase="Pending", Reason="", readiness=false. Elapsed: 16.001162ms
    Nov 15 15:59:54.135: INFO: Pod "pod-bfdcbbe9-9a35-4d89-9542-ebe5409b0435": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033650225s
    Nov 15 15:59:56.142: INFO: Pod "pod-bfdcbbe9-9a35-4d89-9542-ebe5409b0435": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040630397s
    STEP: Saw pod success 11/15/23 15:59:56.142
    Nov 15 15:59:56.143: INFO: Pod "pod-bfdcbbe9-9a35-4d89-9542-ebe5409b0435" satisfied condition "Succeeded or Failed"
    Nov 15 15:59:56.161: INFO: Trying to get logs from node 10.15.40.115 pod pod-bfdcbbe9-9a35-4d89-9542-ebe5409b0435 container test-container: <nil>
    STEP: delete the pod 11/15/23 15:59:56.2
    Nov 15 15:59:56.255: INFO: Waiting for pod pod-bfdcbbe9-9a35-4d89-9542-ebe5409b0435 to disappear
    Nov 15 15:59:56.273: INFO: Pod pod-bfdcbbe9-9a35-4d89-9542-ebe5409b0435 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 15:59:56.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2426" for this suite. 11/15/23 15:59:56.292
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 15:59:56.326
Nov 15 15:59:56.326: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename projected 11/15/23 15:59:56.328
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:59:56.364
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:59:56.377
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-4d55e3b0-2d81-41e6-ab99-0d207485d816 11/15/23 15:59:56.391
STEP: Creating a pod to test consume configMaps 11/15/23 15:59:56.411
Nov 15 15:59:56.440: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4ffdf8b9-4930-4d90-a517-630c5724fd2b" in namespace "projected-3917" to be "Succeeded or Failed"
Nov 15 15:59:56.455: INFO: Pod "pod-projected-configmaps-4ffdf8b9-4930-4d90-a517-630c5724fd2b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.933861ms
Nov 15 15:59:58.474: INFO: Pod "pod-projected-configmaps-4ffdf8b9-4930-4d90-a517-630c5724fd2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033860801s
Nov 15 16:00:00.473: INFO: Pod "pod-projected-configmaps-4ffdf8b9-4930-4d90-a517-630c5724fd2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033225686s
STEP: Saw pod success 11/15/23 16:00:00.474
Nov 15 16:00:00.474: INFO: Pod "pod-projected-configmaps-4ffdf8b9-4930-4d90-a517-630c5724fd2b" satisfied condition "Succeeded or Failed"
Nov 15 16:00:00.492: INFO: Trying to get logs from node 10.15.40.115 pod pod-projected-configmaps-4ffdf8b9-4930-4d90-a517-630c5724fd2b container projected-configmap-volume-test: <nil>
STEP: delete the pod 11/15/23 16:00:00.531
Nov 15 16:00:00.586: INFO: Waiting for pod pod-projected-configmaps-4ffdf8b9-4930-4d90-a517-630c5724fd2b to disappear
Nov 15 16:00:00.602: INFO: Pod pod-projected-configmaps-4ffdf8b9-4930-4d90-a517-630c5724fd2b no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Nov 15 16:00:00.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3917" for this suite. 11/15/23 16:00:00.621
------------------------------
• [4.315 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 15:59:56.326
    Nov 15 15:59:56.326: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename projected 11/15/23 15:59:56.328
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 15:59:56.364
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 15:59:56.377
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-4d55e3b0-2d81-41e6-ab99-0d207485d816 11/15/23 15:59:56.391
    STEP: Creating a pod to test consume configMaps 11/15/23 15:59:56.411
    Nov 15 15:59:56.440: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4ffdf8b9-4930-4d90-a517-630c5724fd2b" in namespace "projected-3917" to be "Succeeded or Failed"
    Nov 15 15:59:56.455: INFO: Pod "pod-projected-configmaps-4ffdf8b9-4930-4d90-a517-630c5724fd2b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.933861ms
    Nov 15 15:59:58.474: INFO: Pod "pod-projected-configmaps-4ffdf8b9-4930-4d90-a517-630c5724fd2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033860801s
    Nov 15 16:00:00.473: INFO: Pod "pod-projected-configmaps-4ffdf8b9-4930-4d90-a517-630c5724fd2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033225686s
    STEP: Saw pod success 11/15/23 16:00:00.474
    Nov 15 16:00:00.474: INFO: Pod "pod-projected-configmaps-4ffdf8b9-4930-4d90-a517-630c5724fd2b" satisfied condition "Succeeded or Failed"
    Nov 15 16:00:00.492: INFO: Trying to get logs from node 10.15.40.115 pod pod-projected-configmaps-4ffdf8b9-4930-4d90-a517-630c5724fd2b container projected-configmap-volume-test: <nil>
    STEP: delete the pod 11/15/23 16:00:00.531
    Nov 15 16:00:00.586: INFO: Waiting for pod pod-projected-configmaps-4ffdf8b9-4930-4d90-a517-630c5724fd2b to disappear
    Nov 15 16:00:00.602: INFO: Pod pod-projected-configmaps-4ffdf8b9-4930-4d90-a517-630c5724fd2b no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:00:00.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3917" for this suite. 11/15/23 16:00:00.621
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:00:00.654
Nov 15 16:00:00.655: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename kubectl 11/15/23 16:00:00.656
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:00:00.693
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:00:00.707
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 11/15/23 16:00:00.721
Nov 15 16:00:00.721: INFO: namespace kubectl-2499
Nov 15 16:00:00.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-2499 create -f -'
Nov 15 16:00:01.051: INFO: stderr: ""
Nov 15 16:00:01.051: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 11/15/23 16:00:01.051
Nov 15 16:00:02.075: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 15 16:00:02.075: INFO: Found 0 / 1
Nov 15 16:00:03.070: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 15 16:00:03.070: INFO: Found 0 / 1
Nov 15 16:00:04.070: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 15 16:00:04.070: INFO: Found 1 / 1
Nov 15 16:00:04.070: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov 15 16:00:04.091: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 15 16:00:04.091: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 15 16:00:04.091: INFO: wait on agnhost-primary startup in kubectl-2499 
Nov 15 16:00:04.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-2499 logs agnhost-primary-l67n2 agnhost-primary'
Nov 15 16:00:04.403: INFO: stderr: ""
Nov 15 16:00:04.403: INFO: stdout: "Paused\n"
STEP: exposing RC 11/15/23 16:00:04.403
Nov 15 16:00:04.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-2499 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Nov 15 16:00:04.612: INFO: stderr: ""
Nov 15 16:00:04.612: INFO: stdout: "service/rm2 exposed\n"
Nov 15 16:00:04.623: INFO: Service rm2 in namespace kubectl-2499 found.
STEP: exposing service 11/15/23 16:00:06.649
Nov 15 16:00:06.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-2499 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Nov 15 16:00:06.884: INFO: stderr: ""
Nov 15 16:00:06.884: INFO: stdout: "service/rm3 exposed\n"
Nov 15 16:00:06.897: INFO: Service rm3 in namespace kubectl-2499 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 15 16:00:08.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2499" for this suite. 11/15/23 16:00:08.954
------------------------------
• [SLOW TEST] [8.318 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:00:00.654
    Nov 15 16:00:00.655: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename kubectl 11/15/23 16:00:00.656
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:00:00.693
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:00:00.707
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 11/15/23 16:00:00.721
    Nov 15 16:00:00.721: INFO: namespace kubectl-2499
    Nov 15 16:00:00.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-2499 create -f -'
    Nov 15 16:00:01.051: INFO: stderr: ""
    Nov 15 16:00:01.051: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 11/15/23 16:00:01.051
    Nov 15 16:00:02.075: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 15 16:00:02.075: INFO: Found 0 / 1
    Nov 15 16:00:03.070: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 15 16:00:03.070: INFO: Found 0 / 1
    Nov 15 16:00:04.070: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 15 16:00:04.070: INFO: Found 1 / 1
    Nov 15 16:00:04.070: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Nov 15 16:00:04.091: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 15 16:00:04.091: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Nov 15 16:00:04.091: INFO: wait on agnhost-primary startup in kubectl-2499 
    Nov 15 16:00:04.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-2499 logs agnhost-primary-l67n2 agnhost-primary'
    Nov 15 16:00:04.403: INFO: stderr: ""
    Nov 15 16:00:04.403: INFO: stdout: "Paused\n"
    STEP: exposing RC 11/15/23 16:00:04.403
    Nov 15 16:00:04.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-2499 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Nov 15 16:00:04.612: INFO: stderr: ""
    Nov 15 16:00:04.612: INFO: stdout: "service/rm2 exposed\n"
    Nov 15 16:00:04.623: INFO: Service rm2 in namespace kubectl-2499 found.
    STEP: exposing service 11/15/23 16:00:06.649
    Nov 15 16:00:06.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-2499 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Nov 15 16:00:06.884: INFO: stderr: ""
    Nov 15 16:00:06.884: INFO: stdout: "service/rm3 exposed\n"
    Nov 15 16:00:06.897: INFO: Service rm3 in namespace kubectl-2499 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:00:08.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2499" for this suite. 11/15/23 16:00:08.954
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:00:08.975
Nov 15 16:00:08.976: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename pods 11/15/23 16:00:08.982
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:00:09.04
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:00:09.052
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Nov 15 16:00:09.103: INFO: Waiting up to 5m0s for pod "server-envvars-ad8b5e9d-e13e-4ff1-be95-273961127c6b" in namespace "pods-7429" to be "running and ready"
Nov 15 16:00:09.121: INFO: Pod "server-envvars-ad8b5e9d-e13e-4ff1-be95-273961127c6b": Phase="Pending", Reason="", readiness=false. Elapsed: 17.61962ms
Nov 15 16:00:09.121: INFO: The phase of Pod server-envvars-ad8b5e9d-e13e-4ff1-be95-273961127c6b is Pending, waiting for it to be Running (with Ready = true)
Nov 15 16:00:11.143: INFO: Pod "server-envvars-ad8b5e9d-e13e-4ff1-be95-273961127c6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038897644s
Nov 15 16:00:11.143: INFO: The phase of Pod server-envvars-ad8b5e9d-e13e-4ff1-be95-273961127c6b is Pending, waiting for it to be Running (with Ready = true)
Nov 15 16:00:13.140: INFO: Pod "server-envvars-ad8b5e9d-e13e-4ff1-be95-273961127c6b": Phase="Running", Reason="", readiness=true. Elapsed: 4.036017071s
Nov 15 16:00:13.140: INFO: The phase of Pod server-envvars-ad8b5e9d-e13e-4ff1-be95-273961127c6b is Running (Ready = true)
Nov 15 16:00:13.140: INFO: Pod "server-envvars-ad8b5e9d-e13e-4ff1-be95-273961127c6b" satisfied condition "running and ready"
Nov 15 16:00:13.220: INFO: Waiting up to 5m0s for pod "client-envvars-b32d4992-8e0b-47b4-ba89-b7bffd458979" in namespace "pods-7429" to be "Succeeded or Failed"
Nov 15 16:00:13.238: INFO: Pod "client-envvars-b32d4992-8e0b-47b4-ba89-b7bffd458979": Phase="Pending", Reason="", readiness=false. Elapsed: 17.918464ms
Nov 15 16:00:15.256: INFO: Pod "client-envvars-b32d4992-8e0b-47b4-ba89-b7bffd458979": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035610801s
Nov 15 16:00:17.266: INFO: Pod "client-envvars-b32d4992-8e0b-47b4-ba89-b7bffd458979": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045831797s
Nov 15 16:00:19.280: INFO: Pod "client-envvars-b32d4992-8e0b-47b4-ba89-b7bffd458979": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.059366906s
STEP: Saw pod success 11/15/23 16:00:19.28
Nov 15 16:00:19.281: INFO: Pod "client-envvars-b32d4992-8e0b-47b4-ba89-b7bffd458979" satisfied condition "Succeeded or Failed"
Nov 15 16:00:19.318: INFO: Trying to get logs from node 10.15.40.115 pod client-envvars-b32d4992-8e0b-47b4-ba89-b7bffd458979 container env3cont: <nil>
STEP: delete the pod 11/15/23 16:00:19.363
Nov 15 16:00:19.416: INFO: Waiting for pod client-envvars-b32d4992-8e0b-47b4-ba89-b7bffd458979 to disappear
Nov 15 16:00:19.476: INFO: Pod client-envvars-b32d4992-8e0b-47b4-ba89-b7bffd458979 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Nov 15 16:00:19.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7429" for this suite. 11/15/23 16:00:19.501
------------------------------
• [SLOW TEST] [10.547 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:00:08.975
    Nov 15 16:00:08.976: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename pods 11/15/23 16:00:08.982
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:00:09.04
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:00:09.052
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Nov 15 16:00:09.103: INFO: Waiting up to 5m0s for pod "server-envvars-ad8b5e9d-e13e-4ff1-be95-273961127c6b" in namespace "pods-7429" to be "running and ready"
    Nov 15 16:00:09.121: INFO: Pod "server-envvars-ad8b5e9d-e13e-4ff1-be95-273961127c6b": Phase="Pending", Reason="", readiness=false. Elapsed: 17.61962ms
    Nov 15 16:00:09.121: INFO: The phase of Pod server-envvars-ad8b5e9d-e13e-4ff1-be95-273961127c6b is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 16:00:11.143: INFO: Pod "server-envvars-ad8b5e9d-e13e-4ff1-be95-273961127c6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038897644s
    Nov 15 16:00:11.143: INFO: The phase of Pod server-envvars-ad8b5e9d-e13e-4ff1-be95-273961127c6b is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 16:00:13.140: INFO: Pod "server-envvars-ad8b5e9d-e13e-4ff1-be95-273961127c6b": Phase="Running", Reason="", readiness=true. Elapsed: 4.036017071s
    Nov 15 16:00:13.140: INFO: The phase of Pod server-envvars-ad8b5e9d-e13e-4ff1-be95-273961127c6b is Running (Ready = true)
    Nov 15 16:00:13.140: INFO: Pod "server-envvars-ad8b5e9d-e13e-4ff1-be95-273961127c6b" satisfied condition "running and ready"
    Nov 15 16:00:13.220: INFO: Waiting up to 5m0s for pod "client-envvars-b32d4992-8e0b-47b4-ba89-b7bffd458979" in namespace "pods-7429" to be "Succeeded or Failed"
    Nov 15 16:00:13.238: INFO: Pod "client-envvars-b32d4992-8e0b-47b4-ba89-b7bffd458979": Phase="Pending", Reason="", readiness=false. Elapsed: 17.918464ms
    Nov 15 16:00:15.256: INFO: Pod "client-envvars-b32d4992-8e0b-47b4-ba89-b7bffd458979": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035610801s
    Nov 15 16:00:17.266: INFO: Pod "client-envvars-b32d4992-8e0b-47b4-ba89-b7bffd458979": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045831797s
    Nov 15 16:00:19.280: INFO: Pod "client-envvars-b32d4992-8e0b-47b4-ba89-b7bffd458979": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.059366906s
    STEP: Saw pod success 11/15/23 16:00:19.28
    Nov 15 16:00:19.281: INFO: Pod "client-envvars-b32d4992-8e0b-47b4-ba89-b7bffd458979" satisfied condition "Succeeded or Failed"
    Nov 15 16:00:19.318: INFO: Trying to get logs from node 10.15.40.115 pod client-envvars-b32d4992-8e0b-47b4-ba89-b7bffd458979 container env3cont: <nil>
    STEP: delete the pod 11/15/23 16:00:19.363
    Nov 15 16:00:19.416: INFO: Waiting for pod client-envvars-b32d4992-8e0b-47b4-ba89-b7bffd458979 to disappear
    Nov 15 16:00:19.476: INFO: Pod client-envvars-b32d4992-8e0b-47b4-ba89-b7bffd458979 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:00:19.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7429" for this suite. 11/15/23 16:00:19.501
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:00:19.525
Nov 15 16:00:19.525: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename statefulset 11/15/23 16:00:19.533
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:00:19.61
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:00:19.63
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7898 11/15/23 16:00:19.673
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 11/15/23 16:00:19.691
STEP: Creating stateful set ss in namespace statefulset-7898 11/15/23 16:00:19.717
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7898 11/15/23 16:00:19.741
Nov 15 16:00:19.758: INFO: Found 0 stateful pods, waiting for 1
Nov 15 16:00:29.782: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 11/15/23 16:00:29.782
Nov 15 16:00:29.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-7898 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 15 16:00:30.261: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 15 16:00:30.261: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 15 16:00:30.261: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 15 16:00:30.279: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Nov 15 16:00:40.304: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 15 16:00:40.304: INFO: Waiting for statefulset status.replicas updated to 0
Nov 15 16:00:40.383: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999336s
Nov 15 16:00:41.404: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.980571707s
Nov 15 16:00:42.424: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.959822508s
Nov 15 16:00:43.443: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.939397752s
Nov 15 16:00:44.462: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.920757685s
Nov 15 16:00:45.482: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.901346468s
Nov 15 16:00:46.500: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.882483042s
Nov 15 16:00:47.519: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.8645072s
Nov 15 16:00:48.538: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.844448061s
Nov 15 16:00:49.557: INFO: Verifying statefulset ss doesn't scale past 1 for another 826.048411ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7898 11/15/23 16:00:50.558
Nov 15 16:00:50.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-7898 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 15 16:00:51.074: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 15 16:00:51.074: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 15 16:00:51.074: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 15 16:00:51.092: INFO: Found 1 stateful pods, waiting for 3
Nov 15 16:01:01.115: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 16:01:01.115: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 16:01:01.115: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 11/15/23 16:01:01.115
STEP: Scale down will halt with unhealthy stateful pod 11/15/23 16:01:01.116
Nov 15 16:01:01.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-7898 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 15 16:01:01.617: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 15 16:01:01.617: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 15 16:01:01.617: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 15 16:01:01.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-7898 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 15 16:01:02.091: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 15 16:01:02.092: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 15 16:01:02.092: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 15 16:01:02.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-7898 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 15 16:01:02.555: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 15 16:01:02.555: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 15 16:01:02.555: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 15 16:01:02.555: INFO: Waiting for statefulset status.replicas updated to 0
Nov 15 16:01:02.582: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Nov 15 16:01:12.623: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 15 16:01:12.623: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov 15 16:01:12.623: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov 15 16:01:12.681: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999994632s
Nov 15 16:01:13.700: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.981328111s
Nov 15 16:01:14.718: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.96241314s
Nov 15 16:01:15.737: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.943648535s
Nov 15 16:01:16.756: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.925142479s
Nov 15 16:01:17.776: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.905902768s
Nov 15 16:01:18.795: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.886490763s
Nov 15 16:01:19.815: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.866173897s
Nov 15 16:01:20.834: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.846517705s
Nov 15 16:01:21.854: INFO: Verifying statefulset ss doesn't scale past 3 for another 828.188515ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7898 11/15/23 16:01:22.854
Nov 15 16:01:22.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-7898 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 15 16:01:23.306: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 15 16:01:23.306: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 15 16:01:23.306: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 15 16:01:23.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-7898 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 15 16:01:23.750: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 15 16:01:23.750: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 15 16:01:23.750: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 15 16:01:23.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-7898 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 15 16:01:24.172: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 15 16:01:24.172: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 15 16:01:24.172: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 15 16:01:24.172: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 11/15/23 16:01:34.259
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Nov 15 16:01:34.260: INFO: Deleting all statefulset in ns statefulset-7898
Nov 15 16:01:34.277: INFO: Scaling statefulset ss to 0
Nov 15 16:01:34.334: INFO: Waiting for statefulset status.replicas updated to 0
Nov 15 16:01:34.351: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Nov 15 16:01:34.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7898" for this suite. 11/15/23 16:01:34.452
------------------------------
• [SLOW TEST] [74.945 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:00:19.525
    Nov 15 16:00:19.525: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename statefulset 11/15/23 16:00:19.533
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:00:19.61
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:00:19.63
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7898 11/15/23 16:00:19.673
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 11/15/23 16:00:19.691
    STEP: Creating stateful set ss in namespace statefulset-7898 11/15/23 16:00:19.717
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7898 11/15/23 16:00:19.741
    Nov 15 16:00:19.758: INFO: Found 0 stateful pods, waiting for 1
    Nov 15 16:00:29.782: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 11/15/23 16:00:29.782
    Nov 15 16:00:29.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-7898 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 15 16:00:30.261: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 15 16:00:30.261: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 15 16:00:30.261: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Nov 15 16:00:30.279: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Nov 15 16:00:40.304: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Nov 15 16:00:40.304: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 15 16:00:40.383: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999336s
    Nov 15 16:00:41.404: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.980571707s
    Nov 15 16:00:42.424: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.959822508s
    Nov 15 16:00:43.443: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.939397752s
    Nov 15 16:00:44.462: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.920757685s
    Nov 15 16:00:45.482: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.901346468s
    Nov 15 16:00:46.500: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.882483042s
    Nov 15 16:00:47.519: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.8645072s
    Nov 15 16:00:48.538: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.844448061s
    Nov 15 16:00:49.557: INFO: Verifying statefulset ss doesn't scale past 1 for another 826.048411ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7898 11/15/23 16:00:50.558
    Nov 15 16:00:50.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-7898 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Nov 15 16:00:51.074: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Nov 15 16:00:51.074: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Nov 15 16:00:51.074: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Nov 15 16:00:51.092: INFO: Found 1 stateful pods, waiting for 3
    Nov 15 16:01:01.115: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Nov 15 16:01:01.115: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Nov 15 16:01:01.115: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 11/15/23 16:01:01.115
    STEP: Scale down will halt with unhealthy stateful pod 11/15/23 16:01:01.116
    Nov 15 16:01:01.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-7898 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 15 16:01:01.617: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 15 16:01:01.617: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 15 16:01:01.617: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Nov 15 16:01:01.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-7898 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 15 16:01:02.091: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 15 16:01:02.092: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 15 16:01:02.092: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Nov 15 16:01:02.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-7898 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 15 16:01:02.555: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 15 16:01:02.555: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 15 16:01:02.555: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Nov 15 16:01:02.555: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 15 16:01:02.582: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Nov 15 16:01:12.623: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Nov 15 16:01:12.623: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Nov 15 16:01:12.623: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Nov 15 16:01:12.681: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999994632s
    Nov 15 16:01:13.700: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.981328111s
    Nov 15 16:01:14.718: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.96241314s
    Nov 15 16:01:15.737: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.943648535s
    Nov 15 16:01:16.756: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.925142479s
    Nov 15 16:01:17.776: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.905902768s
    Nov 15 16:01:18.795: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.886490763s
    Nov 15 16:01:19.815: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.866173897s
    Nov 15 16:01:20.834: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.846517705s
    Nov 15 16:01:21.854: INFO: Verifying statefulset ss doesn't scale past 3 for another 828.188515ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7898 11/15/23 16:01:22.854
    Nov 15 16:01:22.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-7898 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Nov 15 16:01:23.306: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Nov 15 16:01:23.306: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Nov 15 16:01:23.306: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Nov 15 16:01:23.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-7898 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Nov 15 16:01:23.750: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Nov 15 16:01:23.750: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Nov 15 16:01:23.750: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Nov 15 16:01:23.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-7898 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Nov 15 16:01:24.172: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Nov 15 16:01:24.172: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Nov 15 16:01:24.172: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Nov 15 16:01:24.172: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 11/15/23 16:01:34.259
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Nov 15 16:01:34.260: INFO: Deleting all statefulset in ns statefulset-7898
    Nov 15 16:01:34.277: INFO: Scaling statefulset ss to 0
    Nov 15 16:01:34.334: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 15 16:01:34.351: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:01:34.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7898" for this suite. 11/15/23 16:01:34.452
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:01:34.472
Nov 15 16:01:34.472: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename kubectl 11/15/23 16:01:34.475
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:01:34.522
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:01:34.535
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 11/15/23 16:01:34.549
Nov 15 16:01:34.550: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-8851 proxy --unix-socket=/tmp/kubectl-proxy-unix1652772031/test'
STEP: retrieving proxy /api/ output 11/15/23 16:01:34.647
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 15 16:01:34.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8851" for this suite. 11/15/23 16:01:34.667
------------------------------
• [0.214 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:01:34.472
    Nov 15 16:01:34.472: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename kubectl 11/15/23 16:01:34.475
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:01:34.522
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:01:34.535
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 11/15/23 16:01:34.549
    Nov 15 16:01:34.550: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-8851 proxy --unix-socket=/tmp/kubectl-proxy-unix1652772031/test'
    STEP: retrieving proxy /api/ output 11/15/23 16:01:34.647
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:01:34.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8851" for this suite. 11/15/23 16:01:34.667
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:01:34.69
Nov 15 16:01:34.690: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename replicaset 11/15/23 16:01:34.693
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:01:34.733
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:01:34.748
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 11/15/23 16:01:34.761
Nov 15 16:01:34.789: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-7595" to be "running and ready"
Nov 15 16:01:34.805: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 16.454199ms
Nov 15 16:01:34.805: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Nov 15 16:01:36.824: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.035205319s
Nov 15 16:01:36.824: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Nov 15 16:01:36.824: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 11/15/23 16:01:36.842
STEP: Then the orphan pod is adopted 11/15/23 16:01:36.858
STEP: When the matched label of one of its pods change 11/15/23 16:01:37.894
Nov 15 16:01:37.913: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 11/15/23 16:01:37.958
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Nov 15 16:01:38.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-7595" for this suite. 11/15/23 16:01:39.024
------------------------------
• [4.357 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:01:34.69
    Nov 15 16:01:34.690: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename replicaset 11/15/23 16:01:34.693
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:01:34.733
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:01:34.748
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 11/15/23 16:01:34.761
    Nov 15 16:01:34.789: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-7595" to be "running and ready"
    Nov 15 16:01:34.805: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 16.454199ms
    Nov 15 16:01:34.805: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 16:01:36.824: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.035205319s
    Nov 15 16:01:36.824: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Nov 15 16:01:36.824: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 11/15/23 16:01:36.842
    STEP: Then the orphan pod is adopted 11/15/23 16:01:36.858
    STEP: When the matched label of one of its pods change 11/15/23 16:01:37.894
    Nov 15 16:01:37.913: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 11/15/23 16:01:37.958
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:01:38.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-7595" for this suite. 11/15/23 16:01:39.024
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:01:39.05
Nov 15 16:01:39.051: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename containers 11/15/23 16:01:39.054
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:01:39.092
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:01:39.105
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Nov 15 16:01:39.207: INFO: Waiting up to 5m0s for pod "client-containers-301108fa-0cc4-49e7-92bf-c36d8d02acc0" in namespace "containers-2741" to be "running"
Nov 15 16:01:39.226: INFO: Pod "client-containers-301108fa-0cc4-49e7-92bf-c36d8d02acc0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.540326ms
Nov 15 16:01:41.250: INFO: Pod "client-containers-301108fa-0cc4-49e7-92bf-c36d8d02acc0": Phase="Running", Reason="", readiness=true. Elapsed: 2.042978168s
Nov 15 16:01:41.250: INFO: Pod "client-containers-301108fa-0cc4-49e7-92bf-c36d8d02acc0" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Nov 15 16:01:41.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-2741" for this suite. 11/15/23 16:01:41.413
------------------------------
• [2.408 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:01:39.05
    Nov 15 16:01:39.051: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename containers 11/15/23 16:01:39.054
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:01:39.092
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:01:39.105
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Nov 15 16:01:39.207: INFO: Waiting up to 5m0s for pod "client-containers-301108fa-0cc4-49e7-92bf-c36d8d02acc0" in namespace "containers-2741" to be "running"
    Nov 15 16:01:39.226: INFO: Pod "client-containers-301108fa-0cc4-49e7-92bf-c36d8d02acc0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.540326ms
    Nov 15 16:01:41.250: INFO: Pod "client-containers-301108fa-0cc4-49e7-92bf-c36d8d02acc0": Phase="Running", Reason="", readiness=true. Elapsed: 2.042978168s
    Nov 15 16:01:41.250: INFO: Pod "client-containers-301108fa-0cc4-49e7-92bf-c36d8d02acc0" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:01:41.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-2741" for this suite. 11/15/23 16:01:41.413
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:01:41.459
Nov 15 16:01:41.459: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename webhook 11/15/23 16:01:41.463
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:01:41.525
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:01:41.547
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/15/23 16:01:41.624
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 16:01:42.122
STEP: Deploying the webhook pod 11/15/23 16:01:42.157
STEP: Wait for the deployment to be ready 11/15/23 16:01:42.222
Nov 15 16:01:42.269: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 15 16:01:44.332: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 16, 1, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 16, 1, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 16, 1, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 16, 1, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 11/15/23 16:01:46.352
STEP: Verifying the service has paired with the endpoint 11/15/23 16:01:46.426
Nov 15 16:01:47.427: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 11/15/23 16:01:47.454
STEP: create a configmap that should be updated by the webhook 11/15/23 16:01:47.576
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 16:01:47.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6888" for this suite. 11/15/23 16:01:47.939
STEP: Destroying namespace "webhook-6888-markers" for this suite. 11/15/23 16:01:47.969
------------------------------
• [SLOW TEST] [6.541 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:01:41.459
    Nov 15 16:01:41.459: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename webhook 11/15/23 16:01:41.463
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:01:41.525
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:01:41.547
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/15/23 16:01:41.624
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 16:01:42.122
    STEP: Deploying the webhook pod 11/15/23 16:01:42.157
    STEP: Wait for the deployment to be ready 11/15/23 16:01:42.222
    Nov 15 16:01:42.269: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Nov 15 16:01:44.332: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 16, 1, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 16, 1, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 16, 1, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 16, 1, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 11/15/23 16:01:46.352
    STEP: Verifying the service has paired with the endpoint 11/15/23 16:01:46.426
    Nov 15 16:01:47.427: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 11/15/23 16:01:47.454
    STEP: create a configmap that should be updated by the webhook 11/15/23 16:01:47.576
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:01:47.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6888" for this suite. 11/15/23 16:01:47.939
    STEP: Destroying namespace "webhook-6888-markers" for this suite. 11/15/23 16:01:47.969
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:01:48.02
Nov 15 16:01:48.021: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename crd-publish-openapi 11/15/23 16:01:48.022
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:01:48.088
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:01:48.108
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 11/15/23 16:01:48.129
Nov 15 16:01:48.131: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
Nov 15 16:01:50.764: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 16:02:00.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8312" for this suite. 11/15/23 16:02:00.804
------------------------------
• [SLOW TEST] [12.815 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:01:48.02
    Nov 15 16:01:48.021: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename crd-publish-openapi 11/15/23 16:01:48.022
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:01:48.088
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:01:48.108
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 11/15/23 16:01:48.129
    Nov 15 16:01:48.131: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    Nov 15 16:01:50.764: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:02:00.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8312" for this suite. 11/15/23 16:02:00.804
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:02:00.841
Nov 15 16:02:00.841: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename init-container 11/15/23 16:02:00.844
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:02:00.916
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:02:00.936
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 11/15/23 16:02:00.958
Nov 15 16:02:00.959: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Nov 15 16:02:06.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-5124" for this suite. 11/15/23 16:02:06.981
------------------------------
• [SLOW TEST] [6.171 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:02:00.841
    Nov 15 16:02:00.841: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename init-container 11/15/23 16:02:00.844
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:02:00.916
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:02:00.936
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 11/15/23 16:02:00.958
    Nov 15 16:02:00.959: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:02:06.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-5124" for this suite. 11/15/23 16:02:06.981
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:02:07.014
Nov 15 16:02:07.014: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename pods 11/15/23 16:02:07.017
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:02:07.077
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:02:07.097
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 11/15/23 16:02:07.116
STEP: submitting the pod to kubernetes 11/15/23 16:02:07.117
STEP: verifying QOS class is set on the pod 11/15/23 16:02:07.153
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Nov 15 16:02:07.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2105" for this suite. 11/15/23 16:02:07.211
------------------------------
• [0.226 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:02:07.014
    Nov 15 16:02:07.014: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename pods 11/15/23 16:02:07.017
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:02:07.077
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:02:07.097
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 11/15/23 16:02:07.116
    STEP: submitting the pod to kubernetes 11/15/23 16:02:07.117
    STEP: verifying QOS class is set on the pod 11/15/23 16:02:07.153
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:02:07.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2105" for this suite. 11/15/23 16:02:07.211
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:02:07.25
Nov 15 16:02:07.250: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename subpath 11/15/23 16:02:07.252
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:02:07.308
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:02:07.328
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 11/15/23 16:02:07.351
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-9dz8 11/15/23 16:02:07.395
STEP: Creating a pod to test atomic-volume-subpath 11/15/23 16:02:07.395
Nov 15 16:02:07.431: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-9dz8" in namespace "subpath-6436" to be "Succeeded or Failed"
Nov 15 16:02:07.454: INFO: Pod "pod-subpath-test-downwardapi-9dz8": Phase="Pending", Reason="", readiness=false. Elapsed: 22.558299ms
Nov 15 16:02:09.474: INFO: Pod "pod-subpath-test-downwardapi-9dz8": Phase="Running", Reason="", readiness=true. Elapsed: 2.042778174s
Nov 15 16:02:11.473: INFO: Pod "pod-subpath-test-downwardapi-9dz8": Phase="Running", Reason="", readiness=true. Elapsed: 4.041386959s
Nov 15 16:02:13.474: INFO: Pod "pod-subpath-test-downwardapi-9dz8": Phase="Running", Reason="", readiness=true. Elapsed: 6.042782256s
Nov 15 16:02:15.478: INFO: Pod "pod-subpath-test-downwardapi-9dz8": Phase="Running", Reason="", readiness=true. Elapsed: 8.046291052s
Nov 15 16:02:17.473: INFO: Pod "pod-subpath-test-downwardapi-9dz8": Phase="Running", Reason="", readiness=true. Elapsed: 10.041274887s
Nov 15 16:02:19.475: INFO: Pod "pod-subpath-test-downwardapi-9dz8": Phase="Running", Reason="", readiness=true. Elapsed: 12.043655604s
Nov 15 16:02:21.474: INFO: Pod "pod-subpath-test-downwardapi-9dz8": Phase="Running", Reason="", readiness=true. Elapsed: 14.042298182s
Nov 15 16:02:23.474: INFO: Pod "pod-subpath-test-downwardapi-9dz8": Phase="Running", Reason="", readiness=true. Elapsed: 16.042726974s
Nov 15 16:02:25.474: INFO: Pod "pod-subpath-test-downwardapi-9dz8": Phase="Running", Reason="", readiness=true. Elapsed: 18.042398664s
Nov 15 16:02:27.477: INFO: Pod "pod-subpath-test-downwardapi-9dz8": Phase="Running", Reason="", readiness=true. Elapsed: 20.045598314s
Nov 15 16:02:29.474: INFO: Pod "pod-subpath-test-downwardapi-9dz8": Phase="Running", Reason="", readiness=true. Elapsed: 22.043243609s
Nov 15 16:02:31.473: INFO: Pod "pod-subpath-test-downwardapi-9dz8": Phase="Running", Reason="", readiness=false. Elapsed: 24.042237885s
Nov 15 16:02:33.475: INFO: Pod "pod-subpath-test-downwardapi-9dz8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.043357862s
STEP: Saw pod success 11/15/23 16:02:33.475
Nov 15 16:02:33.476: INFO: Pod "pod-subpath-test-downwardapi-9dz8" satisfied condition "Succeeded or Failed"
Nov 15 16:02:33.494: INFO: Trying to get logs from node 10.15.40.115 pod pod-subpath-test-downwardapi-9dz8 container test-container-subpath-downwardapi-9dz8: <nil>
STEP: delete the pod 11/15/23 16:02:33.543
Nov 15 16:02:33.594: INFO: Waiting for pod pod-subpath-test-downwardapi-9dz8 to disappear
Nov 15 16:02:33.613: INFO: Pod pod-subpath-test-downwardapi-9dz8 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-9dz8 11/15/23 16:02:33.613
Nov 15 16:02:33.613: INFO: Deleting pod "pod-subpath-test-downwardapi-9dz8" in namespace "subpath-6436"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Nov 15 16:02:33.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-6436" for this suite. 11/15/23 16:02:33.661
------------------------------
• [SLOW TEST] [26.439 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:02:07.25
    Nov 15 16:02:07.250: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename subpath 11/15/23 16:02:07.252
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:02:07.308
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:02:07.328
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 11/15/23 16:02:07.351
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-9dz8 11/15/23 16:02:07.395
    STEP: Creating a pod to test atomic-volume-subpath 11/15/23 16:02:07.395
    Nov 15 16:02:07.431: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-9dz8" in namespace "subpath-6436" to be "Succeeded or Failed"
    Nov 15 16:02:07.454: INFO: Pod "pod-subpath-test-downwardapi-9dz8": Phase="Pending", Reason="", readiness=false. Elapsed: 22.558299ms
    Nov 15 16:02:09.474: INFO: Pod "pod-subpath-test-downwardapi-9dz8": Phase="Running", Reason="", readiness=true. Elapsed: 2.042778174s
    Nov 15 16:02:11.473: INFO: Pod "pod-subpath-test-downwardapi-9dz8": Phase="Running", Reason="", readiness=true. Elapsed: 4.041386959s
    Nov 15 16:02:13.474: INFO: Pod "pod-subpath-test-downwardapi-9dz8": Phase="Running", Reason="", readiness=true. Elapsed: 6.042782256s
    Nov 15 16:02:15.478: INFO: Pod "pod-subpath-test-downwardapi-9dz8": Phase="Running", Reason="", readiness=true. Elapsed: 8.046291052s
    Nov 15 16:02:17.473: INFO: Pod "pod-subpath-test-downwardapi-9dz8": Phase="Running", Reason="", readiness=true. Elapsed: 10.041274887s
    Nov 15 16:02:19.475: INFO: Pod "pod-subpath-test-downwardapi-9dz8": Phase="Running", Reason="", readiness=true. Elapsed: 12.043655604s
    Nov 15 16:02:21.474: INFO: Pod "pod-subpath-test-downwardapi-9dz8": Phase="Running", Reason="", readiness=true. Elapsed: 14.042298182s
    Nov 15 16:02:23.474: INFO: Pod "pod-subpath-test-downwardapi-9dz8": Phase="Running", Reason="", readiness=true. Elapsed: 16.042726974s
    Nov 15 16:02:25.474: INFO: Pod "pod-subpath-test-downwardapi-9dz8": Phase="Running", Reason="", readiness=true. Elapsed: 18.042398664s
    Nov 15 16:02:27.477: INFO: Pod "pod-subpath-test-downwardapi-9dz8": Phase="Running", Reason="", readiness=true. Elapsed: 20.045598314s
    Nov 15 16:02:29.474: INFO: Pod "pod-subpath-test-downwardapi-9dz8": Phase="Running", Reason="", readiness=true. Elapsed: 22.043243609s
    Nov 15 16:02:31.473: INFO: Pod "pod-subpath-test-downwardapi-9dz8": Phase="Running", Reason="", readiness=false. Elapsed: 24.042237885s
    Nov 15 16:02:33.475: INFO: Pod "pod-subpath-test-downwardapi-9dz8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.043357862s
    STEP: Saw pod success 11/15/23 16:02:33.475
    Nov 15 16:02:33.476: INFO: Pod "pod-subpath-test-downwardapi-9dz8" satisfied condition "Succeeded or Failed"
    Nov 15 16:02:33.494: INFO: Trying to get logs from node 10.15.40.115 pod pod-subpath-test-downwardapi-9dz8 container test-container-subpath-downwardapi-9dz8: <nil>
    STEP: delete the pod 11/15/23 16:02:33.543
    Nov 15 16:02:33.594: INFO: Waiting for pod pod-subpath-test-downwardapi-9dz8 to disappear
    Nov 15 16:02:33.613: INFO: Pod pod-subpath-test-downwardapi-9dz8 no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-9dz8 11/15/23 16:02:33.613
    Nov 15 16:02:33.613: INFO: Deleting pod "pod-subpath-test-downwardapi-9dz8" in namespace "subpath-6436"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:02:33.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-6436" for this suite. 11/15/23 16:02:33.661
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:02:33.706
Nov 15 16:02:33.706: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename kubectl 11/15/23 16:02:33.708
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:02:33.764
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:02:33.788
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 11/15/23 16:02:33.808
Nov 15 16:02:33.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-3497 create -f -'
Nov 15 16:02:34.645: INFO: stderr: ""
Nov 15 16:02:34.645: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 11/15/23 16:02:34.645
Nov 15 16:02:34.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-3497 diff -f -'
Nov 15 16:02:35.536: INFO: rc: 1
Nov 15 16:02:35.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-3497 delete -f -'
Nov 15 16:02:35.722: INFO: stderr: ""
Nov 15 16:02:35.722: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 15 16:02:35.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3497" for this suite. 11/15/23 16:02:35.753
------------------------------
• [2.079 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:02:33.706
    Nov 15 16:02:33.706: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename kubectl 11/15/23 16:02:33.708
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:02:33.764
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:02:33.788
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 11/15/23 16:02:33.808
    Nov 15 16:02:33.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-3497 create -f -'
    Nov 15 16:02:34.645: INFO: stderr: ""
    Nov 15 16:02:34.645: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 11/15/23 16:02:34.645
    Nov 15 16:02:34.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-3497 diff -f -'
    Nov 15 16:02:35.536: INFO: rc: 1
    Nov 15 16:02:35.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-3497 delete -f -'
    Nov 15 16:02:35.722: INFO: stderr: ""
    Nov 15 16:02:35.722: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:02:35.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3497" for this suite. 11/15/23 16:02:35.753
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:02:35.789
Nov 15 16:02:35.789: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename secrets 11/15/23 16:02:35.792
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:02:35.855
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:02:35.874
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-2796/secret-test-07d347bd-58f8-48e9-ba78-89536035c17d 11/15/23 16:02:35.896
STEP: Creating a pod to test consume secrets 11/15/23 16:02:35.918
Nov 15 16:02:35.958: INFO: Waiting up to 5m0s for pod "pod-configmaps-e1c62baf-37c2-404d-b5af-e3b36c300899" in namespace "secrets-2796" to be "Succeeded or Failed"
Nov 15 16:02:35.980: INFO: Pod "pod-configmaps-e1c62baf-37c2-404d-b5af-e3b36c300899": Phase="Pending", Reason="", readiness=false. Elapsed: 21.621861ms
Nov 15 16:02:38.006: INFO: Pod "pod-configmaps-e1c62baf-37c2-404d-b5af-e3b36c300899": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047318333s
Nov 15 16:02:39.999: INFO: Pod "pod-configmaps-e1c62baf-37c2-404d-b5af-e3b36c300899": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040525576s
Nov 15 16:02:41.999: INFO: Pod "pod-configmaps-e1c62baf-37c2-404d-b5af-e3b36c300899": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040481475s
STEP: Saw pod success 11/15/23 16:02:41.999
Nov 15 16:02:42.000: INFO: Pod "pod-configmaps-e1c62baf-37c2-404d-b5af-e3b36c300899" satisfied condition "Succeeded or Failed"
Nov 15 16:02:42.019: INFO: Trying to get logs from node 10.15.40.115 pod pod-configmaps-e1c62baf-37c2-404d-b5af-e3b36c300899 container env-test: <nil>
STEP: delete the pod 11/15/23 16:02:42.065
Nov 15 16:02:42.120: INFO: Waiting for pod pod-configmaps-e1c62baf-37c2-404d-b5af-e3b36c300899 to disappear
Nov 15 16:02:42.137: INFO: Pod pod-configmaps-e1c62baf-37c2-404d-b5af-e3b36c300899 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 15 16:02:42.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2796" for this suite. 11/15/23 16:02:42.169
------------------------------
• [SLOW TEST] [6.412 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:02:35.789
    Nov 15 16:02:35.789: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename secrets 11/15/23 16:02:35.792
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:02:35.855
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:02:35.874
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-2796/secret-test-07d347bd-58f8-48e9-ba78-89536035c17d 11/15/23 16:02:35.896
    STEP: Creating a pod to test consume secrets 11/15/23 16:02:35.918
    Nov 15 16:02:35.958: INFO: Waiting up to 5m0s for pod "pod-configmaps-e1c62baf-37c2-404d-b5af-e3b36c300899" in namespace "secrets-2796" to be "Succeeded or Failed"
    Nov 15 16:02:35.980: INFO: Pod "pod-configmaps-e1c62baf-37c2-404d-b5af-e3b36c300899": Phase="Pending", Reason="", readiness=false. Elapsed: 21.621861ms
    Nov 15 16:02:38.006: INFO: Pod "pod-configmaps-e1c62baf-37c2-404d-b5af-e3b36c300899": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047318333s
    Nov 15 16:02:39.999: INFO: Pod "pod-configmaps-e1c62baf-37c2-404d-b5af-e3b36c300899": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040525576s
    Nov 15 16:02:41.999: INFO: Pod "pod-configmaps-e1c62baf-37c2-404d-b5af-e3b36c300899": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040481475s
    STEP: Saw pod success 11/15/23 16:02:41.999
    Nov 15 16:02:42.000: INFO: Pod "pod-configmaps-e1c62baf-37c2-404d-b5af-e3b36c300899" satisfied condition "Succeeded or Failed"
    Nov 15 16:02:42.019: INFO: Trying to get logs from node 10.15.40.115 pod pod-configmaps-e1c62baf-37c2-404d-b5af-e3b36c300899 container env-test: <nil>
    STEP: delete the pod 11/15/23 16:02:42.065
    Nov 15 16:02:42.120: INFO: Waiting for pod pod-configmaps-e1c62baf-37c2-404d-b5af-e3b36c300899 to disappear
    Nov 15 16:02:42.137: INFO: Pod pod-configmaps-e1c62baf-37c2-404d-b5af-e3b36c300899 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:02:42.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2796" for this suite. 11/15/23 16:02:42.169
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:02:42.201
Nov 15 16:02:42.201: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename subpath 11/15/23 16:02:42.204
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:02:42.26
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:02:42.283
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 11/15/23 16:02:42.301
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-xt57 11/15/23 16:02:42.34
STEP: Creating a pod to test atomic-volume-subpath 11/15/23 16:02:42.34
Nov 15 16:02:42.373: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-xt57" in namespace "subpath-2826" to be "Succeeded or Failed"
Nov 15 16:02:42.392: INFO: Pod "pod-subpath-test-configmap-xt57": Phase="Pending", Reason="", readiness=false. Elapsed: 19.305521ms
Nov 15 16:02:44.412: INFO: Pod "pod-subpath-test-configmap-xt57": Phase="Running", Reason="", readiness=true. Elapsed: 2.03935117s
Nov 15 16:02:46.413: INFO: Pod "pod-subpath-test-configmap-xt57": Phase="Running", Reason="", readiness=true. Elapsed: 4.039763199s
Nov 15 16:02:48.417: INFO: Pod "pod-subpath-test-configmap-xt57": Phase="Running", Reason="", readiness=true. Elapsed: 6.044327687s
Nov 15 16:02:50.413: INFO: Pod "pod-subpath-test-configmap-xt57": Phase="Running", Reason="", readiness=true. Elapsed: 8.039469589s
Nov 15 16:02:52.412: INFO: Pod "pod-subpath-test-configmap-xt57": Phase="Running", Reason="", readiness=true. Elapsed: 10.038781778s
Nov 15 16:02:54.414: INFO: Pod "pod-subpath-test-configmap-xt57": Phase="Running", Reason="", readiness=true. Elapsed: 12.040853178s
Nov 15 16:02:56.412: INFO: Pod "pod-subpath-test-configmap-xt57": Phase="Running", Reason="", readiness=true. Elapsed: 14.039068194s
Nov 15 16:02:58.412: INFO: Pod "pod-subpath-test-configmap-xt57": Phase="Running", Reason="", readiness=true. Elapsed: 16.038586692s
Nov 15 16:03:00.414: INFO: Pod "pod-subpath-test-configmap-xt57": Phase="Running", Reason="", readiness=true. Elapsed: 18.041198661s
Nov 15 16:03:02.417: INFO: Pod "pod-subpath-test-configmap-xt57": Phase="Running", Reason="", readiness=true. Elapsed: 20.043441444s
Nov 15 16:03:04.411: INFO: Pod "pod-subpath-test-configmap-xt57": Phase="Running", Reason="", readiness=false. Elapsed: 22.038268368s
Nov 15 16:03:06.411: INFO: Pod "pod-subpath-test-configmap-xt57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.038176622s
STEP: Saw pod success 11/15/23 16:03:06.411
Nov 15 16:03:06.412: INFO: Pod "pod-subpath-test-configmap-xt57" satisfied condition "Succeeded or Failed"
Nov 15 16:03:06.431: INFO: Trying to get logs from node 10.15.40.115 pod pod-subpath-test-configmap-xt57 container test-container-subpath-configmap-xt57: <nil>
STEP: delete the pod 11/15/23 16:03:06.476
Nov 15 16:03:06.527: INFO: Waiting for pod pod-subpath-test-configmap-xt57 to disappear
Nov 15 16:03:06.545: INFO: Pod pod-subpath-test-configmap-xt57 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-xt57 11/15/23 16:03:06.545
Nov 15 16:03:06.546: INFO: Deleting pod "pod-subpath-test-configmap-xt57" in namespace "subpath-2826"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Nov 15 16:03:06.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-2826" for this suite. 11/15/23 16:03:06.598
------------------------------
• [SLOW TEST] [24.426 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:02:42.201
    Nov 15 16:02:42.201: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename subpath 11/15/23 16:02:42.204
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:02:42.26
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:02:42.283
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 11/15/23 16:02:42.301
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-xt57 11/15/23 16:02:42.34
    STEP: Creating a pod to test atomic-volume-subpath 11/15/23 16:02:42.34
    Nov 15 16:02:42.373: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-xt57" in namespace "subpath-2826" to be "Succeeded or Failed"
    Nov 15 16:02:42.392: INFO: Pod "pod-subpath-test-configmap-xt57": Phase="Pending", Reason="", readiness=false. Elapsed: 19.305521ms
    Nov 15 16:02:44.412: INFO: Pod "pod-subpath-test-configmap-xt57": Phase="Running", Reason="", readiness=true. Elapsed: 2.03935117s
    Nov 15 16:02:46.413: INFO: Pod "pod-subpath-test-configmap-xt57": Phase="Running", Reason="", readiness=true. Elapsed: 4.039763199s
    Nov 15 16:02:48.417: INFO: Pod "pod-subpath-test-configmap-xt57": Phase="Running", Reason="", readiness=true. Elapsed: 6.044327687s
    Nov 15 16:02:50.413: INFO: Pod "pod-subpath-test-configmap-xt57": Phase="Running", Reason="", readiness=true. Elapsed: 8.039469589s
    Nov 15 16:02:52.412: INFO: Pod "pod-subpath-test-configmap-xt57": Phase="Running", Reason="", readiness=true. Elapsed: 10.038781778s
    Nov 15 16:02:54.414: INFO: Pod "pod-subpath-test-configmap-xt57": Phase="Running", Reason="", readiness=true. Elapsed: 12.040853178s
    Nov 15 16:02:56.412: INFO: Pod "pod-subpath-test-configmap-xt57": Phase="Running", Reason="", readiness=true. Elapsed: 14.039068194s
    Nov 15 16:02:58.412: INFO: Pod "pod-subpath-test-configmap-xt57": Phase="Running", Reason="", readiness=true. Elapsed: 16.038586692s
    Nov 15 16:03:00.414: INFO: Pod "pod-subpath-test-configmap-xt57": Phase="Running", Reason="", readiness=true. Elapsed: 18.041198661s
    Nov 15 16:03:02.417: INFO: Pod "pod-subpath-test-configmap-xt57": Phase="Running", Reason="", readiness=true. Elapsed: 20.043441444s
    Nov 15 16:03:04.411: INFO: Pod "pod-subpath-test-configmap-xt57": Phase="Running", Reason="", readiness=false. Elapsed: 22.038268368s
    Nov 15 16:03:06.411: INFO: Pod "pod-subpath-test-configmap-xt57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.038176622s
    STEP: Saw pod success 11/15/23 16:03:06.411
    Nov 15 16:03:06.412: INFO: Pod "pod-subpath-test-configmap-xt57" satisfied condition "Succeeded or Failed"
    Nov 15 16:03:06.431: INFO: Trying to get logs from node 10.15.40.115 pod pod-subpath-test-configmap-xt57 container test-container-subpath-configmap-xt57: <nil>
    STEP: delete the pod 11/15/23 16:03:06.476
    Nov 15 16:03:06.527: INFO: Waiting for pod pod-subpath-test-configmap-xt57 to disappear
    Nov 15 16:03:06.545: INFO: Pod pod-subpath-test-configmap-xt57 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-xt57 11/15/23 16:03:06.545
    Nov 15 16:03:06.546: INFO: Deleting pod "pod-subpath-test-configmap-xt57" in namespace "subpath-2826"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:03:06.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-2826" for this suite. 11/15/23 16:03:06.598
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:03:06.633
Nov 15 16:03:06.633: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename configmap 11/15/23 16:03:06.637
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:03:06.699
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:03:06.718
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-384bccbf-fe97-4c66-94eb-ea437c4e8089 11/15/23 16:03:06.771
STEP: Creating the pod 11/15/23 16:03:06.813
Nov 15 16:03:06.850: INFO: Waiting up to 5m0s for pod "pod-configmaps-85e834f5-d10c-4174-bc82-a296bd1960cf" in namespace "configmap-4505" to be "running"
Nov 15 16:03:06.868: INFO: Pod "pod-configmaps-85e834f5-d10c-4174-bc82-a296bd1960cf": Phase="Pending", Reason="", readiness=false. Elapsed: 18.348977ms
Nov 15 16:03:08.888: INFO: Pod "pod-configmaps-85e834f5-d10c-4174-bc82-a296bd1960cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038022543s
Nov 15 16:03:10.896: INFO: Pod "pod-configmaps-85e834f5-d10c-4174-bc82-a296bd1960cf": Phase="Running", Reason="", readiness=false. Elapsed: 4.046333741s
Nov 15 16:03:10.896: INFO: Pod "pod-configmaps-85e834f5-d10c-4174-bc82-a296bd1960cf" satisfied condition "running"
STEP: Waiting for pod with text data 11/15/23 16:03:10.896
STEP: Waiting for pod with binary data 11/15/23 16:03:10.945
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 15 16:03:10.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4505" for this suite. 11/15/23 16:03:11.012
------------------------------
• [4.411 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:03:06.633
    Nov 15 16:03:06.633: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename configmap 11/15/23 16:03:06.637
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:03:06.699
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:03:06.718
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-384bccbf-fe97-4c66-94eb-ea437c4e8089 11/15/23 16:03:06.771
    STEP: Creating the pod 11/15/23 16:03:06.813
    Nov 15 16:03:06.850: INFO: Waiting up to 5m0s for pod "pod-configmaps-85e834f5-d10c-4174-bc82-a296bd1960cf" in namespace "configmap-4505" to be "running"
    Nov 15 16:03:06.868: INFO: Pod "pod-configmaps-85e834f5-d10c-4174-bc82-a296bd1960cf": Phase="Pending", Reason="", readiness=false. Elapsed: 18.348977ms
    Nov 15 16:03:08.888: INFO: Pod "pod-configmaps-85e834f5-d10c-4174-bc82-a296bd1960cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038022543s
    Nov 15 16:03:10.896: INFO: Pod "pod-configmaps-85e834f5-d10c-4174-bc82-a296bd1960cf": Phase="Running", Reason="", readiness=false. Elapsed: 4.046333741s
    Nov 15 16:03:10.896: INFO: Pod "pod-configmaps-85e834f5-d10c-4174-bc82-a296bd1960cf" satisfied condition "running"
    STEP: Waiting for pod with text data 11/15/23 16:03:10.896
    STEP: Waiting for pod with binary data 11/15/23 16:03:10.945
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:03:10.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4505" for this suite. 11/15/23 16:03:11.012
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:03:11.052
Nov 15 16:03:11.053: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename cronjob 11/15/23 16:03:11.055
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:03:11.111
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:03:11.131
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 11/15/23 16:03:11.15
STEP: Ensuring a job is scheduled 11/15/23 16:03:11.177
STEP: Ensuring exactly one is scheduled 11/15/23 16:04:01.198
STEP: Ensuring exactly one running job exists by listing jobs explicitly 11/15/23 16:04:01.219
STEP: Ensuring the job is replaced with a new one 11/15/23 16:04:01.245
STEP: Removing cronjob 11/15/23 16:05:01.278
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Nov 15 16:05:01.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-6825" for this suite. 11/15/23 16:05:01.344
------------------------------
• [SLOW TEST] [110.330 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:03:11.052
    Nov 15 16:03:11.053: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename cronjob 11/15/23 16:03:11.055
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:03:11.111
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:03:11.131
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 11/15/23 16:03:11.15
    STEP: Ensuring a job is scheduled 11/15/23 16:03:11.177
    STEP: Ensuring exactly one is scheduled 11/15/23 16:04:01.198
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 11/15/23 16:04:01.219
    STEP: Ensuring the job is replaced with a new one 11/15/23 16:04:01.245
    STEP: Removing cronjob 11/15/23 16:05:01.278
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:05:01.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-6825" for this suite. 11/15/23 16:05:01.344
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:05:01.396
Nov 15 16:05:01.397: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename emptydir 11/15/23 16:05:01.398
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:05:01.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:05:01.49
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 11/15/23 16:05:01.51
Nov 15 16:05:01.570: INFO: Waiting up to 5m0s for pod "pod-9c069baf-3802-46e6-9580-1195de6df161" in namespace "emptydir-599" to be "Succeeded or Failed"
Nov 15 16:05:01.588: INFO: Pod "pod-9c069baf-3802-46e6-9580-1195de6df161": Phase="Pending", Reason="", readiness=false. Elapsed: 18.375642ms
Nov 15 16:05:03.607: INFO: Pod "pod-9c069baf-3802-46e6-9580-1195de6df161": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036485372s
Nov 15 16:05:05.609: INFO: Pod "pod-9c069baf-3802-46e6-9580-1195de6df161": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038808963s
Nov 15 16:05:07.607: INFO: Pod "pod-9c069baf-3802-46e6-9580-1195de6df161": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037220333s
STEP: Saw pod success 11/15/23 16:05:07.607
Nov 15 16:05:07.608: INFO: Pod "pod-9c069baf-3802-46e6-9580-1195de6df161" satisfied condition "Succeeded or Failed"
Nov 15 16:05:07.626: INFO: Trying to get logs from node 10.15.40.115 pod pod-9c069baf-3802-46e6-9580-1195de6df161 container test-container: <nil>
STEP: delete the pod 11/15/23 16:05:07.75
Nov 15 16:05:07.829: INFO: Waiting for pod pod-9c069baf-3802-46e6-9580-1195de6df161 to disappear
Nov 15 16:05:07.849: INFO: Pod pod-9c069baf-3802-46e6-9580-1195de6df161 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 15 16:05:07.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-599" for this suite. 11/15/23 16:05:07.883
------------------------------
• [SLOW TEST] [6.515 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:05:01.396
    Nov 15 16:05:01.397: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename emptydir 11/15/23 16:05:01.398
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:05:01.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:05:01.49
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 11/15/23 16:05:01.51
    Nov 15 16:05:01.570: INFO: Waiting up to 5m0s for pod "pod-9c069baf-3802-46e6-9580-1195de6df161" in namespace "emptydir-599" to be "Succeeded or Failed"
    Nov 15 16:05:01.588: INFO: Pod "pod-9c069baf-3802-46e6-9580-1195de6df161": Phase="Pending", Reason="", readiness=false. Elapsed: 18.375642ms
    Nov 15 16:05:03.607: INFO: Pod "pod-9c069baf-3802-46e6-9580-1195de6df161": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036485372s
    Nov 15 16:05:05.609: INFO: Pod "pod-9c069baf-3802-46e6-9580-1195de6df161": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038808963s
    Nov 15 16:05:07.607: INFO: Pod "pod-9c069baf-3802-46e6-9580-1195de6df161": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037220333s
    STEP: Saw pod success 11/15/23 16:05:07.607
    Nov 15 16:05:07.608: INFO: Pod "pod-9c069baf-3802-46e6-9580-1195de6df161" satisfied condition "Succeeded or Failed"
    Nov 15 16:05:07.626: INFO: Trying to get logs from node 10.15.40.115 pod pod-9c069baf-3802-46e6-9580-1195de6df161 container test-container: <nil>
    STEP: delete the pod 11/15/23 16:05:07.75
    Nov 15 16:05:07.829: INFO: Waiting for pod pod-9c069baf-3802-46e6-9580-1195de6df161 to disappear
    Nov 15 16:05:07.849: INFO: Pod pod-9c069baf-3802-46e6-9580-1195de6df161 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:05:07.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-599" for this suite. 11/15/23 16:05:07.883
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:05:07.914
Nov 15 16:05:07.914: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename init-container 11/15/23 16:05:07.918
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:05:07.986
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:05:08.006
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 11/15/23 16:05:08.026
Nov 15 16:05:08.027: INFO: PodSpec: initContainers in spec.initContainers
Nov 15 16:05:54.177: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-aecf159d-237f-49ab-957c-849ae4accb7b", GenerateName:"", Namespace:"init-container-6242", SelfLink:"", UID:"9ea79d69-f592-4e7f-8d6b-e7e46da91306", ResourceVersion:"31933", Generation:0, CreationTimestamp:time.Date(2023, time.November, 15, 16, 5, 8, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"27459986"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"46d2c7e1150e8b0dbbefeebf8d2ad8e2fce4ef847a4bc28fe3068860f630a307", "cni.projectcalico.org/podIP":"172.30.164.58/32", "cni.projectcalico.org/podIPs":"172.30.164.58/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.November, 15, 16, 5, 8, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005405110), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.November, 15, 16, 5, 9, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005405140), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.November, 15, 16, 5, 54, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005405170), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-dbmm4", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00468d960), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-dbmm4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-dbmm4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-dbmm4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0037206c8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.15.40.115", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0002b0460), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003720750)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003720770)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003720778), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00372077c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000f62d30), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.November, 15, 16, 5, 8, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.November, 15, 16, 5, 8, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.November, 15, 16, 5, 8, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.November, 15, 16, 5, 8, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.15.40.115", PodIP:"172.30.164.58", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.164.58"}}, StartTime:time.Date(2023, time.November, 15, 16, 5, 8, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0002b0700)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0002b0770)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://aab28915f3f80b129d3e03f77b586d8ce3351792173a4cdafd817e62293794ac", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00468d9e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00468d9c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc0037207ff)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Nov 15 16:05:54.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-6242" for this suite. 11/15/23 16:05:54.205
------------------------------
• [SLOW TEST] [46.326 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:05:07.914
    Nov 15 16:05:07.914: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename init-container 11/15/23 16:05:07.918
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:05:07.986
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:05:08.006
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 11/15/23 16:05:08.026
    Nov 15 16:05:08.027: INFO: PodSpec: initContainers in spec.initContainers
    Nov 15 16:05:54.177: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-aecf159d-237f-49ab-957c-849ae4accb7b", GenerateName:"", Namespace:"init-container-6242", SelfLink:"", UID:"9ea79d69-f592-4e7f-8d6b-e7e46da91306", ResourceVersion:"31933", Generation:0, CreationTimestamp:time.Date(2023, time.November, 15, 16, 5, 8, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"27459986"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"46d2c7e1150e8b0dbbefeebf8d2ad8e2fce4ef847a4bc28fe3068860f630a307", "cni.projectcalico.org/podIP":"172.30.164.58/32", "cni.projectcalico.org/podIPs":"172.30.164.58/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.November, 15, 16, 5, 8, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005405110), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.November, 15, 16, 5, 9, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005405140), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.November, 15, 16, 5, 54, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005405170), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-dbmm4", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00468d960), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-dbmm4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-dbmm4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-dbmm4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0037206c8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.15.40.115", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0002b0460), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003720750)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003720770)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003720778), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00372077c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000f62d30), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.November, 15, 16, 5, 8, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.November, 15, 16, 5, 8, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.November, 15, 16, 5, 8, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.November, 15, 16, 5, 8, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.15.40.115", PodIP:"172.30.164.58", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.164.58"}}, StartTime:time.Date(2023, time.November, 15, 16, 5, 8, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0002b0700)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0002b0770)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://aab28915f3f80b129d3e03f77b586d8ce3351792173a4cdafd817e62293794ac", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00468d9e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00468d9c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc0037207ff)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:05:54.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-6242" for this suite. 11/15/23 16:05:54.205
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:05:54.25
Nov 15 16:05:54.250: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename resourcequota 11/15/23 16:05:54.252
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:05:54.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:05:54.336
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 11/15/23 16:05:54.357
STEP: Ensuring ResourceQuota status is calculated 11/15/23 16:05:54.384
STEP: Creating a ResourceQuota with not terminating scope 11/15/23 16:05:56.411
STEP: Ensuring ResourceQuota status is calculated 11/15/23 16:05:56.436
STEP: Creating a long running pod 11/15/23 16:05:58.458
STEP: Ensuring resource quota with not terminating scope captures the pod usage 11/15/23 16:05:58.534
STEP: Ensuring resource quota with terminating scope ignored the pod usage 11/15/23 16:06:00.559
STEP: Deleting the pod 11/15/23 16:06:02.581
STEP: Ensuring resource quota status released the pod usage 11/15/23 16:06:02.621
STEP: Creating a terminating pod 11/15/23 16:06:04.642
STEP: Ensuring resource quota with terminating scope captures the pod usage 11/15/23 16:06:04.692
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 11/15/23 16:06:06.718
STEP: Deleting the pod 11/15/23 16:06:08.74
STEP: Ensuring resource quota status released the pod usage 11/15/23 16:06:08.789
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Nov 15 16:06:10.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3418" for this suite. 11/15/23 16:06:10.844
------------------------------
• [SLOW TEST] [16.625 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:05:54.25
    Nov 15 16:05:54.250: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename resourcequota 11/15/23 16:05:54.252
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:05:54.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:05:54.336
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 11/15/23 16:05:54.357
    STEP: Ensuring ResourceQuota status is calculated 11/15/23 16:05:54.384
    STEP: Creating a ResourceQuota with not terminating scope 11/15/23 16:05:56.411
    STEP: Ensuring ResourceQuota status is calculated 11/15/23 16:05:56.436
    STEP: Creating a long running pod 11/15/23 16:05:58.458
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 11/15/23 16:05:58.534
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 11/15/23 16:06:00.559
    STEP: Deleting the pod 11/15/23 16:06:02.581
    STEP: Ensuring resource quota status released the pod usage 11/15/23 16:06:02.621
    STEP: Creating a terminating pod 11/15/23 16:06:04.642
    STEP: Ensuring resource quota with terminating scope captures the pod usage 11/15/23 16:06:04.692
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 11/15/23 16:06:06.718
    STEP: Deleting the pod 11/15/23 16:06:08.74
    STEP: Ensuring resource quota status released the pod usage 11/15/23 16:06:08.789
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:06:10.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3418" for this suite. 11/15/23 16:06:10.844
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:06:10.882
Nov 15 16:06:10.882: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename webhook 11/15/23 16:06:10.884
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:06:10.941
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:06:10.962
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/15/23 16:06:11.038
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 16:06:11.523
STEP: Deploying the webhook pod 11/15/23 16:06:11.558
STEP: Wait for the deployment to be ready 11/15/23 16:06:11.602
Nov 15 16:06:11.656: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/15/23 16:06:13.715
STEP: Verifying the service has paired with the endpoint 11/15/23 16:06:13.769
Nov 15 16:06:14.770: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Nov 15 16:06:14.795: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Registering the custom resource webhook via the AdmissionRegistration API 11/15/23 16:06:15.338
STEP: Creating a custom resource that should be denied by the webhook 11/15/23 16:06:15.473
STEP: Creating a custom resource whose deletion would be denied by the webhook 11/15/23 16:06:17.652
STEP: Updating the custom resource with disallowed data should be denied 11/15/23 16:06:17.698
STEP: Deleting the custom resource should be denied 11/15/23 16:06:17.757
STEP: Remove the offending key and value from the custom resource data 11/15/23 16:06:17.803
STEP: Deleting the updated custom resource should be successful 11/15/23 16:06:17.872
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 16:06:18.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9918" for this suite. 11/15/23 16:06:18.731
STEP: Destroying namespace "webhook-9918-markers" for this suite. 11/15/23 16:06:18.775
------------------------------
• [SLOW TEST] [7.922 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:06:10.882
    Nov 15 16:06:10.882: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename webhook 11/15/23 16:06:10.884
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:06:10.941
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:06:10.962
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/15/23 16:06:11.038
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 16:06:11.523
    STEP: Deploying the webhook pod 11/15/23 16:06:11.558
    STEP: Wait for the deployment to be ready 11/15/23 16:06:11.602
    Nov 15 16:06:11.656: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/15/23 16:06:13.715
    STEP: Verifying the service has paired with the endpoint 11/15/23 16:06:13.769
    Nov 15 16:06:14.770: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Nov 15 16:06:14.795: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 11/15/23 16:06:15.338
    STEP: Creating a custom resource that should be denied by the webhook 11/15/23 16:06:15.473
    STEP: Creating a custom resource whose deletion would be denied by the webhook 11/15/23 16:06:17.652
    STEP: Updating the custom resource with disallowed data should be denied 11/15/23 16:06:17.698
    STEP: Deleting the custom resource should be denied 11/15/23 16:06:17.757
    STEP: Remove the offending key and value from the custom resource data 11/15/23 16:06:17.803
    STEP: Deleting the updated custom resource should be successful 11/15/23 16:06:17.872
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:06:18.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9918" for this suite. 11/15/23 16:06:18.731
    STEP: Destroying namespace "webhook-9918-markers" for this suite. 11/15/23 16:06:18.775
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:06:18.809
Nov 15 16:06:18.809: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename csiinlinevolumes 11/15/23 16:06:18.811
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:06:18.874
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:06:18.893
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 11/15/23 16:06:18.912
STEP: getting 11/15/23 16:06:19.005
STEP: listing in namespace 11/15/23 16:06:19.029
STEP: patching 11/15/23 16:06:19.069
STEP: deleting 11/15/23 16:06:19.096
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Nov 15 16:06:19.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-9091" for this suite. 11/15/23 16:06:19.178
------------------------------
• [0.399 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:06:18.809
    Nov 15 16:06:18.809: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename csiinlinevolumes 11/15/23 16:06:18.811
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:06:18.874
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:06:18.893
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 11/15/23 16:06:18.912
    STEP: getting 11/15/23 16:06:19.005
    STEP: listing in namespace 11/15/23 16:06:19.029
    STEP: patching 11/15/23 16:06:19.069
    STEP: deleting 11/15/23 16:06:19.096
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:06:19.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-9091" for this suite. 11/15/23 16:06:19.178
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:06:19.214
Nov 15 16:06:19.214: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename prestop 11/15/23 16:06:19.217
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:06:19.298
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:06:19.318
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-1431 11/15/23 16:06:19.339
STEP: Waiting for pods to come up. 11/15/23 16:06:19.385
Nov 15 16:06:19.386: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-1431" to be "running"
Nov 15 16:06:19.405: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 18.870168ms
Nov 15 16:06:21.423: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.037445647s
Nov 15 16:06:21.423: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-1431 11/15/23 16:06:21.441
Nov 15 16:06:21.463: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-1431" to be "running"
Nov 15 16:06:21.480: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 17.055102ms
Nov 15 16:06:23.500: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.037126293s
Nov 15 16:06:23.500: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 11/15/23 16:06:23.5
Nov 15 16:06:28.630: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 11/15/23 16:06:28.631
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Nov 15 16:06:28.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-1431" for this suite. 11/15/23 16:06:28.712
------------------------------
• [SLOW TEST] [9.529 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:06:19.214
    Nov 15 16:06:19.214: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename prestop 11/15/23 16:06:19.217
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:06:19.298
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:06:19.318
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-1431 11/15/23 16:06:19.339
    STEP: Waiting for pods to come up. 11/15/23 16:06:19.385
    Nov 15 16:06:19.386: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-1431" to be "running"
    Nov 15 16:06:19.405: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 18.870168ms
    Nov 15 16:06:21.423: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.037445647s
    Nov 15 16:06:21.423: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-1431 11/15/23 16:06:21.441
    Nov 15 16:06:21.463: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-1431" to be "running"
    Nov 15 16:06:21.480: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 17.055102ms
    Nov 15 16:06:23.500: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.037126293s
    Nov 15 16:06:23.500: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 11/15/23 16:06:23.5
    Nov 15 16:06:28.630: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 11/15/23 16:06:28.631
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:06:28.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-1431" for this suite. 11/15/23 16:06:28.712
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:06:28.745
Nov 15 16:06:28.746: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename deployment 11/15/23 16:06:28.748
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:06:28.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:06:28.833
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Nov 15 16:06:28.853: INFO: Creating deployment "test-recreate-deployment"
Nov 15 16:06:28.886: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Nov 15 16:06:28.925: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Nov 15 16:06:30.966: INFO: Waiting deployment "test-recreate-deployment" to complete
Nov 15 16:06:30.988: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Nov 15 16:06:31.031: INFO: Updating deployment test-recreate-deployment
Nov 15 16:06:31.031: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Nov 15 16:06:31.223: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-2074  425c4f57-ff3c-4f0b-936d-7b3ac30fbf51 32204 2 2023-11-15 16:06:28 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-11-15 16:06:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 16:06:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00466a4d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-11-15 16:06:31 +0000 UTC,LastTransitionTime:2023-11-15 16:06:31 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-11-15 16:06:31 +0000 UTC,LastTransitionTime:2023-11-15 16:06:28 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Nov 15 16:06:31.244: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-2074  24bd9bef-bdd7-424c-aaad-1725259fb609 32203 1 2023-11-15 16:06:31 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 425c4f57-ff3c-4f0b-936d-7b3ac30fbf51 0xc001115b40 0xc001115b41}] [] [{kube-controller-manager Update apps/v1 2023-11-15 16:06:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"425c4f57-ff3c-4f0b-936d-7b3ac30fbf51\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 16:06:31 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001115bd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 15 16:06:31.245: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Nov 15 16:06:31.246: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-2074  bceaeb6b-6d72-429b-9464-649f53e98309 32194 2 2023-11-15 16:06:28 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 425c4f57-ff3c-4f0b-936d-7b3ac30fbf51 0xc001115a27 0xc001115a28}] [] [{kube-controller-manager Update apps/v1 2023-11-15 16:06:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"425c4f57-ff3c-4f0b-936d-7b3ac30fbf51\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 16:06:31 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001115ad8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 15 16:06:31.266: INFO: Pod "test-recreate-deployment-cff6dc657-f288c" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-f288c test-recreate-deployment-cff6dc657- deployment-2074  4ffe041f-abf4-44a5-ab6f-23be08593e0e 32206 0 2023-11-15 16:06:31 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 24bd9bef-bdd7-424c-aaad-1725259fb609 0xc00466b350 0xc00466b351}] [] [{kube-controller-manager Update v1 2023-11-15 16:06:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"24bd9bef-bdd7-424c-aaad-1725259fb609\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 16:06:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f7nkf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f7nkf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:06:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:06:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:06:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:06:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:,StartTime:2023-11-15 16:06:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Nov 15 16:06:31.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2074" for this suite. 11/15/23 16:06:31.299
------------------------------
• [2.585 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:06:28.745
    Nov 15 16:06:28.746: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename deployment 11/15/23 16:06:28.748
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:06:28.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:06:28.833
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Nov 15 16:06:28.853: INFO: Creating deployment "test-recreate-deployment"
    Nov 15 16:06:28.886: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Nov 15 16:06:28.925: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Nov 15 16:06:30.966: INFO: Waiting deployment "test-recreate-deployment" to complete
    Nov 15 16:06:30.988: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Nov 15 16:06:31.031: INFO: Updating deployment test-recreate-deployment
    Nov 15 16:06:31.031: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Nov 15 16:06:31.223: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-2074  425c4f57-ff3c-4f0b-936d-7b3ac30fbf51 32204 2 2023-11-15 16:06:28 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-11-15 16:06:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 16:06:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00466a4d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-11-15 16:06:31 +0000 UTC,LastTransitionTime:2023-11-15 16:06:31 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-11-15 16:06:31 +0000 UTC,LastTransitionTime:2023-11-15 16:06:28 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Nov 15 16:06:31.244: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-2074  24bd9bef-bdd7-424c-aaad-1725259fb609 32203 1 2023-11-15 16:06:31 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 425c4f57-ff3c-4f0b-936d-7b3ac30fbf51 0xc001115b40 0xc001115b41}] [] [{kube-controller-manager Update apps/v1 2023-11-15 16:06:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"425c4f57-ff3c-4f0b-936d-7b3ac30fbf51\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 16:06:31 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001115bd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Nov 15 16:06:31.245: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Nov 15 16:06:31.246: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-2074  bceaeb6b-6d72-429b-9464-649f53e98309 32194 2 2023-11-15 16:06:28 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 425c4f57-ff3c-4f0b-936d-7b3ac30fbf51 0xc001115a27 0xc001115a28}] [] [{kube-controller-manager Update apps/v1 2023-11-15 16:06:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"425c4f57-ff3c-4f0b-936d-7b3ac30fbf51\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 16:06:31 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001115ad8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Nov 15 16:06:31.266: INFO: Pod "test-recreate-deployment-cff6dc657-f288c" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-f288c test-recreate-deployment-cff6dc657- deployment-2074  4ffe041f-abf4-44a5-ab6f-23be08593e0e 32206 0 2023-11-15 16:06:31 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 24bd9bef-bdd7-424c-aaad-1725259fb609 0xc00466b350 0xc00466b351}] [] [{kube-controller-manager Update v1 2023-11-15 16:06:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"24bd9bef-bdd7-424c-aaad-1725259fb609\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 16:06:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f7nkf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f7nkf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:06:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:06:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:06:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:06:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:,StartTime:2023-11-15 16:06:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:06:31.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2074" for this suite. 11/15/23 16:06:31.299
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:06:31.347
Nov 15 16:06:31.347: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename crd-publish-openapi 11/15/23 16:06:31.349
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:06:31.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:06:31.423
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Nov 15 16:06:31.443: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 11/15/23 16:06:33.989
Nov 15 16:06:33.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-3984 --namespace=crd-publish-openapi-3984 create -f -'
Nov 15 16:06:35.062: INFO: stderr: ""
Nov 15 16:06:35.062: INFO: stdout: "e2e-test-crd-publish-openapi-2808-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Nov 15 16:06:35.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-3984 --namespace=crd-publish-openapi-3984 delete e2e-test-crd-publish-openapi-2808-crds test-cr'
Nov 15 16:06:35.299: INFO: stderr: ""
Nov 15 16:06:35.300: INFO: stdout: "e2e-test-crd-publish-openapi-2808-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Nov 15 16:06:35.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-3984 --namespace=crd-publish-openapi-3984 apply -f -'
Nov 15 16:06:35.706: INFO: stderr: ""
Nov 15 16:06:35.706: INFO: stdout: "e2e-test-crd-publish-openapi-2808-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Nov 15 16:06:35.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-3984 --namespace=crd-publish-openapi-3984 delete e2e-test-crd-publish-openapi-2808-crds test-cr'
Nov 15 16:06:35.858: INFO: stderr: ""
Nov 15 16:06:35.858: INFO: stdout: "e2e-test-crd-publish-openapi-2808-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 11/15/23 16:06:35.858
Nov 15 16:06:35.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-3984 explain e2e-test-crd-publish-openapi-2808-crds'
Nov 15 16:06:36.521: INFO: stderr: ""
Nov 15 16:06:36.521: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2808-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 16:06:38.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3984" for this suite. 11/15/23 16:06:38.897
------------------------------
• [SLOW TEST] [7.571 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:06:31.347
    Nov 15 16:06:31.347: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename crd-publish-openapi 11/15/23 16:06:31.349
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:06:31.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:06:31.423
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Nov 15 16:06:31.443: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 11/15/23 16:06:33.989
    Nov 15 16:06:33.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-3984 --namespace=crd-publish-openapi-3984 create -f -'
    Nov 15 16:06:35.062: INFO: stderr: ""
    Nov 15 16:06:35.062: INFO: stdout: "e2e-test-crd-publish-openapi-2808-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Nov 15 16:06:35.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-3984 --namespace=crd-publish-openapi-3984 delete e2e-test-crd-publish-openapi-2808-crds test-cr'
    Nov 15 16:06:35.299: INFO: stderr: ""
    Nov 15 16:06:35.300: INFO: stdout: "e2e-test-crd-publish-openapi-2808-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Nov 15 16:06:35.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-3984 --namespace=crd-publish-openapi-3984 apply -f -'
    Nov 15 16:06:35.706: INFO: stderr: ""
    Nov 15 16:06:35.706: INFO: stdout: "e2e-test-crd-publish-openapi-2808-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Nov 15 16:06:35.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-3984 --namespace=crd-publish-openapi-3984 delete e2e-test-crd-publish-openapi-2808-crds test-cr'
    Nov 15 16:06:35.858: INFO: stderr: ""
    Nov 15 16:06:35.858: INFO: stdout: "e2e-test-crd-publish-openapi-2808-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 11/15/23 16:06:35.858
    Nov 15 16:06:35.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-3984 explain e2e-test-crd-publish-openapi-2808-crds'
    Nov 15 16:06:36.521: INFO: stderr: ""
    Nov 15 16:06:36.521: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2808-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:06:38.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3984" for this suite. 11/15/23 16:06:38.897
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:06:38.92
Nov 15 16:06:38.920: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename downward-api 11/15/23 16:06:38.922
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:06:38.981
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:06:38.996
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 11/15/23 16:06:39.012
Nov 15 16:06:39.052: INFO: Waiting up to 5m0s for pod "downwardapi-volume-13286432-0055-4189-a8a5-82fb81134794" in namespace "downward-api-6885" to be "Succeeded or Failed"
Nov 15 16:06:39.073: INFO: Pod "downwardapi-volume-13286432-0055-4189-a8a5-82fb81134794": Phase="Pending", Reason="", readiness=false. Elapsed: 20.87857ms
Nov 15 16:06:41.090: INFO: Pod "downwardapi-volume-13286432-0055-4189-a8a5-82fb81134794": Phase="Running", Reason="", readiness=true. Elapsed: 2.037799069s
Nov 15 16:06:43.101: INFO: Pod "downwardapi-volume-13286432-0055-4189-a8a5-82fb81134794": Phase="Running", Reason="", readiness=false. Elapsed: 4.048676672s
Nov 15 16:06:45.095: INFO: Pod "downwardapi-volume-13286432-0055-4189-a8a5-82fb81134794": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043131425s
STEP: Saw pod success 11/15/23 16:06:45.095
Nov 15 16:06:45.095: INFO: Pod "downwardapi-volume-13286432-0055-4189-a8a5-82fb81134794" satisfied condition "Succeeded or Failed"
Nov 15 16:06:45.114: INFO: Trying to get logs from node 10.15.40.115 pod downwardapi-volume-13286432-0055-4189-a8a5-82fb81134794 container client-container: <nil>
STEP: delete the pod 11/15/23 16:06:45.248
Nov 15 16:06:45.297: INFO: Waiting for pod downwardapi-volume-13286432-0055-4189-a8a5-82fb81134794 to disappear
Nov 15 16:06:45.312: INFO: Pod downwardapi-volume-13286432-0055-4189-a8a5-82fb81134794 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Nov 15 16:06:45.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6885" for this suite. 11/15/23 16:06:45.33
------------------------------
• [SLOW TEST] [6.428 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:06:38.92
    Nov 15 16:06:38.920: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename downward-api 11/15/23 16:06:38.922
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:06:38.981
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:06:38.996
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 11/15/23 16:06:39.012
    Nov 15 16:06:39.052: INFO: Waiting up to 5m0s for pod "downwardapi-volume-13286432-0055-4189-a8a5-82fb81134794" in namespace "downward-api-6885" to be "Succeeded or Failed"
    Nov 15 16:06:39.073: INFO: Pod "downwardapi-volume-13286432-0055-4189-a8a5-82fb81134794": Phase="Pending", Reason="", readiness=false. Elapsed: 20.87857ms
    Nov 15 16:06:41.090: INFO: Pod "downwardapi-volume-13286432-0055-4189-a8a5-82fb81134794": Phase="Running", Reason="", readiness=true. Elapsed: 2.037799069s
    Nov 15 16:06:43.101: INFO: Pod "downwardapi-volume-13286432-0055-4189-a8a5-82fb81134794": Phase="Running", Reason="", readiness=false. Elapsed: 4.048676672s
    Nov 15 16:06:45.095: INFO: Pod "downwardapi-volume-13286432-0055-4189-a8a5-82fb81134794": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043131425s
    STEP: Saw pod success 11/15/23 16:06:45.095
    Nov 15 16:06:45.095: INFO: Pod "downwardapi-volume-13286432-0055-4189-a8a5-82fb81134794" satisfied condition "Succeeded or Failed"
    Nov 15 16:06:45.114: INFO: Trying to get logs from node 10.15.40.115 pod downwardapi-volume-13286432-0055-4189-a8a5-82fb81134794 container client-container: <nil>
    STEP: delete the pod 11/15/23 16:06:45.248
    Nov 15 16:06:45.297: INFO: Waiting for pod downwardapi-volume-13286432-0055-4189-a8a5-82fb81134794 to disappear
    Nov 15 16:06:45.312: INFO: Pod downwardapi-volume-13286432-0055-4189-a8a5-82fb81134794 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:06:45.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6885" for this suite. 11/15/23 16:06:45.33
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:06:45.356
Nov 15 16:06:45.357: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename emptydir-wrapper 11/15/23 16:06:45.361
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:06:45.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:06:45.427
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 11/15/23 16:06:45.442
STEP: Creating RC which spawns configmap-volume pods 11/15/23 16:06:46.597
Nov 15 16:06:46.642: INFO: Pod name wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10: Found 0 pods out of 5
Nov 15 16:06:51.681: INFO: Pod name wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10: Found 5 pods out of 5
STEP: Ensuring each pod is running 11/15/23 16:06:51.681
Nov 15 16:06:51.681: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10-55sx9" in namespace "emptydir-wrapper-7095" to be "running"
Nov 15 16:06:51.699: INFO: Pod "wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10-55sx9": Phase="Running", Reason="", readiness=true. Elapsed: 18.43497ms
Nov 15 16:06:51.700: INFO: Pod "wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10-55sx9" satisfied condition "running"
Nov 15 16:06:51.700: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10-5pcmd" in namespace "emptydir-wrapper-7095" to be "running"
Nov 15 16:06:51.718: INFO: Pod "wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10-5pcmd": Phase="Running", Reason="", readiness=true. Elapsed: 18.334112ms
Nov 15 16:06:51.718: INFO: Pod "wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10-5pcmd" satisfied condition "running"
Nov 15 16:06:51.718: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10-qc6pz" in namespace "emptydir-wrapper-7095" to be "running"
Nov 15 16:06:51.737: INFO: Pod "wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10-qc6pz": Phase="Running", Reason="", readiness=true. Elapsed: 18.063351ms
Nov 15 16:06:51.737: INFO: Pod "wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10-qc6pz" satisfied condition "running"
Nov 15 16:06:51.737: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10-qcwfr" in namespace "emptydir-wrapper-7095" to be "running"
Nov 15 16:06:51.769: INFO: Pod "wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10-qcwfr": Phase="Running", Reason="", readiness=true. Elapsed: 32.323695ms
Nov 15 16:06:51.769: INFO: Pod "wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10-qcwfr" satisfied condition "running"
Nov 15 16:06:51.769: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10-x7wwq" in namespace "emptydir-wrapper-7095" to be "running"
Nov 15 16:06:51.788: INFO: Pod "wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10-x7wwq": Phase="Running", Reason="", readiness=true. Elapsed: 18.425436ms
Nov 15 16:06:51.788: INFO: Pod "wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10-x7wwq" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10 in namespace emptydir-wrapper-7095, will wait for the garbage collector to delete the pods 11/15/23 16:06:51.788
Nov 15 16:06:51.876: INFO: Deleting ReplicationController wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10 took: 21.558109ms
Nov 15 16:06:51.976: INFO: Terminating ReplicationController wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10 pods took: 100.835359ms
STEP: Creating RC which spawns configmap-volume pods 11/15/23 16:06:54.994
Nov 15 16:06:55.040: INFO: Pod name wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b: Found 0 pods out of 5
Nov 15 16:07:00.090: INFO: Pod name wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b: Found 5 pods out of 5
STEP: Ensuring each pod is running 11/15/23 16:07:00.09
Nov 15 16:07:00.090: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b-h2gc5" in namespace "emptydir-wrapper-7095" to be "running"
Nov 15 16:07:00.124: INFO: Pod "wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b-h2gc5": Phase="Running", Reason="", readiness=true. Elapsed: 33.748589ms
Nov 15 16:07:00.124: INFO: Pod "wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b-h2gc5" satisfied condition "running"
Nov 15 16:07:00.124: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b-l9tm9" in namespace "emptydir-wrapper-7095" to be "running"
Nov 15 16:07:00.147: INFO: Pod "wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b-l9tm9": Phase="Running", Reason="", readiness=true. Elapsed: 22.242505ms
Nov 15 16:07:00.147: INFO: Pod "wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b-l9tm9" satisfied condition "running"
Nov 15 16:07:00.147: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b-pscsw" in namespace "emptydir-wrapper-7095" to be "running"
Nov 15 16:07:00.166: INFO: Pod "wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b-pscsw": Phase="Running", Reason="", readiness=true. Elapsed: 19.164166ms
Nov 15 16:07:00.166: INFO: Pod "wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b-pscsw" satisfied condition "running"
Nov 15 16:07:00.166: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b-rnfxh" in namespace "emptydir-wrapper-7095" to be "running"
Nov 15 16:07:00.184: INFO: Pod "wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b-rnfxh": Phase="Running", Reason="", readiness=true. Elapsed: 18.357295ms
Nov 15 16:07:00.185: INFO: Pod "wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b-rnfxh" satisfied condition "running"
Nov 15 16:07:00.185: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b-v9tln" in namespace "emptydir-wrapper-7095" to be "running"
Nov 15 16:07:00.203: INFO: Pod "wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b-v9tln": Phase="Running", Reason="", readiness=true. Elapsed: 18.810091ms
Nov 15 16:07:00.204: INFO: Pod "wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b-v9tln" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b in namespace emptydir-wrapper-7095, will wait for the garbage collector to delete the pods 11/15/23 16:07:00.204
Nov 15 16:07:00.295: INFO: Deleting ReplicationController wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b took: 20.345142ms
Nov 15 16:07:00.596: INFO: Terminating ReplicationController wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b pods took: 301.329908ms
STEP: Creating RC which spawns configmap-volume pods 11/15/23 16:07:03.516
Nov 15 16:07:03.565: INFO: Pod name wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c: Found 0 pods out of 5
Nov 15 16:07:08.624: INFO: Pod name wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c: Found 5 pods out of 5
STEP: Ensuring each pod is running 11/15/23 16:07:08.624
Nov 15 16:07:08.625: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c-85hsk" in namespace "emptydir-wrapper-7095" to be "running"
Nov 15 16:07:08.643: INFO: Pod "wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c-85hsk": Phase="Running", Reason="", readiness=true. Elapsed: 18.228313ms
Nov 15 16:07:08.643: INFO: Pod "wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c-85hsk" satisfied condition "running"
Nov 15 16:07:08.644: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c-dg29l" in namespace "emptydir-wrapper-7095" to be "running"
Nov 15 16:07:08.662: INFO: Pod "wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c-dg29l": Phase="Running", Reason="", readiness=true. Elapsed: 17.706661ms
Nov 15 16:07:08.662: INFO: Pod "wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c-dg29l" satisfied condition "running"
Nov 15 16:07:08.662: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c-jh2q4" in namespace "emptydir-wrapper-7095" to be "running"
Nov 15 16:07:08.682: INFO: Pod "wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c-jh2q4": Phase="Running", Reason="", readiness=true. Elapsed: 20.266277ms
Nov 15 16:07:08.682: INFO: Pod "wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c-jh2q4" satisfied condition "running"
Nov 15 16:07:08.682: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c-wb9k9" in namespace "emptydir-wrapper-7095" to be "running"
Nov 15 16:07:08.699: INFO: Pod "wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c-wb9k9": Phase="Running", Reason="", readiness=true. Elapsed: 16.645016ms
Nov 15 16:07:08.699: INFO: Pod "wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c-wb9k9" satisfied condition "running"
Nov 15 16:07:08.699: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c-xfkg4" in namespace "emptydir-wrapper-7095" to be "running"
Nov 15 16:07:08.717: INFO: Pod "wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c-xfkg4": Phase="Running", Reason="", readiness=true. Elapsed: 17.784607ms
Nov 15 16:07:08.717: INFO: Pod "wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c-xfkg4" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c in namespace emptydir-wrapper-7095, will wait for the garbage collector to delete the pods 11/15/23 16:07:08.717
Nov 15 16:07:08.809: INFO: Deleting ReplicationController wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c took: 20.631438ms
Nov 15 16:07:08.910: INFO: Terminating ReplicationController wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c pods took: 100.667876ms
STEP: Cleaning up the configMaps 11/15/23 16:07:12.011
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Nov 15 16:07:13.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-7095" for this suite. 11/15/23 16:07:13.514
------------------------------
• [SLOW TEST] [28.181 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:06:45.356
    Nov 15 16:06:45.357: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename emptydir-wrapper 11/15/23 16:06:45.361
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:06:45.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:06:45.427
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 11/15/23 16:06:45.442
    STEP: Creating RC which spawns configmap-volume pods 11/15/23 16:06:46.597
    Nov 15 16:06:46.642: INFO: Pod name wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10: Found 0 pods out of 5
    Nov 15 16:06:51.681: INFO: Pod name wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10: Found 5 pods out of 5
    STEP: Ensuring each pod is running 11/15/23 16:06:51.681
    Nov 15 16:06:51.681: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10-55sx9" in namespace "emptydir-wrapper-7095" to be "running"
    Nov 15 16:06:51.699: INFO: Pod "wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10-55sx9": Phase="Running", Reason="", readiness=true. Elapsed: 18.43497ms
    Nov 15 16:06:51.700: INFO: Pod "wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10-55sx9" satisfied condition "running"
    Nov 15 16:06:51.700: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10-5pcmd" in namespace "emptydir-wrapper-7095" to be "running"
    Nov 15 16:06:51.718: INFO: Pod "wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10-5pcmd": Phase="Running", Reason="", readiness=true. Elapsed: 18.334112ms
    Nov 15 16:06:51.718: INFO: Pod "wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10-5pcmd" satisfied condition "running"
    Nov 15 16:06:51.718: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10-qc6pz" in namespace "emptydir-wrapper-7095" to be "running"
    Nov 15 16:06:51.737: INFO: Pod "wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10-qc6pz": Phase="Running", Reason="", readiness=true. Elapsed: 18.063351ms
    Nov 15 16:06:51.737: INFO: Pod "wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10-qc6pz" satisfied condition "running"
    Nov 15 16:06:51.737: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10-qcwfr" in namespace "emptydir-wrapper-7095" to be "running"
    Nov 15 16:06:51.769: INFO: Pod "wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10-qcwfr": Phase="Running", Reason="", readiness=true. Elapsed: 32.323695ms
    Nov 15 16:06:51.769: INFO: Pod "wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10-qcwfr" satisfied condition "running"
    Nov 15 16:06:51.769: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10-x7wwq" in namespace "emptydir-wrapper-7095" to be "running"
    Nov 15 16:06:51.788: INFO: Pod "wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10-x7wwq": Phase="Running", Reason="", readiness=true. Elapsed: 18.425436ms
    Nov 15 16:06:51.788: INFO: Pod "wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10-x7wwq" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10 in namespace emptydir-wrapper-7095, will wait for the garbage collector to delete the pods 11/15/23 16:06:51.788
    Nov 15 16:06:51.876: INFO: Deleting ReplicationController wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10 took: 21.558109ms
    Nov 15 16:06:51.976: INFO: Terminating ReplicationController wrapped-volume-race-44a6d0df-3385-43c8-967e-06c55d4bde10 pods took: 100.835359ms
    STEP: Creating RC which spawns configmap-volume pods 11/15/23 16:06:54.994
    Nov 15 16:06:55.040: INFO: Pod name wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b: Found 0 pods out of 5
    Nov 15 16:07:00.090: INFO: Pod name wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b: Found 5 pods out of 5
    STEP: Ensuring each pod is running 11/15/23 16:07:00.09
    Nov 15 16:07:00.090: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b-h2gc5" in namespace "emptydir-wrapper-7095" to be "running"
    Nov 15 16:07:00.124: INFO: Pod "wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b-h2gc5": Phase="Running", Reason="", readiness=true. Elapsed: 33.748589ms
    Nov 15 16:07:00.124: INFO: Pod "wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b-h2gc5" satisfied condition "running"
    Nov 15 16:07:00.124: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b-l9tm9" in namespace "emptydir-wrapper-7095" to be "running"
    Nov 15 16:07:00.147: INFO: Pod "wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b-l9tm9": Phase="Running", Reason="", readiness=true. Elapsed: 22.242505ms
    Nov 15 16:07:00.147: INFO: Pod "wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b-l9tm9" satisfied condition "running"
    Nov 15 16:07:00.147: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b-pscsw" in namespace "emptydir-wrapper-7095" to be "running"
    Nov 15 16:07:00.166: INFO: Pod "wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b-pscsw": Phase="Running", Reason="", readiness=true. Elapsed: 19.164166ms
    Nov 15 16:07:00.166: INFO: Pod "wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b-pscsw" satisfied condition "running"
    Nov 15 16:07:00.166: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b-rnfxh" in namespace "emptydir-wrapper-7095" to be "running"
    Nov 15 16:07:00.184: INFO: Pod "wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b-rnfxh": Phase="Running", Reason="", readiness=true. Elapsed: 18.357295ms
    Nov 15 16:07:00.185: INFO: Pod "wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b-rnfxh" satisfied condition "running"
    Nov 15 16:07:00.185: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b-v9tln" in namespace "emptydir-wrapper-7095" to be "running"
    Nov 15 16:07:00.203: INFO: Pod "wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b-v9tln": Phase="Running", Reason="", readiness=true. Elapsed: 18.810091ms
    Nov 15 16:07:00.204: INFO: Pod "wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b-v9tln" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b in namespace emptydir-wrapper-7095, will wait for the garbage collector to delete the pods 11/15/23 16:07:00.204
    Nov 15 16:07:00.295: INFO: Deleting ReplicationController wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b took: 20.345142ms
    Nov 15 16:07:00.596: INFO: Terminating ReplicationController wrapped-volume-race-f78c3a95-6049-4ce5-80a8-887ae53ced0b pods took: 301.329908ms
    STEP: Creating RC which spawns configmap-volume pods 11/15/23 16:07:03.516
    Nov 15 16:07:03.565: INFO: Pod name wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c: Found 0 pods out of 5
    Nov 15 16:07:08.624: INFO: Pod name wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c: Found 5 pods out of 5
    STEP: Ensuring each pod is running 11/15/23 16:07:08.624
    Nov 15 16:07:08.625: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c-85hsk" in namespace "emptydir-wrapper-7095" to be "running"
    Nov 15 16:07:08.643: INFO: Pod "wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c-85hsk": Phase="Running", Reason="", readiness=true. Elapsed: 18.228313ms
    Nov 15 16:07:08.643: INFO: Pod "wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c-85hsk" satisfied condition "running"
    Nov 15 16:07:08.644: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c-dg29l" in namespace "emptydir-wrapper-7095" to be "running"
    Nov 15 16:07:08.662: INFO: Pod "wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c-dg29l": Phase="Running", Reason="", readiness=true. Elapsed: 17.706661ms
    Nov 15 16:07:08.662: INFO: Pod "wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c-dg29l" satisfied condition "running"
    Nov 15 16:07:08.662: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c-jh2q4" in namespace "emptydir-wrapper-7095" to be "running"
    Nov 15 16:07:08.682: INFO: Pod "wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c-jh2q4": Phase="Running", Reason="", readiness=true. Elapsed: 20.266277ms
    Nov 15 16:07:08.682: INFO: Pod "wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c-jh2q4" satisfied condition "running"
    Nov 15 16:07:08.682: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c-wb9k9" in namespace "emptydir-wrapper-7095" to be "running"
    Nov 15 16:07:08.699: INFO: Pod "wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c-wb9k9": Phase="Running", Reason="", readiness=true. Elapsed: 16.645016ms
    Nov 15 16:07:08.699: INFO: Pod "wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c-wb9k9" satisfied condition "running"
    Nov 15 16:07:08.699: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c-xfkg4" in namespace "emptydir-wrapper-7095" to be "running"
    Nov 15 16:07:08.717: INFO: Pod "wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c-xfkg4": Phase="Running", Reason="", readiness=true. Elapsed: 17.784607ms
    Nov 15 16:07:08.717: INFO: Pod "wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c-xfkg4" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c in namespace emptydir-wrapper-7095, will wait for the garbage collector to delete the pods 11/15/23 16:07:08.717
    Nov 15 16:07:08.809: INFO: Deleting ReplicationController wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c took: 20.631438ms
    Nov 15 16:07:08.910: INFO: Terminating ReplicationController wrapped-volume-race-d522dc89-9350-4170-bf18-a2ee53874a4c pods took: 100.667876ms
    STEP: Cleaning up the configMaps 11/15/23 16:07:12.011
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:07:13.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-7095" for this suite. 11/15/23 16:07:13.514
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:07:13.555
Nov 15 16:07:13.555: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename crd-publish-openapi 11/15/23 16:07:13.557
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:07:13.602
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:07:13.618
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 11/15/23 16:07:13.639
Nov 15 16:07:13.642: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: rename a version 11/15/23 16:07:19.33
STEP: check the new version name is served 11/15/23 16:07:19.39
STEP: check the old version name is removed 11/15/23 16:07:21.61
STEP: check the other version is not changed 11/15/23 16:07:22.914
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 16:07:27.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3961" for this suite. 11/15/23 16:07:27.526
------------------------------
• [SLOW TEST] [13.997 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:07:13.555
    Nov 15 16:07:13.555: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename crd-publish-openapi 11/15/23 16:07:13.557
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:07:13.602
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:07:13.618
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 11/15/23 16:07:13.639
    Nov 15 16:07:13.642: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: rename a version 11/15/23 16:07:19.33
    STEP: check the new version name is served 11/15/23 16:07:19.39
    STEP: check the old version name is removed 11/15/23 16:07:21.61
    STEP: check the other version is not changed 11/15/23 16:07:22.914
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:07:27.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3961" for this suite. 11/15/23 16:07:27.526
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:07:27.562
Nov 15 16:07:27.562: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename downward-api 11/15/23 16:07:27.565
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:07:27.613
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:07:27.628
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 11/15/23 16:07:27.645
Nov 15 16:07:27.678: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c47c754b-9474-4be9-872b-07be26989320" in namespace "downward-api-5467" to be "Succeeded or Failed"
Nov 15 16:07:27.692: INFO: Pod "downwardapi-volume-c47c754b-9474-4be9-872b-07be26989320": Phase="Pending", Reason="", readiness=false. Elapsed: 14.432136ms
Nov 15 16:07:29.709: INFO: Pod "downwardapi-volume-c47c754b-9474-4be9-872b-07be26989320": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031093595s
Nov 15 16:07:31.709: INFO: Pod "downwardapi-volume-c47c754b-9474-4be9-872b-07be26989320": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030875889s
Nov 15 16:07:33.708: INFO: Pod "downwardapi-volume-c47c754b-9474-4be9-872b-07be26989320": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030107314s
STEP: Saw pod success 11/15/23 16:07:33.708
Nov 15 16:07:33.708: INFO: Pod "downwardapi-volume-c47c754b-9474-4be9-872b-07be26989320" satisfied condition "Succeeded or Failed"
Nov 15 16:07:33.726: INFO: Trying to get logs from node 10.15.40.115 pod downwardapi-volume-c47c754b-9474-4be9-872b-07be26989320 container client-container: <nil>
STEP: delete the pod 11/15/23 16:07:33.858
Nov 15 16:07:33.909: INFO: Waiting for pod downwardapi-volume-c47c754b-9474-4be9-872b-07be26989320 to disappear
Nov 15 16:07:33.923: INFO: Pod downwardapi-volume-c47c754b-9474-4be9-872b-07be26989320 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Nov 15 16:07:33.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5467" for this suite. 11/15/23 16:07:33.947
------------------------------
• [SLOW TEST] [6.411 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:07:27.562
    Nov 15 16:07:27.562: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename downward-api 11/15/23 16:07:27.565
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:07:27.613
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:07:27.628
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 11/15/23 16:07:27.645
    Nov 15 16:07:27.678: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c47c754b-9474-4be9-872b-07be26989320" in namespace "downward-api-5467" to be "Succeeded or Failed"
    Nov 15 16:07:27.692: INFO: Pod "downwardapi-volume-c47c754b-9474-4be9-872b-07be26989320": Phase="Pending", Reason="", readiness=false. Elapsed: 14.432136ms
    Nov 15 16:07:29.709: INFO: Pod "downwardapi-volume-c47c754b-9474-4be9-872b-07be26989320": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031093595s
    Nov 15 16:07:31.709: INFO: Pod "downwardapi-volume-c47c754b-9474-4be9-872b-07be26989320": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030875889s
    Nov 15 16:07:33.708: INFO: Pod "downwardapi-volume-c47c754b-9474-4be9-872b-07be26989320": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030107314s
    STEP: Saw pod success 11/15/23 16:07:33.708
    Nov 15 16:07:33.708: INFO: Pod "downwardapi-volume-c47c754b-9474-4be9-872b-07be26989320" satisfied condition "Succeeded or Failed"
    Nov 15 16:07:33.726: INFO: Trying to get logs from node 10.15.40.115 pod downwardapi-volume-c47c754b-9474-4be9-872b-07be26989320 container client-container: <nil>
    STEP: delete the pod 11/15/23 16:07:33.858
    Nov 15 16:07:33.909: INFO: Waiting for pod downwardapi-volume-c47c754b-9474-4be9-872b-07be26989320 to disappear
    Nov 15 16:07:33.923: INFO: Pod downwardapi-volume-c47c754b-9474-4be9-872b-07be26989320 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:07:33.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5467" for this suite. 11/15/23 16:07:33.947
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:07:33.979
Nov 15 16:07:33.979: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename var-expansion 11/15/23 16:07:33.981
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:07:34.036
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:07:34.052
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 11/15/23 16:07:34.068
Nov 15 16:07:34.103: INFO: Waiting up to 5m0s for pod "var-expansion-a186c912-3786-4e24-8905-9cea3223e04d" in namespace "var-expansion-4449" to be "Succeeded or Failed"
Nov 15 16:07:34.119: INFO: Pod "var-expansion-a186c912-3786-4e24-8905-9cea3223e04d": Phase="Pending", Reason="", readiness=false. Elapsed: 16.323794ms
Nov 15 16:07:36.141: INFO: Pod "var-expansion-a186c912-3786-4e24-8905-9cea3223e04d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037693287s
Nov 15 16:07:38.137: INFO: Pod "var-expansion-a186c912-3786-4e24-8905-9cea3223e04d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03361381s
STEP: Saw pod success 11/15/23 16:07:38.137
Nov 15 16:07:38.137: INFO: Pod "var-expansion-a186c912-3786-4e24-8905-9cea3223e04d" satisfied condition "Succeeded or Failed"
Nov 15 16:07:38.154: INFO: Trying to get logs from node 10.15.40.115 pod var-expansion-a186c912-3786-4e24-8905-9cea3223e04d container dapi-container: <nil>
STEP: delete the pod 11/15/23 16:07:38.217
Nov 15 16:07:38.258: INFO: Waiting for pod var-expansion-a186c912-3786-4e24-8905-9cea3223e04d to disappear
Nov 15 16:07:38.272: INFO: Pod var-expansion-a186c912-3786-4e24-8905-9cea3223e04d no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Nov 15 16:07:38.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4449" for this suite. 11/15/23 16:07:38.296
------------------------------
• [4.343 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:07:33.979
    Nov 15 16:07:33.979: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename var-expansion 11/15/23 16:07:33.981
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:07:34.036
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:07:34.052
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 11/15/23 16:07:34.068
    Nov 15 16:07:34.103: INFO: Waiting up to 5m0s for pod "var-expansion-a186c912-3786-4e24-8905-9cea3223e04d" in namespace "var-expansion-4449" to be "Succeeded or Failed"
    Nov 15 16:07:34.119: INFO: Pod "var-expansion-a186c912-3786-4e24-8905-9cea3223e04d": Phase="Pending", Reason="", readiness=false. Elapsed: 16.323794ms
    Nov 15 16:07:36.141: INFO: Pod "var-expansion-a186c912-3786-4e24-8905-9cea3223e04d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037693287s
    Nov 15 16:07:38.137: INFO: Pod "var-expansion-a186c912-3786-4e24-8905-9cea3223e04d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03361381s
    STEP: Saw pod success 11/15/23 16:07:38.137
    Nov 15 16:07:38.137: INFO: Pod "var-expansion-a186c912-3786-4e24-8905-9cea3223e04d" satisfied condition "Succeeded or Failed"
    Nov 15 16:07:38.154: INFO: Trying to get logs from node 10.15.40.115 pod var-expansion-a186c912-3786-4e24-8905-9cea3223e04d container dapi-container: <nil>
    STEP: delete the pod 11/15/23 16:07:38.217
    Nov 15 16:07:38.258: INFO: Waiting for pod var-expansion-a186c912-3786-4e24-8905-9cea3223e04d to disappear
    Nov 15 16:07:38.272: INFO: Pod var-expansion-a186c912-3786-4e24-8905-9cea3223e04d no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:07:38.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4449" for this suite. 11/15/23 16:07:38.296
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:07:38.338
Nov 15 16:07:38.338: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename sysctl 11/15/23 16:07:38.34
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:07:38.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:07:38.406
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 11/15/23 16:07:38.423
STEP: Watching for error events or started pod 11/15/23 16:07:38.456
STEP: Waiting for pod completion 11/15/23 16:07:40.478
Nov 15 16:07:40.479: INFO: Waiting up to 3m0s for pod "sysctl-39914171-df44-48e2-bc85-c2712cb6a6bd" in namespace "sysctl-4644" to be "completed"
Nov 15 16:07:40.492: INFO: Pod "sysctl-39914171-df44-48e2-bc85-c2712cb6a6bd": Phase="Pending", Reason="", readiness=false. Elapsed: 13.337074ms
Nov 15 16:07:42.508: INFO: Pod "sysctl-39914171-df44-48e2-bc85-c2712cb6a6bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028465764s
Nov 15 16:07:42.508: INFO: Pod "sysctl-39914171-df44-48e2-bc85-c2712cb6a6bd" satisfied condition "completed"
STEP: Checking that the pod succeeded 11/15/23 16:07:42.523
STEP: Getting logs from the pod 11/15/23 16:07:42.524
STEP: Checking that the sysctl is actually updated 11/15/23 16:07:42.565
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Nov 15 16:07:42.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-4644" for this suite. 11/15/23 16:07:42.594
------------------------------
• [4.284 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:07:38.338
    Nov 15 16:07:38.338: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename sysctl 11/15/23 16:07:38.34
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:07:38.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:07:38.406
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 11/15/23 16:07:38.423
    STEP: Watching for error events or started pod 11/15/23 16:07:38.456
    STEP: Waiting for pod completion 11/15/23 16:07:40.478
    Nov 15 16:07:40.479: INFO: Waiting up to 3m0s for pod "sysctl-39914171-df44-48e2-bc85-c2712cb6a6bd" in namespace "sysctl-4644" to be "completed"
    Nov 15 16:07:40.492: INFO: Pod "sysctl-39914171-df44-48e2-bc85-c2712cb6a6bd": Phase="Pending", Reason="", readiness=false. Elapsed: 13.337074ms
    Nov 15 16:07:42.508: INFO: Pod "sysctl-39914171-df44-48e2-bc85-c2712cb6a6bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028465764s
    Nov 15 16:07:42.508: INFO: Pod "sysctl-39914171-df44-48e2-bc85-c2712cb6a6bd" satisfied condition "completed"
    STEP: Checking that the pod succeeded 11/15/23 16:07:42.523
    STEP: Getting logs from the pod 11/15/23 16:07:42.524
    STEP: Checking that the sysctl is actually updated 11/15/23 16:07:42.565
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:07:42.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-4644" for this suite. 11/15/23 16:07:42.594
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:07:42.634
Nov 15 16:07:42.634: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename dns 11/15/23 16:07:42.636
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:07:42.689
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:07:42.703
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 11/15/23 16:07:42.72
Nov 15 16:07:42.757: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1839  02fd6d20-558f-4ed8-aa54-bbacce5564f3 33033 0 2023-11-15 16:07:42 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-11-15 16:07:42 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nnr85,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nnr85,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 16:07:42.757: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-1839" to be "running and ready"
Nov 15 16:07:42.771: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 14.095599ms
Nov 15 16:07:42.771: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Nov 15 16:07:44.786: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029068357s
Nov 15 16:07:44.786: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Nov 15 16:07:46.794: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 4.036663294s
Nov 15 16:07:46.794: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Nov 15 16:07:46.794: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 11/15/23 16:07:46.795
Nov 15 16:07:46.795: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1839 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 16:07:46.796: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
Nov 15 16:07:46.797: INFO: ExecWithOptions: Clientset creation
Nov 15 16:07:46.798: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-1839/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 11/15/23 16:07:47.164
Nov 15 16:07:47.165: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1839 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 16:07:47.165: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
Nov 15 16:07:47.166: INFO: ExecWithOptions: Clientset creation
Nov 15 16:07:47.167: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-1839/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Nov 15 16:07:47.475: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Nov 15 16:07:47.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-1839" for this suite. 11/15/23 16:07:47.558
------------------------------
• [4.950 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:07:42.634
    Nov 15 16:07:42.634: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename dns 11/15/23 16:07:42.636
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:07:42.689
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:07:42.703
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 11/15/23 16:07:42.72
    Nov 15 16:07:42.757: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1839  02fd6d20-558f-4ed8-aa54-bbacce5564f3 33033 0 2023-11-15 16:07:42 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-11-15 16:07:42 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nnr85,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nnr85,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 16:07:42.757: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-1839" to be "running and ready"
    Nov 15 16:07:42.771: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 14.095599ms
    Nov 15 16:07:42.771: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 16:07:44.786: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029068357s
    Nov 15 16:07:44.786: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 16:07:46.794: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 4.036663294s
    Nov 15 16:07:46.794: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Nov 15 16:07:46.794: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 11/15/23 16:07:46.795
    Nov 15 16:07:46.795: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1839 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 16:07:46.796: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    Nov 15 16:07:46.797: INFO: ExecWithOptions: Clientset creation
    Nov 15 16:07:46.798: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-1839/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 11/15/23 16:07:47.164
    Nov 15 16:07:47.165: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1839 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 16:07:47.165: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    Nov 15 16:07:47.166: INFO: ExecWithOptions: Clientset creation
    Nov 15 16:07:47.167: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-1839/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Nov 15 16:07:47.475: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:07:47.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-1839" for this suite. 11/15/23 16:07:47.558
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:07:47.586
Nov 15 16:07:47.586: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename sched-preemption 11/15/23 16:07:47.59
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:07:47.643
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:07:47.659
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Nov 15 16:07:47.737: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 15 16:08:47.873: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:08:47.889
Nov 15 16:08:47.889: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename sched-preemption-path 11/15/23 16:08:47.895
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:08:47.952
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:08:47.969
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 11/15/23 16:08:47.984
STEP: Trying to launch a pod without a label to get a node which can launch it. 11/15/23 16:08:47.985
Nov 15 16:08:48.016: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-6007" to be "running"
Nov 15 16:08:48.029: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 13.146426ms
Nov 15 16:08:50.044: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028097833s
Nov 15 16:08:52.050: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.033475662s
Nov 15 16:08:52.050: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 11/15/23 16:08:52.063
Nov 15 16:08:52.096: INFO: found a healthy node: 10.15.40.115
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
Nov 15 16:09:04.394: INFO: pods created so far: [1 1 1]
Nov 15 16:09:04.394: INFO: length of pods created so far: 3
Nov 15 16:09:08.437: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Nov 15 16:09:15.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 16:09:15.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-6007" for this suite. 11/15/23 16:09:15.887
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-7730" for this suite. 11/15/23 16:09:15.915
------------------------------
• [SLOW TEST] [88.354 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:07:47.586
    Nov 15 16:07:47.586: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename sched-preemption 11/15/23 16:07:47.59
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:07:47.643
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:07:47.659
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Nov 15 16:07:47.737: INFO: Waiting up to 1m0s for all nodes to be ready
    Nov 15 16:08:47.873: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:08:47.889
    Nov 15 16:08:47.889: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename sched-preemption-path 11/15/23 16:08:47.895
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:08:47.952
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:08:47.969
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 11/15/23 16:08:47.984
    STEP: Trying to launch a pod without a label to get a node which can launch it. 11/15/23 16:08:47.985
    Nov 15 16:08:48.016: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-6007" to be "running"
    Nov 15 16:08:48.029: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 13.146426ms
    Nov 15 16:08:50.044: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028097833s
    Nov 15 16:08:52.050: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.033475662s
    Nov 15 16:08:52.050: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 11/15/23 16:08:52.063
    Nov 15 16:08:52.096: INFO: found a healthy node: 10.15.40.115
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    Nov 15 16:09:04.394: INFO: pods created so far: [1 1 1]
    Nov 15 16:09:04.394: INFO: length of pods created so far: 3
    Nov 15 16:09:08.437: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:09:15.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:09:15.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-6007" for this suite. 11/15/23 16:09:15.887
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-7730" for this suite. 11/15/23 16:09:15.915
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:09:15.943
Nov 15 16:09:15.943: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename projected 11/15/23 16:09:15.946
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:09:16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:09:16.02
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 11/15/23 16:09:16.036
Nov 15 16:09:16.070: INFO: Waiting up to 5m0s for pod "downwardapi-volume-07dbc578-7dd4-4501-a0e9-68fd1d06c68e" in namespace "projected-6655" to be "Succeeded or Failed"
Nov 15 16:09:16.084: INFO: Pod "downwardapi-volume-07dbc578-7dd4-4501-a0e9-68fd1d06c68e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.977568ms
Nov 15 16:09:18.110: INFO: Pod "downwardapi-volume-07dbc578-7dd4-4501-a0e9-68fd1d06c68e": Phase="Running", Reason="", readiness=true. Elapsed: 2.03976422s
Nov 15 16:09:20.098: INFO: Pod "downwardapi-volume-07dbc578-7dd4-4501-a0e9-68fd1d06c68e": Phase="Running", Reason="", readiness=false. Elapsed: 4.028282831s
Nov 15 16:09:22.100: INFO: Pod "downwardapi-volume-07dbc578-7dd4-4501-a0e9-68fd1d06c68e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030421137s
STEP: Saw pod success 11/15/23 16:09:22.101
Nov 15 16:09:22.101: INFO: Pod "downwardapi-volume-07dbc578-7dd4-4501-a0e9-68fd1d06c68e" satisfied condition "Succeeded or Failed"
Nov 15 16:09:22.119: INFO: Trying to get logs from node 10.15.40.115 pod downwardapi-volume-07dbc578-7dd4-4501-a0e9-68fd1d06c68e container client-container: <nil>
STEP: delete the pod 11/15/23 16:09:22.313
Nov 15 16:09:22.360: INFO: Waiting for pod downwardapi-volume-07dbc578-7dd4-4501-a0e9-68fd1d06c68e to disappear
Nov 15 16:09:22.374: INFO: Pod downwardapi-volume-07dbc578-7dd4-4501-a0e9-68fd1d06c68e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Nov 15 16:09:22.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6655" for this suite. 11/15/23 16:09:22.401
------------------------------
• [SLOW TEST] [6.484 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:09:15.943
    Nov 15 16:09:15.943: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename projected 11/15/23 16:09:15.946
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:09:16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:09:16.02
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 11/15/23 16:09:16.036
    Nov 15 16:09:16.070: INFO: Waiting up to 5m0s for pod "downwardapi-volume-07dbc578-7dd4-4501-a0e9-68fd1d06c68e" in namespace "projected-6655" to be "Succeeded or Failed"
    Nov 15 16:09:16.084: INFO: Pod "downwardapi-volume-07dbc578-7dd4-4501-a0e9-68fd1d06c68e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.977568ms
    Nov 15 16:09:18.110: INFO: Pod "downwardapi-volume-07dbc578-7dd4-4501-a0e9-68fd1d06c68e": Phase="Running", Reason="", readiness=true. Elapsed: 2.03976422s
    Nov 15 16:09:20.098: INFO: Pod "downwardapi-volume-07dbc578-7dd4-4501-a0e9-68fd1d06c68e": Phase="Running", Reason="", readiness=false. Elapsed: 4.028282831s
    Nov 15 16:09:22.100: INFO: Pod "downwardapi-volume-07dbc578-7dd4-4501-a0e9-68fd1d06c68e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030421137s
    STEP: Saw pod success 11/15/23 16:09:22.101
    Nov 15 16:09:22.101: INFO: Pod "downwardapi-volume-07dbc578-7dd4-4501-a0e9-68fd1d06c68e" satisfied condition "Succeeded or Failed"
    Nov 15 16:09:22.119: INFO: Trying to get logs from node 10.15.40.115 pod downwardapi-volume-07dbc578-7dd4-4501-a0e9-68fd1d06c68e container client-container: <nil>
    STEP: delete the pod 11/15/23 16:09:22.313
    Nov 15 16:09:22.360: INFO: Waiting for pod downwardapi-volume-07dbc578-7dd4-4501-a0e9-68fd1d06c68e to disappear
    Nov 15 16:09:22.374: INFO: Pod downwardapi-volume-07dbc578-7dd4-4501-a0e9-68fd1d06c68e no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:09:22.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6655" for this suite. 11/15/23 16:09:22.401
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:09:22.435
Nov 15 16:09:22.436: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename security-context-test 11/15/23 16:09:22.437
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:09:22.489
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:09:22.507
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Nov 15 16:09:22.557: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-8a1bca97-c608-454e-bc02-7328968c5078" in namespace "security-context-test-5434" to be "Succeeded or Failed"
Nov 15 16:09:22.579: INFO: Pod "busybox-privileged-false-8a1bca97-c608-454e-bc02-7328968c5078": Phase="Pending", Reason="", readiness=false. Elapsed: 22.09595ms
Nov 15 16:09:24.595: INFO: Pod "busybox-privileged-false-8a1bca97-c608-454e-bc02-7328968c5078": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037786013s
Nov 15 16:09:26.599: INFO: Pod "busybox-privileged-false-8a1bca97-c608-454e-bc02-7328968c5078": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04196718s
Nov 15 16:09:28.592: INFO: Pod "busybox-privileged-false-8a1bca97-c608-454e-bc02-7328968c5078": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035295553s
Nov 15 16:09:28.593: INFO: Pod "busybox-privileged-false-8a1bca97-c608-454e-bc02-7328968c5078" satisfied condition "Succeeded or Failed"
Nov 15 16:09:28.632: INFO: Got logs for pod "busybox-privileged-false-8a1bca97-c608-454e-bc02-7328968c5078": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Nov 15 16:09:28.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-5434" for this suite. 11/15/23 16:09:28.653
------------------------------
• [SLOW TEST] [6.240 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:09:22.435
    Nov 15 16:09:22.436: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename security-context-test 11/15/23 16:09:22.437
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:09:22.489
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:09:22.507
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Nov 15 16:09:22.557: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-8a1bca97-c608-454e-bc02-7328968c5078" in namespace "security-context-test-5434" to be "Succeeded or Failed"
    Nov 15 16:09:22.579: INFO: Pod "busybox-privileged-false-8a1bca97-c608-454e-bc02-7328968c5078": Phase="Pending", Reason="", readiness=false. Elapsed: 22.09595ms
    Nov 15 16:09:24.595: INFO: Pod "busybox-privileged-false-8a1bca97-c608-454e-bc02-7328968c5078": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037786013s
    Nov 15 16:09:26.599: INFO: Pod "busybox-privileged-false-8a1bca97-c608-454e-bc02-7328968c5078": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04196718s
    Nov 15 16:09:28.592: INFO: Pod "busybox-privileged-false-8a1bca97-c608-454e-bc02-7328968c5078": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035295553s
    Nov 15 16:09:28.593: INFO: Pod "busybox-privileged-false-8a1bca97-c608-454e-bc02-7328968c5078" satisfied condition "Succeeded or Failed"
    Nov 15 16:09:28.632: INFO: Got logs for pod "busybox-privileged-false-8a1bca97-c608-454e-bc02-7328968c5078": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:09:28.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-5434" for this suite. 11/15/23 16:09:28.653
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:09:28.68
Nov 15 16:09:28.680: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename init-container 11/15/23 16:09:28.684
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:09:28.735
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:09:28.748
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 11/15/23 16:09:28.767
Nov 15 16:09:28.767: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Nov 15 16:09:33.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-5959" for this suite. 11/15/23 16:09:33.796
------------------------------
• [SLOW TEST] [5.145 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:09:28.68
    Nov 15 16:09:28.680: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename init-container 11/15/23 16:09:28.684
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:09:28.735
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:09:28.748
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 11/15/23 16:09:28.767
    Nov 15 16:09:28.767: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:09:33.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-5959" for this suite. 11/15/23 16:09:33.796
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:09:33.829
Nov 15 16:09:33.830: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename crd-webhook 11/15/23 16:09:33.832
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:09:33.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:09:33.922
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 11/15/23 16:09:33.937
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 11/15/23 16:09:34.282
STEP: Deploying the custom resource conversion webhook pod 11/15/23 16:09:34.318
STEP: Wait for the deployment to be ready 11/15/23 16:09:34.371
Nov 15 16:09:34.415: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/15/23 16:09:36.476
STEP: Verifying the service has paired with the endpoint 11/15/23 16:09:36.513
Nov 15 16:09:37.515: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Nov 15 16:09:37.533: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Creating a v1 custom resource 11/15/23 16:09:40.385
STEP: v2 custom resource should be converted 11/15/23 16:09:40.416
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 16:09:41.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-1325" for this suite. 11/15/23 16:09:41.201
------------------------------
• [SLOW TEST] [7.413 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:09:33.829
    Nov 15 16:09:33.830: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename crd-webhook 11/15/23 16:09:33.832
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:09:33.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:09:33.922
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 11/15/23 16:09:33.937
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 11/15/23 16:09:34.282
    STEP: Deploying the custom resource conversion webhook pod 11/15/23 16:09:34.318
    STEP: Wait for the deployment to be ready 11/15/23 16:09:34.371
    Nov 15 16:09:34.415: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/15/23 16:09:36.476
    STEP: Verifying the service has paired with the endpoint 11/15/23 16:09:36.513
    Nov 15 16:09:37.515: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Nov 15 16:09:37.533: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Creating a v1 custom resource 11/15/23 16:09:40.385
    STEP: v2 custom resource should be converted 11/15/23 16:09:40.416
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:09:41.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-1325" for this suite. 11/15/23 16:09:41.201
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:09:41.243
Nov 15 16:09:41.243: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename disruption 11/15/23 16:09:41.244
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:09:41.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:09:41.339
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 11/15/23 16:09:41.354
STEP: Waiting for the pdb to be processed 11/15/23 16:09:41.385
STEP: updating the pdb 11/15/23 16:09:43.417
STEP: Waiting for the pdb to be processed 11/15/23 16:09:43.454
STEP: patching the pdb 11/15/23 16:09:45.498
STEP: Waiting for the pdb to be processed 11/15/23 16:09:45.537
STEP: Waiting for the pdb to be deleted 11/15/23 16:09:47.615
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Nov 15 16:09:47.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-9598" for this suite. 11/15/23 16:09:47.654
------------------------------
• [SLOW TEST] [6.437 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:09:41.243
    Nov 15 16:09:41.243: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename disruption 11/15/23 16:09:41.244
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:09:41.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:09:41.339
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 11/15/23 16:09:41.354
    STEP: Waiting for the pdb to be processed 11/15/23 16:09:41.385
    STEP: updating the pdb 11/15/23 16:09:43.417
    STEP: Waiting for the pdb to be processed 11/15/23 16:09:43.454
    STEP: patching the pdb 11/15/23 16:09:45.498
    STEP: Waiting for the pdb to be processed 11/15/23 16:09:45.537
    STEP: Waiting for the pdb to be deleted 11/15/23 16:09:47.615
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:09:47.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-9598" for this suite. 11/15/23 16:09:47.654
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:09:47.684
Nov 15 16:09:47.684: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename projected 11/15/23 16:09:47.687
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:09:47.741
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:09:47.757
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-0fd4b241-3ac7-4eb7-b999-f535e13ea28c 11/15/23 16:09:47.777
STEP: Creating a pod to test consume secrets 11/15/23 16:09:47.798
Nov 15 16:09:47.832: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-20788d00-7634-4855-be95-ed7d8a037fd5" in namespace "projected-3158" to be "Succeeded or Failed"
Nov 15 16:09:47.846: INFO: Pod "pod-projected-secrets-20788d00-7634-4855-be95-ed7d8a037fd5": Phase="Pending", Reason="", readiness=false. Elapsed: 13.910972ms
Nov 15 16:09:49.861: INFO: Pod "pod-projected-secrets-20788d00-7634-4855-be95-ed7d8a037fd5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029766813s
Nov 15 16:09:51.860: INFO: Pod "pod-projected-secrets-20788d00-7634-4855-be95-ed7d8a037fd5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028809446s
Nov 15 16:09:53.866: INFO: Pod "pod-projected-secrets-20788d00-7634-4855-be95-ed7d8a037fd5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034232357s
STEP: Saw pod success 11/15/23 16:09:53.866
Nov 15 16:09:53.866: INFO: Pod "pod-projected-secrets-20788d00-7634-4855-be95-ed7d8a037fd5" satisfied condition "Succeeded or Failed"
Nov 15 16:09:53.881: INFO: Trying to get logs from node 10.15.40.115 pod pod-projected-secrets-20788d00-7634-4855-be95-ed7d8a037fd5 container projected-secret-volume-test: <nil>
STEP: delete the pod 11/15/23 16:09:53.921
Nov 15 16:09:53.958: INFO: Waiting for pod pod-projected-secrets-20788d00-7634-4855-be95-ed7d8a037fd5 to disappear
Nov 15 16:09:53.974: INFO: Pod pod-projected-secrets-20788d00-7634-4855-be95-ed7d8a037fd5 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Nov 15 16:09:53.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3158" for this suite. 11/15/23 16:09:54.001
------------------------------
• [SLOW TEST] [6.341 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:09:47.684
    Nov 15 16:09:47.684: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename projected 11/15/23 16:09:47.687
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:09:47.741
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:09:47.757
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-0fd4b241-3ac7-4eb7-b999-f535e13ea28c 11/15/23 16:09:47.777
    STEP: Creating a pod to test consume secrets 11/15/23 16:09:47.798
    Nov 15 16:09:47.832: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-20788d00-7634-4855-be95-ed7d8a037fd5" in namespace "projected-3158" to be "Succeeded or Failed"
    Nov 15 16:09:47.846: INFO: Pod "pod-projected-secrets-20788d00-7634-4855-be95-ed7d8a037fd5": Phase="Pending", Reason="", readiness=false. Elapsed: 13.910972ms
    Nov 15 16:09:49.861: INFO: Pod "pod-projected-secrets-20788d00-7634-4855-be95-ed7d8a037fd5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029766813s
    Nov 15 16:09:51.860: INFO: Pod "pod-projected-secrets-20788d00-7634-4855-be95-ed7d8a037fd5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028809446s
    Nov 15 16:09:53.866: INFO: Pod "pod-projected-secrets-20788d00-7634-4855-be95-ed7d8a037fd5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034232357s
    STEP: Saw pod success 11/15/23 16:09:53.866
    Nov 15 16:09:53.866: INFO: Pod "pod-projected-secrets-20788d00-7634-4855-be95-ed7d8a037fd5" satisfied condition "Succeeded or Failed"
    Nov 15 16:09:53.881: INFO: Trying to get logs from node 10.15.40.115 pod pod-projected-secrets-20788d00-7634-4855-be95-ed7d8a037fd5 container projected-secret-volume-test: <nil>
    STEP: delete the pod 11/15/23 16:09:53.921
    Nov 15 16:09:53.958: INFO: Waiting for pod pod-projected-secrets-20788d00-7634-4855-be95-ed7d8a037fd5 to disappear
    Nov 15 16:09:53.974: INFO: Pod pod-projected-secrets-20788d00-7634-4855-be95-ed7d8a037fd5 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:09:53.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3158" for this suite. 11/15/23 16:09:54.001
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:09:54.033
Nov 15 16:09:54.033: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename svcaccounts 11/15/23 16:09:54.035
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:09:54.089
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:09:54.102
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 11/15/23 16:09:54.114
STEP: watching for the ServiceAccount to be added 11/15/23 16:09:54.142
STEP: patching the ServiceAccount 11/15/23 16:09:54.147
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 11/15/23 16:09:54.165
STEP: deleting the ServiceAccount 11/15/23 16:09:54.183
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Nov 15 16:09:54.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-940" for this suite. 11/15/23 16:09:54.256
------------------------------
• [0.250 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:09:54.033
    Nov 15 16:09:54.033: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename svcaccounts 11/15/23 16:09:54.035
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:09:54.089
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:09:54.102
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 11/15/23 16:09:54.114
    STEP: watching for the ServiceAccount to be added 11/15/23 16:09:54.142
    STEP: patching the ServiceAccount 11/15/23 16:09:54.147
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 11/15/23 16:09:54.165
    STEP: deleting the ServiceAccount 11/15/23 16:09:54.183
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:09:54.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-940" for this suite. 11/15/23 16:09:54.256
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:09:54.284
Nov 15 16:09:54.284: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename kubectl 11/15/23 16:09:54.287
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:09:54.337
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:09:54.351
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 11/15/23 16:09:54.365
Nov 15 16:09:54.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-9923 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Nov 15 16:09:54.519: INFO: stderr: ""
Nov 15 16:09:54.519: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 11/15/23 16:09:54.519
Nov 15 16:09:54.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-9923 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Nov 15 16:09:55.620: INFO: stderr: ""
Nov 15 16:09:55.620: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 11/15/23 16:09:55.62
Nov 15 16:09:55.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-9923 delete pods e2e-test-httpd-pod'
Nov 15 16:09:56.995: INFO: stderr: ""
Nov 15 16:09:56.995: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 15 16:09:56.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9923" for this suite. 11/15/23 16:09:57.021
------------------------------
• [2.764 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:09:54.284
    Nov 15 16:09:54.284: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename kubectl 11/15/23 16:09:54.287
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:09:54.337
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:09:54.351
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 11/15/23 16:09:54.365
    Nov 15 16:09:54.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-9923 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Nov 15 16:09:54.519: INFO: stderr: ""
    Nov 15 16:09:54.519: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 11/15/23 16:09:54.519
    Nov 15 16:09:54.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-9923 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Nov 15 16:09:55.620: INFO: stderr: ""
    Nov 15 16:09:55.620: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 11/15/23 16:09:55.62
    Nov 15 16:09:55.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-9923 delete pods e2e-test-httpd-pod'
    Nov 15 16:09:56.995: INFO: stderr: ""
    Nov 15 16:09:56.995: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:09:56.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9923" for this suite. 11/15/23 16:09:57.021
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:09:57.05
Nov 15 16:09:57.050: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename downward-api 11/15/23 16:09:57.054
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:09:57.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:09:57.112
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 11/15/23 16:09:57.126
Nov 15 16:09:57.161: INFO: Waiting up to 5m0s for pod "downward-api-e736481d-8049-4c14-bf34-03f0ddc2c51d" in namespace "downward-api-2027" to be "Succeeded or Failed"
Nov 15 16:09:57.173: INFO: Pod "downward-api-e736481d-8049-4c14-bf34-03f0ddc2c51d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.72516ms
Nov 15 16:09:59.191: INFO: Pod "downward-api-e736481d-8049-4c14-bf34-03f0ddc2c51d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030163553s
Nov 15 16:10:01.189: INFO: Pod "downward-api-e736481d-8049-4c14-bf34-03f0ddc2c51d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028762023s
STEP: Saw pod success 11/15/23 16:10:01.189
Nov 15 16:10:01.190: INFO: Pod "downward-api-e736481d-8049-4c14-bf34-03f0ddc2c51d" satisfied condition "Succeeded or Failed"
Nov 15 16:10:01.205: INFO: Trying to get logs from node 10.15.40.115 pod downward-api-e736481d-8049-4c14-bf34-03f0ddc2c51d container dapi-container: <nil>
STEP: delete the pod 11/15/23 16:10:01.246
Nov 15 16:10:01.291: INFO: Waiting for pod downward-api-e736481d-8049-4c14-bf34-03f0ddc2c51d to disappear
Nov 15 16:10:01.306: INFO: Pod downward-api-e736481d-8049-4c14-bf34-03f0ddc2c51d no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Nov 15 16:10:01.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2027" for this suite. 11/15/23 16:10:01.333
------------------------------
• [4.308 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:09:57.05
    Nov 15 16:09:57.050: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename downward-api 11/15/23 16:09:57.054
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:09:57.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:09:57.112
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 11/15/23 16:09:57.126
    Nov 15 16:09:57.161: INFO: Waiting up to 5m0s for pod "downward-api-e736481d-8049-4c14-bf34-03f0ddc2c51d" in namespace "downward-api-2027" to be "Succeeded or Failed"
    Nov 15 16:09:57.173: INFO: Pod "downward-api-e736481d-8049-4c14-bf34-03f0ddc2c51d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.72516ms
    Nov 15 16:09:59.191: INFO: Pod "downward-api-e736481d-8049-4c14-bf34-03f0ddc2c51d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030163553s
    Nov 15 16:10:01.189: INFO: Pod "downward-api-e736481d-8049-4c14-bf34-03f0ddc2c51d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028762023s
    STEP: Saw pod success 11/15/23 16:10:01.189
    Nov 15 16:10:01.190: INFO: Pod "downward-api-e736481d-8049-4c14-bf34-03f0ddc2c51d" satisfied condition "Succeeded or Failed"
    Nov 15 16:10:01.205: INFO: Trying to get logs from node 10.15.40.115 pod downward-api-e736481d-8049-4c14-bf34-03f0ddc2c51d container dapi-container: <nil>
    STEP: delete the pod 11/15/23 16:10:01.246
    Nov 15 16:10:01.291: INFO: Waiting for pod downward-api-e736481d-8049-4c14-bf34-03f0ddc2c51d to disappear
    Nov 15 16:10:01.306: INFO: Pod downward-api-e736481d-8049-4c14-bf34-03f0ddc2c51d no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:10:01.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2027" for this suite. 11/15/23 16:10:01.333
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:10:01.371
Nov 15 16:10:01.371: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename cronjob 11/15/23 16:10:01.372
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:10:01.426
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:10:01.441
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 11/15/23 16:10:01.454
STEP: Ensuring a job is scheduled 11/15/23 16:10:01.477
STEP: Ensuring exactly one is scheduled 11/15/23 16:11:01.52
STEP: Ensuring exactly one running job exists by listing jobs explicitly 11/15/23 16:11:01.537
STEP: Ensuring no more jobs are scheduled 11/15/23 16:11:01.552
STEP: Removing cronjob 11/15/23 16:16:01.609
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Nov 15 16:16:01.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-914" for this suite. 11/15/23 16:16:01.662
------------------------------
• [SLOW TEST] [360.317 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:10:01.371
    Nov 15 16:10:01.371: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename cronjob 11/15/23 16:10:01.372
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:10:01.426
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:10:01.441
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 11/15/23 16:10:01.454
    STEP: Ensuring a job is scheduled 11/15/23 16:10:01.477
    STEP: Ensuring exactly one is scheduled 11/15/23 16:11:01.52
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 11/15/23 16:11:01.537
    STEP: Ensuring no more jobs are scheduled 11/15/23 16:11:01.552
    STEP: Removing cronjob 11/15/23 16:16:01.609
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:16:01.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-914" for this suite. 11/15/23 16:16:01.662
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:16:01.694
Nov 15 16:16:01.694: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename cronjob 11/15/23 16:16:01.696
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:16:01.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:16:01.792
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 11/15/23 16:16:01.807
STEP: Ensuring more than one job is running at a time 11/15/23 16:16:01.828
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 11/15/23 16:18:01.856
STEP: Removing cronjob 11/15/23 16:18:01.895
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Nov 15 16:18:01.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-8306" for this suite. 11/15/23 16:18:01.964
------------------------------
• [SLOW TEST] [120.306 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:16:01.694
    Nov 15 16:16:01.694: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename cronjob 11/15/23 16:16:01.696
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:16:01.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:16:01.792
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 11/15/23 16:16:01.807
    STEP: Ensuring more than one job is running at a time 11/15/23 16:16:01.828
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 11/15/23 16:18:01.856
    STEP: Removing cronjob 11/15/23 16:18:01.895
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:18:01.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-8306" for this suite. 11/15/23 16:18:01.964
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:18:02.001
Nov 15 16:18:02.001: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename kubectl 11/15/23 16:18:02.004
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:18:02.058
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:18:02.072
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Nov 15 16:18:02.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-425 version'
Nov 15 16:18:02.223: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Nov 15 16:18:02.223: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.10\", GitCommit:\"b8609d4dd75c5d6fba4a5eaa63a5507cb39a6e99\", GitTreeState:\"clean\", BuildDate:\"2023-10-18T11:44:31Z\", GoVersion:\"go1.20.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.10+IKS\", GitCommit:\"f70b9038b5717b1618d091499b4a60ca3f8cd8af\", GitTreeState:\"clean\", BuildDate:\"2023-11-07T04:32:22Z\", GoVersion:\"go1.20.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 15 16:18:02.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-425" for this suite. 11/15/23 16:18:02.248
------------------------------
• [0.290 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:18:02.001
    Nov 15 16:18:02.001: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename kubectl 11/15/23 16:18:02.004
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:18:02.058
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:18:02.072
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Nov 15 16:18:02.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-425 version'
    Nov 15 16:18:02.223: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Nov 15 16:18:02.223: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.10\", GitCommit:\"b8609d4dd75c5d6fba4a5eaa63a5507cb39a6e99\", GitTreeState:\"clean\", BuildDate:\"2023-10-18T11:44:31Z\", GoVersion:\"go1.20.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.10+IKS\", GitCommit:\"f70b9038b5717b1618d091499b4a60ca3f8cd8af\", GitTreeState:\"clean\", BuildDate:\"2023-11-07T04:32:22Z\", GoVersion:\"go1.20.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:18:02.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-425" for this suite. 11/15/23 16:18:02.248
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:18:02.293
Nov 15 16:18:02.293: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename sched-pred 11/15/23 16:18:02.297
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:18:02.35
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:18:02.365
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Nov 15 16:18:02.381: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 15 16:18:02.431: INFO: Waiting for terminating namespaces to be deleted...
Nov 15 16:18:02.453: INFO: 
Logging pods the apiserver thinks is on node 10.15.40.106 before test
Nov 15 16:18:02.494: INFO: ibm-cloud-provider-ip-163-109-74-98-5d59656977-bgllw from ibm-system started at 2023-11-15 15:39:54 +0000 UTC (1 container statuses recorded)
Nov 15 16:18:02.494: INFO: 	Container ibm-cloud-provider-ip-163-109-74-98 ready: true, restart count 0
Nov 15 16:18:02.494: INFO: calico-kube-controllers-7847f7647d-srqd4 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 16:18:02.494: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Nov 15 16:18:02.494: INFO: calico-node-67csc from kube-system started at 2023-11-15 13:13:21 +0000 UTC (1 container statuses recorded)
Nov 15 16:18:02.494: INFO: 	Container calico-node ready: true, restart count 0
Nov 15 16:18:02.494: INFO: calico-typha-5b5db87f55-nwmh4 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 16:18:02.494: INFO: 	Container calico-typha ready: true, restart count 0
Nov 15 16:18:02.494: INFO: coredns-5845f98d4-2lnrf from kube-system started at 2023-11-15 13:23:15 +0000 UTC (1 container statuses recorded)
Nov 15 16:18:02.494: INFO: 	Container coredns ready: true, restart count 0
Nov 15 16:18:02.494: INFO: coredns-autoscaler-85f4bdddf6-qwxmw from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 16:18:02.494: INFO: 	Container autoscaler ready: true, restart count 0
Nov 15 16:18:02.494: INFO: dashboard-metrics-scraper-7cf679fbdf-frv7t from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 16:18:02.495: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Nov 15 16:18:02.495: INFO: ibm-file-plugin-557f875d5f-jrfnv from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 16:18:02.495: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Nov 15 16:18:02.495: INFO: ibm-keepalived-watcher-dqknf from kube-system started at 2023-11-15 13:13:21 +0000 UTC (1 container statuses recorded)
Nov 15 16:18:02.495: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 15 16:18:02.495: INFO: ibm-master-proxy-static-10.15.40.106 from kube-system started at 2023-11-15 13:13:08 +0000 UTC (2 container statuses recorded)
Nov 15 16:18:02.495: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 15 16:18:02.495: INFO: 	Container pause ready: true, restart count 0
Nov 15 16:18:02.495: INFO: ibm-storage-watcher-7d897847f4-k64t5 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 16:18:02.495: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Nov 15 16:18:02.495: INFO: ibmcloud-block-storage-driver-f97nm from kube-system started at 2023-11-15 13:13:30 +0000 UTC (1 container statuses recorded)
Nov 15 16:18:02.496: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 15 16:18:02.496: INFO: ibmcloud-block-storage-plugin-5bd59f7b48-2dxcj from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 16:18:02.496: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Nov 15 16:18:02.496: INFO: ingress-cluster-healthcheck-5985f966bb-jgjx8 from kube-system started at 2023-11-15 15:39:54 +0000 UTC (1 container statuses recorded)
Nov 15 16:18:02.496: INFO: 	Container ingress-cluster-healthcheck ready: true, restart count 0
Nov 15 16:18:02.496: INFO: konnectivity-agent-5brpz from kube-system started at 2023-11-15 13:22:38 +0000 UTC (1 container statuses recorded)
Nov 15 16:18:02.496: INFO: 	Container konnectivity-agent ready: true, restart count 0
Nov 15 16:18:02.496: INFO: kubernetes-dashboard-5ccdc9cbb8-wbmfz from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 16:18:02.496: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Nov 15 16:18:02.496: INFO: metrics-server-7cbd9c9b48-8tr74 from kube-system started at 2023-11-15 15:39:54 +0000 UTC (3 container statuses recorded)
Nov 15 16:18:02.496: INFO: 	Container config-watcher ready: true, restart count 0
Nov 15 16:18:02.496: INFO: 	Container metrics-server ready: true, restart count 0
Nov 15 16:18:02.496: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Nov 15 16:18:02.496: INFO: public-crclac150z0u38f277tnpg-alb1-6df9445bb6-qw2bt from kube-system started at 2023-11-15 15:39:54 +0000 UTC (1 container statuses recorded)
Nov 15 16:18:02.497: INFO: 	Container nginx-ingress ready: true, restart count 0
Nov 15 16:18:02.497: INFO: snapshot-controller-6db47fc545-5nnph from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 16:18:02.497: INFO: 	Container snapshot-controller ready: true, restart count 0
Nov 15 16:18:02.497: INFO: snapshot-controller-6db47fc545-h4q49 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 16:18:02.497: INFO: 	Container snapshot-controller ready: true, restart count 0
Nov 15 16:18:02.497: INFO: snapshot-controller-6db47fc545-khkb2 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 16:18:02.498: INFO: 	Container snapshot-controller ready: true, restart count 0
Nov 15 16:18:02.498: INFO: sonobuoy-systemd-logs-daemon-set-eabb9aa298c743f7-zxdnv from sonobuoy started at 2023-11-15 15:24:45 +0000 UTC (2 container statuses recorded)
Nov 15 16:18:02.498: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 15 16:18:02.498: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 15 16:18:02.498: INFO: 
Logging pods the apiserver thinks is on node 10.15.40.114 before test
Nov 15 16:18:02.534: INFO: ibm-cloud-provider-ip-163-109-74-98-5d59656977-wxlx6 from ibm-system started at 2023-11-15 13:18:32 +0000 UTC (1 container statuses recorded)
Nov 15 16:18:02.534: INFO: 	Container ibm-cloud-provider-ip-163-109-74-98 ready: true, restart count 0
Nov 15 16:18:02.535: INFO: calico-node-wqvj8 from kube-system started at 2023-11-15 13:13:28 +0000 UTC (1 container statuses recorded)
Nov 15 16:18:02.535: INFO: 	Container calico-node ready: true, restart count 0
Nov 15 16:18:02.535: INFO: calico-typha-5b5db87f55-4qv4c from kube-system started at 2023-11-15 13:13:53 +0000 UTC (1 container statuses recorded)
Nov 15 16:18:02.535: INFO: 	Container calico-typha ready: true, restart count 0
Nov 15 16:18:02.536: INFO: coredns-5845f98d4-6cld8 from kube-system started at 2023-11-15 13:23:15 +0000 UTC (1 container statuses recorded)
Nov 15 16:18:02.536: INFO: 	Container coredns ready: true, restart count 0
Nov 15 16:18:02.536: INFO: ibm-keepalived-watcher-xsslz from kube-system started at 2023-11-15 13:13:28 +0000 UTC (1 container statuses recorded)
Nov 15 16:18:02.537: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 15 16:18:02.537: INFO: ibm-master-proxy-static-10.15.40.114 from kube-system started at 2023-11-15 13:13:15 +0000 UTC (2 container statuses recorded)
Nov 15 16:18:02.537: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 15 16:18:02.537: INFO: 	Container pause ready: true, restart count 0
Nov 15 16:18:02.538: INFO: ibmcloud-block-storage-driver-4txx8 from kube-system started at 2023-11-15 13:13:37 +0000 UTC (1 container statuses recorded)
Nov 15 16:18:02.538: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 15 16:18:02.538: INFO: konnectivity-agent-zr4zl from kube-system started at 2023-11-15 13:22:41 +0000 UTC (1 container statuses recorded)
Nov 15 16:18:02.538: INFO: 	Container konnectivity-agent ready: true, restart count 0
Nov 15 16:18:02.539: INFO: metrics-server-7cbd9c9b48-cnskr from kube-system started at 2023-11-15 13:56:24 +0000 UTC (3 container statuses recorded)
Nov 15 16:18:02.539: INFO: 	Container config-watcher ready: true, restart count 0
Nov 15 16:18:02.539: INFO: 	Container metrics-server ready: true, restart count 0
Nov 15 16:18:02.539: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Nov 15 16:18:02.540: INFO: public-crclac150z0u38f277tnpg-alb1-6df9445bb6-jkpcd from kube-system started at 2023-11-15 13:23:30 +0000 UTC (1 container statuses recorded)
Nov 15 16:18:02.540: INFO: 	Container nginx-ingress ready: true, restart count 0
Nov 15 16:18:02.540: INFO: sonobuoy-e2e-job-3a51e32fa56b4156 from sonobuoy started at 2023-11-15 15:24:45 +0000 UTC (2 container statuses recorded)
Nov 15 16:18:02.540: INFO: 	Container e2e ready: true, restart count 0
Nov 15 16:18:02.540: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 15 16:18:02.541: INFO: sonobuoy-systemd-logs-daemon-set-eabb9aa298c743f7-sxg5b from sonobuoy started at 2023-11-15 15:24:45 +0000 UTC (2 container statuses recorded)
Nov 15 16:18:02.541: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 15 16:18:02.541: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 15 16:18:02.541: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-11-15 13:15:59 +0000 UTC (1 container statuses recorded)
Nov 15 16:18:02.542: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Nov 15 16:18:02.542: INFO: 
Logging pods the apiserver thinks is on node 10.15.40.115 before test
Nov 15 16:18:02.572: INFO: concurrent-28334417-98b2w from cronjob-8306 started at 2023-11-15 16:17:00 +0000 UTC (1 container statuses recorded)
Nov 15 16:18:02.572: INFO: 	Container c ready: true, restart count 0
Nov 15 16:18:02.572: INFO: concurrent-28334418-75crr from cronjob-8306 started at 2023-11-15 16:18:00 +0000 UTC (1 container statuses recorded)
Nov 15 16:18:02.572: INFO: 	Container c ready: true, restart count 0
Nov 15 16:18:02.572: INFO: calico-node-gknnt from kube-system started at 2023-11-15 13:13:40 +0000 UTC (1 container statuses recorded)
Nov 15 16:18:02.572: INFO: 	Container calico-node ready: true, restart count 0
Nov 15 16:18:02.572: INFO: calico-typha-5b5db87f55-fchsg from kube-system started at 2023-11-15 15:40:02 +0000 UTC (1 container statuses recorded)
Nov 15 16:18:02.572: INFO: 	Container calico-typha ready: true, restart count 0
Nov 15 16:18:02.572: INFO: coredns-5845f98d4-rt4sq from kube-system started at 2023-11-15 15:39:54 +0000 UTC (1 container statuses recorded)
Nov 15 16:18:02.572: INFO: 	Container coredns ready: true, restart count 0
Nov 15 16:18:02.572: INFO: ibm-keepalived-watcher-75lpq from kube-system started at 2023-11-15 13:13:40 +0000 UTC (1 container statuses recorded)
Nov 15 16:18:02.572: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 15 16:18:02.572: INFO: ibm-master-proxy-static-10.15.40.115 from kube-system started at 2023-11-15 13:13:28 +0000 UTC (2 container statuses recorded)
Nov 15 16:18:02.572: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 15 16:18:02.572: INFO: 	Container pause ready: true, restart count 0
Nov 15 16:18:02.573: INFO: ibmcloud-block-storage-driver-gbqth from kube-system started at 2023-11-15 13:13:49 +0000 UTC (1 container statuses recorded)
Nov 15 16:18:02.573: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 15 16:18:02.573: INFO: konnectivity-agent-9pkld from kube-system started at 2023-11-15 13:22:45 +0000 UTC (1 container statuses recorded)
Nov 15 16:18:02.573: INFO: 	Container konnectivity-agent ready: true, restart count 0
Nov 15 16:18:02.573: INFO: sonobuoy from sonobuoy started at 2023-11-15 15:24:39 +0000 UTC (1 container statuses recorded)
Nov 15 16:18:02.573: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 15 16:18:02.573: INFO: sonobuoy-systemd-logs-daemon-set-eabb9aa298c743f7-hmfn7 from sonobuoy started at 2023-11-15 15:24:45 +0000 UTC (2 container statuses recorded)
Nov 15 16:18:02.573: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 15 16:18:02.573: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 11/15/23 16:18:02.573
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1797d82f74705789], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 11/15/23 16:18:02.723
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 16:18:03.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-5398" for this suite. 11/15/23 16:18:03.766
------------------------------
• [1.507 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:18:02.293
    Nov 15 16:18:02.293: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename sched-pred 11/15/23 16:18:02.297
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:18:02.35
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:18:02.365
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Nov 15 16:18:02.381: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Nov 15 16:18:02.431: INFO: Waiting for terminating namespaces to be deleted...
    Nov 15 16:18:02.453: INFO: 
    Logging pods the apiserver thinks is on node 10.15.40.106 before test
    Nov 15 16:18:02.494: INFO: ibm-cloud-provider-ip-163-109-74-98-5d59656977-bgllw from ibm-system started at 2023-11-15 15:39:54 +0000 UTC (1 container statuses recorded)
    Nov 15 16:18:02.494: INFO: 	Container ibm-cloud-provider-ip-163-109-74-98 ready: true, restart count 0
    Nov 15 16:18:02.494: INFO: calico-kube-controllers-7847f7647d-srqd4 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 16:18:02.494: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Nov 15 16:18:02.494: INFO: calico-node-67csc from kube-system started at 2023-11-15 13:13:21 +0000 UTC (1 container statuses recorded)
    Nov 15 16:18:02.494: INFO: 	Container calico-node ready: true, restart count 0
    Nov 15 16:18:02.494: INFO: calico-typha-5b5db87f55-nwmh4 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 16:18:02.494: INFO: 	Container calico-typha ready: true, restart count 0
    Nov 15 16:18:02.494: INFO: coredns-5845f98d4-2lnrf from kube-system started at 2023-11-15 13:23:15 +0000 UTC (1 container statuses recorded)
    Nov 15 16:18:02.494: INFO: 	Container coredns ready: true, restart count 0
    Nov 15 16:18:02.494: INFO: coredns-autoscaler-85f4bdddf6-qwxmw from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 16:18:02.494: INFO: 	Container autoscaler ready: true, restart count 0
    Nov 15 16:18:02.494: INFO: dashboard-metrics-scraper-7cf679fbdf-frv7t from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 16:18:02.495: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Nov 15 16:18:02.495: INFO: ibm-file-plugin-557f875d5f-jrfnv from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 16:18:02.495: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    Nov 15 16:18:02.495: INFO: ibm-keepalived-watcher-dqknf from kube-system started at 2023-11-15 13:13:21 +0000 UTC (1 container statuses recorded)
    Nov 15 16:18:02.495: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Nov 15 16:18:02.495: INFO: ibm-master-proxy-static-10.15.40.106 from kube-system started at 2023-11-15 13:13:08 +0000 UTC (2 container statuses recorded)
    Nov 15 16:18:02.495: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Nov 15 16:18:02.495: INFO: 	Container pause ready: true, restart count 0
    Nov 15 16:18:02.495: INFO: ibm-storage-watcher-7d897847f4-k64t5 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 16:18:02.495: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    Nov 15 16:18:02.495: INFO: ibmcloud-block-storage-driver-f97nm from kube-system started at 2023-11-15 13:13:30 +0000 UTC (1 container statuses recorded)
    Nov 15 16:18:02.496: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Nov 15 16:18:02.496: INFO: ibmcloud-block-storage-plugin-5bd59f7b48-2dxcj from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 16:18:02.496: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    Nov 15 16:18:02.496: INFO: ingress-cluster-healthcheck-5985f966bb-jgjx8 from kube-system started at 2023-11-15 15:39:54 +0000 UTC (1 container statuses recorded)
    Nov 15 16:18:02.496: INFO: 	Container ingress-cluster-healthcheck ready: true, restart count 0
    Nov 15 16:18:02.496: INFO: konnectivity-agent-5brpz from kube-system started at 2023-11-15 13:22:38 +0000 UTC (1 container statuses recorded)
    Nov 15 16:18:02.496: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Nov 15 16:18:02.496: INFO: kubernetes-dashboard-5ccdc9cbb8-wbmfz from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 16:18:02.496: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Nov 15 16:18:02.496: INFO: metrics-server-7cbd9c9b48-8tr74 from kube-system started at 2023-11-15 15:39:54 +0000 UTC (3 container statuses recorded)
    Nov 15 16:18:02.496: INFO: 	Container config-watcher ready: true, restart count 0
    Nov 15 16:18:02.496: INFO: 	Container metrics-server ready: true, restart count 0
    Nov 15 16:18:02.496: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Nov 15 16:18:02.496: INFO: public-crclac150z0u38f277tnpg-alb1-6df9445bb6-qw2bt from kube-system started at 2023-11-15 15:39:54 +0000 UTC (1 container statuses recorded)
    Nov 15 16:18:02.497: INFO: 	Container nginx-ingress ready: true, restart count 0
    Nov 15 16:18:02.497: INFO: snapshot-controller-6db47fc545-5nnph from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 16:18:02.497: INFO: 	Container snapshot-controller ready: true, restart count 0
    Nov 15 16:18:02.497: INFO: snapshot-controller-6db47fc545-h4q49 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 16:18:02.497: INFO: 	Container snapshot-controller ready: true, restart count 0
    Nov 15 16:18:02.497: INFO: snapshot-controller-6db47fc545-khkb2 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 16:18:02.498: INFO: 	Container snapshot-controller ready: true, restart count 0
    Nov 15 16:18:02.498: INFO: sonobuoy-systemd-logs-daemon-set-eabb9aa298c743f7-zxdnv from sonobuoy started at 2023-11-15 15:24:45 +0000 UTC (2 container statuses recorded)
    Nov 15 16:18:02.498: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 15 16:18:02.498: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 15 16:18:02.498: INFO: 
    Logging pods the apiserver thinks is on node 10.15.40.114 before test
    Nov 15 16:18:02.534: INFO: ibm-cloud-provider-ip-163-109-74-98-5d59656977-wxlx6 from ibm-system started at 2023-11-15 13:18:32 +0000 UTC (1 container statuses recorded)
    Nov 15 16:18:02.534: INFO: 	Container ibm-cloud-provider-ip-163-109-74-98 ready: true, restart count 0
    Nov 15 16:18:02.535: INFO: calico-node-wqvj8 from kube-system started at 2023-11-15 13:13:28 +0000 UTC (1 container statuses recorded)
    Nov 15 16:18:02.535: INFO: 	Container calico-node ready: true, restart count 0
    Nov 15 16:18:02.535: INFO: calico-typha-5b5db87f55-4qv4c from kube-system started at 2023-11-15 13:13:53 +0000 UTC (1 container statuses recorded)
    Nov 15 16:18:02.535: INFO: 	Container calico-typha ready: true, restart count 0
    Nov 15 16:18:02.536: INFO: coredns-5845f98d4-6cld8 from kube-system started at 2023-11-15 13:23:15 +0000 UTC (1 container statuses recorded)
    Nov 15 16:18:02.536: INFO: 	Container coredns ready: true, restart count 0
    Nov 15 16:18:02.536: INFO: ibm-keepalived-watcher-xsslz from kube-system started at 2023-11-15 13:13:28 +0000 UTC (1 container statuses recorded)
    Nov 15 16:18:02.537: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Nov 15 16:18:02.537: INFO: ibm-master-proxy-static-10.15.40.114 from kube-system started at 2023-11-15 13:13:15 +0000 UTC (2 container statuses recorded)
    Nov 15 16:18:02.537: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Nov 15 16:18:02.537: INFO: 	Container pause ready: true, restart count 0
    Nov 15 16:18:02.538: INFO: ibmcloud-block-storage-driver-4txx8 from kube-system started at 2023-11-15 13:13:37 +0000 UTC (1 container statuses recorded)
    Nov 15 16:18:02.538: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Nov 15 16:18:02.538: INFO: konnectivity-agent-zr4zl from kube-system started at 2023-11-15 13:22:41 +0000 UTC (1 container statuses recorded)
    Nov 15 16:18:02.538: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Nov 15 16:18:02.539: INFO: metrics-server-7cbd9c9b48-cnskr from kube-system started at 2023-11-15 13:56:24 +0000 UTC (3 container statuses recorded)
    Nov 15 16:18:02.539: INFO: 	Container config-watcher ready: true, restart count 0
    Nov 15 16:18:02.539: INFO: 	Container metrics-server ready: true, restart count 0
    Nov 15 16:18:02.539: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Nov 15 16:18:02.540: INFO: public-crclac150z0u38f277tnpg-alb1-6df9445bb6-jkpcd from kube-system started at 2023-11-15 13:23:30 +0000 UTC (1 container statuses recorded)
    Nov 15 16:18:02.540: INFO: 	Container nginx-ingress ready: true, restart count 0
    Nov 15 16:18:02.540: INFO: sonobuoy-e2e-job-3a51e32fa56b4156 from sonobuoy started at 2023-11-15 15:24:45 +0000 UTC (2 container statuses recorded)
    Nov 15 16:18:02.540: INFO: 	Container e2e ready: true, restart count 0
    Nov 15 16:18:02.540: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 15 16:18:02.541: INFO: sonobuoy-systemd-logs-daemon-set-eabb9aa298c743f7-sxg5b from sonobuoy started at 2023-11-15 15:24:45 +0000 UTC (2 container statuses recorded)
    Nov 15 16:18:02.541: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 15 16:18:02.541: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 15 16:18:02.541: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-11-15 13:15:59 +0000 UTC (1 container statuses recorded)
    Nov 15 16:18:02.542: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
    Nov 15 16:18:02.542: INFO: 
    Logging pods the apiserver thinks is on node 10.15.40.115 before test
    Nov 15 16:18:02.572: INFO: concurrent-28334417-98b2w from cronjob-8306 started at 2023-11-15 16:17:00 +0000 UTC (1 container statuses recorded)
    Nov 15 16:18:02.572: INFO: 	Container c ready: true, restart count 0
    Nov 15 16:18:02.572: INFO: concurrent-28334418-75crr from cronjob-8306 started at 2023-11-15 16:18:00 +0000 UTC (1 container statuses recorded)
    Nov 15 16:18:02.572: INFO: 	Container c ready: true, restart count 0
    Nov 15 16:18:02.572: INFO: calico-node-gknnt from kube-system started at 2023-11-15 13:13:40 +0000 UTC (1 container statuses recorded)
    Nov 15 16:18:02.572: INFO: 	Container calico-node ready: true, restart count 0
    Nov 15 16:18:02.572: INFO: calico-typha-5b5db87f55-fchsg from kube-system started at 2023-11-15 15:40:02 +0000 UTC (1 container statuses recorded)
    Nov 15 16:18:02.572: INFO: 	Container calico-typha ready: true, restart count 0
    Nov 15 16:18:02.572: INFO: coredns-5845f98d4-rt4sq from kube-system started at 2023-11-15 15:39:54 +0000 UTC (1 container statuses recorded)
    Nov 15 16:18:02.572: INFO: 	Container coredns ready: true, restart count 0
    Nov 15 16:18:02.572: INFO: ibm-keepalived-watcher-75lpq from kube-system started at 2023-11-15 13:13:40 +0000 UTC (1 container statuses recorded)
    Nov 15 16:18:02.572: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Nov 15 16:18:02.572: INFO: ibm-master-proxy-static-10.15.40.115 from kube-system started at 2023-11-15 13:13:28 +0000 UTC (2 container statuses recorded)
    Nov 15 16:18:02.572: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Nov 15 16:18:02.572: INFO: 	Container pause ready: true, restart count 0
    Nov 15 16:18:02.573: INFO: ibmcloud-block-storage-driver-gbqth from kube-system started at 2023-11-15 13:13:49 +0000 UTC (1 container statuses recorded)
    Nov 15 16:18:02.573: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Nov 15 16:18:02.573: INFO: konnectivity-agent-9pkld from kube-system started at 2023-11-15 13:22:45 +0000 UTC (1 container statuses recorded)
    Nov 15 16:18:02.573: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Nov 15 16:18:02.573: INFO: sonobuoy from sonobuoy started at 2023-11-15 15:24:39 +0000 UTC (1 container statuses recorded)
    Nov 15 16:18:02.573: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Nov 15 16:18:02.573: INFO: sonobuoy-systemd-logs-daemon-set-eabb9aa298c743f7-hmfn7 from sonobuoy started at 2023-11-15 15:24:45 +0000 UTC (2 container statuses recorded)
    Nov 15 16:18:02.573: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 15 16:18:02.573: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 11/15/23 16:18:02.573
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.1797d82f74705789], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 11/15/23 16:18:02.723
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:18:03.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-5398" for this suite. 11/15/23 16:18:03.766
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:18:03.8
Nov 15 16:18:03.801: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename resourcequota 11/15/23 16:18:03.803
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:18:03.861
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:18:03.887
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-c7wsq" 11/15/23 16:18:03.929
Nov 15 16:18:03.977: INFO: Resource quota "e2e-rq-status-c7wsq" reports spec: hard cpu limit of 500m
Nov 15 16:18:03.977: INFO: Resource quota "e2e-rq-status-c7wsq" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-c7wsq" /status 11/15/23 16:18:03.977
STEP: Confirm /status for "e2e-rq-status-c7wsq" resourceQuota via watch 11/15/23 16:18:04.032
Nov 15 16:18:04.042: INFO: observed resourceQuota "e2e-rq-status-c7wsq" in namespace "resourcequota-7387" with hard status: v1.ResourceList(nil)
Nov 15 16:18:04.042: INFO: Found resourceQuota "e2e-rq-status-c7wsq" in namespace "resourcequota-7387" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Nov 15 16:18:04.043: INFO: ResourceQuota "e2e-rq-status-c7wsq" /status was updated
STEP: Patching hard spec values for cpu & memory 11/15/23 16:18:04.064
Nov 15 16:18:04.091: INFO: Resource quota "e2e-rq-status-c7wsq" reports spec: hard cpu limit of 1
Nov 15 16:18:04.091: INFO: Resource quota "e2e-rq-status-c7wsq" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-c7wsq" /status 11/15/23 16:18:04.091
STEP: Confirm /status for "e2e-rq-status-c7wsq" resourceQuota via watch 11/15/23 16:18:04.116
Nov 15 16:18:04.126: INFO: observed resourceQuota "e2e-rq-status-c7wsq" in namespace "resourcequota-7387" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Nov 15 16:18:04.126: INFO: Found resourceQuota "e2e-rq-status-c7wsq" in namespace "resourcequota-7387" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Nov 15 16:18:04.126: INFO: ResourceQuota "e2e-rq-status-c7wsq" /status was patched
STEP: Get "e2e-rq-status-c7wsq" /status 11/15/23 16:18:04.126
Nov 15 16:18:04.147: INFO: Resourcequota "e2e-rq-status-c7wsq" reports status: hard cpu of 1
Nov 15 16:18:04.148: INFO: Resourcequota "e2e-rq-status-c7wsq" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-c7wsq" /status before checking Spec is unchanged 11/15/23 16:18:04.169
Nov 15 16:18:04.191: INFO: Resourcequota "e2e-rq-status-c7wsq" reports status: hard cpu of 2
Nov 15 16:18:04.192: INFO: Resourcequota "e2e-rq-status-c7wsq" reports status: hard memory of 2Gi
Nov 15 16:18:04.201: INFO: Found resourceQuota "e2e-rq-status-c7wsq" in namespace "resourcequota-7387" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Nov 15 16:20:29.243: INFO: ResourceQuota "e2e-rq-status-c7wsq" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Nov 15 16:20:29.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7387" for this suite. 11/15/23 16:20:29.282
------------------------------
• [SLOW TEST] [145.511 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:18:03.8
    Nov 15 16:18:03.801: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename resourcequota 11/15/23 16:18:03.803
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:18:03.861
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:18:03.887
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-c7wsq" 11/15/23 16:18:03.929
    Nov 15 16:18:03.977: INFO: Resource quota "e2e-rq-status-c7wsq" reports spec: hard cpu limit of 500m
    Nov 15 16:18:03.977: INFO: Resource quota "e2e-rq-status-c7wsq" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-c7wsq" /status 11/15/23 16:18:03.977
    STEP: Confirm /status for "e2e-rq-status-c7wsq" resourceQuota via watch 11/15/23 16:18:04.032
    Nov 15 16:18:04.042: INFO: observed resourceQuota "e2e-rq-status-c7wsq" in namespace "resourcequota-7387" with hard status: v1.ResourceList(nil)
    Nov 15 16:18:04.042: INFO: Found resourceQuota "e2e-rq-status-c7wsq" in namespace "resourcequota-7387" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Nov 15 16:18:04.043: INFO: ResourceQuota "e2e-rq-status-c7wsq" /status was updated
    STEP: Patching hard spec values for cpu & memory 11/15/23 16:18:04.064
    Nov 15 16:18:04.091: INFO: Resource quota "e2e-rq-status-c7wsq" reports spec: hard cpu limit of 1
    Nov 15 16:18:04.091: INFO: Resource quota "e2e-rq-status-c7wsq" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-c7wsq" /status 11/15/23 16:18:04.091
    STEP: Confirm /status for "e2e-rq-status-c7wsq" resourceQuota via watch 11/15/23 16:18:04.116
    Nov 15 16:18:04.126: INFO: observed resourceQuota "e2e-rq-status-c7wsq" in namespace "resourcequota-7387" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Nov 15 16:18:04.126: INFO: Found resourceQuota "e2e-rq-status-c7wsq" in namespace "resourcequota-7387" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Nov 15 16:18:04.126: INFO: ResourceQuota "e2e-rq-status-c7wsq" /status was patched
    STEP: Get "e2e-rq-status-c7wsq" /status 11/15/23 16:18:04.126
    Nov 15 16:18:04.147: INFO: Resourcequota "e2e-rq-status-c7wsq" reports status: hard cpu of 1
    Nov 15 16:18:04.148: INFO: Resourcequota "e2e-rq-status-c7wsq" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-c7wsq" /status before checking Spec is unchanged 11/15/23 16:18:04.169
    Nov 15 16:18:04.191: INFO: Resourcequota "e2e-rq-status-c7wsq" reports status: hard cpu of 2
    Nov 15 16:18:04.192: INFO: Resourcequota "e2e-rq-status-c7wsq" reports status: hard memory of 2Gi
    Nov 15 16:18:04.201: INFO: Found resourceQuota "e2e-rq-status-c7wsq" in namespace "resourcequota-7387" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Nov 15 16:20:29.243: INFO: ResourceQuota "e2e-rq-status-c7wsq" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:20:29.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7387" for this suite. 11/15/23 16:20:29.282
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:20:29.321
Nov 15 16:20:29.321: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename deployment 11/15/23 16:20:29.323
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:20:29.385
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:20:29.407
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 11/15/23 16:20:29.45
STEP: waiting for Deployment to be created 11/15/23 16:20:29.473
STEP: waiting for all Replicas to be Ready 11/15/23 16:20:29.483
Nov 15 16:20:29.493: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 15 16:20:29.494: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 15 16:20:29.507: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 15 16:20:29.507: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 15 16:20:29.530: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 15 16:20:29.530: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 15 16:20:29.579: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 15 16:20:29.583: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 15 16:20:31.147: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Nov 15 16:20:31.148: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Nov 15 16:20:31.788: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 11/15/23 16:20:31.789
W1115 16:20:31.814131      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Nov 15 16:20:31.824: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 11/15/23 16:20:31.824
Nov 15 16:20:31.836: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 0
Nov 15 16:20:31.836: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 0
Nov 15 16:20:31.836: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 0
Nov 15 16:20:31.837: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 0
Nov 15 16:20:31.837: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 0
Nov 15 16:20:31.837: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 0
Nov 15 16:20:31.838: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 0
Nov 15 16:20:31.838: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 0
Nov 15 16:20:31.838: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1
Nov 15 16:20:31.839: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1
Nov 15 16:20:31.839: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2
Nov 15 16:20:31.839: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2
Nov 15 16:20:31.839: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2
Nov 15 16:20:31.840: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2
Nov 15 16:20:31.840: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2
Nov 15 16:20:31.840: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2
Nov 15 16:20:31.888: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2
Nov 15 16:20:31.888: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2
Nov 15 16:20:31.912: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1
Nov 15 16:20:31.912: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1
Nov 15 16:20:31.935: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1
Nov 15 16:20:31.935: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1
Nov 15 16:20:33.818: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2
Nov 15 16:20:33.818: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2
Nov 15 16:20:33.876: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1
STEP: listing Deployments 11/15/23 16:20:33.876
Nov 15 16:20:33.906: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 11/15/23 16:20:33.906
Nov 15 16:20:33.942: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 11/15/23 16:20:33.942
Nov 15 16:20:33.980: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Nov 15 16:20:33.980: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Nov 15 16:20:33.988: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Nov 15 16:20:34.017: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Nov 15 16:20:34.073: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Nov 15 16:20:35.837: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Nov 15 16:20:35.870: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Nov 15 16:20:35.886: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Nov 15 16:20:35.919: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Nov 15 16:20:38.208: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 11/15/23 16:20:38.264
STEP: fetching the DeploymentStatus 11/15/23 16:20:38.301
Nov 15 16:20:38.332: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1
Nov 15 16:20:38.333: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1
Nov 15 16:20:38.334: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1
Nov 15 16:20:38.334: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1
Nov 15 16:20:38.334: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1
Nov 15 16:20:38.335: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2
Nov 15 16:20:38.335: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2
Nov 15 16:20:38.336: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2
Nov 15 16:20:38.336: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2
Nov 15 16:20:38.336: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 3
STEP: deleting the Deployment 11/15/23 16:20:38.337
Nov 15 16:20:38.384: INFO: observed event type MODIFIED
Nov 15 16:20:38.385: INFO: observed event type MODIFIED
Nov 15 16:20:38.385: INFO: observed event type MODIFIED
Nov 15 16:20:38.386: INFO: observed event type MODIFIED
Nov 15 16:20:38.387: INFO: observed event type MODIFIED
Nov 15 16:20:38.387: INFO: observed event type MODIFIED
Nov 15 16:20:38.387: INFO: observed event type MODIFIED
Nov 15 16:20:38.388: INFO: observed event type MODIFIED
Nov 15 16:20:38.389: INFO: observed event type MODIFIED
Nov 15 16:20:38.390: INFO: observed event type MODIFIED
Nov 15 16:20:38.390: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Nov 15 16:20:38.410: INFO: Log out all the ReplicaSets if there is no deployment created
Nov 15 16:20:38.432: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-1140  598aea77-39b3-4bac-b329-6abb627776b7 34859 2 2023-11-15 16:20:33 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment e2ea39fb-9346-4511-a218-a0739311428c 0xc002ef3ce7 0xc002ef3ce8}] [] [{kube-controller-manager Update apps/v1 2023-11-15 16:20:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2ea39fb-9346-4511-a218-a0739311428c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 16:20:38 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002ef3d70 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Nov 15 16:20:38.453: INFO: pod: "test-deployment-7b7876f9d6-4f6m4":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-4f6m4 test-deployment-7b7876f9d6- deployment-1140  44933799-f569-4c6b-bdc7-9f6693ec3ba3 34818 0 2023-11-15 16:20:33 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:5fbf9e7980ffa6b304072021ec7dea6f8590442984be4e1f0476d1234b515646 cni.projectcalico.org/podIP:172.30.164.24/32 cni.projectcalico.org/podIPs:172.30.164.24/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 598aea77-39b3-4bac-b329-6abb627776b7 0xc0036d05e7 0xc0036d05e8}] [] [{kube-controller-manager Update v1 2023-11-15 16:20:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"598aea77-39b3-4bac-b329-6abb627776b7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 16:20:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 16:20:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.164.24\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2pzc9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2pzc9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:20:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:20:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:20:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:20:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:172.30.164.24,StartTime:2023-11-15 16:20:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 16:20:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://887e8cb3f56f6191e73c24f6cd78a43e8bad23898879630f62bfa1e5fc21ca89,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.164.24,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Nov 15 16:20:38.454: INFO: pod: "test-deployment-7b7876f9d6-xnsm8":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-xnsm8 test-deployment-7b7876f9d6- deployment-1140  2915e4f0-7c31-4b38-b91e-87ac7b1bd268 34858 0 2023-11-15 16:20:35 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:aa09bf1f73be7551ffa4f60ec22eed48d1d0408e59feae59dab8a251dfb0ee58 cni.projectcalico.org/podIP:172.30.205.219/32 cni.projectcalico.org/podIPs:172.30.205.219/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 598aea77-39b3-4bac-b329-6abb627776b7 0xc0036d0d37 0xc0036d0d38}] [] [{kube-controller-manager Update v1 2023-11-15 16:20:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"598aea77-39b3-4bac-b329-6abb627776b7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 16:20:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 16:20:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.205.219\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cwlrp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cwlrp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:20:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:20:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:20:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:20:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.114,PodIP:172.30.205.219,StartTime:2023-11-15 16:20:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 16:20:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c412bbcd0633b6284efed96e5f71b4e4860dc282cc2ef2a8bdfe2dd40e532c66,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.205.219,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Nov 15 16:20:38.455: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-1140  379880af-24dd-400c-afd6-6022a603cd07 34868 4 2023-11-15 16:20:31 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment e2ea39fb-9346-4511-a218-a0739311428c 0xc002ef3dd7 0xc002ef3dd8}] [] [{kube-controller-manager Update apps/v1 2023-11-15 16:20:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2ea39fb-9346-4511-a218-a0739311428c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 16:20:38 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002ef3e60 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Nov 15 16:20:38.474: INFO: pod: "test-deployment-7df74c55ff-4pc5s":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-4pc5s test-deployment-7df74c55ff- deployment-1140  817a12e4-0a29-4cc0-9b2b-15ff1d864d1c 34864 0 2023-11-15 16:20:31 +0000 UTC 2023-11-15 16:20:39 +0000 UTC 0xc003836a78 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:c62e98a0fb8e74f2f1d4d84c9f6a59e0788dfffc1fc3860345e5818405d0a12d cni.projectcalico.org/podIP:172.30.164.22/32 cni.projectcalico.org/podIPs:172.30.164.22/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 379880af-24dd-400c-afd6-6022a603cd07 0xc003836ac7 0xc003836ac8}] [] [{kube-controller-manager Update v1 2023-11-15 16:20:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"379880af-24dd-400c-afd6-6022a603cd07\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 16:20:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 16:20:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.164.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v5cbd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v5cbd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:20:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:20:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:20:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:20:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:172.30.164.22,StartTime:2023-11-15 16:20:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 16:20:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://536185201dd5f17ad040a7a77835473613ecc671eef8c70c460055a92b499dc8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.164.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Nov 15 16:20:38.475: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-1140  f10f7f89-60fe-45c0-81e9-a5365b07fa90 34761 3 2023-11-15 16:20:29 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment e2ea39fb-9346-4511-a218-a0739311428c 0xc002ef3ec7 0xc002ef3ec8}] [] [{kube-controller-manager Update apps/v1 2023-11-15 16:20:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2ea39fb-9346-4511-a218-a0739311428c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 16:20:33 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002ef3f50 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Nov 15 16:20:38.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1140" for this suite. 11/15/23 16:20:38.526
------------------------------
• [SLOW TEST] [9.236 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:20:29.321
    Nov 15 16:20:29.321: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename deployment 11/15/23 16:20:29.323
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:20:29.385
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:20:29.407
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 11/15/23 16:20:29.45
    STEP: waiting for Deployment to be created 11/15/23 16:20:29.473
    STEP: waiting for all Replicas to be Ready 11/15/23 16:20:29.483
    Nov 15 16:20:29.493: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Nov 15 16:20:29.494: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Nov 15 16:20:29.507: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Nov 15 16:20:29.507: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Nov 15 16:20:29.530: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Nov 15 16:20:29.530: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Nov 15 16:20:29.579: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Nov 15 16:20:29.583: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Nov 15 16:20:31.147: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Nov 15 16:20:31.148: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Nov 15 16:20:31.788: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 11/15/23 16:20:31.789
    W1115 16:20:31.814131      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Nov 15 16:20:31.824: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 11/15/23 16:20:31.824
    Nov 15 16:20:31.836: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 0
    Nov 15 16:20:31.836: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 0
    Nov 15 16:20:31.836: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 0
    Nov 15 16:20:31.837: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 0
    Nov 15 16:20:31.837: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 0
    Nov 15 16:20:31.837: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 0
    Nov 15 16:20:31.838: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 0
    Nov 15 16:20:31.838: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 0
    Nov 15 16:20:31.838: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1
    Nov 15 16:20:31.839: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1
    Nov 15 16:20:31.839: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2
    Nov 15 16:20:31.839: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2
    Nov 15 16:20:31.839: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2
    Nov 15 16:20:31.840: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2
    Nov 15 16:20:31.840: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2
    Nov 15 16:20:31.840: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2
    Nov 15 16:20:31.888: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2
    Nov 15 16:20:31.888: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2
    Nov 15 16:20:31.912: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1
    Nov 15 16:20:31.912: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1
    Nov 15 16:20:31.935: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1
    Nov 15 16:20:31.935: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1
    Nov 15 16:20:33.818: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2
    Nov 15 16:20:33.818: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2
    Nov 15 16:20:33.876: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1
    STEP: listing Deployments 11/15/23 16:20:33.876
    Nov 15 16:20:33.906: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 11/15/23 16:20:33.906
    Nov 15 16:20:33.942: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 11/15/23 16:20:33.942
    Nov 15 16:20:33.980: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Nov 15 16:20:33.980: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Nov 15 16:20:33.988: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Nov 15 16:20:34.017: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Nov 15 16:20:34.073: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Nov 15 16:20:35.837: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Nov 15 16:20:35.870: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Nov 15 16:20:35.886: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Nov 15 16:20:35.919: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Nov 15 16:20:38.208: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 11/15/23 16:20:38.264
    STEP: fetching the DeploymentStatus 11/15/23 16:20:38.301
    Nov 15 16:20:38.332: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1
    Nov 15 16:20:38.333: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1
    Nov 15 16:20:38.334: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1
    Nov 15 16:20:38.334: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1
    Nov 15 16:20:38.334: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 1
    Nov 15 16:20:38.335: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2
    Nov 15 16:20:38.335: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2
    Nov 15 16:20:38.336: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2
    Nov 15 16:20:38.336: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 2
    Nov 15 16:20:38.336: INFO: observed Deployment test-deployment in namespace deployment-1140 with ReadyReplicas 3
    STEP: deleting the Deployment 11/15/23 16:20:38.337
    Nov 15 16:20:38.384: INFO: observed event type MODIFIED
    Nov 15 16:20:38.385: INFO: observed event type MODIFIED
    Nov 15 16:20:38.385: INFO: observed event type MODIFIED
    Nov 15 16:20:38.386: INFO: observed event type MODIFIED
    Nov 15 16:20:38.387: INFO: observed event type MODIFIED
    Nov 15 16:20:38.387: INFO: observed event type MODIFIED
    Nov 15 16:20:38.387: INFO: observed event type MODIFIED
    Nov 15 16:20:38.388: INFO: observed event type MODIFIED
    Nov 15 16:20:38.389: INFO: observed event type MODIFIED
    Nov 15 16:20:38.390: INFO: observed event type MODIFIED
    Nov 15 16:20:38.390: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Nov 15 16:20:38.410: INFO: Log out all the ReplicaSets if there is no deployment created
    Nov 15 16:20:38.432: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-1140  598aea77-39b3-4bac-b329-6abb627776b7 34859 2 2023-11-15 16:20:33 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment e2ea39fb-9346-4511-a218-a0739311428c 0xc002ef3ce7 0xc002ef3ce8}] [] [{kube-controller-manager Update apps/v1 2023-11-15 16:20:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2ea39fb-9346-4511-a218-a0739311428c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 16:20:38 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002ef3d70 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Nov 15 16:20:38.453: INFO: pod: "test-deployment-7b7876f9d6-4f6m4":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-4f6m4 test-deployment-7b7876f9d6- deployment-1140  44933799-f569-4c6b-bdc7-9f6693ec3ba3 34818 0 2023-11-15 16:20:33 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:5fbf9e7980ffa6b304072021ec7dea6f8590442984be4e1f0476d1234b515646 cni.projectcalico.org/podIP:172.30.164.24/32 cni.projectcalico.org/podIPs:172.30.164.24/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 598aea77-39b3-4bac-b329-6abb627776b7 0xc0036d05e7 0xc0036d05e8}] [] [{kube-controller-manager Update v1 2023-11-15 16:20:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"598aea77-39b3-4bac-b329-6abb627776b7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 16:20:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 16:20:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.164.24\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2pzc9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2pzc9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:20:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:20:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:20:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:20:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:172.30.164.24,StartTime:2023-11-15 16:20:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 16:20:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://887e8cb3f56f6191e73c24f6cd78a43e8bad23898879630f62bfa1e5fc21ca89,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.164.24,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Nov 15 16:20:38.454: INFO: pod: "test-deployment-7b7876f9d6-xnsm8":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-xnsm8 test-deployment-7b7876f9d6- deployment-1140  2915e4f0-7c31-4b38-b91e-87ac7b1bd268 34858 0 2023-11-15 16:20:35 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:aa09bf1f73be7551ffa4f60ec22eed48d1d0408e59feae59dab8a251dfb0ee58 cni.projectcalico.org/podIP:172.30.205.219/32 cni.projectcalico.org/podIPs:172.30.205.219/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 598aea77-39b3-4bac-b329-6abb627776b7 0xc0036d0d37 0xc0036d0d38}] [] [{kube-controller-manager Update v1 2023-11-15 16:20:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"598aea77-39b3-4bac-b329-6abb627776b7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 16:20:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 16:20:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.205.219\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cwlrp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cwlrp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:20:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:20:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:20:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:20:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.114,PodIP:172.30.205.219,StartTime:2023-11-15 16:20:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 16:20:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c412bbcd0633b6284efed96e5f71b4e4860dc282cc2ef2a8bdfe2dd40e532c66,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.205.219,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Nov 15 16:20:38.455: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-1140  379880af-24dd-400c-afd6-6022a603cd07 34868 4 2023-11-15 16:20:31 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment e2ea39fb-9346-4511-a218-a0739311428c 0xc002ef3dd7 0xc002ef3dd8}] [] [{kube-controller-manager Update apps/v1 2023-11-15 16:20:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2ea39fb-9346-4511-a218-a0739311428c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 16:20:38 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002ef3e60 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Nov 15 16:20:38.474: INFO: pod: "test-deployment-7df74c55ff-4pc5s":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-4pc5s test-deployment-7df74c55ff- deployment-1140  817a12e4-0a29-4cc0-9b2b-15ff1d864d1c 34864 0 2023-11-15 16:20:31 +0000 UTC 2023-11-15 16:20:39 +0000 UTC 0xc003836a78 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:c62e98a0fb8e74f2f1d4d84c9f6a59e0788dfffc1fc3860345e5818405d0a12d cni.projectcalico.org/podIP:172.30.164.22/32 cni.projectcalico.org/podIPs:172.30.164.22/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 379880af-24dd-400c-afd6-6022a603cd07 0xc003836ac7 0xc003836ac8}] [] [{kube-controller-manager Update v1 2023-11-15 16:20:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"379880af-24dd-400c-afd6-6022a603cd07\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 16:20:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 16:20:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.164.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v5cbd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v5cbd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:20:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:20:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:20:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:20:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:172.30.164.22,StartTime:2023-11-15 16:20:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 16:20:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://536185201dd5f17ad040a7a77835473613ecc671eef8c70c460055a92b499dc8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.164.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Nov 15 16:20:38.475: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-1140  f10f7f89-60fe-45c0-81e9-a5365b07fa90 34761 3 2023-11-15 16:20:29 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment e2ea39fb-9346-4511-a218-a0739311428c 0xc002ef3ec7 0xc002ef3ec8}] [] [{kube-controller-manager Update apps/v1 2023-11-15 16:20:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2ea39fb-9346-4511-a218-a0739311428c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 16:20:33 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002ef3f50 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:20:38.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1140" for this suite. 11/15/23 16:20:38.526
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:20:38.566
Nov 15 16:20:38.566: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename downward-api 11/15/23 16:20:38.569
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:20:38.622
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:20:38.643
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 11/15/23 16:20:38.663
Nov 15 16:20:38.707: INFO: Waiting up to 5m0s for pod "downward-api-b0461448-40b7-479e-833a-0d35a9db8616" in namespace "downward-api-9185" to be "Succeeded or Failed"
Nov 15 16:20:38.725: INFO: Pod "downward-api-b0461448-40b7-479e-833a-0d35a9db8616": Phase="Pending", Reason="", readiness=false. Elapsed: 17.744452ms
Nov 15 16:20:40.744: INFO: Pod "downward-api-b0461448-40b7-479e-833a-0d35a9db8616": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03738854s
Nov 15 16:20:42.745: INFO: Pod "downward-api-b0461448-40b7-479e-833a-0d35a9db8616": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038055351s
Nov 15 16:20:44.746: INFO: Pod "downward-api-b0461448-40b7-479e-833a-0d35a9db8616": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039463796s
STEP: Saw pod success 11/15/23 16:20:44.747
Nov 15 16:20:44.747: INFO: Pod "downward-api-b0461448-40b7-479e-833a-0d35a9db8616" satisfied condition "Succeeded or Failed"
Nov 15 16:20:44.765: INFO: Trying to get logs from node 10.15.40.115 pod downward-api-b0461448-40b7-479e-833a-0d35a9db8616 container dapi-container: <nil>
STEP: delete the pod 11/15/23 16:20:44.91
Nov 15 16:20:44.949: INFO: Waiting for pod downward-api-b0461448-40b7-479e-833a-0d35a9db8616 to disappear
Nov 15 16:20:44.966: INFO: Pod downward-api-b0461448-40b7-479e-833a-0d35a9db8616 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Nov 15 16:20:44.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9185" for this suite. 11/15/23 16:20:44.998
------------------------------
• [SLOW TEST] [6.465 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:20:38.566
    Nov 15 16:20:38.566: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename downward-api 11/15/23 16:20:38.569
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:20:38.622
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:20:38.643
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 11/15/23 16:20:38.663
    Nov 15 16:20:38.707: INFO: Waiting up to 5m0s for pod "downward-api-b0461448-40b7-479e-833a-0d35a9db8616" in namespace "downward-api-9185" to be "Succeeded or Failed"
    Nov 15 16:20:38.725: INFO: Pod "downward-api-b0461448-40b7-479e-833a-0d35a9db8616": Phase="Pending", Reason="", readiness=false. Elapsed: 17.744452ms
    Nov 15 16:20:40.744: INFO: Pod "downward-api-b0461448-40b7-479e-833a-0d35a9db8616": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03738854s
    Nov 15 16:20:42.745: INFO: Pod "downward-api-b0461448-40b7-479e-833a-0d35a9db8616": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038055351s
    Nov 15 16:20:44.746: INFO: Pod "downward-api-b0461448-40b7-479e-833a-0d35a9db8616": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039463796s
    STEP: Saw pod success 11/15/23 16:20:44.747
    Nov 15 16:20:44.747: INFO: Pod "downward-api-b0461448-40b7-479e-833a-0d35a9db8616" satisfied condition "Succeeded or Failed"
    Nov 15 16:20:44.765: INFO: Trying to get logs from node 10.15.40.115 pod downward-api-b0461448-40b7-479e-833a-0d35a9db8616 container dapi-container: <nil>
    STEP: delete the pod 11/15/23 16:20:44.91
    Nov 15 16:20:44.949: INFO: Waiting for pod downward-api-b0461448-40b7-479e-833a-0d35a9db8616 to disappear
    Nov 15 16:20:44.966: INFO: Pod downward-api-b0461448-40b7-479e-833a-0d35a9db8616 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:20:44.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9185" for this suite. 11/15/23 16:20:44.998
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:20:45.032
Nov 15 16:20:45.032: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename watch 11/15/23 16:20:45.034
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:20:45.091
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:20:45.11
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 11/15/23 16:20:45.13
STEP: creating a new configmap 11/15/23 16:20:45.139
STEP: modifying the configmap once 11/15/23 16:20:45.182
STEP: closing the watch once it receives two notifications 11/15/23 16:20:45.215
Nov 15 16:20:45.215: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4684  32bee800-158a-4fd5-ab73-8a9ca83044aa 34978 0 2023-11-15 16:20:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-11-15 16:20:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 15 16:20:45.216: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4684  32bee800-158a-4fd5-ab73-8a9ca83044aa 34980 0 2023-11-15 16:20:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-11-15 16:20:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 11/15/23 16:20:45.216
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 11/15/23 16:20:45.249
STEP: deleting the configmap 11/15/23 16:20:45.258
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 11/15/23 16:20:45.281
Nov 15 16:20:45.282: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4684  32bee800-158a-4fd5-ab73-8a9ca83044aa 34982 0 2023-11-15 16:20:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-11-15 16:20:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 15 16:20:45.282: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4684  32bee800-158a-4fd5-ab73-8a9ca83044aa 34984 0 2023-11-15 16:20:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-11-15 16:20:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Nov 15 16:20:45.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-4684" for this suite. 11/15/23 16:20:45.312
------------------------------
• [0.313 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:20:45.032
    Nov 15 16:20:45.032: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename watch 11/15/23 16:20:45.034
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:20:45.091
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:20:45.11
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 11/15/23 16:20:45.13
    STEP: creating a new configmap 11/15/23 16:20:45.139
    STEP: modifying the configmap once 11/15/23 16:20:45.182
    STEP: closing the watch once it receives two notifications 11/15/23 16:20:45.215
    Nov 15 16:20:45.215: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4684  32bee800-158a-4fd5-ab73-8a9ca83044aa 34978 0 2023-11-15 16:20:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-11-15 16:20:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 15 16:20:45.216: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4684  32bee800-158a-4fd5-ab73-8a9ca83044aa 34980 0 2023-11-15 16:20:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-11-15 16:20:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 11/15/23 16:20:45.216
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 11/15/23 16:20:45.249
    STEP: deleting the configmap 11/15/23 16:20:45.258
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 11/15/23 16:20:45.281
    Nov 15 16:20:45.282: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4684  32bee800-158a-4fd5-ab73-8a9ca83044aa 34982 0 2023-11-15 16:20:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-11-15 16:20:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 15 16:20:45.282: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4684  32bee800-158a-4fd5-ab73-8a9ca83044aa 34984 0 2023-11-15 16:20:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-11-15 16:20:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:20:45.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-4684" for this suite. 11/15/23 16:20:45.312
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:20:45.353
Nov 15 16:20:45.353: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename configmap 11/15/23 16:20:45.356
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:20:45.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:20:45.436
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-d1f7ece2-70e6-434b-99f5-af0d9a56cfd1 11/15/23 16:20:45.456
STEP: Creating a pod to test consume configMaps 11/15/23 16:20:45.477
Nov 15 16:20:45.512: INFO: Waiting up to 5m0s for pod "pod-configmaps-cd9ee0ce-b364-49e3-a62d-103523f2b76f" in namespace "configmap-4389" to be "Succeeded or Failed"
Nov 15 16:20:45.530: INFO: Pod "pod-configmaps-cd9ee0ce-b364-49e3-a62d-103523f2b76f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.113803ms
Nov 15 16:20:47.549: INFO: Pod "pod-configmaps-cd9ee0ce-b364-49e3-a62d-103523f2b76f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037543983s
Nov 15 16:20:49.554: INFO: Pod "pod-configmaps-cd9ee0ce-b364-49e3-a62d-103523f2b76f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042597708s
Nov 15 16:20:51.550: INFO: Pod "pod-configmaps-cd9ee0ce-b364-49e3-a62d-103523f2b76f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03791518s
STEP: Saw pod success 11/15/23 16:20:51.55
Nov 15 16:20:51.550: INFO: Pod "pod-configmaps-cd9ee0ce-b364-49e3-a62d-103523f2b76f" satisfied condition "Succeeded or Failed"
Nov 15 16:20:51.568: INFO: Trying to get logs from node 10.15.40.115 pod pod-configmaps-cd9ee0ce-b364-49e3-a62d-103523f2b76f container agnhost-container: <nil>
STEP: delete the pod 11/15/23 16:20:51.607
Nov 15 16:20:51.664: INFO: Waiting for pod pod-configmaps-cd9ee0ce-b364-49e3-a62d-103523f2b76f to disappear
Nov 15 16:20:51.681: INFO: Pod pod-configmaps-cd9ee0ce-b364-49e3-a62d-103523f2b76f no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 15 16:20:51.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4389" for this suite. 11/15/23 16:20:51.709
------------------------------
• [SLOW TEST] [6.388 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:20:45.353
    Nov 15 16:20:45.353: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename configmap 11/15/23 16:20:45.356
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:20:45.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:20:45.436
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-d1f7ece2-70e6-434b-99f5-af0d9a56cfd1 11/15/23 16:20:45.456
    STEP: Creating a pod to test consume configMaps 11/15/23 16:20:45.477
    Nov 15 16:20:45.512: INFO: Waiting up to 5m0s for pod "pod-configmaps-cd9ee0ce-b364-49e3-a62d-103523f2b76f" in namespace "configmap-4389" to be "Succeeded or Failed"
    Nov 15 16:20:45.530: INFO: Pod "pod-configmaps-cd9ee0ce-b364-49e3-a62d-103523f2b76f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.113803ms
    Nov 15 16:20:47.549: INFO: Pod "pod-configmaps-cd9ee0ce-b364-49e3-a62d-103523f2b76f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037543983s
    Nov 15 16:20:49.554: INFO: Pod "pod-configmaps-cd9ee0ce-b364-49e3-a62d-103523f2b76f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042597708s
    Nov 15 16:20:51.550: INFO: Pod "pod-configmaps-cd9ee0ce-b364-49e3-a62d-103523f2b76f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03791518s
    STEP: Saw pod success 11/15/23 16:20:51.55
    Nov 15 16:20:51.550: INFO: Pod "pod-configmaps-cd9ee0ce-b364-49e3-a62d-103523f2b76f" satisfied condition "Succeeded or Failed"
    Nov 15 16:20:51.568: INFO: Trying to get logs from node 10.15.40.115 pod pod-configmaps-cd9ee0ce-b364-49e3-a62d-103523f2b76f container agnhost-container: <nil>
    STEP: delete the pod 11/15/23 16:20:51.607
    Nov 15 16:20:51.664: INFO: Waiting for pod pod-configmaps-cd9ee0ce-b364-49e3-a62d-103523f2b76f to disappear
    Nov 15 16:20:51.681: INFO: Pod pod-configmaps-cd9ee0ce-b364-49e3-a62d-103523f2b76f no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:20:51.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4389" for this suite. 11/15/23 16:20:51.709
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:20:51.76
Nov 15 16:20:51.760: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename webhook 11/15/23 16:20:51.762
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:20:51.818
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:20:51.843
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/15/23 16:20:51.929
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 16:20:52.556
STEP: Deploying the webhook pod 11/15/23 16:20:52.616
STEP: Wait for the deployment to be ready 11/15/23 16:20:52.663
Nov 15 16:20:52.713: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 15 16:20:54.771: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 16, 20, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 16, 20, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 16, 20, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 16, 20, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 11/15/23 16:20:56.79
STEP: Verifying the service has paired with the endpoint 11/15/23 16:20:56.843
Nov 15 16:20:57.844: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 11/15/23 16:20:58.089
STEP: Creating a configMap that does not comply to the validation webhook rules 11/15/23 16:20:58.238
STEP: Deleting the collection of validation webhooks 11/15/23 16:20:58.375
STEP: Creating a configMap that does not comply to the validation webhook rules 11/15/23 16:20:58.567
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 16:20:58.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9940" for this suite. 11/15/23 16:20:58.825
STEP: Destroying namespace "webhook-9940-markers" for this suite. 11/15/23 16:20:58.859
------------------------------
• [SLOW TEST] [7.129 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:20:51.76
    Nov 15 16:20:51.760: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename webhook 11/15/23 16:20:51.762
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:20:51.818
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:20:51.843
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/15/23 16:20:51.929
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 16:20:52.556
    STEP: Deploying the webhook pod 11/15/23 16:20:52.616
    STEP: Wait for the deployment to be ready 11/15/23 16:20:52.663
    Nov 15 16:20:52.713: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Nov 15 16:20:54.771: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 16, 20, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 16, 20, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 16, 20, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 16, 20, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 11/15/23 16:20:56.79
    STEP: Verifying the service has paired with the endpoint 11/15/23 16:20:56.843
    Nov 15 16:20:57.844: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 11/15/23 16:20:58.089
    STEP: Creating a configMap that does not comply to the validation webhook rules 11/15/23 16:20:58.238
    STEP: Deleting the collection of validation webhooks 11/15/23 16:20:58.375
    STEP: Creating a configMap that does not comply to the validation webhook rules 11/15/23 16:20:58.567
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:20:58.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9940" for this suite. 11/15/23 16:20:58.825
    STEP: Destroying namespace "webhook-9940-markers" for this suite. 11/15/23 16:20:58.859
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:20:58.893
Nov 15 16:20:58.894: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename namespaces 11/15/23 16:20:58.895
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:20:58.95
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:20:58.968
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-3805" 11/15/23 16:20:58.991
Nov 15 16:20:59.032: INFO: Namespace "namespaces-3805" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"3b21b822-8b44-408e-b236-bfbd8cf999aa", "kubernetes.io/metadata.name":"namespaces-3805", "namespaces-3805":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 16:20:59.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-3805" for this suite. 11/15/23 16:20:59.062
------------------------------
• [0.199 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:20:58.893
    Nov 15 16:20:58.894: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename namespaces 11/15/23 16:20:58.895
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:20:58.95
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:20:58.968
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-3805" 11/15/23 16:20:58.991
    Nov 15 16:20:59.032: INFO: Namespace "namespaces-3805" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"3b21b822-8b44-408e-b236-bfbd8cf999aa", "kubernetes.io/metadata.name":"namespaces-3805", "namespaces-3805":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:20:59.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-3805" for this suite. 11/15/23 16:20:59.062
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:20:59.107
Nov 15 16:20:59.107: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename gc 11/15/23 16:20:59.109
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:20:59.172
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:20:59.192
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 11/15/23 16:20:59.242
STEP: create the rc2 11/15/23 16:20:59.265
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 11/15/23 16:21:04.311
STEP: delete the rc simpletest-rc-to-be-deleted 11/15/23 16:21:05.971
STEP: wait for the rc to be deleted 11/15/23 16:21:06.017
STEP: Gathering metrics 11/15/23 16:21:11.08
W1115 16:21:11.126392      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Nov 15 16:21:11.126: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Nov 15 16:21:11.126: INFO: Deleting pod "simpletest-rc-to-be-deleted-255jp" in namespace "gc-7618"
Nov 15 16:21:11.171: INFO: Deleting pod "simpletest-rc-to-be-deleted-29cmc" in namespace "gc-7618"
Nov 15 16:21:11.216: INFO: Deleting pod "simpletest-rc-to-be-deleted-2gq62" in namespace "gc-7618"
Nov 15 16:21:11.267: INFO: Deleting pod "simpletest-rc-to-be-deleted-2m5xt" in namespace "gc-7618"
Nov 15 16:21:11.308: INFO: Deleting pod "simpletest-rc-to-be-deleted-2p6ln" in namespace "gc-7618"
Nov 15 16:21:11.366: INFO: Deleting pod "simpletest-rc-to-be-deleted-2s9dk" in namespace "gc-7618"
Nov 15 16:21:11.449: INFO: Deleting pod "simpletest-rc-to-be-deleted-5mcfw" in namespace "gc-7618"
Nov 15 16:21:11.505: INFO: Deleting pod "simpletest-rc-to-be-deleted-6b4xx" in namespace "gc-7618"
Nov 15 16:21:11.546: INFO: Deleting pod "simpletest-rc-to-be-deleted-6bpp2" in namespace "gc-7618"
Nov 15 16:21:11.589: INFO: Deleting pod "simpletest-rc-to-be-deleted-6ct6n" in namespace "gc-7618"
Nov 15 16:21:11.635: INFO: Deleting pod "simpletest-rc-to-be-deleted-6fzvp" in namespace "gc-7618"
Nov 15 16:21:11.698: INFO: Deleting pod "simpletest-rc-to-be-deleted-6qxhn" in namespace "gc-7618"
Nov 15 16:21:11.748: INFO: Deleting pod "simpletest-rc-to-be-deleted-6zgjf" in namespace "gc-7618"
Nov 15 16:21:11.798: INFO: Deleting pod "simpletest-rc-to-be-deleted-7k7r8" in namespace "gc-7618"
Nov 15 16:21:11.837: INFO: Deleting pod "simpletest-rc-to-be-deleted-7pwc8" in namespace "gc-7618"
Nov 15 16:21:11.885: INFO: Deleting pod "simpletest-rc-to-be-deleted-7z88r" in namespace "gc-7618"
Nov 15 16:21:11.941: INFO: Deleting pod "simpletest-rc-to-be-deleted-8qvtt" in namespace "gc-7618"
Nov 15 16:21:11.993: INFO: Deleting pod "simpletest-rc-to-be-deleted-8thff" in namespace "gc-7618"
Nov 15 16:21:12.036: INFO: Deleting pod "simpletest-rc-to-be-deleted-8z4m9" in namespace "gc-7618"
Nov 15 16:21:12.092: INFO: Deleting pod "simpletest-rc-to-be-deleted-9s9wq" in namespace "gc-7618"
Nov 15 16:21:12.138: INFO: Deleting pod "simpletest-rc-to-be-deleted-bd4z5" in namespace "gc-7618"
Nov 15 16:21:12.191: INFO: Deleting pod "simpletest-rc-to-be-deleted-bq65j" in namespace "gc-7618"
Nov 15 16:21:12.236: INFO: Deleting pod "simpletest-rc-to-be-deleted-bw5t5" in namespace "gc-7618"
Nov 15 16:21:12.286: INFO: Deleting pod "simpletest-rc-to-be-deleted-c5wtc" in namespace "gc-7618"
Nov 15 16:21:12.327: INFO: Deleting pod "simpletest-rc-to-be-deleted-c9rl4" in namespace "gc-7618"
Nov 15 16:21:12.391: INFO: Deleting pod "simpletest-rc-to-be-deleted-cplnh" in namespace "gc-7618"
Nov 15 16:21:12.453: INFO: Deleting pod "simpletest-rc-to-be-deleted-ctrdg" in namespace "gc-7618"
Nov 15 16:21:12.503: INFO: Deleting pod "simpletest-rc-to-be-deleted-d284h" in namespace "gc-7618"
Nov 15 16:21:12.551: INFO: Deleting pod "simpletest-rc-to-be-deleted-d2crp" in namespace "gc-7618"
Nov 15 16:21:12.599: INFO: Deleting pod "simpletest-rc-to-be-deleted-d64ws" in namespace "gc-7618"
Nov 15 16:21:12.649: INFO: Deleting pod "simpletest-rc-to-be-deleted-d6rw2" in namespace "gc-7618"
Nov 15 16:21:12.722: INFO: Deleting pod "simpletest-rc-to-be-deleted-dk2v6" in namespace "gc-7618"
Nov 15 16:21:12.765: INFO: Deleting pod "simpletest-rc-to-be-deleted-drtrk" in namespace "gc-7618"
Nov 15 16:21:12.815: INFO: Deleting pod "simpletest-rc-to-be-deleted-fl78r" in namespace "gc-7618"
Nov 15 16:21:12.862: INFO: Deleting pod "simpletest-rc-to-be-deleted-fmrdd" in namespace "gc-7618"
Nov 15 16:21:12.917: INFO: Deleting pod "simpletest-rc-to-be-deleted-fpt6g" in namespace "gc-7618"
Nov 15 16:21:12.961: INFO: Deleting pod "simpletest-rc-to-be-deleted-fsxp9" in namespace "gc-7618"
Nov 15 16:21:13.001: INFO: Deleting pod "simpletest-rc-to-be-deleted-ft9hv" in namespace "gc-7618"
Nov 15 16:21:13.053: INFO: Deleting pod "simpletest-rc-to-be-deleted-g2tlx" in namespace "gc-7618"
Nov 15 16:21:13.103: INFO: Deleting pod "simpletest-rc-to-be-deleted-g6dcw" in namespace "gc-7618"
Nov 15 16:21:13.143: INFO: Deleting pod "simpletest-rc-to-be-deleted-g6xk2" in namespace "gc-7618"
Nov 15 16:21:13.188: INFO: Deleting pod "simpletest-rc-to-be-deleted-gcjfc" in namespace "gc-7618"
Nov 15 16:21:13.250: INFO: Deleting pod "simpletest-rc-to-be-deleted-ghbzw" in namespace "gc-7618"
Nov 15 16:21:13.301: INFO: Deleting pod "simpletest-rc-to-be-deleted-h2d6c" in namespace "gc-7618"
Nov 15 16:21:13.356: INFO: Deleting pod "simpletest-rc-to-be-deleted-hdgfv" in namespace "gc-7618"
Nov 15 16:21:13.415: INFO: Deleting pod "simpletest-rc-to-be-deleted-jbl46" in namespace "gc-7618"
Nov 15 16:21:13.549: INFO: Deleting pod "simpletest-rc-to-be-deleted-jjrgp" in namespace "gc-7618"
Nov 15 16:21:13.604: INFO: Deleting pod "simpletest-rc-to-be-deleted-jpvws" in namespace "gc-7618"
Nov 15 16:21:13.648: INFO: Deleting pod "simpletest-rc-to-be-deleted-jwvm5" in namespace "gc-7618"
Nov 15 16:21:13.701: INFO: Deleting pod "simpletest-rc-to-be-deleted-k6l4f" in namespace "gc-7618"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Nov 15 16:21:13.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-7618" for this suite. 11/15/23 16:21:13.789
------------------------------
• [SLOW TEST] [14.733 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:20:59.107
    Nov 15 16:20:59.107: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename gc 11/15/23 16:20:59.109
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:20:59.172
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:20:59.192
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 11/15/23 16:20:59.242
    STEP: create the rc2 11/15/23 16:20:59.265
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 11/15/23 16:21:04.311
    STEP: delete the rc simpletest-rc-to-be-deleted 11/15/23 16:21:05.971
    STEP: wait for the rc to be deleted 11/15/23 16:21:06.017
    STEP: Gathering metrics 11/15/23 16:21:11.08
    W1115 16:21:11.126392      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Nov 15 16:21:11.126: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Nov 15 16:21:11.126: INFO: Deleting pod "simpletest-rc-to-be-deleted-255jp" in namespace "gc-7618"
    Nov 15 16:21:11.171: INFO: Deleting pod "simpletest-rc-to-be-deleted-29cmc" in namespace "gc-7618"
    Nov 15 16:21:11.216: INFO: Deleting pod "simpletest-rc-to-be-deleted-2gq62" in namespace "gc-7618"
    Nov 15 16:21:11.267: INFO: Deleting pod "simpletest-rc-to-be-deleted-2m5xt" in namespace "gc-7618"
    Nov 15 16:21:11.308: INFO: Deleting pod "simpletest-rc-to-be-deleted-2p6ln" in namespace "gc-7618"
    Nov 15 16:21:11.366: INFO: Deleting pod "simpletest-rc-to-be-deleted-2s9dk" in namespace "gc-7618"
    Nov 15 16:21:11.449: INFO: Deleting pod "simpletest-rc-to-be-deleted-5mcfw" in namespace "gc-7618"
    Nov 15 16:21:11.505: INFO: Deleting pod "simpletest-rc-to-be-deleted-6b4xx" in namespace "gc-7618"
    Nov 15 16:21:11.546: INFO: Deleting pod "simpletest-rc-to-be-deleted-6bpp2" in namespace "gc-7618"
    Nov 15 16:21:11.589: INFO: Deleting pod "simpletest-rc-to-be-deleted-6ct6n" in namespace "gc-7618"
    Nov 15 16:21:11.635: INFO: Deleting pod "simpletest-rc-to-be-deleted-6fzvp" in namespace "gc-7618"
    Nov 15 16:21:11.698: INFO: Deleting pod "simpletest-rc-to-be-deleted-6qxhn" in namespace "gc-7618"
    Nov 15 16:21:11.748: INFO: Deleting pod "simpletest-rc-to-be-deleted-6zgjf" in namespace "gc-7618"
    Nov 15 16:21:11.798: INFO: Deleting pod "simpletest-rc-to-be-deleted-7k7r8" in namespace "gc-7618"
    Nov 15 16:21:11.837: INFO: Deleting pod "simpletest-rc-to-be-deleted-7pwc8" in namespace "gc-7618"
    Nov 15 16:21:11.885: INFO: Deleting pod "simpletest-rc-to-be-deleted-7z88r" in namespace "gc-7618"
    Nov 15 16:21:11.941: INFO: Deleting pod "simpletest-rc-to-be-deleted-8qvtt" in namespace "gc-7618"
    Nov 15 16:21:11.993: INFO: Deleting pod "simpletest-rc-to-be-deleted-8thff" in namespace "gc-7618"
    Nov 15 16:21:12.036: INFO: Deleting pod "simpletest-rc-to-be-deleted-8z4m9" in namespace "gc-7618"
    Nov 15 16:21:12.092: INFO: Deleting pod "simpletest-rc-to-be-deleted-9s9wq" in namespace "gc-7618"
    Nov 15 16:21:12.138: INFO: Deleting pod "simpletest-rc-to-be-deleted-bd4z5" in namespace "gc-7618"
    Nov 15 16:21:12.191: INFO: Deleting pod "simpletest-rc-to-be-deleted-bq65j" in namespace "gc-7618"
    Nov 15 16:21:12.236: INFO: Deleting pod "simpletest-rc-to-be-deleted-bw5t5" in namespace "gc-7618"
    Nov 15 16:21:12.286: INFO: Deleting pod "simpletest-rc-to-be-deleted-c5wtc" in namespace "gc-7618"
    Nov 15 16:21:12.327: INFO: Deleting pod "simpletest-rc-to-be-deleted-c9rl4" in namespace "gc-7618"
    Nov 15 16:21:12.391: INFO: Deleting pod "simpletest-rc-to-be-deleted-cplnh" in namespace "gc-7618"
    Nov 15 16:21:12.453: INFO: Deleting pod "simpletest-rc-to-be-deleted-ctrdg" in namespace "gc-7618"
    Nov 15 16:21:12.503: INFO: Deleting pod "simpletest-rc-to-be-deleted-d284h" in namespace "gc-7618"
    Nov 15 16:21:12.551: INFO: Deleting pod "simpletest-rc-to-be-deleted-d2crp" in namespace "gc-7618"
    Nov 15 16:21:12.599: INFO: Deleting pod "simpletest-rc-to-be-deleted-d64ws" in namespace "gc-7618"
    Nov 15 16:21:12.649: INFO: Deleting pod "simpletest-rc-to-be-deleted-d6rw2" in namespace "gc-7618"
    Nov 15 16:21:12.722: INFO: Deleting pod "simpletest-rc-to-be-deleted-dk2v6" in namespace "gc-7618"
    Nov 15 16:21:12.765: INFO: Deleting pod "simpletest-rc-to-be-deleted-drtrk" in namespace "gc-7618"
    Nov 15 16:21:12.815: INFO: Deleting pod "simpletest-rc-to-be-deleted-fl78r" in namespace "gc-7618"
    Nov 15 16:21:12.862: INFO: Deleting pod "simpletest-rc-to-be-deleted-fmrdd" in namespace "gc-7618"
    Nov 15 16:21:12.917: INFO: Deleting pod "simpletest-rc-to-be-deleted-fpt6g" in namespace "gc-7618"
    Nov 15 16:21:12.961: INFO: Deleting pod "simpletest-rc-to-be-deleted-fsxp9" in namespace "gc-7618"
    Nov 15 16:21:13.001: INFO: Deleting pod "simpletest-rc-to-be-deleted-ft9hv" in namespace "gc-7618"
    Nov 15 16:21:13.053: INFO: Deleting pod "simpletest-rc-to-be-deleted-g2tlx" in namespace "gc-7618"
    Nov 15 16:21:13.103: INFO: Deleting pod "simpletest-rc-to-be-deleted-g6dcw" in namespace "gc-7618"
    Nov 15 16:21:13.143: INFO: Deleting pod "simpletest-rc-to-be-deleted-g6xk2" in namespace "gc-7618"
    Nov 15 16:21:13.188: INFO: Deleting pod "simpletest-rc-to-be-deleted-gcjfc" in namespace "gc-7618"
    Nov 15 16:21:13.250: INFO: Deleting pod "simpletest-rc-to-be-deleted-ghbzw" in namespace "gc-7618"
    Nov 15 16:21:13.301: INFO: Deleting pod "simpletest-rc-to-be-deleted-h2d6c" in namespace "gc-7618"
    Nov 15 16:21:13.356: INFO: Deleting pod "simpletest-rc-to-be-deleted-hdgfv" in namespace "gc-7618"
    Nov 15 16:21:13.415: INFO: Deleting pod "simpletest-rc-to-be-deleted-jbl46" in namespace "gc-7618"
    Nov 15 16:21:13.549: INFO: Deleting pod "simpletest-rc-to-be-deleted-jjrgp" in namespace "gc-7618"
    Nov 15 16:21:13.604: INFO: Deleting pod "simpletest-rc-to-be-deleted-jpvws" in namespace "gc-7618"
    Nov 15 16:21:13.648: INFO: Deleting pod "simpletest-rc-to-be-deleted-jwvm5" in namespace "gc-7618"
    Nov 15 16:21:13.701: INFO: Deleting pod "simpletest-rc-to-be-deleted-k6l4f" in namespace "gc-7618"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:21:13.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-7618" for this suite. 11/15/23 16:21:13.789
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:21:13.845
Nov 15 16:21:13.845: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename daemonsets 11/15/23 16:21:13.853
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:21:13.917
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:21:13.939
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873
STEP: Creating simple DaemonSet "daemon-set" 11/15/23 16:21:14.081
STEP: Check that daemon pods launch on every node of the cluster. 11/15/23 16:21:14.104
Nov 15 16:21:14.152: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 16:21:14.152: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
Nov 15 16:21:15.217: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 16:21:15.217: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
Nov 15 16:21:16.201: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 16:21:16.201: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
Nov 15 16:21:17.203: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 16:21:17.203: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
Nov 15 16:21:18.213: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 16:21:18.213: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
Nov 15 16:21:19.200: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 16:21:19.201: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
Nov 15 16:21:20.200: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 16:21:20.200: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
Nov 15 16:21:21.205: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 16:21:21.205: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
Nov 15 16:21:22.202: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Nov 15 16:21:22.202: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
Nov 15 16:21:23.208: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 15 16:21:23.209: INFO: Node 10.15.40.115 is running 0 daemon pod, expected 1
Nov 15 16:21:24.200: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 15 16:21:24.200: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 11/15/23 16:21:24.218
Nov 15 16:21:24.237: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 11/15/23 16:21:24.237
Nov 15 16:21:24.285: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 11/15/23 16:21:24.285
Nov 15 16:21:24.297: INFO: Observed &DaemonSet event: ADDED
Nov 15 16:21:24.298: INFO: Observed &DaemonSet event: MODIFIED
Nov 15 16:21:24.299: INFO: Observed &DaemonSet event: MODIFIED
Nov 15 16:21:24.300: INFO: Observed &DaemonSet event: MODIFIED
Nov 15 16:21:24.301: INFO: Observed &DaemonSet event: MODIFIED
Nov 15 16:21:24.301: INFO: Found daemon set daemon-set in namespace daemonsets-1583 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Nov 15 16:21:24.301: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 11/15/23 16:21:24.301
STEP: watching for the daemon set status to be patched 11/15/23 16:21:24.331
Nov 15 16:21:24.342: INFO: Observed &DaemonSet event: ADDED
Nov 15 16:21:24.343: INFO: Observed &DaemonSet event: MODIFIED
Nov 15 16:21:24.343: INFO: Observed &DaemonSet event: MODIFIED
Nov 15 16:21:24.344: INFO: Observed &DaemonSet event: MODIFIED
Nov 15 16:21:24.345: INFO: Observed &DaemonSet event: MODIFIED
Nov 15 16:21:24.346: INFO: Observed daemon set daemon-set in namespace daemonsets-1583 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Nov 15 16:21:24.346: INFO: Observed &DaemonSet event: MODIFIED
Nov 15 16:21:24.346: INFO: Found daemon set daemon-set in namespace daemonsets-1583 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Nov 15 16:21:24.346: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 11/15/23 16:21:24.364
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1583, will wait for the garbage collector to delete the pods 11/15/23 16:21:24.364
Nov 15 16:21:24.464: INFO: Deleting DaemonSet.extensions daemon-set took: 28.202565ms
Nov 15 16:21:24.565: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.842535ms
Nov 15 16:21:26.687: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 16:21:26.687: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Nov 15 16:21:26.704: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"37246"},"items":null}

Nov 15 16:21:26.721: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"37246"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 16:21:26.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1583" for this suite. 11/15/23 16:21:26.832
------------------------------
• [SLOW TEST] [13.017 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:21:13.845
    Nov 15 16:21:13.845: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename daemonsets 11/15/23 16:21:13.853
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:21:13.917
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:21:13.939
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:873
    STEP: Creating simple DaemonSet "daemon-set" 11/15/23 16:21:14.081
    STEP: Check that daemon pods launch on every node of the cluster. 11/15/23 16:21:14.104
    Nov 15 16:21:14.152: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 16:21:14.152: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
    Nov 15 16:21:15.217: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 16:21:15.217: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
    Nov 15 16:21:16.201: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 16:21:16.201: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
    Nov 15 16:21:17.203: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 16:21:17.203: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
    Nov 15 16:21:18.213: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 16:21:18.213: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
    Nov 15 16:21:19.200: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 16:21:19.201: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
    Nov 15 16:21:20.200: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 16:21:20.200: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
    Nov 15 16:21:21.205: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 16:21:21.205: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
    Nov 15 16:21:22.202: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Nov 15 16:21:22.202: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
    Nov 15 16:21:23.208: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Nov 15 16:21:23.209: INFO: Node 10.15.40.115 is running 0 daemon pod, expected 1
    Nov 15 16:21:24.200: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Nov 15 16:21:24.200: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 11/15/23 16:21:24.218
    Nov 15 16:21:24.237: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 11/15/23 16:21:24.237
    Nov 15 16:21:24.285: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 11/15/23 16:21:24.285
    Nov 15 16:21:24.297: INFO: Observed &DaemonSet event: ADDED
    Nov 15 16:21:24.298: INFO: Observed &DaemonSet event: MODIFIED
    Nov 15 16:21:24.299: INFO: Observed &DaemonSet event: MODIFIED
    Nov 15 16:21:24.300: INFO: Observed &DaemonSet event: MODIFIED
    Nov 15 16:21:24.301: INFO: Observed &DaemonSet event: MODIFIED
    Nov 15 16:21:24.301: INFO: Found daemon set daemon-set in namespace daemonsets-1583 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Nov 15 16:21:24.301: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 11/15/23 16:21:24.301
    STEP: watching for the daemon set status to be patched 11/15/23 16:21:24.331
    Nov 15 16:21:24.342: INFO: Observed &DaemonSet event: ADDED
    Nov 15 16:21:24.343: INFO: Observed &DaemonSet event: MODIFIED
    Nov 15 16:21:24.343: INFO: Observed &DaemonSet event: MODIFIED
    Nov 15 16:21:24.344: INFO: Observed &DaemonSet event: MODIFIED
    Nov 15 16:21:24.345: INFO: Observed &DaemonSet event: MODIFIED
    Nov 15 16:21:24.346: INFO: Observed daemon set daemon-set in namespace daemonsets-1583 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Nov 15 16:21:24.346: INFO: Observed &DaemonSet event: MODIFIED
    Nov 15 16:21:24.346: INFO: Found daemon set daemon-set in namespace daemonsets-1583 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Nov 15 16:21:24.346: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 11/15/23 16:21:24.364
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1583, will wait for the garbage collector to delete the pods 11/15/23 16:21:24.364
    Nov 15 16:21:24.464: INFO: Deleting DaemonSet.extensions daemon-set took: 28.202565ms
    Nov 15 16:21:24.565: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.842535ms
    Nov 15 16:21:26.687: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 16:21:26.687: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Nov 15 16:21:26.704: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"37246"},"items":null}

    Nov 15 16:21:26.721: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"37246"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:21:26.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1583" for this suite. 11/15/23 16:21:26.832
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:21:26.868
Nov 15 16:21:26.869: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename endpointslice 11/15/23 16:21:26.871
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:21:26.939
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:21:26.957
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 11/15/23 16:21:26.977
STEP: getting /apis/discovery.k8s.io 11/15/23 16:21:27.001
STEP: getting /apis/discovery.k8s.iov1 11/15/23 16:21:27.011
STEP: creating 11/15/23 16:21:27.02
STEP: getting 11/15/23 16:21:27.088
STEP: listing 11/15/23 16:21:27.11
STEP: watching 11/15/23 16:21:27.128
Nov 15 16:21:27.128: INFO: starting watch
STEP: cluster-wide listing 11/15/23 16:21:27.142
STEP: cluster-wide watching 11/15/23 16:21:27.16
Nov 15 16:21:27.160: INFO: starting watch
STEP: patching 11/15/23 16:21:27.169
STEP: updating 11/15/23 16:21:27.192
Nov 15 16:21:27.232: INFO: waiting for watch events with expected annotations
Nov 15 16:21:27.233: INFO: saw patched and updated annotations
STEP: deleting 11/15/23 16:21:27.233
STEP: deleting a collection 11/15/23 16:21:27.297
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Nov 15 16:21:27.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-8811" for this suite. 11/15/23 16:21:27.398
------------------------------
• [0.569 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:21:26.868
    Nov 15 16:21:26.869: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename endpointslice 11/15/23 16:21:26.871
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:21:26.939
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:21:26.957
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 11/15/23 16:21:26.977
    STEP: getting /apis/discovery.k8s.io 11/15/23 16:21:27.001
    STEP: getting /apis/discovery.k8s.iov1 11/15/23 16:21:27.011
    STEP: creating 11/15/23 16:21:27.02
    STEP: getting 11/15/23 16:21:27.088
    STEP: listing 11/15/23 16:21:27.11
    STEP: watching 11/15/23 16:21:27.128
    Nov 15 16:21:27.128: INFO: starting watch
    STEP: cluster-wide listing 11/15/23 16:21:27.142
    STEP: cluster-wide watching 11/15/23 16:21:27.16
    Nov 15 16:21:27.160: INFO: starting watch
    STEP: patching 11/15/23 16:21:27.169
    STEP: updating 11/15/23 16:21:27.192
    Nov 15 16:21:27.232: INFO: waiting for watch events with expected annotations
    Nov 15 16:21:27.233: INFO: saw patched and updated annotations
    STEP: deleting 11/15/23 16:21:27.233
    STEP: deleting a collection 11/15/23 16:21:27.297
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:21:27.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-8811" for this suite. 11/15/23 16:21:27.398
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:21:27.44
Nov 15 16:21:27.440: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename limitrange 11/15/23 16:21:27.443
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:21:27.499
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:21:27.519
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-vxbqv" in namespace "limitrange-3108" 11/15/23 16:21:27.54
STEP: Creating another limitRange in another namespace 11/15/23 16:21:27.565
Nov 15 16:21:27.627: INFO: Namespace "e2e-limitrange-vxbqv-2235" created
Nov 15 16:21:27.627: INFO: Creating LimitRange "e2e-limitrange-vxbqv" in namespace "e2e-limitrange-vxbqv-2235"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-vxbqv" 11/15/23 16:21:27.651
Nov 15 16:21:27.672: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-vxbqv" in "limitrange-3108" namespace 11/15/23 16:21:27.672
Nov 15 16:21:27.701: INFO: LimitRange "e2e-limitrange-vxbqv" has been patched
STEP: Delete LimitRange "e2e-limitrange-vxbqv" by Collection with labelSelector: "e2e-limitrange-vxbqv=patched" 11/15/23 16:21:27.701
STEP: Confirm that the limitRange "e2e-limitrange-vxbqv" has been deleted 11/15/23 16:21:27.75
Nov 15 16:21:27.750: INFO: Requesting list of LimitRange to confirm quantity
Nov 15 16:21:27.771: INFO: Found 0 LimitRange with label "e2e-limitrange-vxbqv=patched"
Nov 15 16:21:27.771: INFO: LimitRange "e2e-limitrange-vxbqv" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-vxbqv" 11/15/23 16:21:27.771
Nov 15 16:21:27.793: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Nov 15 16:21:27.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-3108" for this suite. 11/15/23 16:21:27.821
STEP: Destroying namespace "e2e-limitrange-vxbqv-2235" for this suite. 11/15/23 16:21:27.855
------------------------------
• [0.444 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:21:27.44
    Nov 15 16:21:27.440: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename limitrange 11/15/23 16:21:27.443
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:21:27.499
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:21:27.519
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-vxbqv" in namespace "limitrange-3108" 11/15/23 16:21:27.54
    STEP: Creating another limitRange in another namespace 11/15/23 16:21:27.565
    Nov 15 16:21:27.627: INFO: Namespace "e2e-limitrange-vxbqv-2235" created
    Nov 15 16:21:27.627: INFO: Creating LimitRange "e2e-limitrange-vxbqv" in namespace "e2e-limitrange-vxbqv-2235"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-vxbqv" 11/15/23 16:21:27.651
    Nov 15 16:21:27.672: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-vxbqv" in "limitrange-3108" namespace 11/15/23 16:21:27.672
    Nov 15 16:21:27.701: INFO: LimitRange "e2e-limitrange-vxbqv" has been patched
    STEP: Delete LimitRange "e2e-limitrange-vxbqv" by Collection with labelSelector: "e2e-limitrange-vxbqv=patched" 11/15/23 16:21:27.701
    STEP: Confirm that the limitRange "e2e-limitrange-vxbqv" has been deleted 11/15/23 16:21:27.75
    Nov 15 16:21:27.750: INFO: Requesting list of LimitRange to confirm quantity
    Nov 15 16:21:27.771: INFO: Found 0 LimitRange with label "e2e-limitrange-vxbqv=patched"
    Nov 15 16:21:27.771: INFO: LimitRange "e2e-limitrange-vxbqv" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-vxbqv" 11/15/23 16:21:27.771
    Nov 15 16:21:27.793: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:21:27.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-3108" for this suite. 11/15/23 16:21:27.821
    STEP: Destroying namespace "e2e-limitrange-vxbqv-2235" for this suite. 11/15/23 16:21:27.855
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:21:27.887
Nov 15 16:21:27.887: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename namespaces 11/15/23 16:21:27.891
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:21:27.95
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:21:27.97
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 11/15/23 16:21:27.991
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:21:28.048
STEP: Creating a pod in the namespace 11/15/23 16:21:28.067
STEP: Waiting for the pod to have running status 11/15/23 16:21:28.105
Nov 15 16:21:28.106: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-9423" to be "running"
Nov 15 16:21:28.124: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 17.720083ms
Nov 15 16:21:30.144: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037316594s
Nov 15 16:21:32.148: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.041362634s
Nov 15 16:21:32.148: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 11/15/23 16:21:32.148
STEP: Waiting for the namespace to be removed. 11/15/23 16:21:32.178
STEP: Recreating the namespace 11/15/23 16:21:44.197
STEP: Verifying there are no pods in the namespace 11/15/23 16:21:44.257
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 16:21:44.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-7763" for this suite. 11/15/23 16:21:44.306
STEP: Destroying namespace "nsdeletetest-9423" for this suite. 11/15/23 16:21:44.335
Nov 15 16:21:44.354: INFO: Namespace nsdeletetest-9423 was already deleted
STEP: Destroying namespace "nsdeletetest-1090" for this suite. 11/15/23 16:21:44.354
------------------------------
• [SLOW TEST] [16.499 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:21:27.887
    Nov 15 16:21:27.887: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename namespaces 11/15/23 16:21:27.891
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:21:27.95
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:21:27.97
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 11/15/23 16:21:27.991
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:21:28.048
    STEP: Creating a pod in the namespace 11/15/23 16:21:28.067
    STEP: Waiting for the pod to have running status 11/15/23 16:21:28.105
    Nov 15 16:21:28.106: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-9423" to be "running"
    Nov 15 16:21:28.124: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 17.720083ms
    Nov 15 16:21:30.144: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037316594s
    Nov 15 16:21:32.148: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.041362634s
    Nov 15 16:21:32.148: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 11/15/23 16:21:32.148
    STEP: Waiting for the namespace to be removed. 11/15/23 16:21:32.178
    STEP: Recreating the namespace 11/15/23 16:21:44.197
    STEP: Verifying there are no pods in the namespace 11/15/23 16:21:44.257
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:21:44.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-7763" for this suite. 11/15/23 16:21:44.306
    STEP: Destroying namespace "nsdeletetest-9423" for this suite. 11/15/23 16:21:44.335
    Nov 15 16:21:44.354: INFO: Namespace nsdeletetest-9423 was already deleted
    STEP: Destroying namespace "nsdeletetest-1090" for this suite. 11/15/23 16:21:44.354
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:21:44.399
Nov 15 16:21:44.400: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename pods 11/15/23 16:21:44.402
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:21:44.46
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:21:44.48
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 11/15/23 16:21:44.5
Nov 15 16:21:44.538: INFO: created test-pod-1
Nov 15 16:21:44.559: INFO: created test-pod-2
Nov 15 16:21:44.581: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 11/15/23 16:21:44.581
Nov 15 16:21:44.582: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-6113' to be running and ready
Nov 15 16:21:44.656: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Nov 15 16:21:44.656: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Nov 15 16:21:44.656: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Nov 15 16:21:44.656: INFO: 0 / 3 pods in namespace 'pods-6113' are running and ready (0 seconds elapsed)
Nov 15 16:21:44.656: INFO: expected 0 pod replicas in namespace 'pods-6113', 0 are Running and Ready.
Nov 15 16:21:44.656: INFO: POD         NODE          PHASE    GRACE  CONDITIONS
Nov 15 16:21:44.657: INFO: test-pod-1  10.15.40.115  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 16:21:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 16:21:44 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 16:21:44 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 16:21:44 +0000 UTC  }]
Nov 15 16:21:44.657: INFO: test-pod-2  10.15.40.115  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 16:21:44 +0000 UTC  }]
Nov 15 16:21:44.657: INFO: test-pod-3  10.15.40.115  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 16:21:44 +0000 UTC  }]
Nov 15 16:21:44.657: INFO: 
Nov 15 16:21:46.717: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Nov 15 16:21:46.717: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Nov 15 16:21:46.717: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Nov 15 16:21:46.717: INFO: 0 / 3 pods in namespace 'pods-6113' are running and ready (2 seconds elapsed)
Nov 15 16:21:46.717: INFO: expected 0 pod replicas in namespace 'pods-6113', 0 are Running and Ready.
Nov 15 16:21:46.717: INFO: POD         NODE          PHASE    GRACE  CONDITIONS
Nov 15 16:21:46.717: INFO: test-pod-1  10.15.40.115  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 16:21:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 16:21:44 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 16:21:44 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 16:21:44 +0000 UTC  }]
Nov 15 16:21:46.717: INFO: test-pod-2  10.15.40.115  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 16:21:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 16:21:44 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 16:21:44 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 16:21:44 +0000 UTC  }]
Nov 15 16:21:46.717: INFO: test-pod-3  10.15.40.115  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 16:21:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 16:21:44 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 16:21:44 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 16:21:44 +0000 UTC  }]
Nov 15 16:21:46.717: INFO: 
Nov 15 16:21:48.724: INFO: 3 / 3 pods in namespace 'pods-6113' are running and ready (4 seconds elapsed)
Nov 15 16:21:48.724: INFO: expected 0 pod replicas in namespace 'pods-6113', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 11/15/23 16:21:48.809
Nov 15 16:21:48.829: INFO: Pod quantity 3 is different from expected quantity 0
Nov 15 16:21:49.850: INFO: Pod quantity 3 is different from expected quantity 0
Nov 15 16:21:50.851: INFO: Pod quantity 2 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Nov 15 16:21:51.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6113" for this suite. 11/15/23 16:21:51.882
------------------------------
• [SLOW TEST] [7.512 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:21:44.399
    Nov 15 16:21:44.400: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename pods 11/15/23 16:21:44.402
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:21:44.46
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:21:44.48
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 11/15/23 16:21:44.5
    Nov 15 16:21:44.538: INFO: created test-pod-1
    Nov 15 16:21:44.559: INFO: created test-pod-2
    Nov 15 16:21:44.581: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 11/15/23 16:21:44.581
    Nov 15 16:21:44.582: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-6113' to be running and ready
    Nov 15 16:21:44.656: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Nov 15 16:21:44.656: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Nov 15 16:21:44.656: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Nov 15 16:21:44.656: INFO: 0 / 3 pods in namespace 'pods-6113' are running and ready (0 seconds elapsed)
    Nov 15 16:21:44.656: INFO: expected 0 pod replicas in namespace 'pods-6113', 0 are Running and Ready.
    Nov 15 16:21:44.656: INFO: POD         NODE          PHASE    GRACE  CONDITIONS
    Nov 15 16:21:44.657: INFO: test-pod-1  10.15.40.115  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 16:21:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 16:21:44 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 16:21:44 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 16:21:44 +0000 UTC  }]
    Nov 15 16:21:44.657: INFO: test-pod-2  10.15.40.115  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 16:21:44 +0000 UTC  }]
    Nov 15 16:21:44.657: INFO: test-pod-3  10.15.40.115  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 16:21:44 +0000 UTC  }]
    Nov 15 16:21:44.657: INFO: 
    Nov 15 16:21:46.717: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Nov 15 16:21:46.717: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Nov 15 16:21:46.717: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Nov 15 16:21:46.717: INFO: 0 / 3 pods in namespace 'pods-6113' are running and ready (2 seconds elapsed)
    Nov 15 16:21:46.717: INFO: expected 0 pod replicas in namespace 'pods-6113', 0 are Running and Ready.
    Nov 15 16:21:46.717: INFO: POD         NODE          PHASE    GRACE  CONDITIONS
    Nov 15 16:21:46.717: INFO: test-pod-1  10.15.40.115  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 16:21:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 16:21:44 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 16:21:44 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 16:21:44 +0000 UTC  }]
    Nov 15 16:21:46.717: INFO: test-pod-2  10.15.40.115  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 16:21:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 16:21:44 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 16:21:44 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 16:21:44 +0000 UTC  }]
    Nov 15 16:21:46.717: INFO: test-pod-3  10.15.40.115  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 16:21:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 16:21:44 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 16:21:44 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 16:21:44 +0000 UTC  }]
    Nov 15 16:21:46.717: INFO: 
    Nov 15 16:21:48.724: INFO: 3 / 3 pods in namespace 'pods-6113' are running and ready (4 seconds elapsed)
    Nov 15 16:21:48.724: INFO: expected 0 pod replicas in namespace 'pods-6113', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 11/15/23 16:21:48.809
    Nov 15 16:21:48.829: INFO: Pod quantity 3 is different from expected quantity 0
    Nov 15 16:21:49.850: INFO: Pod quantity 3 is different from expected quantity 0
    Nov 15 16:21:50.851: INFO: Pod quantity 2 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:21:51.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6113" for this suite. 11/15/23 16:21:51.882
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:21:51.919
Nov 15 16:21:51.919: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename container-runtime 11/15/23 16:21:51.92
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:21:51.982
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:21:52
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 11/15/23 16:21:52.02
STEP: wait for the container to reach Succeeded 11/15/23 16:21:52.057
STEP: get the container status 11/15/23 16:21:57.173
STEP: the container should be terminated 11/15/23 16:21:57.195
STEP: the termination message should be set 11/15/23 16:21:57.196
Nov 15 16:21:57.196: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 11/15/23 16:21:57.196
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Nov 15 16:21:57.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-4683" for this suite. 11/15/23 16:21:57.301
------------------------------
• [SLOW TEST] [5.411 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:21:51.919
    Nov 15 16:21:51.919: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename container-runtime 11/15/23 16:21:51.92
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:21:51.982
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:21:52
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 11/15/23 16:21:52.02
    STEP: wait for the container to reach Succeeded 11/15/23 16:21:52.057
    STEP: get the container status 11/15/23 16:21:57.173
    STEP: the container should be terminated 11/15/23 16:21:57.195
    STEP: the termination message should be set 11/15/23 16:21:57.196
    Nov 15 16:21:57.196: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 11/15/23 16:21:57.196
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:21:57.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-4683" for this suite. 11/15/23 16:21:57.301
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:21:57.334
Nov 15 16:21:57.334: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename statefulset 11/15/23 16:21:57.335
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:21:57.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:21:57.408
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-6925 11/15/23 16:21:57.426
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-6925 11/15/23 16:21:57.47
Nov 15 16:21:57.514: INFO: Found 0 stateful pods, waiting for 1
Nov 15 16:22:07.534: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 11/15/23 16:22:07.574
STEP: Getting /status 11/15/23 16:22:07.618
Nov 15 16:22:07.649: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 11/15/23 16:22:07.649
Nov 15 16:22:07.696: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 11/15/23 16:22:07.696
Nov 15 16:22:07.706: INFO: Observed &StatefulSet event: ADDED
Nov 15 16:22:07.706: INFO: Found Statefulset ss in namespace statefulset-6925 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Nov 15 16:22:07.706: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 11/15/23 16:22:07.706
Nov 15 16:22:07.707: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Nov 15 16:22:07.732: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 11/15/23 16:22:07.732
Nov 15 16:22:07.743: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Nov 15 16:22:07.743: INFO: Deleting all statefulset in ns statefulset-6925
Nov 15 16:22:07.767: INFO: Scaling statefulset ss to 0
Nov 15 16:22:17.859: INFO: Waiting for statefulset status.replicas updated to 0
Nov 15 16:22:17.882: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Nov 15 16:22:17.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-6925" for this suite. 11/15/23 16:22:17.985
------------------------------
• [SLOW TEST] [20.680 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:21:57.334
    Nov 15 16:21:57.334: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename statefulset 11/15/23 16:21:57.335
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:21:57.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:21:57.408
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-6925 11/15/23 16:21:57.426
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-6925 11/15/23 16:21:57.47
    Nov 15 16:21:57.514: INFO: Found 0 stateful pods, waiting for 1
    Nov 15 16:22:07.534: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 11/15/23 16:22:07.574
    STEP: Getting /status 11/15/23 16:22:07.618
    Nov 15 16:22:07.649: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 11/15/23 16:22:07.649
    Nov 15 16:22:07.696: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 11/15/23 16:22:07.696
    Nov 15 16:22:07.706: INFO: Observed &StatefulSet event: ADDED
    Nov 15 16:22:07.706: INFO: Found Statefulset ss in namespace statefulset-6925 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Nov 15 16:22:07.706: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 11/15/23 16:22:07.706
    Nov 15 16:22:07.707: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Nov 15 16:22:07.732: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 11/15/23 16:22:07.732
    Nov 15 16:22:07.743: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Nov 15 16:22:07.743: INFO: Deleting all statefulset in ns statefulset-6925
    Nov 15 16:22:07.767: INFO: Scaling statefulset ss to 0
    Nov 15 16:22:17.859: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 15 16:22:17.882: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:22:17.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-6925" for this suite. 11/15/23 16:22:17.985
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:22:18.025
Nov 15 16:22:18.026: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename dns 11/15/23 16:22:18.028
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:22:18.09
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:22:18.113
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 11/15/23 16:22:18.136
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4731 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4731;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4731 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4731;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4731.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4731.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4731.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4731.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4731.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4731.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4731.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4731.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4731.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4731.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4731.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4731.svc;check="$$(dig +notcp +noall +answer +search 245.91.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.91.245_udp@PTR;check="$$(dig +tcp +noall +answer +search 245.91.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.91.245_tcp@PTR;sleep 1; done
 11/15/23 16:22:18.217
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4731 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4731;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4731 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4731;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4731.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4731.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4731.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4731.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4731.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4731.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4731.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4731.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4731.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4731.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4731.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4731.svc;check="$$(dig +notcp +noall +answer +search 245.91.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.91.245_udp@PTR;check="$$(dig +tcp +noall +answer +search 245.91.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.91.245_tcp@PTR;sleep 1; done
 11/15/23 16:22:18.217
STEP: creating a pod to probe DNS 11/15/23 16:22:18.218
STEP: submitting the pod to kubernetes 11/15/23 16:22:18.218
Nov 15 16:22:18.256: INFO: Waiting up to 15m0s for pod "dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b" in namespace "dns-4731" to be "running"
Nov 15 16:22:18.276: INFO: Pod "dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b": Phase="Pending", Reason="", readiness=false. Elapsed: 19.283786ms
Nov 15 16:22:20.297: INFO: Pod "dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040882291s
Nov 15 16:22:22.296: INFO: Pod "dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b": Phase="Running", Reason="", readiness=true. Elapsed: 4.039714321s
Nov 15 16:22:22.296: INFO: Pod "dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b" satisfied condition "running"
STEP: retrieving the pod 11/15/23 16:22:22.296
STEP: looking for the results for each expected name from probers 11/15/23 16:22:22.315
Nov 15 16:22:22.397: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:22.430: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:22.457: INFO: Unable to read wheezy_udp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:22.485: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:22.512: INFO: Unable to read wheezy_udp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:22.539: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:22.566: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:22.594: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:22.730: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:22.757: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:22.786: INFO: Unable to read jessie_udp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:22.812: INFO: Unable to read jessie_tcp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:22.838: INFO: Unable to read jessie_udp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:22.863: INFO: Unable to read jessie_tcp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:22.888: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:22.912: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:23.020: INFO: Lookups using dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4731 wheezy_tcp@dns-test-service.dns-4731 wheezy_udp@dns-test-service.dns-4731.svc wheezy_tcp@dns-test-service.dns-4731.svc wheezy_udp@_http._tcp.dns-test-service.dns-4731.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4731.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4731 jessie_tcp@dns-test-service.dns-4731 jessie_udp@dns-test-service.dns-4731.svc jessie_tcp@dns-test-service.dns-4731.svc jessie_udp@_http._tcp.dns-test-service.dns-4731.svc jessie_tcp@_http._tcp.dns-test-service.dns-4731.svc]

Nov 15 16:22:28.053: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:28.083: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:28.108: INFO: Unable to read wheezy_udp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:28.134: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:28.167: INFO: Unable to read wheezy_udp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:28.194: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:28.222: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:28.248: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:28.377: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:28.404: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:28.430: INFO: Unable to read jessie_udp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:28.454: INFO: Unable to read jessie_tcp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:28.483: INFO: Unable to read jessie_udp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:28.510: INFO: Unable to read jessie_tcp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:28.534: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:28.560: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:28.664: INFO: Lookups using dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4731 wheezy_tcp@dns-test-service.dns-4731 wheezy_udp@dns-test-service.dns-4731.svc wheezy_tcp@dns-test-service.dns-4731.svc wheezy_udp@_http._tcp.dns-test-service.dns-4731.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4731.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4731 jessie_tcp@dns-test-service.dns-4731 jessie_udp@dns-test-service.dns-4731.svc jessie_tcp@dns-test-service.dns-4731.svc jessie_udp@_http._tcp.dns-test-service.dns-4731.svc jessie_tcp@_http._tcp.dns-test-service.dns-4731.svc]

Nov 15 16:22:33.050: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:33.078: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:33.103: INFO: Unable to read wheezy_udp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:33.129: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:33.154: INFO: Unable to read wheezy_udp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:33.181: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:33.207: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:33.232: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:33.374: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:33.405: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:33.437: INFO: Unable to read jessie_udp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:33.461: INFO: Unable to read jessie_tcp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:33.487: INFO: Unable to read jessie_udp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:33.513: INFO: Unable to read jessie_tcp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:33.538: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:33.564: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:33.667: INFO: Lookups using dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4731 wheezy_tcp@dns-test-service.dns-4731 wheezy_udp@dns-test-service.dns-4731.svc wheezy_tcp@dns-test-service.dns-4731.svc wheezy_udp@_http._tcp.dns-test-service.dns-4731.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4731.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4731 jessie_tcp@dns-test-service.dns-4731 jessie_udp@dns-test-service.dns-4731.svc jessie_tcp@dns-test-service.dns-4731.svc jessie_udp@_http._tcp.dns-test-service.dns-4731.svc jessie_tcp@_http._tcp.dns-test-service.dns-4731.svc]

Nov 15 16:22:38.057: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:38.088: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:38.114: INFO: Unable to read wheezy_udp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:38.140: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:38.164: INFO: Unable to read wheezy_udp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:38.198: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:38.221: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:38.248: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:38.385: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:38.411: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:38.441: INFO: Unable to read jessie_udp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:38.467: INFO: Unable to read jessie_tcp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:38.499: INFO: Unable to read jessie_udp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:38.525: INFO: Unable to read jessie_tcp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:38.551: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:38.577: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:38.679: INFO: Lookups using dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4731 wheezy_tcp@dns-test-service.dns-4731 wheezy_udp@dns-test-service.dns-4731.svc wheezy_tcp@dns-test-service.dns-4731.svc wheezy_udp@_http._tcp.dns-test-service.dns-4731.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4731.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4731 jessie_tcp@dns-test-service.dns-4731 jessie_udp@dns-test-service.dns-4731.svc jessie_tcp@dns-test-service.dns-4731.svc jessie_udp@_http._tcp.dns-test-service.dns-4731.svc jessie_tcp@_http._tcp.dns-test-service.dns-4731.svc]

Nov 15 16:22:43.049: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:43.074: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:43.101: INFO: Unable to read wheezy_udp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:43.126: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:43.157: INFO: Unable to read wheezy_udp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:43.193: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:43.218: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:43.244: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:43.375: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:43.402: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:43.437: INFO: Unable to read jessie_udp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:43.462: INFO: Unable to read jessie_tcp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:43.487: INFO: Unable to read jessie_udp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:43.513: INFO: Unable to read jessie_tcp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:43.539: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:43.566: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:43.671: INFO: Lookups using dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4731 wheezy_tcp@dns-test-service.dns-4731 wheezy_udp@dns-test-service.dns-4731.svc wheezy_tcp@dns-test-service.dns-4731.svc wheezy_udp@_http._tcp.dns-test-service.dns-4731.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4731.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4731 jessie_tcp@dns-test-service.dns-4731 jessie_udp@dns-test-service.dns-4731.svc jessie_tcp@dns-test-service.dns-4731.svc jessie_udp@_http._tcp.dns-test-service.dns-4731.svc jessie_tcp@_http._tcp.dns-test-service.dns-4731.svc]

Nov 15 16:22:48.054: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:48.078: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:48.109: INFO: Unable to read wheezy_udp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:48.134: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:48.159: INFO: Unable to read wheezy_udp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:48.184: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:48.210: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:48.236: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:48.369: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:48.395: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:48.421: INFO: Unable to read jessie_udp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:48.445: INFO: Unable to read jessie_tcp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:48.470: INFO: Unable to read jessie_udp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:48.496: INFO: Unable to read jessie_tcp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:48.521: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:48.551: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
Nov 15 16:22:48.661: INFO: Lookups using dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4731 wheezy_tcp@dns-test-service.dns-4731 wheezy_udp@dns-test-service.dns-4731.svc wheezy_tcp@dns-test-service.dns-4731.svc wheezy_udp@_http._tcp.dns-test-service.dns-4731.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4731.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4731 jessie_tcp@dns-test-service.dns-4731 jessie_udp@dns-test-service.dns-4731.svc jessie_tcp@dns-test-service.dns-4731.svc jessie_udp@_http._tcp.dns-test-service.dns-4731.svc jessie_tcp@_http._tcp.dns-test-service.dns-4731.svc]

Nov 15 16:22:53.669: INFO: DNS probes using dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b succeeded

STEP: deleting the pod 11/15/23 16:22:53.669
STEP: deleting the test service 11/15/23 16:22:53.712
STEP: deleting the test headless service 11/15/23 16:22:53.798
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Nov 15 16:22:53.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4731" for this suite. 11/15/23 16:22:53.879
------------------------------
• [SLOW TEST] [35.894 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:22:18.025
    Nov 15 16:22:18.026: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename dns 11/15/23 16:22:18.028
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:22:18.09
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:22:18.113
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 11/15/23 16:22:18.136
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4731 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4731;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4731 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4731;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4731.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4731.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4731.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4731.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4731.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4731.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4731.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4731.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4731.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4731.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4731.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4731.svc;check="$$(dig +notcp +noall +answer +search 245.91.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.91.245_udp@PTR;check="$$(dig +tcp +noall +answer +search 245.91.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.91.245_tcp@PTR;sleep 1; done
     11/15/23 16:22:18.217
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4731 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4731;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4731 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4731;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4731.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4731.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4731.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4731.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4731.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4731.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4731.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4731.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4731.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4731.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4731.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4731.svc;check="$$(dig +notcp +noall +answer +search 245.91.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.91.245_udp@PTR;check="$$(dig +tcp +noall +answer +search 245.91.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.91.245_tcp@PTR;sleep 1; done
     11/15/23 16:22:18.217
    STEP: creating a pod to probe DNS 11/15/23 16:22:18.218
    STEP: submitting the pod to kubernetes 11/15/23 16:22:18.218
    Nov 15 16:22:18.256: INFO: Waiting up to 15m0s for pod "dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b" in namespace "dns-4731" to be "running"
    Nov 15 16:22:18.276: INFO: Pod "dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b": Phase="Pending", Reason="", readiness=false. Elapsed: 19.283786ms
    Nov 15 16:22:20.297: INFO: Pod "dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040882291s
    Nov 15 16:22:22.296: INFO: Pod "dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b": Phase="Running", Reason="", readiness=true. Elapsed: 4.039714321s
    Nov 15 16:22:22.296: INFO: Pod "dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b" satisfied condition "running"
    STEP: retrieving the pod 11/15/23 16:22:22.296
    STEP: looking for the results for each expected name from probers 11/15/23 16:22:22.315
    Nov 15 16:22:22.397: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:22.430: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:22.457: INFO: Unable to read wheezy_udp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:22.485: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:22.512: INFO: Unable to read wheezy_udp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:22.539: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:22.566: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:22.594: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:22.730: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:22.757: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:22.786: INFO: Unable to read jessie_udp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:22.812: INFO: Unable to read jessie_tcp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:22.838: INFO: Unable to read jessie_udp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:22.863: INFO: Unable to read jessie_tcp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:22.888: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:22.912: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:23.020: INFO: Lookups using dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4731 wheezy_tcp@dns-test-service.dns-4731 wheezy_udp@dns-test-service.dns-4731.svc wheezy_tcp@dns-test-service.dns-4731.svc wheezy_udp@_http._tcp.dns-test-service.dns-4731.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4731.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4731 jessie_tcp@dns-test-service.dns-4731 jessie_udp@dns-test-service.dns-4731.svc jessie_tcp@dns-test-service.dns-4731.svc jessie_udp@_http._tcp.dns-test-service.dns-4731.svc jessie_tcp@_http._tcp.dns-test-service.dns-4731.svc]

    Nov 15 16:22:28.053: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:28.083: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:28.108: INFO: Unable to read wheezy_udp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:28.134: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:28.167: INFO: Unable to read wheezy_udp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:28.194: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:28.222: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:28.248: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:28.377: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:28.404: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:28.430: INFO: Unable to read jessie_udp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:28.454: INFO: Unable to read jessie_tcp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:28.483: INFO: Unable to read jessie_udp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:28.510: INFO: Unable to read jessie_tcp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:28.534: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:28.560: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:28.664: INFO: Lookups using dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4731 wheezy_tcp@dns-test-service.dns-4731 wheezy_udp@dns-test-service.dns-4731.svc wheezy_tcp@dns-test-service.dns-4731.svc wheezy_udp@_http._tcp.dns-test-service.dns-4731.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4731.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4731 jessie_tcp@dns-test-service.dns-4731 jessie_udp@dns-test-service.dns-4731.svc jessie_tcp@dns-test-service.dns-4731.svc jessie_udp@_http._tcp.dns-test-service.dns-4731.svc jessie_tcp@_http._tcp.dns-test-service.dns-4731.svc]

    Nov 15 16:22:33.050: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:33.078: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:33.103: INFO: Unable to read wheezy_udp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:33.129: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:33.154: INFO: Unable to read wheezy_udp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:33.181: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:33.207: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:33.232: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:33.374: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:33.405: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:33.437: INFO: Unable to read jessie_udp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:33.461: INFO: Unable to read jessie_tcp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:33.487: INFO: Unable to read jessie_udp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:33.513: INFO: Unable to read jessie_tcp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:33.538: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:33.564: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:33.667: INFO: Lookups using dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4731 wheezy_tcp@dns-test-service.dns-4731 wheezy_udp@dns-test-service.dns-4731.svc wheezy_tcp@dns-test-service.dns-4731.svc wheezy_udp@_http._tcp.dns-test-service.dns-4731.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4731.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4731 jessie_tcp@dns-test-service.dns-4731 jessie_udp@dns-test-service.dns-4731.svc jessie_tcp@dns-test-service.dns-4731.svc jessie_udp@_http._tcp.dns-test-service.dns-4731.svc jessie_tcp@_http._tcp.dns-test-service.dns-4731.svc]

    Nov 15 16:22:38.057: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:38.088: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:38.114: INFO: Unable to read wheezy_udp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:38.140: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:38.164: INFO: Unable to read wheezy_udp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:38.198: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:38.221: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:38.248: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:38.385: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:38.411: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:38.441: INFO: Unable to read jessie_udp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:38.467: INFO: Unable to read jessie_tcp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:38.499: INFO: Unable to read jessie_udp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:38.525: INFO: Unable to read jessie_tcp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:38.551: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:38.577: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:38.679: INFO: Lookups using dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4731 wheezy_tcp@dns-test-service.dns-4731 wheezy_udp@dns-test-service.dns-4731.svc wheezy_tcp@dns-test-service.dns-4731.svc wheezy_udp@_http._tcp.dns-test-service.dns-4731.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4731.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4731 jessie_tcp@dns-test-service.dns-4731 jessie_udp@dns-test-service.dns-4731.svc jessie_tcp@dns-test-service.dns-4731.svc jessie_udp@_http._tcp.dns-test-service.dns-4731.svc jessie_tcp@_http._tcp.dns-test-service.dns-4731.svc]

    Nov 15 16:22:43.049: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:43.074: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:43.101: INFO: Unable to read wheezy_udp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:43.126: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:43.157: INFO: Unable to read wheezy_udp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:43.193: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:43.218: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:43.244: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:43.375: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:43.402: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:43.437: INFO: Unable to read jessie_udp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:43.462: INFO: Unable to read jessie_tcp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:43.487: INFO: Unable to read jessie_udp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:43.513: INFO: Unable to read jessie_tcp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:43.539: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:43.566: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:43.671: INFO: Lookups using dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4731 wheezy_tcp@dns-test-service.dns-4731 wheezy_udp@dns-test-service.dns-4731.svc wheezy_tcp@dns-test-service.dns-4731.svc wheezy_udp@_http._tcp.dns-test-service.dns-4731.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4731.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4731 jessie_tcp@dns-test-service.dns-4731 jessie_udp@dns-test-service.dns-4731.svc jessie_tcp@dns-test-service.dns-4731.svc jessie_udp@_http._tcp.dns-test-service.dns-4731.svc jessie_tcp@_http._tcp.dns-test-service.dns-4731.svc]

    Nov 15 16:22:48.054: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:48.078: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:48.109: INFO: Unable to read wheezy_udp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:48.134: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:48.159: INFO: Unable to read wheezy_udp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:48.184: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:48.210: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:48.236: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:48.369: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:48.395: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:48.421: INFO: Unable to read jessie_udp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:48.445: INFO: Unable to read jessie_tcp@dns-test-service.dns-4731 from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:48.470: INFO: Unable to read jessie_udp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:48.496: INFO: Unable to read jessie_tcp@dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:48.521: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:48.551: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4731.svc from pod dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b: the server could not find the requested resource (get pods dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b)
    Nov 15 16:22:48.661: INFO: Lookups using dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4731 wheezy_tcp@dns-test-service.dns-4731 wheezy_udp@dns-test-service.dns-4731.svc wheezy_tcp@dns-test-service.dns-4731.svc wheezy_udp@_http._tcp.dns-test-service.dns-4731.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4731.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4731 jessie_tcp@dns-test-service.dns-4731 jessie_udp@dns-test-service.dns-4731.svc jessie_tcp@dns-test-service.dns-4731.svc jessie_udp@_http._tcp.dns-test-service.dns-4731.svc jessie_tcp@_http._tcp.dns-test-service.dns-4731.svc]

    Nov 15 16:22:53.669: INFO: DNS probes using dns-4731/dns-test-5a6dc4e5-89f4-4bbc-8177-20628e903b5b succeeded

    STEP: deleting the pod 11/15/23 16:22:53.669
    STEP: deleting the test service 11/15/23 16:22:53.712
    STEP: deleting the test headless service 11/15/23 16:22:53.798
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:22:53.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4731" for this suite. 11/15/23 16:22:53.879
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:22:53.929
Nov 15 16:22:53.929: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename emptydir 11/15/23 16:22:53.93
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:22:53.997
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:22:54.015
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 11/15/23 16:22:54.036
Nov 15 16:22:54.073: INFO: Waiting up to 5m0s for pod "pod-c25ba65b-9933-4301-a3cb-044f6e0f5302" in namespace "emptydir-1940" to be "Succeeded or Failed"
Nov 15 16:22:54.097: INFO: Pod "pod-c25ba65b-9933-4301-a3cb-044f6e0f5302": Phase="Pending", Reason="", readiness=false. Elapsed: 23.835742ms
Nov 15 16:22:56.117: INFO: Pod "pod-c25ba65b-9933-4301-a3cb-044f6e0f5302": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043922891s
Nov 15 16:22:58.122: INFO: Pod "pod-c25ba65b-9933-4301-a3cb-044f6e0f5302": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048085527s
Nov 15 16:23:00.118: INFO: Pod "pod-c25ba65b-9933-4301-a3cb-044f6e0f5302": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044696554s
STEP: Saw pod success 11/15/23 16:23:00.118
Nov 15 16:23:00.119: INFO: Pod "pod-c25ba65b-9933-4301-a3cb-044f6e0f5302" satisfied condition "Succeeded or Failed"
Nov 15 16:23:00.137: INFO: Trying to get logs from node 10.15.40.115 pod pod-c25ba65b-9933-4301-a3cb-044f6e0f5302 container test-container: <nil>
STEP: delete the pod 11/15/23 16:23:00.271
Nov 15 16:23:00.321: INFO: Waiting for pod pod-c25ba65b-9933-4301-a3cb-044f6e0f5302 to disappear
Nov 15 16:23:00.340: INFO: Pod pod-c25ba65b-9933-4301-a3cb-044f6e0f5302 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 15 16:23:00.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1940" for this suite. 11/15/23 16:23:00.368
------------------------------
• [SLOW TEST] [6.468 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:22:53.929
    Nov 15 16:22:53.929: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename emptydir 11/15/23 16:22:53.93
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:22:53.997
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:22:54.015
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 11/15/23 16:22:54.036
    Nov 15 16:22:54.073: INFO: Waiting up to 5m0s for pod "pod-c25ba65b-9933-4301-a3cb-044f6e0f5302" in namespace "emptydir-1940" to be "Succeeded or Failed"
    Nov 15 16:22:54.097: INFO: Pod "pod-c25ba65b-9933-4301-a3cb-044f6e0f5302": Phase="Pending", Reason="", readiness=false. Elapsed: 23.835742ms
    Nov 15 16:22:56.117: INFO: Pod "pod-c25ba65b-9933-4301-a3cb-044f6e0f5302": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043922891s
    Nov 15 16:22:58.122: INFO: Pod "pod-c25ba65b-9933-4301-a3cb-044f6e0f5302": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048085527s
    Nov 15 16:23:00.118: INFO: Pod "pod-c25ba65b-9933-4301-a3cb-044f6e0f5302": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044696554s
    STEP: Saw pod success 11/15/23 16:23:00.118
    Nov 15 16:23:00.119: INFO: Pod "pod-c25ba65b-9933-4301-a3cb-044f6e0f5302" satisfied condition "Succeeded or Failed"
    Nov 15 16:23:00.137: INFO: Trying to get logs from node 10.15.40.115 pod pod-c25ba65b-9933-4301-a3cb-044f6e0f5302 container test-container: <nil>
    STEP: delete the pod 11/15/23 16:23:00.271
    Nov 15 16:23:00.321: INFO: Waiting for pod pod-c25ba65b-9933-4301-a3cb-044f6e0f5302 to disappear
    Nov 15 16:23:00.340: INFO: Pod pod-c25ba65b-9933-4301-a3cb-044f6e0f5302 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:23:00.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1940" for this suite. 11/15/23 16:23:00.368
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:23:00.399
Nov 15 16:23:00.399: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename job 11/15/23 16:23:00.401
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:23:00.465
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:23:00.484
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 11/15/23 16:23:00.504
STEP: Ensuring job reaches completions 11/15/23 16:23:00.539
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Nov 15 16:23:12.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-3618" for this suite. 11/15/23 16:23:12.588
------------------------------
• [SLOW TEST] [12.217 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:23:00.399
    Nov 15 16:23:00.399: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename job 11/15/23 16:23:00.401
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:23:00.465
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:23:00.484
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 11/15/23 16:23:00.504
    STEP: Ensuring job reaches completions 11/15/23 16:23:00.539
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:23:12.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-3618" for this suite. 11/15/23 16:23:12.588
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:23:12.619
Nov 15 16:23:12.619: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename sched-pred 11/15/23 16:23:12.622
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:23:12.713
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:23:12.733
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Nov 15 16:23:12.753: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 15 16:23:12.805: INFO: Waiting for terminating namespaces to be deleted...
Nov 15 16:23:12.826: INFO: 
Logging pods the apiserver thinks is on node 10.15.40.106 before test
Nov 15 16:23:12.869: INFO: ibm-cloud-provider-ip-163-109-74-98-5d59656977-bgllw from ibm-system started at 2023-11-15 15:39:54 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.869: INFO: 	Container ibm-cloud-provider-ip-163-109-74-98 ready: true, restart count 0
Nov 15 16:23:12.869: INFO: calico-kube-controllers-7847f7647d-srqd4 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.870: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Nov 15 16:23:12.870: INFO: calico-node-67csc from kube-system started at 2023-11-15 13:13:21 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.870: INFO: 	Container calico-node ready: true, restart count 0
Nov 15 16:23:12.870: INFO: calico-typha-5b5db87f55-nwmh4 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.870: INFO: 	Container calico-typha ready: true, restart count 0
Nov 15 16:23:12.870: INFO: coredns-5845f98d4-2lnrf from kube-system started at 2023-11-15 13:23:15 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.870: INFO: 	Container coredns ready: true, restart count 0
Nov 15 16:23:12.870: INFO: coredns-autoscaler-85f4bdddf6-qwxmw from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.871: INFO: 	Container autoscaler ready: true, restart count 0
Nov 15 16:23:12.871: INFO: dashboard-metrics-scraper-7cf679fbdf-frv7t from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.871: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Nov 15 16:23:12.871: INFO: ibm-file-plugin-557f875d5f-jrfnv from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.871: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Nov 15 16:23:12.871: INFO: ibm-keepalived-watcher-dqknf from kube-system started at 2023-11-15 13:13:21 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.871: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 15 16:23:12.872: INFO: ibm-master-proxy-static-10.15.40.106 from kube-system started at 2023-11-15 13:13:08 +0000 UTC (2 container statuses recorded)
Nov 15 16:23:12.872: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 15 16:23:12.872: INFO: 	Container pause ready: true, restart count 0
Nov 15 16:23:12.872: INFO: ibm-storage-watcher-7d897847f4-k64t5 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.872: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Nov 15 16:23:12.873: INFO: ibmcloud-block-storage-driver-f97nm from kube-system started at 2023-11-15 13:13:30 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.873: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 15 16:23:12.873: INFO: ibmcloud-block-storage-plugin-5bd59f7b48-2dxcj from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.873: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Nov 15 16:23:12.873: INFO: ingress-cluster-healthcheck-5985f966bb-jgjx8 from kube-system started at 2023-11-15 15:39:54 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.874: INFO: 	Container ingress-cluster-healthcheck ready: true, restart count 0
Nov 15 16:23:12.874: INFO: konnectivity-agent-5brpz from kube-system started at 2023-11-15 13:22:38 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.874: INFO: 	Container konnectivity-agent ready: true, restart count 0
Nov 15 16:23:12.874: INFO: kubernetes-dashboard-5ccdc9cbb8-wbmfz from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.875: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Nov 15 16:23:12.875: INFO: metrics-server-7cbd9c9b48-8tr74 from kube-system started at 2023-11-15 15:39:54 +0000 UTC (3 container statuses recorded)
Nov 15 16:23:12.875: INFO: 	Container config-watcher ready: true, restart count 0
Nov 15 16:23:12.875: INFO: 	Container metrics-server ready: true, restart count 0
Nov 15 16:23:12.875: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Nov 15 16:23:12.875: INFO: public-crclac150z0u38f277tnpg-alb1-6df9445bb6-qw2bt from kube-system started at 2023-11-15 15:39:54 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.875: INFO: 	Container nginx-ingress ready: true, restart count 0
Nov 15 16:23:12.875: INFO: snapshot-controller-6db47fc545-5nnph from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.875: INFO: 	Container snapshot-controller ready: true, restart count 0
Nov 15 16:23:12.875: INFO: snapshot-controller-6db47fc545-h4q49 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.876: INFO: 	Container snapshot-controller ready: true, restart count 0
Nov 15 16:23:12.876: INFO: snapshot-controller-6db47fc545-khkb2 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.876: INFO: 	Container snapshot-controller ready: true, restart count 0
Nov 15 16:23:12.876: INFO: sonobuoy-systemd-logs-daemon-set-eabb9aa298c743f7-zxdnv from sonobuoy started at 2023-11-15 15:24:45 +0000 UTC (2 container statuses recorded)
Nov 15 16:23:12.877: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 15 16:23:12.877: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 15 16:23:12.877: INFO: 
Logging pods the apiserver thinks is on node 10.15.40.114 before test
Nov 15 16:23:12.922: INFO: ibm-cloud-provider-ip-163-109-74-98-5d59656977-wxlx6 from ibm-system started at 2023-11-15 13:18:32 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.922: INFO: 	Container ibm-cloud-provider-ip-163-109-74-98 ready: true, restart count 0
Nov 15 16:23:12.922: INFO: calico-node-wqvj8 from kube-system started at 2023-11-15 13:13:28 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.922: INFO: 	Container calico-node ready: true, restart count 0
Nov 15 16:23:12.922: INFO: calico-typha-5b5db87f55-4qv4c from kube-system started at 2023-11-15 13:13:53 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.922: INFO: 	Container calico-typha ready: true, restart count 0
Nov 15 16:23:12.922: INFO: coredns-5845f98d4-6cld8 from kube-system started at 2023-11-15 13:23:15 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.922: INFO: 	Container coredns ready: true, restart count 0
Nov 15 16:23:12.922: INFO: ibm-keepalived-watcher-xsslz from kube-system started at 2023-11-15 13:13:28 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.922: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 15 16:23:12.922: INFO: ibm-master-proxy-static-10.15.40.114 from kube-system started at 2023-11-15 13:13:15 +0000 UTC (2 container statuses recorded)
Nov 15 16:23:12.922: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 15 16:23:12.923: INFO: 	Container pause ready: true, restart count 0
Nov 15 16:23:12.923: INFO: ibmcloud-block-storage-driver-4txx8 from kube-system started at 2023-11-15 13:13:37 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.923: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 15 16:23:12.923: INFO: konnectivity-agent-zr4zl from kube-system started at 2023-11-15 13:22:41 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.923: INFO: 	Container konnectivity-agent ready: true, restart count 0
Nov 15 16:23:12.923: INFO: metrics-server-7cbd9c9b48-cnskr from kube-system started at 2023-11-15 13:56:24 +0000 UTC (3 container statuses recorded)
Nov 15 16:23:12.924: INFO: 	Container config-watcher ready: true, restart count 0
Nov 15 16:23:12.924: INFO: 	Container metrics-server ready: true, restart count 0
Nov 15 16:23:12.924: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Nov 15 16:23:12.924: INFO: public-crclac150z0u38f277tnpg-alb1-6df9445bb6-jkpcd from kube-system started at 2023-11-15 13:23:30 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.924: INFO: 	Container nginx-ingress ready: true, restart count 0
Nov 15 16:23:12.924: INFO: sonobuoy-e2e-job-3a51e32fa56b4156 from sonobuoy started at 2023-11-15 15:24:45 +0000 UTC (2 container statuses recorded)
Nov 15 16:23:12.924: INFO: 	Container e2e ready: true, restart count 0
Nov 15 16:23:12.924: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 15 16:23:12.924: INFO: sonobuoy-systemd-logs-daemon-set-eabb9aa298c743f7-sxg5b from sonobuoy started at 2023-11-15 15:24:45 +0000 UTC (2 container statuses recorded)
Nov 15 16:23:12.925: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 15 16:23:12.925: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 15 16:23:12.925: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-11-15 13:15:59 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.925: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Nov 15 16:23:12.925: INFO: 
Logging pods the apiserver thinks is on node 10.15.40.115 before test
Nov 15 16:23:12.970: INFO: fail-once-local-2bp58 from job-3618 started at 2023-11-15 16:23:00 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.971: INFO: 	Container c ready: false, restart count 1
Nov 15 16:23:12.971: INFO: fail-once-local-bh76n from job-3618 started at 2023-11-15 16:23:00 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.971: INFO: 	Container c ready: false, restart count 1
Nov 15 16:23:12.971: INFO: fail-once-local-qrmbh from job-3618 started at 2023-11-15 16:23:06 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.971: INFO: 	Container c ready: false, restart count 1
Nov 15 16:23:12.971: INFO: fail-once-local-tl6jn from job-3618 started at 2023-11-15 16:23:06 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.971: INFO: 	Container c ready: false, restart count 1
Nov 15 16:23:12.971: INFO: calico-node-gknnt from kube-system started at 2023-11-15 13:13:40 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.971: INFO: 	Container calico-node ready: true, restart count 0
Nov 15 16:23:12.971: INFO: calico-typha-5b5db87f55-fchsg from kube-system started at 2023-11-15 15:40:02 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.971: INFO: 	Container calico-typha ready: true, restart count 0
Nov 15 16:23:12.971: INFO: coredns-5845f98d4-rt4sq from kube-system started at 2023-11-15 15:39:54 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.971: INFO: 	Container coredns ready: true, restart count 0
Nov 15 16:23:12.971: INFO: ibm-keepalived-watcher-75lpq from kube-system started at 2023-11-15 13:13:40 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.971: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 15 16:23:12.971: INFO: ibm-master-proxy-static-10.15.40.115 from kube-system started at 2023-11-15 13:13:28 +0000 UTC (2 container statuses recorded)
Nov 15 16:23:12.971: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 15 16:23:12.971: INFO: 	Container pause ready: true, restart count 0
Nov 15 16:23:12.971: INFO: ibmcloud-block-storage-driver-gbqth from kube-system started at 2023-11-15 13:13:49 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.972: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 15 16:23:12.972: INFO: konnectivity-agent-9pkld from kube-system started at 2023-11-15 13:22:45 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.972: INFO: 	Container konnectivity-agent ready: true, restart count 0
Nov 15 16:23:12.972: INFO: sonobuoy from sonobuoy started at 2023-11-15 15:24:39 +0000 UTC (1 container statuses recorded)
Nov 15 16:23:12.972: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 15 16:23:12.972: INFO: sonobuoy-systemd-logs-daemon-set-eabb9aa298c743f7-hmfn7 from sonobuoy started at 2023-11-15 15:24:45 +0000 UTC (2 container statuses recorded)
Nov 15 16:23:12.972: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 15 16:23:12.972: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 11/15/23 16:23:12.973
Nov 15 16:23:13.008: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6770" to be "running"
Nov 15 16:23:13.025: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 17.299493ms
Nov 15 16:23:15.044: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035935968s
Nov 15 16:23:17.044: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.035751559s
Nov 15 16:23:17.044: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 11/15/23 16:23:17.063
STEP: Trying to apply a random label on the found node. 11/15/23 16:23:17.127
STEP: verifying the node has the label kubernetes.io/e2e-f39942fa-02ef-49c1-bf32-a2712e6f2658 95 11/15/23 16:23:17.164
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 11/15/23 16:23:17.184
Nov 15 16:23:17.204: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-6770" to be "not pending"
Nov 15 16:23:17.226: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 22.378115ms
Nov 15 16:23:19.247: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043065471s
Nov 15 16:23:21.246: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 4.042271824s
Nov 15 16:23:21.246: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.15.40.115 on the node which pod4 resides and expect not scheduled 11/15/23 16:23:21.246
Nov 15 16:23:21.277: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-6770" to be "not pending"
Nov 15 16:23:21.299: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.300899ms
Nov 15 16:23:23.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041159276s
Nov 15 16:23:25.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042333396s
Nov 15 16:23:27.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.040644648s
Nov 15 16:23:29.322: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.044569862s
Nov 15 16:23:31.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.041636783s
Nov 15 16:23:33.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.041930772s
Nov 15 16:23:35.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.041612427s
Nov 15 16:23:37.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.042380026s
Nov 15 16:23:39.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.041195904s
Nov 15 16:23:41.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.040602283s
Nov 15 16:23:43.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.042152863s
Nov 15 16:23:45.321: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.043578581s
Nov 15 16:23:47.317: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.039942788s
Nov 15 16:23:49.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.041405718s
Nov 15 16:23:51.323: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.046143654s
Nov 15 16:23:53.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.042518185s
Nov 15 16:23:55.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.042978859s
Nov 15 16:23:57.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.042137439s
Nov 15 16:23:59.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.042228095s
Nov 15 16:24:01.370: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.092777009s
Nov 15 16:24:03.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.042624119s
Nov 15 16:24:05.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.041636723s
Nov 15 16:24:07.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.042411441s
Nov 15 16:24:09.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.042672464s
Nov 15 16:24:11.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.042633055s
Nov 15 16:24:13.328: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.051364605s
Nov 15 16:24:15.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.042202019s
Nov 15 16:24:17.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.042568706s
Nov 15 16:24:19.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.041711431s
Nov 15 16:24:21.317: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.04021724s
Nov 15 16:24:23.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.04279312s
Nov 15 16:24:25.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.042088883s
Nov 15 16:24:27.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.04142074s
Nov 15 16:24:29.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.042745059s
Nov 15 16:24:31.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.041271429s
Nov 15 16:24:33.321: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.044154027s
Nov 15 16:24:35.323: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.045409819s
Nov 15 16:24:37.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.042991596s
Nov 15 16:24:39.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.041498124s
Nov 15 16:24:41.321: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.043415647s
Nov 15 16:24:43.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.04109855s
Nov 15 16:24:45.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.042921482s
Nov 15 16:24:47.324: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.046503946s
Nov 15 16:24:49.317: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.039653277s
Nov 15 16:24:51.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.040810585s
Nov 15 16:24:53.317: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.040341515s
Nov 15 16:24:55.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.042362886s
Nov 15 16:24:57.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.042850913s
Nov 15 16:24:59.322: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.045006996s
Nov 15 16:25:01.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.040660762s
Nov 15 16:25:03.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.040472263s
Nov 15 16:25:05.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.041720399s
Nov 15 16:25:07.323: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.045679945s
Nov 15 16:25:09.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.042052938s
Nov 15 16:25:11.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.040542796s
Nov 15 16:25:13.317: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.040332148s
Nov 15 16:25:15.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.041612317s
Nov 15 16:25:17.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.042823586s
Nov 15 16:25:19.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.042400425s
Nov 15 16:25:21.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.041684584s
Nov 15 16:25:23.322: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.044640126s
Nov 15 16:25:25.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.042373877s
Nov 15 16:25:27.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.040409677s
Nov 15 16:25:29.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.04067078s
Nov 15 16:25:31.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.040631175s
Nov 15 16:25:33.322: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.045010151s
Nov 15 16:25:35.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.041622431s
Nov 15 16:25:37.322: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.044688198s
Nov 15 16:25:39.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.042767031s
Nov 15 16:25:41.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.041348671s
Nov 15 16:25:43.331: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.054178849s
Nov 15 16:25:45.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.042975607s
Nov 15 16:25:47.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.04089605s
Nov 15 16:25:49.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.040954796s
Nov 15 16:25:51.317: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.039718955s
Nov 15 16:25:53.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.042216737s
Nov 15 16:25:55.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.04228193s
Nov 15 16:25:57.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.040768821s
Nov 15 16:25:59.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.0429459s
Nov 15 16:26:01.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.041134239s
Nov 15 16:26:03.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.040426621s
Nov 15 16:26:05.323: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.046049017s
Nov 15 16:26:07.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.041792328s
Nov 15 16:26:09.321: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.043440871s
Nov 15 16:26:11.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.041674349s
Nov 15 16:26:13.321: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.043514307s
Nov 15 16:26:15.321: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.043996789s
Nov 15 16:26:17.323: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.045806821s
Nov 15 16:26:19.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.042736308s
Nov 15 16:26:21.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.042834675s
Nov 15 16:26:23.322: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.045089217s
Nov 15 16:26:25.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.041404474s
Nov 15 16:26:27.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.041674483s
Nov 15 16:26:29.322: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.045309481s
Nov 15 16:26:31.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.040680381s
Nov 15 16:26:33.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.042072055s
Nov 15 16:26:35.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.041590122s
Nov 15 16:26:37.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.040653429s
Nov 15 16:26:39.321: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.04389362s
Nov 15 16:26:41.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.04249166s
Nov 15 16:26:43.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.040901457s
Nov 15 16:26:45.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.042296969s
Nov 15 16:26:47.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.042020109s
Nov 15 16:26:49.322: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.044796302s
Nov 15 16:26:51.324: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.046975727s
Nov 15 16:26:53.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.041863396s
Nov 15 16:26:55.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.040985506s
Nov 15 16:26:57.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.043202075s
Nov 15 16:26:59.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.041970811s
Nov 15 16:27:01.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.042458092s
Nov 15 16:27:03.324: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.046483471s
Nov 15 16:27:05.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.041132466s
Nov 15 16:27:07.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.042973867s
Nov 15 16:27:09.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.042212635s
Nov 15 16:27:11.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.041162028s
Nov 15 16:27:13.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.042216163s
Nov 15 16:27:15.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.042006454s
Nov 15 16:27:17.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.041434211s
Nov 15 16:27:19.317: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.040388425s
Nov 15 16:27:21.317: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.04022921s
Nov 15 16:27:23.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.041652382s
Nov 15 16:27:25.324: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.046526065s
Nov 15 16:27:27.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.042426138s
Nov 15 16:27:29.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.042972468s
Nov 15 16:27:31.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.04194309s
Nov 15 16:27:33.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.041158444s
Nov 15 16:27:35.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.04187284s
Nov 15 16:27:37.324: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.046827416s
Nov 15 16:27:39.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.042662087s
Nov 15 16:27:41.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.042838857s
Nov 15 16:27:43.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.041958706s
Nov 15 16:27:45.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.041106072s
Nov 15 16:27:47.325: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.047940408s
Nov 15 16:27:49.322: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.044835109s
Nov 15 16:27:51.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.041674434s
Nov 15 16:27:53.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.041682391s
Nov 15 16:27:55.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.042701913s
Nov 15 16:27:57.317: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.040265859s
Nov 15 16:27:59.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.042689253s
Nov 15 16:28:01.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.042714102s
Nov 15 16:28:03.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.041389231s
Nov 15 16:28:05.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.042702433s
Nov 15 16:28:07.345: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.067905198s
Nov 15 16:28:09.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.04166432s
Nov 15 16:28:11.322: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.045240395s
Nov 15 16:28:13.317: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.04000601s
Nov 15 16:28:15.317: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.040052989s
Nov 15 16:28:17.327: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.049453221s
Nov 15 16:28:19.317: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.040392999s
Nov 15 16:28:21.317: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.040364129s
Nov 15 16:28:21.338: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.060755524s
STEP: removing the label kubernetes.io/e2e-f39942fa-02ef-49c1-bf32-a2712e6f2658 off the node 10.15.40.115 11/15/23 16:28:21.338
STEP: verifying the node doesn't have the label kubernetes.io/e2e-f39942fa-02ef-49c1-bf32-a2712e6f2658 11/15/23 16:28:21.399
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 16:28:21.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-6770" for this suite. 11/15/23 16:28:21.442
------------------------------
• [SLOW TEST] [308.854 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:23:12.619
    Nov 15 16:23:12.619: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename sched-pred 11/15/23 16:23:12.622
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:23:12.713
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:23:12.733
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Nov 15 16:23:12.753: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Nov 15 16:23:12.805: INFO: Waiting for terminating namespaces to be deleted...
    Nov 15 16:23:12.826: INFO: 
    Logging pods the apiserver thinks is on node 10.15.40.106 before test
    Nov 15 16:23:12.869: INFO: ibm-cloud-provider-ip-163-109-74-98-5d59656977-bgllw from ibm-system started at 2023-11-15 15:39:54 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.869: INFO: 	Container ibm-cloud-provider-ip-163-109-74-98 ready: true, restart count 0
    Nov 15 16:23:12.869: INFO: calico-kube-controllers-7847f7647d-srqd4 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.870: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Nov 15 16:23:12.870: INFO: calico-node-67csc from kube-system started at 2023-11-15 13:13:21 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.870: INFO: 	Container calico-node ready: true, restart count 0
    Nov 15 16:23:12.870: INFO: calico-typha-5b5db87f55-nwmh4 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.870: INFO: 	Container calico-typha ready: true, restart count 0
    Nov 15 16:23:12.870: INFO: coredns-5845f98d4-2lnrf from kube-system started at 2023-11-15 13:23:15 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.870: INFO: 	Container coredns ready: true, restart count 0
    Nov 15 16:23:12.870: INFO: coredns-autoscaler-85f4bdddf6-qwxmw from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.871: INFO: 	Container autoscaler ready: true, restart count 0
    Nov 15 16:23:12.871: INFO: dashboard-metrics-scraper-7cf679fbdf-frv7t from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.871: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Nov 15 16:23:12.871: INFO: ibm-file-plugin-557f875d5f-jrfnv from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.871: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    Nov 15 16:23:12.871: INFO: ibm-keepalived-watcher-dqknf from kube-system started at 2023-11-15 13:13:21 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.871: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Nov 15 16:23:12.872: INFO: ibm-master-proxy-static-10.15.40.106 from kube-system started at 2023-11-15 13:13:08 +0000 UTC (2 container statuses recorded)
    Nov 15 16:23:12.872: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Nov 15 16:23:12.872: INFO: 	Container pause ready: true, restart count 0
    Nov 15 16:23:12.872: INFO: ibm-storage-watcher-7d897847f4-k64t5 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.872: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    Nov 15 16:23:12.873: INFO: ibmcloud-block-storage-driver-f97nm from kube-system started at 2023-11-15 13:13:30 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.873: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Nov 15 16:23:12.873: INFO: ibmcloud-block-storage-plugin-5bd59f7b48-2dxcj from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.873: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    Nov 15 16:23:12.873: INFO: ingress-cluster-healthcheck-5985f966bb-jgjx8 from kube-system started at 2023-11-15 15:39:54 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.874: INFO: 	Container ingress-cluster-healthcheck ready: true, restart count 0
    Nov 15 16:23:12.874: INFO: konnectivity-agent-5brpz from kube-system started at 2023-11-15 13:22:38 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.874: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Nov 15 16:23:12.874: INFO: kubernetes-dashboard-5ccdc9cbb8-wbmfz from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.875: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Nov 15 16:23:12.875: INFO: metrics-server-7cbd9c9b48-8tr74 from kube-system started at 2023-11-15 15:39:54 +0000 UTC (3 container statuses recorded)
    Nov 15 16:23:12.875: INFO: 	Container config-watcher ready: true, restart count 0
    Nov 15 16:23:12.875: INFO: 	Container metrics-server ready: true, restart count 0
    Nov 15 16:23:12.875: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Nov 15 16:23:12.875: INFO: public-crclac150z0u38f277tnpg-alb1-6df9445bb6-qw2bt from kube-system started at 2023-11-15 15:39:54 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.875: INFO: 	Container nginx-ingress ready: true, restart count 0
    Nov 15 16:23:12.875: INFO: snapshot-controller-6db47fc545-5nnph from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.875: INFO: 	Container snapshot-controller ready: true, restart count 0
    Nov 15 16:23:12.875: INFO: snapshot-controller-6db47fc545-h4q49 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.876: INFO: 	Container snapshot-controller ready: true, restart count 0
    Nov 15 16:23:12.876: INFO: snapshot-controller-6db47fc545-khkb2 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.876: INFO: 	Container snapshot-controller ready: true, restart count 0
    Nov 15 16:23:12.876: INFO: sonobuoy-systemd-logs-daemon-set-eabb9aa298c743f7-zxdnv from sonobuoy started at 2023-11-15 15:24:45 +0000 UTC (2 container statuses recorded)
    Nov 15 16:23:12.877: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 15 16:23:12.877: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 15 16:23:12.877: INFO: 
    Logging pods the apiserver thinks is on node 10.15.40.114 before test
    Nov 15 16:23:12.922: INFO: ibm-cloud-provider-ip-163-109-74-98-5d59656977-wxlx6 from ibm-system started at 2023-11-15 13:18:32 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.922: INFO: 	Container ibm-cloud-provider-ip-163-109-74-98 ready: true, restart count 0
    Nov 15 16:23:12.922: INFO: calico-node-wqvj8 from kube-system started at 2023-11-15 13:13:28 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.922: INFO: 	Container calico-node ready: true, restart count 0
    Nov 15 16:23:12.922: INFO: calico-typha-5b5db87f55-4qv4c from kube-system started at 2023-11-15 13:13:53 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.922: INFO: 	Container calico-typha ready: true, restart count 0
    Nov 15 16:23:12.922: INFO: coredns-5845f98d4-6cld8 from kube-system started at 2023-11-15 13:23:15 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.922: INFO: 	Container coredns ready: true, restart count 0
    Nov 15 16:23:12.922: INFO: ibm-keepalived-watcher-xsslz from kube-system started at 2023-11-15 13:13:28 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.922: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Nov 15 16:23:12.922: INFO: ibm-master-proxy-static-10.15.40.114 from kube-system started at 2023-11-15 13:13:15 +0000 UTC (2 container statuses recorded)
    Nov 15 16:23:12.922: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Nov 15 16:23:12.923: INFO: 	Container pause ready: true, restart count 0
    Nov 15 16:23:12.923: INFO: ibmcloud-block-storage-driver-4txx8 from kube-system started at 2023-11-15 13:13:37 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.923: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Nov 15 16:23:12.923: INFO: konnectivity-agent-zr4zl from kube-system started at 2023-11-15 13:22:41 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.923: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Nov 15 16:23:12.923: INFO: metrics-server-7cbd9c9b48-cnskr from kube-system started at 2023-11-15 13:56:24 +0000 UTC (3 container statuses recorded)
    Nov 15 16:23:12.924: INFO: 	Container config-watcher ready: true, restart count 0
    Nov 15 16:23:12.924: INFO: 	Container metrics-server ready: true, restart count 0
    Nov 15 16:23:12.924: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Nov 15 16:23:12.924: INFO: public-crclac150z0u38f277tnpg-alb1-6df9445bb6-jkpcd from kube-system started at 2023-11-15 13:23:30 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.924: INFO: 	Container nginx-ingress ready: true, restart count 0
    Nov 15 16:23:12.924: INFO: sonobuoy-e2e-job-3a51e32fa56b4156 from sonobuoy started at 2023-11-15 15:24:45 +0000 UTC (2 container statuses recorded)
    Nov 15 16:23:12.924: INFO: 	Container e2e ready: true, restart count 0
    Nov 15 16:23:12.924: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 15 16:23:12.924: INFO: sonobuoy-systemd-logs-daemon-set-eabb9aa298c743f7-sxg5b from sonobuoy started at 2023-11-15 15:24:45 +0000 UTC (2 container statuses recorded)
    Nov 15 16:23:12.925: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 15 16:23:12.925: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 15 16:23:12.925: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-11-15 13:15:59 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.925: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
    Nov 15 16:23:12.925: INFO: 
    Logging pods the apiserver thinks is on node 10.15.40.115 before test
    Nov 15 16:23:12.970: INFO: fail-once-local-2bp58 from job-3618 started at 2023-11-15 16:23:00 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.971: INFO: 	Container c ready: false, restart count 1
    Nov 15 16:23:12.971: INFO: fail-once-local-bh76n from job-3618 started at 2023-11-15 16:23:00 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.971: INFO: 	Container c ready: false, restart count 1
    Nov 15 16:23:12.971: INFO: fail-once-local-qrmbh from job-3618 started at 2023-11-15 16:23:06 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.971: INFO: 	Container c ready: false, restart count 1
    Nov 15 16:23:12.971: INFO: fail-once-local-tl6jn from job-3618 started at 2023-11-15 16:23:06 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.971: INFO: 	Container c ready: false, restart count 1
    Nov 15 16:23:12.971: INFO: calico-node-gknnt from kube-system started at 2023-11-15 13:13:40 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.971: INFO: 	Container calico-node ready: true, restart count 0
    Nov 15 16:23:12.971: INFO: calico-typha-5b5db87f55-fchsg from kube-system started at 2023-11-15 15:40:02 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.971: INFO: 	Container calico-typha ready: true, restart count 0
    Nov 15 16:23:12.971: INFO: coredns-5845f98d4-rt4sq from kube-system started at 2023-11-15 15:39:54 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.971: INFO: 	Container coredns ready: true, restart count 0
    Nov 15 16:23:12.971: INFO: ibm-keepalived-watcher-75lpq from kube-system started at 2023-11-15 13:13:40 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.971: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Nov 15 16:23:12.971: INFO: ibm-master-proxy-static-10.15.40.115 from kube-system started at 2023-11-15 13:13:28 +0000 UTC (2 container statuses recorded)
    Nov 15 16:23:12.971: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Nov 15 16:23:12.971: INFO: 	Container pause ready: true, restart count 0
    Nov 15 16:23:12.971: INFO: ibmcloud-block-storage-driver-gbqth from kube-system started at 2023-11-15 13:13:49 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.972: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Nov 15 16:23:12.972: INFO: konnectivity-agent-9pkld from kube-system started at 2023-11-15 13:22:45 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.972: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Nov 15 16:23:12.972: INFO: sonobuoy from sonobuoy started at 2023-11-15 15:24:39 +0000 UTC (1 container statuses recorded)
    Nov 15 16:23:12.972: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Nov 15 16:23:12.972: INFO: sonobuoy-systemd-logs-daemon-set-eabb9aa298c743f7-hmfn7 from sonobuoy started at 2023-11-15 15:24:45 +0000 UTC (2 container statuses recorded)
    Nov 15 16:23:12.972: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 15 16:23:12.972: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 11/15/23 16:23:12.973
    Nov 15 16:23:13.008: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6770" to be "running"
    Nov 15 16:23:13.025: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 17.299493ms
    Nov 15 16:23:15.044: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035935968s
    Nov 15 16:23:17.044: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.035751559s
    Nov 15 16:23:17.044: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 11/15/23 16:23:17.063
    STEP: Trying to apply a random label on the found node. 11/15/23 16:23:17.127
    STEP: verifying the node has the label kubernetes.io/e2e-f39942fa-02ef-49c1-bf32-a2712e6f2658 95 11/15/23 16:23:17.164
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 11/15/23 16:23:17.184
    Nov 15 16:23:17.204: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-6770" to be "not pending"
    Nov 15 16:23:17.226: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 22.378115ms
    Nov 15 16:23:19.247: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043065471s
    Nov 15 16:23:21.246: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 4.042271824s
    Nov 15 16:23:21.246: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.15.40.115 on the node which pod4 resides and expect not scheduled 11/15/23 16:23:21.246
    Nov 15 16:23:21.277: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-6770" to be "not pending"
    Nov 15 16:23:21.299: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.300899ms
    Nov 15 16:23:23.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041159276s
    Nov 15 16:23:25.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042333396s
    Nov 15 16:23:27.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.040644648s
    Nov 15 16:23:29.322: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.044569862s
    Nov 15 16:23:31.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.041636783s
    Nov 15 16:23:33.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.041930772s
    Nov 15 16:23:35.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.041612427s
    Nov 15 16:23:37.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.042380026s
    Nov 15 16:23:39.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.041195904s
    Nov 15 16:23:41.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.040602283s
    Nov 15 16:23:43.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.042152863s
    Nov 15 16:23:45.321: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.043578581s
    Nov 15 16:23:47.317: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.039942788s
    Nov 15 16:23:49.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.041405718s
    Nov 15 16:23:51.323: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.046143654s
    Nov 15 16:23:53.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.042518185s
    Nov 15 16:23:55.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.042978859s
    Nov 15 16:23:57.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.042137439s
    Nov 15 16:23:59.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.042228095s
    Nov 15 16:24:01.370: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.092777009s
    Nov 15 16:24:03.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.042624119s
    Nov 15 16:24:05.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.041636723s
    Nov 15 16:24:07.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.042411441s
    Nov 15 16:24:09.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.042672464s
    Nov 15 16:24:11.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.042633055s
    Nov 15 16:24:13.328: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.051364605s
    Nov 15 16:24:15.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.042202019s
    Nov 15 16:24:17.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.042568706s
    Nov 15 16:24:19.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.041711431s
    Nov 15 16:24:21.317: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.04021724s
    Nov 15 16:24:23.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.04279312s
    Nov 15 16:24:25.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.042088883s
    Nov 15 16:24:27.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.04142074s
    Nov 15 16:24:29.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.042745059s
    Nov 15 16:24:31.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.041271429s
    Nov 15 16:24:33.321: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.044154027s
    Nov 15 16:24:35.323: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.045409819s
    Nov 15 16:24:37.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.042991596s
    Nov 15 16:24:39.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.041498124s
    Nov 15 16:24:41.321: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.043415647s
    Nov 15 16:24:43.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.04109855s
    Nov 15 16:24:45.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.042921482s
    Nov 15 16:24:47.324: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.046503946s
    Nov 15 16:24:49.317: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.039653277s
    Nov 15 16:24:51.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.040810585s
    Nov 15 16:24:53.317: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.040341515s
    Nov 15 16:24:55.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.042362886s
    Nov 15 16:24:57.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.042850913s
    Nov 15 16:24:59.322: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.045006996s
    Nov 15 16:25:01.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.040660762s
    Nov 15 16:25:03.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.040472263s
    Nov 15 16:25:05.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.041720399s
    Nov 15 16:25:07.323: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.045679945s
    Nov 15 16:25:09.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.042052938s
    Nov 15 16:25:11.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.040542796s
    Nov 15 16:25:13.317: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.040332148s
    Nov 15 16:25:15.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.041612317s
    Nov 15 16:25:17.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.042823586s
    Nov 15 16:25:19.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.042400425s
    Nov 15 16:25:21.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.041684584s
    Nov 15 16:25:23.322: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.044640126s
    Nov 15 16:25:25.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.042373877s
    Nov 15 16:25:27.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.040409677s
    Nov 15 16:25:29.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.04067078s
    Nov 15 16:25:31.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.040631175s
    Nov 15 16:25:33.322: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.045010151s
    Nov 15 16:25:35.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.041622431s
    Nov 15 16:25:37.322: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.044688198s
    Nov 15 16:25:39.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.042767031s
    Nov 15 16:25:41.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.041348671s
    Nov 15 16:25:43.331: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.054178849s
    Nov 15 16:25:45.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.042975607s
    Nov 15 16:25:47.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.04089605s
    Nov 15 16:25:49.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.040954796s
    Nov 15 16:25:51.317: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.039718955s
    Nov 15 16:25:53.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.042216737s
    Nov 15 16:25:55.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.04228193s
    Nov 15 16:25:57.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.040768821s
    Nov 15 16:25:59.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.0429459s
    Nov 15 16:26:01.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.041134239s
    Nov 15 16:26:03.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.040426621s
    Nov 15 16:26:05.323: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.046049017s
    Nov 15 16:26:07.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.041792328s
    Nov 15 16:26:09.321: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.043440871s
    Nov 15 16:26:11.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.041674349s
    Nov 15 16:26:13.321: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.043514307s
    Nov 15 16:26:15.321: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.043996789s
    Nov 15 16:26:17.323: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.045806821s
    Nov 15 16:26:19.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.042736308s
    Nov 15 16:26:21.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.042834675s
    Nov 15 16:26:23.322: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.045089217s
    Nov 15 16:26:25.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.041404474s
    Nov 15 16:26:27.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.041674483s
    Nov 15 16:26:29.322: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.045309481s
    Nov 15 16:26:31.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.040680381s
    Nov 15 16:26:33.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.042072055s
    Nov 15 16:26:35.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.041590122s
    Nov 15 16:26:37.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.040653429s
    Nov 15 16:26:39.321: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.04389362s
    Nov 15 16:26:41.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.04249166s
    Nov 15 16:26:43.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.040901457s
    Nov 15 16:26:45.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.042296969s
    Nov 15 16:26:47.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.042020109s
    Nov 15 16:26:49.322: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.044796302s
    Nov 15 16:26:51.324: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.046975727s
    Nov 15 16:26:53.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.041863396s
    Nov 15 16:26:55.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.040985506s
    Nov 15 16:26:57.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.043202075s
    Nov 15 16:26:59.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.041970811s
    Nov 15 16:27:01.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.042458092s
    Nov 15 16:27:03.324: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.046483471s
    Nov 15 16:27:05.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.041132466s
    Nov 15 16:27:07.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.042973867s
    Nov 15 16:27:09.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.042212635s
    Nov 15 16:27:11.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.041162028s
    Nov 15 16:27:13.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.042216163s
    Nov 15 16:27:15.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.042006454s
    Nov 15 16:27:17.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.041434211s
    Nov 15 16:27:19.317: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.040388425s
    Nov 15 16:27:21.317: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.04022921s
    Nov 15 16:27:23.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.041652382s
    Nov 15 16:27:25.324: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.046526065s
    Nov 15 16:27:27.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.042426138s
    Nov 15 16:27:29.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.042972468s
    Nov 15 16:27:31.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.04194309s
    Nov 15 16:27:33.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.041158444s
    Nov 15 16:27:35.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.04187284s
    Nov 15 16:27:37.324: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.046827416s
    Nov 15 16:27:39.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.042662087s
    Nov 15 16:27:41.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.042838857s
    Nov 15 16:27:43.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.041958706s
    Nov 15 16:27:45.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.041106072s
    Nov 15 16:27:47.325: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.047940408s
    Nov 15 16:27:49.322: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.044835109s
    Nov 15 16:27:51.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.041674434s
    Nov 15 16:27:53.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.041682391s
    Nov 15 16:27:55.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.042701913s
    Nov 15 16:27:57.317: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.040265859s
    Nov 15 16:27:59.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.042689253s
    Nov 15 16:28:01.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.042714102s
    Nov 15 16:28:03.318: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.041389231s
    Nov 15 16:28:05.320: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.042702433s
    Nov 15 16:28:07.345: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.067905198s
    Nov 15 16:28:09.319: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.04166432s
    Nov 15 16:28:11.322: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.045240395s
    Nov 15 16:28:13.317: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.04000601s
    Nov 15 16:28:15.317: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.040052989s
    Nov 15 16:28:17.327: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.049453221s
    Nov 15 16:28:19.317: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.040392999s
    Nov 15 16:28:21.317: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.040364129s
    Nov 15 16:28:21.338: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.060755524s
    STEP: removing the label kubernetes.io/e2e-f39942fa-02ef-49c1-bf32-a2712e6f2658 off the node 10.15.40.115 11/15/23 16:28:21.338
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-f39942fa-02ef-49c1-bf32-a2712e6f2658 11/15/23 16:28:21.399
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:28:21.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-6770" for this suite. 11/15/23 16:28:21.442
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:28:21.478
Nov 15 16:28:21.478: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename emptydir 11/15/23 16:28:21.482
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:28:21.567
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:28:21.586
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 11/15/23 16:28:21.605
Nov 15 16:28:21.640: INFO: Waiting up to 5m0s for pod "pod-cb0da2f0-f16a-4a2f-bd78-7a3249c4b1ba" in namespace "emptydir-9452" to be "Succeeded or Failed"
Nov 15 16:28:21.657: INFO: Pod "pod-cb0da2f0-f16a-4a2f-bd78-7a3249c4b1ba": Phase="Pending", Reason="", readiness=false. Elapsed: 16.309564ms
Nov 15 16:28:23.674: INFO: Pod "pod-cb0da2f0-f16a-4a2f-bd78-7a3249c4b1ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033560024s
Nov 15 16:28:25.675: INFO: Pod "pod-cb0da2f0-f16a-4a2f-bd78-7a3249c4b1ba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034407449s
Nov 15 16:28:27.674: INFO: Pod "pod-cb0da2f0-f16a-4a2f-bd78-7a3249c4b1ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033288375s
STEP: Saw pod success 11/15/23 16:28:27.674
Nov 15 16:28:27.674: INFO: Pod "pod-cb0da2f0-f16a-4a2f-bd78-7a3249c4b1ba" satisfied condition "Succeeded or Failed"
Nov 15 16:28:27.693: INFO: Trying to get logs from node 10.15.40.115 pod pod-cb0da2f0-f16a-4a2f-bd78-7a3249c4b1ba container test-container: <nil>
STEP: delete the pod 11/15/23 16:28:27.844
Nov 15 16:28:27.893: INFO: Waiting for pod pod-cb0da2f0-f16a-4a2f-bd78-7a3249c4b1ba to disappear
Nov 15 16:28:27.907: INFO: Pod pod-cb0da2f0-f16a-4a2f-bd78-7a3249c4b1ba no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 15 16:28:27.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9452" for this suite. 11/15/23 16:28:27.931
------------------------------
• [SLOW TEST] [6.536 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:28:21.478
    Nov 15 16:28:21.478: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename emptydir 11/15/23 16:28:21.482
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:28:21.567
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:28:21.586
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 11/15/23 16:28:21.605
    Nov 15 16:28:21.640: INFO: Waiting up to 5m0s for pod "pod-cb0da2f0-f16a-4a2f-bd78-7a3249c4b1ba" in namespace "emptydir-9452" to be "Succeeded or Failed"
    Nov 15 16:28:21.657: INFO: Pod "pod-cb0da2f0-f16a-4a2f-bd78-7a3249c4b1ba": Phase="Pending", Reason="", readiness=false. Elapsed: 16.309564ms
    Nov 15 16:28:23.674: INFO: Pod "pod-cb0da2f0-f16a-4a2f-bd78-7a3249c4b1ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033560024s
    Nov 15 16:28:25.675: INFO: Pod "pod-cb0da2f0-f16a-4a2f-bd78-7a3249c4b1ba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034407449s
    Nov 15 16:28:27.674: INFO: Pod "pod-cb0da2f0-f16a-4a2f-bd78-7a3249c4b1ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033288375s
    STEP: Saw pod success 11/15/23 16:28:27.674
    Nov 15 16:28:27.674: INFO: Pod "pod-cb0da2f0-f16a-4a2f-bd78-7a3249c4b1ba" satisfied condition "Succeeded or Failed"
    Nov 15 16:28:27.693: INFO: Trying to get logs from node 10.15.40.115 pod pod-cb0da2f0-f16a-4a2f-bd78-7a3249c4b1ba container test-container: <nil>
    STEP: delete the pod 11/15/23 16:28:27.844
    Nov 15 16:28:27.893: INFO: Waiting for pod pod-cb0da2f0-f16a-4a2f-bd78-7a3249c4b1ba to disappear
    Nov 15 16:28:27.907: INFO: Pod pod-cb0da2f0-f16a-4a2f-bd78-7a3249c4b1ba no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:28:27.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9452" for this suite. 11/15/23 16:28:27.931
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:28:28.031
Nov 15 16:28:28.031: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename kubectl 11/15/23 16:28:28.033
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:28:28.106
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:28:28.121
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 11/15/23 16:28:28.139
Nov 15 16:28:28.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-899 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Nov 15 16:28:28.324: INFO: stderr: ""
Nov 15 16:28:28.324: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 11/15/23 16:28:28.325
Nov 15 16:28:28.325: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Nov 15 16:28:28.326: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-899" to be "running and ready, or succeeded"
Nov 15 16:28:28.342: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 16.134968ms
Nov 15 16:28:28.342: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '10.15.40.115' to be 'Running' but was 'Pending'
Nov 15 16:28:30.360: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.034204103s
Nov 15 16:28:30.360: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Nov 15 16:28:30.360: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 11/15/23 16:28:30.36
Nov 15 16:28:30.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-899 logs logs-generator logs-generator'
Nov 15 16:28:30.553: INFO: stderr: ""
Nov 15 16:28:30.553: INFO: stdout: "I1115 16:28:29.648209       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/d426 444\nI1115 16:28:29.848560       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/cxg 310\nI1115 16:28:30.048740       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/96zz 522\nI1115 16:28:30.248306       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/8l9 526\nI1115 16:28:30.449074       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/6rj 202\n"
STEP: limiting log lines 11/15/23 16:28:30.553
Nov 15 16:28:30.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-899 logs logs-generator logs-generator --tail=1'
Nov 15 16:28:30.848: INFO: stderr: ""
Nov 15 16:28:30.848: INFO: stdout: "I1115 16:28:30.648507       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/kwtt 512\n"
Nov 15 16:28:30.848: INFO: got output "I1115 16:28:30.648507       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/kwtt 512\n"
STEP: limiting log bytes 11/15/23 16:28:30.848
Nov 15 16:28:30.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-899 logs logs-generator logs-generator --limit-bytes=1'
Nov 15 16:28:31.047: INFO: stderr: ""
Nov 15 16:28:31.047: INFO: stdout: "I"
Nov 15 16:28:31.047: INFO: got output "I"
STEP: exposing timestamps 11/15/23 16:28:31.047
Nov 15 16:28:31.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-899 logs logs-generator logs-generator --tail=1 --timestamps'
Nov 15 16:28:31.259: INFO: stderr: ""
Nov 15 16:28:31.259: INFO: stdout: "2023-11-15T16:28:31.048986859Z I1115 16:28:31.048604       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/wx4z 479\n"
Nov 15 16:28:31.260: INFO: got output "2023-11-15T16:28:31.048986859Z I1115 16:28:31.048604       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/wx4z 479\n"
STEP: restricting to a time range 11/15/23 16:28:31.26
Nov 15 16:28:33.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-899 logs logs-generator logs-generator --since=1s'
Nov 15 16:28:34.058: INFO: stderr: ""
Nov 15 16:28:34.058: INFO: stdout: "I1115 16:28:33.048438       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/f97t 261\nI1115 16:28:33.249029       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/xm2 556\nI1115 16:28:33.448502       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/5hqx 262\nI1115 16:28:33.649243       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/m49p 598\nI1115 16:28:33.848884       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/bc4 468\n"
Nov 15 16:28:34.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-899 logs logs-generator logs-generator --since=24h'
Nov 15 16:28:34.249: INFO: stderr: ""
Nov 15 16:28:34.249: INFO: stdout: "I1115 16:28:29.648209       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/d426 444\nI1115 16:28:29.848560       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/cxg 310\nI1115 16:28:30.048740       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/96zz 522\nI1115 16:28:30.248306       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/8l9 526\nI1115 16:28:30.449074       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/6rj 202\nI1115 16:28:30.648507       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/kwtt 512\nI1115 16:28:30.849131       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/cr8p 513\nI1115 16:28:31.048604       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/wx4z 479\nI1115 16:28:31.249068       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/pnj 556\nI1115 16:28:31.449153       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/56b 539\nI1115 16:28:31.648842       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/jsm2 368\nI1115 16:28:31.848482       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/sp8 261\nI1115 16:28:32.049309       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/cwr 264\nI1115 16:28:32.249054       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/drz6 225\nI1115 16:28:32.448701       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/dj6x 379\nI1115 16:28:32.649383       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/7h8 260\nI1115 16:28:32.849031       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/rqw 442\nI1115 16:28:33.048438       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/f97t 261\nI1115 16:28:33.249029       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/xm2 556\nI1115 16:28:33.448502       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/5hqx 262\nI1115 16:28:33.649243       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/m49p 598\nI1115 16:28:33.848884       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/bc4 468\nI1115 16:28:34.049362       1 logs_generator.go:76] 22 POST /api/v1/namespaces/ns/pods/bjm 414\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Nov 15 16:28:34.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-899 delete pod logs-generator'
Nov 15 16:28:36.038: INFO: stderr: ""
Nov 15 16:28:36.038: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 15 16:28:36.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-899" for this suite. 11/15/23 16:28:36.063
------------------------------
• [SLOW TEST] [8.060 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:28:28.031
    Nov 15 16:28:28.031: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename kubectl 11/15/23 16:28:28.033
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:28:28.106
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:28:28.121
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 11/15/23 16:28:28.139
    Nov 15 16:28:28.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-899 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Nov 15 16:28:28.324: INFO: stderr: ""
    Nov 15 16:28:28.324: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 11/15/23 16:28:28.325
    Nov 15 16:28:28.325: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Nov 15 16:28:28.326: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-899" to be "running and ready, or succeeded"
    Nov 15 16:28:28.342: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 16.134968ms
    Nov 15 16:28:28.342: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '10.15.40.115' to be 'Running' but was 'Pending'
    Nov 15 16:28:30.360: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.034204103s
    Nov 15 16:28:30.360: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Nov 15 16:28:30.360: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 11/15/23 16:28:30.36
    Nov 15 16:28:30.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-899 logs logs-generator logs-generator'
    Nov 15 16:28:30.553: INFO: stderr: ""
    Nov 15 16:28:30.553: INFO: stdout: "I1115 16:28:29.648209       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/d426 444\nI1115 16:28:29.848560       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/cxg 310\nI1115 16:28:30.048740       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/96zz 522\nI1115 16:28:30.248306       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/8l9 526\nI1115 16:28:30.449074       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/6rj 202\n"
    STEP: limiting log lines 11/15/23 16:28:30.553
    Nov 15 16:28:30.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-899 logs logs-generator logs-generator --tail=1'
    Nov 15 16:28:30.848: INFO: stderr: ""
    Nov 15 16:28:30.848: INFO: stdout: "I1115 16:28:30.648507       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/kwtt 512\n"
    Nov 15 16:28:30.848: INFO: got output "I1115 16:28:30.648507       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/kwtt 512\n"
    STEP: limiting log bytes 11/15/23 16:28:30.848
    Nov 15 16:28:30.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-899 logs logs-generator logs-generator --limit-bytes=1'
    Nov 15 16:28:31.047: INFO: stderr: ""
    Nov 15 16:28:31.047: INFO: stdout: "I"
    Nov 15 16:28:31.047: INFO: got output "I"
    STEP: exposing timestamps 11/15/23 16:28:31.047
    Nov 15 16:28:31.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-899 logs logs-generator logs-generator --tail=1 --timestamps'
    Nov 15 16:28:31.259: INFO: stderr: ""
    Nov 15 16:28:31.259: INFO: stdout: "2023-11-15T16:28:31.048986859Z I1115 16:28:31.048604       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/wx4z 479\n"
    Nov 15 16:28:31.260: INFO: got output "2023-11-15T16:28:31.048986859Z I1115 16:28:31.048604       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/wx4z 479\n"
    STEP: restricting to a time range 11/15/23 16:28:31.26
    Nov 15 16:28:33.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-899 logs logs-generator logs-generator --since=1s'
    Nov 15 16:28:34.058: INFO: stderr: ""
    Nov 15 16:28:34.058: INFO: stdout: "I1115 16:28:33.048438       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/f97t 261\nI1115 16:28:33.249029       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/xm2 556\nI1115 16:28:33.448502       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/5hqx 262\nI1115 16:28:33.649243       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/m49p 598\nI1115 16:28:33.848884       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/bc4 468\n"
    Nov 15 16:28:34.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-899 logs logs-generator logs-generator --since=24h'
    Nov 15 16:28:34.249: INFO: stderr: ""
    Nov 15 16:28:34.249: INFO: stdout: "I1115 16:28:29.648209       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/d426 444\nI1115 16:28:29.848560       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/cxg 310\nI1115 16:28:30.048740       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/96zz 522\nI1115 16:28:30.248306       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/8l9 526\nI1115 16:28:30.449074       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/6rj 202\nI1115 16:28:30.648507       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/kwtt 512\nI1115 16:28:30.849131       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/cr8p 513\nI1115 16:28:31.048604       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/wx4z 479\nI1115 16:28:31.249068       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/pnj 556\nI1115 16:28:31.449153       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/56b 539\nI1115 16:28:31.648842       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/jsm2 368\nI1115 16:28:31.848482       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/sp8 261\nI1115 16:28:32.049309       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/cwr 264\nI1115 16:28:32.249054       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/drz6 225\nI1115 16:28:32.448701       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/dj6x 379\nI1115 16:28:32.649383       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/7h8 260\nI1115 16:28:32.849031       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/rqw 442\nI1115 16:28:33.048438       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/f97t 261\nI1115 16:28:33.249029       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/xm2 556\nI1115 16:28:33.448502       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/5hqx 262\nI1115 16:28:33.649243       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/m49p 598\nI1115 16:28:33.848884       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/bc4 468\nI1115 16:28:34.049362       1 logs_generator.go:76] 22 POST /api/v1/namespaces/ns/pods/bjm 414\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Nov 15 16:28:34.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-899 delete pod logs-generator'
    Nov 15 16:28:36.038: INFO: stderr: ""
    Nov 15 16:28:36.038: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:28:36.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-899" for this suite. 11/15/23 16:28:36.063
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:28:36.091
Nov 15 16:28:36.092: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename container-runtime 11/15/23 16:28:36.097
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:28:36.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:28:36.173
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 11/15/23 16:28:36.199
STEP: wait for the container to reach Succeeded 11/15/23 16:28:36.233
STEP: get the container status 11/15/23 16:28:40.324
STEP: the container should be terminated 11/15/23 16:28:40.342
STEP: the termination message should be set 11/15/23 16:28:40.343
Nov 15 16:28:40.343: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 11/15/23 16:28:40.344
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Nov 15 16:28:40.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-9215" for this suite. 11/15/23 16:28:40.438
------------------------------
• [4.373 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:28:36.091
    Nov 15 16:28:36.092: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename container-runtime 11/15/23 16:28:36.097
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:28:36.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:28:36.173
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 11/15/23 16:28:36.199
    STEP: wait for the container to reach Succeeded 11/15/23 16:28:36.233
    STEP: get the container status 11/15/23 16:28:40.324
    STEP: the container should be terminated 11/15/23 16:28:40.342
    STEP: the termination message should be set 11/15/23 16:28:40.343
    Nov 15 16:28:40.343: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 11/15/23 16:28:40.344
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:28:40.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-9215" for this suite. 11/15/23 16:28:40.438
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:28:40.468
Nov 15 16:28:40.468: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename var-expansion 11/15/23 16:28:40.47
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:28:40.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:28:40.54
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 11/15/23 16:28:40.557
Nov 15 16:28:40.589: INFO: Waiting up to 5m0s for pod "var-expansion-48cccd52-c335-4e58-8148-8f23731282bb" in namespace "var-expansion-5395" to be "Succeeded or Failed"
Nov 15 16:28:40.607: INFO: Pod "var-expansion-48cccd52-c335-4e58-8148-8f23731282bb": Phase="Pending", Reason="", readiness=false. Elapsed: 16.936097ms
Nov 15 16:28:42.624: INFO: Pod "var-expansion-48cccd52-c335-4e58-8148-8f23731282bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034511316s
Nov 15 16:28:44.628: INFO: Pod "var-expansion-48cccd52-c335-4e58-8148-8f23731282bb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038292642s
Nov 15 16:28:46.625: INFO: Pod "var-expansion-48cccd52-c335-4e58-8148-8f23731282bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035749929s
STEP: Saw pod success 11/15/23 16:28:46.626
Nov 15 16:28:46.626: INFO: Pod "var-expansion-48cccd52-c335-4e58-8148-8f23731282bb" satisfied condition "Succeeded or Failed"
Nov 15 16:28:46.644: INFO: Trying to get logs from node 10.15.40.115 pod var-expansion-48cccd52-c335-4e58-8148-8f23731282bb container dapi-container: <nil>
STEP: delete the pod 11/15/23 16:28:46.686
Nov 15 16:28:46.725: INFO: Waiting for pod var-expansion-48cccd52-c335-4e58-8148-8f23731282bb to disappear
Nov 15 16:28:46.742: INFO: Pod var-expansion-48cccd52-c335-4e58-8148-8f23731282bb no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Nov 15 16:28:46.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5395" for this suite. 11/15/23 16:28:46.768
------------------------------
• [SLOW TEST] [6.327 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:28:40.468
    Nov 15 16:28:40.468: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename var-expansion 11/15/23 16:28:40.47
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:28:40.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:28:40.54
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 11/15/23 16:28:40.557
    Nov 15 16:28:40.589: INFO: Waiting up to 5m0s for pod "var-expansion-48cccd52-c335-4e58-8148-8f23731282bb" in namespace "var-expansion-5395" to be "Succeeded or Failed"
    Nov 15 16:28:40.607: INFO: Pod "var-expansion-48cccd52-c335-4e58-8148-8f23731282bb": Phase="Pending", Reason="", readiness=false. Elapsed: 16.936097ms
    Nov 15 16:28:42.624: INFO: Pod "var-expansion-48cccd52-c335-4e58-8148-8f23731282bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034511316s
    Nov 15 16:28:44.628: INFO: Pod "var-expansion-48cccd52-c335-4e58-8148-8f23731282bb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038292642s
    Nov 15 16:28:46.625: INFO: Pod "var-expansion-48cccd52-c335-4e58-8148-8f23731282bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035749929s
    STEP: Saw pod success 11/15/23 16:28:46.626
    Nov 15 16:28:46.626: INFO: Pod "var-expansion-48cccd52-c335-4e58-8148-8f23731282bb" satisfied condition "Succeeded or Failed"
    Nov 15 16:28:46.644: INFO: Trying to get logs from node 10.15.40.115 pod var-expansion-48cccd52-c335-4e58-8148-8f23731282bb container dapi-container: <nil>
    STEP: delete the pod 11/15/23 16:28:46.686
    Nov 15 16:28:46.725: INFO: Waiting for pod var-expansion-48cccd52-c335-4e58-8148-8f23731282bb to disappear
    Nov 15 16:28:46.742: INFO: Pod var-expansion-48cccd52-c335-4e58-8148-8f23731282bb no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:28:46.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5395" for this suite. 11/15/23 16:28:46.768
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:28:46.806
Nov 15 16:28:46.806: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename secrets 11/15/23 16:28:46.808
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:28:46.86
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:28:46.879
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-553cef60-de45-48fe-b069-023f3208c918 11/15/23 16:28:46.894
STEP: Creating a pod to test consume secrets 11/15/23 16:28:46.914
Nov 15 16:28:46.947: INFO: Waiting up to 5m0s for pod "pod-secrets-8feab271-584e-4fcf-85df-c25fc03f8252" in namespace "secrets-1758" to be "Succeeded or Failed"
Nov 15 16:28:46.968: INFO: Pod "pod-secrets-8feab271-584e-4fcf-85df-c25fc03f8252": Phase="Pending", Reason="", readiness=false. Elapsed: 20.225528ms
Nov 15 16:28:48.985: INFO: Pod "pod-secrets-8feab271-584e-4fcf-85df-c25fc03f8252": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03796838s
Nov 15 16:28:50.986: INFO: Pod "pod-secrets-8feab271-584e-4fcf-85df-c25fc03f8252": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038502893s
Nov 15 16:28:52.987: INFO: Pod "pod-secrets-8feab271-584e-4fcf-85df-c25fc03f8252": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039736966s
STEP: Saw pod success 11/15/23 16:28:52.987
Nov 15 16:28:52.988: INFO: Pod "pod-secrets-8feab271-584e-4fcf-85df-c25fc03f8252" satisfied condition "Succeeded or Failed"
Nov 15 16:28:53.005: INFO: Trying to get logs from node 10.15.40.115 pod pod-secrets-8feab271-584e-4fcf-85df-c25fc03f8252 container secret-volume-test: <nil>
STEP: delete the pod 11/15/23 16:28:53.049
Nov 15 16:28:53.089: INFO: Waiting for pod pod-secrets-8feab271-584e-4fcf-85df-c25fc03f8252 to disappear
Nov 15 16:28:53.105: INFO: Pod pod-secrets-8feab271-584e-4fcf-85df-c25fc03f8252 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 15 16:28:53.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1758" for this suite. 11/15/23 16:28:53.132
------------------------------
• [SLOW TEST] [6.355 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:28:46.806
    Nov 15 16:28:46.806: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename secrets 11/15/23 16:28:46.808
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:28:46.86
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:28:46.879
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-553cef60-de45-48fe-b069-023f3208c918 11/15/23 16:28:46.894
    STEP: Creating a pod to test consume secrets 11/15/23 16:28:46.914
    Nov 15 16:28:46.947: INFO: Waiting up to 5m0s for pod "pod-secrets-8feab271-584e-4fcf-85df-c25fc03f8252" in namespace "secrets-1758" to be "Succeeded or Failed"
    Nov 15 16:28:46.968: INFO: Pod "pod-secrets-8feab271-584e-4fcf-85df-c25fc03f8252": Phase="Pending", Reason="", readiness=false. Elapsed: 20.225528ms
    Nov 15 16:28:48.985: INFO: Pod "pod-secrets-8feab271-584e-4fcf-85df-c25fc03f8252": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03796838s
    Nov 15 16:28:50.986: INFO: Pod "pod-secrets-8feab271-584e-4fcf-85df-c25fc03f8252": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038502893s
    Nov 15 16:28:52.987: INFO: Pod "pod-secrets-8feab271-584e-4fcf-85df-c25fc03f8252": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039736966s
    STEP: Saw pod success 11/15/23 16:28:52.987
    Nov 15 16:28:52.988: INFO: Pod "pod-secrets-8feab271-584e-4fcf-85df-c25fc03f8252" satisfied condition "Succeeded or Failed"
    Nov 15 16:28:53.005: INFO: Trying to get logs from node 10.15.40.115 pod pod-secrets-8feab271-584e-4fcf-85df-c25fc03f8252 container secret-volume-test: <nil>
    STEP: delete the pod 11/15/23 16:28:53.049
    Nov 15 16:28:53.089: INFO: Waiting for pod pod-secrets-8feab271-584e-4fcf-85df-c25fc03f8252 to disappear
    Nov 15 16:28:53.105: INFO: Pod pod-secrets-8feab271-584e-4fcf-85df-c25fc03f8252 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:28:53.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1758" for this suite. 11/15/23 16:28:53.132
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:28:53.164
Nov 15 16:28:53.164: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename podtemplate 11/15/23 16:28:53.167
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:28:53.221
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:28:53.243
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Nov 15 16:28:53.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-1179" for this suite. 11/15/23 16:28:53.45
------------------------------
• [0.319 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:28:53.164
    Nov 15 16:28:53.164: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename podtemplate 11/15/23 16:28:53.167
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:28:53.221
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:28:53.243
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:28:53.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-1179" for this suite. 11/15/23 16:28:53.45
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:28:53.485
Nov 15 16:28:53.485: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename projected 11/15/23 16:28:53.488
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:28:53.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:28:53.581
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 11/15/23 16:28:53.6
Nov 15 16:28:53.634: INFO: Waiting up to 5m0s for pod "annotationupdate562af869-fce1-4126-8c4b-48c5ed417170" in namespace "projected-5804" to be "running and ready"
Nov 15 16:28:53.651: INFO: Pod "annotationupdate562af869-fce1-4126-8c4b-48c5ed417170": Phase="Pending", Reason="", readiness=false. Elapsed: 16.796821ms
Nov 15 16:28:53.651: INFO: The phase of Pod annotationupdate562af869-fce1-4126-8c4b-48c5ed417170 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 16:28:55.671: INFO: Pod "annotationupdate562af869-fce1-4126-8c4b-48c5ed417170": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036656254s
Nov 15 16:28:55.671: INFO: The phase of Pod annotationupdate562af869-fce1-4126-8c4b-48c5ed417170 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 16:28:57.670: INFO: Pod "annotationupdate562af869-fce1-4126-8c4b-48c5ed417170": Phase="Running", Reason="", readiness=true. Elapsed: 4.035588002s
Nov 15 16:28:57.670: INFO: The phase of Pod annotationupdate562af869-fce1-4126-8c4b-48c5ed417170 is Running (Ready = true)
Nov 15 16:28:57.670: INFO: Pod "annotationupdate562af869-fce1-4126-8c4b-48c5ed417170" satisfied condition "running and ready"
Nov 15 16:28:58.274: INFO: Successfully updated pod "annotationupdate562af869-fce1-4126-8c4b-48c5ed417170"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Nov 15 16:29:00.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5804" for this suite. 11/15/23 16:29:00.4
------------------------------
• [SLOW TEST] [6.945 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:28:53.485
    Nov 15 16:28:53.485: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename projected 11/15/23 16:28:53.488
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:28:53.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:28:53.581
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 11/15/23 16:28:53.6
    Nov 15 16:28:53.634: INFO: Waiting up to 5m0s for pod "annotationupdate562af869-fce1-4126-8c4b-48c5ed417170" in namespace "projected-5804" to be "running and ready"
    Nov 15 16:28:53.651: INFO: Pod "annotationupdate562af869-fce1-4126-8c4b-48c5ed417170": Phase="Pending", Reason="", readiness=false. Elapsed: 16.796821ms
    Nov 15 16:28:53.651: INFO: The phase of Pod annotationupdate562af869-fce1-4126-8c4b-48c5ed417170 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 16:28:55.671: INFO: Pod "annotationupdate562af869-fce1-4126-8c4b-48c5ed417170": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036656254s
    Nov 15 16:28:55.671: INFO: The phase of Pod annotationupdate562af869-fce1-4126-8c4b-48c5ed417170 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 16:28:57.670: INFO: Pod "annotationupdate562af869-fce1-4126-8c4b-48c5ed417170": Phase="Running", Reason="", readiness=true. Elapsed: 4.035588002s
    Nov 15 16:28:57.670: INFO: The phase of Pod annotationupdate562af869-fce1-4126-8c4b-48c5ed417170 is Running (Ready = true)
    Nov 15 16:28:57.670: INFO: Pod "annotationupdate562af869-fce1-4126-8c4b-48c5ed417170" satisfied condition "running and ready"
    Nov 15 16:28:58.274: INFO: Successfully updated pod "annotationupdate562af869-fce1-4126-8c4b-48c5ed417170"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:29:00.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5804" for this suite. 11/15/23 16:29:00.4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:29:00.448
Nov 15 16:29:00.448: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename var-expansion 11/15/23 16:29:00.45
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:29:00.507
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:29:00.523
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 11/15/23 16:29:00.543
Nov 15 16:29:00.578: INFO: Waiting up to 5m0s for pod "var-expansion-b5389e7f-288f-4362-ab10-d8db103791d4" in namespace "var-expansion-5722" to be "Succeeded or Failed"
Nov 15 16:29:00.594: INFO: Pod "var-expansion-b5389e7f-288f-4362-ab10-d8db103791d4": Phase="Pending", Reason="", readiness=false. Elapsed: 16.642894ms
Nov 15 16:29:02.614: INFO: Pod "var-expansion-b5389e7f-288f-4362-ab10-d8db103791d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035822641s
Nov 15 16:29:04.613: INFO: Pod "var-expansion-b5389e7f-288f-4362-ab10-d8db103791d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035392591s
STEP: Saw pod success 11/15/23 16:29:04.613
Nov 15 16:29:04.614: INFO: Pod "var-expansion-b5389e7f-288f-4362-ab10-d8db103791d4" satisfied condition "Succeeded or Failed"
Nov 15 16:29:04.631: INFO: Trying to get logs from node 10.15.40.115 pod var-expansion-b5389e7f-288f-4362-ab10-d8db103791d4 container dapi-container: <nil>
STEP: delete the pod 11/15/23 16:29:04.68
Nov 15 16:29:04.769: INFO: Waiting for pod var-expansion-b5389e7f-288f-4362-ab10-d8db103791d4 to disappear
Nov 15 16:29:04.788: INFO: Pod var-expansion-b5389e7f-288f-4362-ab10-d8db103791d4 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Nov 15 16:29:04.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5722" for this suite. 11/15/23 16:29:04.815
------------------------------
• [4.397 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:29:00.448
    Nov 15 16:29:00.448: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename var-expansion 11/15/23 16:29:00.45
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:29:00.507
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:29:00.523
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 11/15/23 16:29:00.543
    Nov 15 16:29:00.578: INFO: Waiting up to 5m0s for pod "var-expansion-b5389e7f-288f-4362-ab10-d8db103791d4" in namespace "var-expansion-5722" to be "Succeeded or Failed"
    Nov 15 16:29:00.594: INFO: Pod "var-expansion-b5389e7f-288f-4362-ab10-d8db103791d4": Phase="Pending", Reason="", readiness=false. Elapsed: 16.642894ms
    Nov 15 16:29:02.614: INFO: Pod "var-expansion-b5389e7f-288f-4362-ab10-d8db103791d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035822641s
    Nov 15 16:29:04.613: INFO: Pod "var-expansion-b5389e7f-288f-4362-ab10-d8db103791d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035392591s
    STEP: Saw pod success 11/15/23 16:29:04.613
    Nov 15 16:29:04.614: INFO: Pod "var-expansion-b5389e7f-288f-4362-ab10-d8db103791d4" satisfied condition "Succeeded or Failed"
    Nov 15 16:29:04.631: INFO: Trying to get logs from node 10.15.40.115 pod var-expansion-b5389e7f-288f-4362-ab10-d8db103791d4 container dapi-container: <nil>
    STEP: delete the pod 11/15/23 16:29:04.68
    Nov 15 16:29:04.769: INFO: Waiting for pod var-expansion-b5389e7f-288f-4362-ab10-d8db103791d4 to disappear
    Nov 15 16:29:04.788: INFO: Pod var-expansion-b5389e7f-288f-4362-ab10-d8db103791d4 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:29:04.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5722" for this suite. 11/15/23 16:29:04.815
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:29:04.855
Nov 15 16:29:04.856: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename subpath 11/15/23 16:29:04.857
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:29:04.912
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:29:04.929
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 11/15/23 16:29:04.952
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-m64h 11/15/23 16:29:04.988
STEP: Creating a pod to test atomic-volume-subpath 11/15/23 16:29:04.989
Nov 15 16:29:05.023: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-m64h" in namespace "subpath-2768" to be "Succeeded or Failed"
Nov 15 16:29:05.040: INFO: Pod "pod-subpath-test-secret-m64h": Phase="Pending", Reason="", readiness=false. Elapsed: 16.918862ms
Nov 15 16:29:07.058: INFO: Pod "pod-subpath-test-secret-m64h": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034416292s
Nov 15 16:29:09.061: INFO: Pod "pod-subpath-test-secret-m64h": Phase="Running", Reason="", readiness=true. Elapsed: 4.037291873s
Nov 15 16:29:11.058: INFO: Pod "pod-subpath-test-secret-m64h": Phase="Running", Reason="", readiness=true. Elapsed: 6.034363369s
Nov 15 16:29:13.059: INFO: Pod "pod-subpath-test-secret-m64h": Phase="Running", Reason="", readiness=true. Elapsed: 8.03531479s
Nov 15 16:29:15.067: INFO: Pod "pod-subpath-test-secret-m64h": Phase="Running", Reason="", readiness=true. Elapsed: 10.044049223s
Nov 15 16:29:17.058: INFO: Pod "pod-subpath-test-secret-m64h": Phase="Running", Reason="", readiness=true. Elapsed: 12.034523634s
Nov 15 16:29:19.058: INFO: Pod "pod-subpath-test-secret-m64h": Phase="Running", Reason="", readiness=true. Elapsed: 14.034447493s
Nov 15 16:29:21.058: INFO: Pod "pod-subpath-test-secret-m64h": Phase="Running", Reason="", readiness=true. Elapsed: 16.034706263s
Nov 15 16:29:23.057: INFO: Pod "pod-subpath-test-secret-m64h": Phase="Running", Reason="", readiness=true. Elapsed: 18.034050166s
Nov 15 16:29:25.062: INFO: Pod "pod-subpath-test-secret-m64h": Phase="Running", Reason="", readiness=true. Elapsed: 20.038641884s
Nov 15 16:29:27.062: INFO: Pod "pod-subpath-test-secret-m64h": Phase="Running", Reason="", readiness=true. Elapsed: 22.038988679s
Nov 15 16:29:29.057: INFO: Pod "pod-subpath-test-secret-m64h": Phase="Running", Reason="", readiness=false. Elapsed: 24.033353581s
Nov 15 16:29:31.059: INFO: Pod "pod-subpath-test-secret-m64h": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.035245014s
STEP: Saw pod success 11/15/23 16:29:31.059
Nov 15 16:29:31.059: INFO: Pod "pod-subpath-test-secret-m64h" satisfied condition "Succeeded or Failed"
Nov 15 16:29:31.076: INFO: Trying to get logs from node 10.15.40.115 pod pod-subpath-test-secret-m64h container test-container-subpath-secret-m64h: <nil>
STEP: delete the pod 11/15/23 16:29:31.129
Nov 15 16:29:31.177: INFO: Waiting for pod pod-subpath-test-secret-m64h to disappear
Nov 15 16:29:31.193: INFO: Pod pod-subpath-test-secret-m64h no longer exists
STEP: Deleting pod pod-subpath-test-secret-m64h 11/15/23 16:29:31.193
Nov 15 16:29:31.194: INFO: Deleting pod "pod-subpath-test-secret-m64h" in namespace "subpath-2768"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Nov 15 16:29:31.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-2768" for this suite. 11/15/23 16:29:31.236
------------------------------
• [SLOW TEST] [26.409 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:29:04.855
    Nov 15 16:29:04.856: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename subpath 11/15/23 16:29:04.857
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:29:04.912
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:29:04.929
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 11/15/23 16:29:04.952
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-m64h 11/15/23 16:29:04.988
    STEP: Creating a pod to test atomic-volume-subpath 11/15/23 16:29:04.989
    Nov 15 16:29:05.023: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-m64h" in namespace "subpath-2768" to be "Succeeded or Failed"
    Nov 15 16:29:05.040: INFO: Pod "pod-subpath-test-secret-m64h": Phase="Pending", Reason="", readiness=false. Elapsed: 16.918862ms
    Nov 15 16:29:07.058: INFO: Pod "pod-subpath-test-secret-m64h": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034416292s
    Nov 15 16:29:09.061: INFO: Pod "pod-subpath-test-secret-m64h": Phase="Running", Reason="", readiness=true. Elapsed: 4.037291873s
    Nov 15 16:29:11.058: INFO: Pod "pod-subpath-test-secret-m64h": Phase="Running", Reason="", readiness=true. Elapsed: 6.034363369s
    Nov 15 16:29:13.059: INFO: Pod "pod-subpath-test-secret-m64h": Phase="Running", Reason="", readiness=true. Elapsed: 8.03531479s
    Nov 15 16:29:15.067: INFO: Pod "pod-subpath-test-secret-m64h": Phase="Running", Reason="", readiness=true. Elapsed: 10.044049223s
    Nov 15 16:29:17.058: INFO: Pod "pod-subpath-test-secret-m64h": Phase="Running", Reason="", readiness=true. Elapsed: 12.034523634s
    Nov 15 16:29:19.058: INFO: Pod "pod-subpath-test-secret-m64h": Phase="Running", Reason="", readiness=true. Elapsed: 14.034447493s
    Nov 15 16:29:21.058: INFO: Pod "pod-subpath-test-secret-m64h": Phase="Running", Reason="", readiness=true. Elapsed: 16.034706263s
    Nov 15 16:29:23.057: INFO: Pod "pod-subpath-test-secret-m64h": Phase="Running", Reason="", readiness=true. Elapsed: 18.034050166s
    Nov 15 16:29:25.062: INFO: Pod "pod-subpath-test-secret-m64h": Phase="Running", Reason="", readiness=true. Elapsed: 20.038641884s
    Nov 15 16:29:27.062: INFO: Pod "pod-subpath-test-secret-m64h": Phase="Running", Reason="", readiness=true. Elapsed: 22.038988679s
    Nov 15 16:29:29.057: INFO: Pod "pod-subpath-test-secret-m64h": Phase="Running", Reason="", readiness=false. Elapsed: 24.033353581s
    Nov 15 16:29:31.059: INFO: Pod "pod-subpath-test-secret-m64h": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.035245014s
    STEP: Saw pod success 11/15/23 16:29:31.059
    Nov 15 16:29:31.059: INFO: Pod "pod-subpath-test-secret-m64h" satisfied condition "Succeeded or Failed"
    Nov 15 16:29:31.076: INFO: Trying to get logs from node 10.15.40.115 pod pod-subpath-test-secret-m64h container test-container-subpath-secret-m64h: <nil>
    STEP: delete the pod 11/15/23 16:29:31.129
    Nov 15 16:29:31.177: INFO: Waiting for pod pod-subpath-test-secret-m64h to disappear
    Nov 15 16:29:31.193: INFO: Pod pod-subpath-test-secret-m64h no longer exists
    STEP: Deleting pod pod-subpath-test-secret-m64h 11/15/23 16:29:31.193
    Nov 15 16:29:31.194: INFO: Deleting pod "pod-subpath-test-secret-m64h" in namespace "subpath-2768"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:29:31.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-2768" for this suite. 11/15/23 16:29:31.236
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:29:31.277
Nov 15 16:29:31.277: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename webhook 11/15/23 16:29:31.278
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:29:31.335
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:29:31.35
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/15/23 16:29:31.419
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 16:29:31.737
STEP: Deploying the webhook pod 11/15/23 16:29:31.77
STEP: Wait for the deployment to be ready 11/15/23 16:29:31.829
Nov 15 16:29:31.860: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Nov 15 16:29:33.915: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 16, 29, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 16, 29, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 16, 29, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 16, 29, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 11/15/23 16:29:35.929
STEP: Verifying the service has paired with the endpoint 11/15/23 16:29:35.979
Nov 15 16:29:36.980: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 11/15/23 16:29:37.001
STEP: Registering slow webhook via the AdmissionRegistration API 11/15/23 16:29:37.001
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 11/15/23 16:29:37.119
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 11/15/23 16:29:38.155
STEP: Registering slow webhook via the AdmissionRegistration API 11/15/23 16:29:38.156
STEP: Having no error when timeout is longer than webhook latency 11/15/23 16:29:39.254
STEP: Registering slow webhook via the AdmissionRegistration API 11/15/23 16:29:39.255
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 11/15/23 16:29:44.436
STEP: Registering slow webhook via the AdmissionRegistration API 11/15/23 16:29:44.436
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 16:29:49.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2062" for this suite. 11/15/23 16:29:49.731
STEP: Destroying namespace "webhook-2062-markers" for this suite. 11/15/23 16:29:49.762
------------------------------
• [SLOW TEST] [18.513 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:29:31.277
    Nov 15 16:29:31.277: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename webhook 11/15/23 16:29:31.278
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:29:31.335
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:29:31.35
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/15/23 16:29:31.419
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 16:29:31.737
    STEP: Deploying the webhook pod 11/15/23 16:29:31.77
    STEP: Wait for the deployment to be ready 11/15/23 16:29:31.829
    Nov 15 16:29:31.860: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Nov 15 16:29:33.915: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 16, 29, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 16, 29, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 16, 29, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 16, 29, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 11/15/23 16:29:35.929
    STEP: Verifying the service has paired with the endpoint 11/15/23 16:29:35.979
    Nov 15 16:29:36.980: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 11/15/23 16:29:37.001
    STEP: Registering slow webhook via the AdmissionRegistration API 11/15/23 16:29:37.001
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 11/15/23 16:29:37.119
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 11/15/23 16:29:38.155
    STEP: Registering slow webhook via the AdmissionRegistration API 11/15/23 16:29:38.156
    STEP: Having no error when timeout is longer than webhook latency 11/15/23 16:29:39.254
    STEP: Registering slow webhook via the AdmissionRegistration API 11/15/23 16:29:39.255
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 11/15/23 16:29:44.436
    STEP: Registering slow webhook via the AdmissionRegistration API 11/15/23 16:29:44.436
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:29:49.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2062" for this suite. 11/15/23 16:29:49.731
    STEP: Destroying namespace "webhook-2062-markers" for this suite. 11/15/23 16:29:49.762
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:29:49.792
Nov 15 16:29:49.792: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename webhook 11/15/23 16:29:49.797
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:29:49.85
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:29:49.862
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/15/23 16:29:49.923
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 16:29:50.386
STEP: Deploying the webhook pod 11/15/23 16:29:50.401
STEP: Wait for the deployment to be ready 11/15/23 16:29:50.437
Nov 15 16:29:50.472: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 15 16:29:52.519: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 16, 29, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 16, 29, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 16, 29, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 16, 29, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 11/15/23 16:29:54.548
STEP: Verifying the service has paired with the endpoint 11/15/23 16:29:54.597
Nov 15 16:29:55.598: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 11/15/23 16:29:55.644
STEP: create a pod that should be updated by the webhook 11/15/23 16:29:55.774
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 16:29:55.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2858" for this suite. 11/15/23 16:29:56.124
STEP: Destroying namespace "webhook-2858-markers" for this suite. 11/15/23 16:29:56.149
------------------------------
• [SLOW TEST] [6.382 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:29:49.792
    Nov 15 16:29:49.792: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename webhook 11/15/23 16:29:49.797
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:29:49.85
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:29:49.862
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/15/23 16:29:49.923
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 16:29:50.386
    STEP: Deploying the webhook pod 11/15/23 16:29:50.401
    STEP: Wait for the deployment to be ready 11/15/23 16:29:50.437
    Nov 15 16:29:50.472: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Nov 15 16:29:52.519: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 16, 29, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 16, 29, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 16, 29, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 16, 29, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 11/15/23 16:29:54.548
    STEP: Verifying the service has paired with the endpoint 11/15/23 16:29:54.597
    Nov 15 16:29:55.598: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 11/15/23 16:29:55.644
    STEP: create a pod that should be updated by the webhook 11/15/23 16:29:55.774
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:29:55.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2858" for this suite. 11/15/23 16:29:56.124
    STEP: Destroying namespace "webhook-2858-markers" for this suite. 11/15/23 16:29:56.149
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:29:56.176
Nov 15 16:29:56.176: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename svcaccounts 11/15/23 16:29:56.178
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:29:56.231
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:29:56.242
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Nov 15 16:29:56.303: INFO: Waiting up to 5m0s for pod "pod-service-account-2b51bdff-deb2-4839-9666-ee595b59b9ca" in namespace "svcaccounts-5972" to be "running"
Nov 15 16:29:56.316: INFO: Pod "pod-service-account-2b51bdff-deb2-4839-9666-ee595b59b9ca": Phase="Pending", Reason="", readiness=false. Elapsed: 13.347945ms
Nov 15 16:29:58.333: INFO: Pod "pod-service-account-2b51bdff-deb2-4839-9666-ee595b59b9ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029845732s
Nov 15 16:30:00.332: INFO: Pod "pod-service-account-2b51bdff-deb2-4839-9666-ee595b59b9ca": Phase="Running", Reason="", readiness=true. Elapsed: 4.02874258s
Nov 15 16:30:00.332: INFO: Pod "pod-service-account-2b51bdff-deb2-4839-9666-ee595b59b9ca" satisfied condition "running"
STEP: reading a file in the container 11/15/23 16:30:00.332
Nov 15 16:30:00.332: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5972 pod-service-account-2b51bdff-deb2-4839-9666-ee595b59b9ca -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 11/15/23 16:30:00.874
Nov 15 16:30:00.874: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5972 pod-service-account-2b51bdff-deb2-4839-9666-ee595b59b9ca -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 11/15/23 16:30:01.316
Nov 15 16:30:01.316: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5972 pod-service-account-2b51bdff-deb2-4839-9666-ee595b59b9ca -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Nov 15 16:30:01.848: INFO: Got root ca configmap in namespace "svcaccounts-5972"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Nov 15 16:30:01.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-5972" for this suite. 11/15/23 16:30:01.881
------------------------------
• [SLOW TEST] [5.731 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:29:56.176
    Nov 15 16:29:56.176: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename svcaccounts 11/15/23 16:29:56.178
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:29:56.231
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:29:56.242
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Nov 15 16:29:56.303: INFO: Waiting up to 5m0s for pod "pod-service-account-2b51bdff-deb2-4839-9666-ee595b59b9ca" in namespace "svcaccounts-5972" to be "running"
    Nov 15 16:29:56.316: INFO: Pod "pod-service-account-2b51bdff-deb2-4839-9666-ee595b59b9ca": Phase="Pending", Reason="", readiness=false. Elapsed: 13.347945ms
    Nov 15 16:29:58.333: INFO: Pod "pod-service-account-2b51bdff-deb2-4839-9666-ee595b59b9ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029845732s
    Nov 15 16:30:00.332: INFO: Pod "pod-service-account-2b51bdff-deb2-4839-9666-ee595b59b9ca": Phase="Running", Reason="", readiness=true. Elapsed: 4.02874258s
    Nov 15 16:30:00.332: INFO: Pod "pod-service-account-2b51bdff-deb2-4839-9666-ee595b59b9ca" satisfied condition "running"
    STEP: reading a file in the container 11/15/23 16:30:00.332
    Nov 15 16:30:00.332: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5972 pod-service-account-2b51bdff-deb2-4839-9666-ee595b59b9ca -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 11/15/23 16:30:00.874
    Nov 15 16:30:00.874: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5972 pod-service-account-2b51bdff-deb2-4839-9666-ee595b59b9ca -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 11/15/23 16:30:01.316
    Nov 15 16:30:01.316: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5972 pod-service-account-2b51bdff-deb2-4839-9666-ee595b59b9ca -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Nov 15 16:30:01.848: INFO: Got root ca configmap in namespace "svcaccounts-5972"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:30:01.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-5972" for this suite. 11/15/23 16:30:01.881
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:30:01.908
Nov 15 16:30:01.908: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename downward-api 11/15/23 16:30:01.911
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:30:01.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:30:01.992
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 11/15/23 16:30:02.009
Nov 15 16:30:02.067: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5b467934-cf78-477e-8e71-2d3c76d04550" in namespace "downward-api-2285" to be "Succeeded or Failed"
Nov 15 16:30:02.085: INFO: Pod "downwardapi-volume-5b467934-cf78-477e-8e71-2d3c76d04550": Phase="Pending", Reason="", readiness=false. Elapsed: 17.921689ms
Nov 15 16:30:04.101: INFO: Pod "downwardapi-volume-5b467934-cf78-477e-8e71-2d3c76d04550": Phase="Running", Reason="", readiness=true. Elapsed: 2.03416117s
Nov 15 16:30:06.100: INFO: Pod "downwardapi-volume-5b467934-cf78-477e-8e71-2d3c76d04550": Phase="Running", Reason="", readiness=false. Elapsed: 4.032947715s
Nov 15 16:30:08.100: INFO: Pod "downwardapi-volume-5b467934-cf78-477e-8e71-2d3c76d04550": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033614975s
STEP: Saw pod success 11/15/23 16:30:08.1
Nov 15 16:30:08.101: INFO: Pod "downwardapi-volume-5b467934-cf78-477e-8e71-2d3c76d04550" satisfied condition "Succeeded or Failed"
Nov 15 16:30:08.114: INFO: Trying to get logs from node 10.15.40.115 pod downwardapi-volume-5b467934-cf78-477e-8e71-2d3c76d04550 container client-container: <nil>
STEP: delete the pod 11/15/23 16:30:08.157
Nov 15 16:30:08.193: INFO: Waiting for pod downwardapi-volume-5b467934-cf78-477e-8e71-2d3c76d04550 to disappear
Nov 15 16:30:08.207: INFO: Pod downwardapi-volume-5b467934-cf78-477e-8e71-2d3c76d04550 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Nov 15 16:30:08.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2285" for this suite. 11/15/23 16:30:08.226
------------------------------
• [SLOW TEST] [6.342 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:30:01.908
    Nov 15 16:30:01.908: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename downward-api 11/15/23 16:30:01.911
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:30:01.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:30:01.992
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 11/15/23 16:30:02.009
    Nov 15 16:30:02.067: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5b467934-cf78-477e-8e71-2d3c76d04550" in namespace "downward-api-2285" to be "Succeeded or Failed"
    Nov 15 16:30:02.085: INFO: Pod "downwardapi-volume-5b467934-cf78-477e-8e71-2d3c76d04550": Phase="Pending", Reason="", readiness=false. Elapsed: 17.921689ms
    Nov 15 16:30:04.101: INFO: Pod "downwardapi-volume-5b467934-cf78-477e-8e71-2d3c76d04550": Phase="Running", Reason="", readiness=true. Elapsed: 2.03416117s
    Nov 15 16:30:06.100: INFO: Pod "downwardapi-volume-5b467934-cf78-477e-8e71-2d3c76d04550": Phase="Running", Reason="", readiness=false. Elapsed: 4.032947715s
    Nov 15 16:30:08.100: INFO: Pod "downwardapi-volume-5b467934-cf78-477e-8e71-2d3c76d04550": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033614975s
    STEP: Saw pod success 11/15/23 16:30:08.1
    Nov 15 16:30:08.101: INFO: Pod "downwardapi-volume-5b467934-cf78-477e-8e71-2d3c76d04550" satisfied condition "Succeeded or Failed"
    Nov 15 16:30:08.114: INFO: Trying to get logs from node 10.15.40.115 pod downwardapi-volume-5b467934-cf78-477e-8e71-2d3c76d04550 container client-container: <nil>
    STEP: delete the pod 11/15/23 16:30:08.157
    Nov 15 16:30:08.193: INFO: Waiting for pod downwardapi-volume-5b467934-cf78-477e-8e71-2d3c76d04550 to disappear
    Nov 15 16:30:08.207: INFO: Pod downwardapi-volume-5b467934-cf78-477e-8e71-2d3c76d04550 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:30:08.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2285" for this suite. 11/15/23 16:30:08.226
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:30:08.257
Nov 15 16:30:08.257: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename secrets 11/15/23 16:30:08.258
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:30:08.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:30:08.323
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-7ded07ba-2131-4f18-9e4e-2ad9cd0766ed 11/15/23 16:30:08.353
STEP: Creating secret with name s-test-opt-upd-dbb5ad0b-ae10-406c-a6cd-80a30cb11d01 11/15/23 16:30:08.369
STEP: Creating the pod 11/15/23 16:30:08.386
Nov 15 16:30:08.424: INFO: Waiting up to 5m0s for pod "pod-secrets-3b55f2fc-16d0-42be-aa42-2169cbc55c2f" in namespace "secrets-9533" to be "running and ready"
Nov 15 16:30:08.438: INFO: Pod "pod-secrets-3b55f2fc-16d0-42be-aa42-2169cbc55c2f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.660203ms
Nov 15 16:30:08.438: INFO: The phase of Pod pod-secrets-3b55f2fc-16d0-42be-aa42-2169cbc55c2f is Pending, waiting for it to be Running (with Ready = true)
Nov 15 16:30:10.461: INFO: Pod "pod-secrets-3b55f2fc-16d0-42be-aa42-2169cbc55c2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03715995s
Nov 15 16:30:10.461: INFO: The phase of Pod pod-secrets-3b55f2fc-16d0-42be-aa42-2169cbc55c2f is Pending, waiting for it to be Running (with Ready = true)
Nov 15 16:30:12.454: INFO: Pod "pod-secrets-3b55f2fc-16d0-42be-aa42-2169cbc55c2f": Phase="Running", Reason="", readiness=true. Elapsed: 4.029665055s
Nov 15 16:30:12.454: INFO: The phase of Pod pod-secrets-3b55f2fc-16d0-42be-aa42-2169cbc55c2f is Running (Ready = true)
Nov 15 16:30:12.454: INFO: Pod "pod-secrets-3b55f2fc-16d0-42be-aa42-2169cbc55c2f" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-7ded07ba-2131-4f18-9e4e-2ad9cd0766ed 11/15/23 16:30:12.59
STEP: Updating secret s-test-opt-upd-dbb5ad0b-ae10-406c-a6cd-80a30cb11d01 11/15/23 16:30:12.615
STEP: Creating secret with name s-test-opt-create-e8e65600-2984-42dc-9d6e-df0c3efbdc3e 11/15/23 16:30:12.632
STEP: waiting to observe update in volume 11/15/23 16:30:12.649
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 15 16:31:32.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9533" for this suite. 11/15/23 16:31:32.738
------------------------------
• [SLOW TEST] [84.507 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:30:08.257
    Nov 15 16:30:08.257: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename secrets 11/15/23 16:30:08.258
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:30:08.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:30:08.323
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-7ded07ba-2131-4f18-9e4e-2ad9cd0766ed 11/15/23 16:30:08.353
    STEP: Creating secret with name s-test-opt-upd-dbb5ad0b-ae10-406c-a6cd-80a30cb11d01 11/15/23 16:30:08.369
    STEP: Creating the pod 11/15/23 16:30:08.386
    Nov 15 16:30:08.424: INFO: Waiting up to 5m0s for pod "pod-secrets-3b55f2fc-16d0-42be-aa42-2169cbc55c2f" in namespace "secrets-9533" to be "running and ready"
    Nov 15 16:30:08.438: INFO: Pod "pod-secrets-3b55f2fc-16d0-42be-aa42-2169cbc55c2f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.660203ms
    Nov 15 16:30:08.438: INFO: The phase of Pod pod-secrets-3b55f2fc-16d0-42be-aa42-2169cbc55c2f is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 16:30:10.461: INFO: Pod "pod-secrets-3b55f2fc-16d0-42be-aa42-2169cbc55c2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03715995s
    Nov 15 16:30:10.461: INFO: The phase of Pod pod-secrets-3b55f2fc-16d0-42be-aa42-2169cbc55c2f is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 16:30:12.454: INFO: Pod "pod-secrets-3b55f2fc-16d0-42be-aa42-2169cbc55c2f": Phase="Running", Reason="", readiness=true. Elapsed: 4.029665055s
    Nov 15 16:30:12.454: INFO: The phase of Pod pod-secrets-3b55f2fc-16d0-42be-aa42-2169cbc55c2f is Running (Ready = true)
    Nov 15 16:30:12.454: INFO: Pod "pod-secrets-3b55f2fc-16d0-42be-aa42-2169cbc55c2f" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-7ded07ba-2131-4f18-9e4e-2ad9cd0766ed 11/15/23 16:30:12.59
    STEP: Updating secret s-test-opt-upd-dbb5ad0b-ae10-406c-a6cd-80a30cb11d01 11/15/23 16:30:12.615
    STEP: Creating secret with name s-test-opt-create-e8e65600-2984-42dc-9d6e-df0c3efbdc3e 11/15/23 16:30:12.632
    STEP: waiting to observe update in volume 11/15/23 16:30:12.649
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:31:32.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9533" for this suite. 11/15/23 16:31:32.738
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:31:32.77
Nov 15 16:31:32.770: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename dns 11/15/23 16:31:32.773
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:31:32.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:31:32.845
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 11/15/23 16:31:32.857
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 11/15/23 16:31:32.857
STEP: creating a pod to probe DNS 11/15/23 16:31:32.858
STEP: submitting the pod to kubernetes 11/15/23 16:31:32.859
Nov 15 16:31:32.894: INFO: Waiting up to 15m0s for pod "dns-test-a45b2d97-4bf5-44bf-b207-db3c9c2c7cff" in namespace "dns-5711" to be "running"
Nov 15 16:31:32.908: INFO: Pod "dns-test-a45b2d97-4bf5-44bf-b207-db3c9c2c7cff": Phase="Pending", Reason="", readiness=false. Elapsed: 14.59176ms
Nov 15 16:31:34.925: INFO: Pod "dns-test-a45b2d97-4bf5-44bf-b207-db3c9c2c7cff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030854302s
Nov 15 16:31:36.926: INFO: Pod "dns-test-a45b2d97-4bf5-44bf-b207-db3c9c2c7cff": Phase="Running", Reason="", readiness=true. Elapsed: 4.031899572s
Nov 15 16:31:36.926: INFO: Pod "dns-test-a45b2d97-4bf5-44bf-b207-db3c9c2c7cff" satisfied condition "running"
STEP: retrieving the pod 11/15/23 16:31:36.926
STEP: looking for the results for each expected name from probers 11/15/23 16:31:36.939
Nov 15 16:31:37.098: INFO: DNS probes using dns-5711/dns-test-a45b2d97-4bf5-44bf-b207-db3c9c2c7cff succeeded

STEP: deleting the pod 11/15/23 16:31:37.098
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Nov 15 16:31:37.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5711" for this suite. 11/15/23 16:31:37.166
------------------------------
• [4.421 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:31:32.77
    Nov 15 16:31:32.770: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename dns 11/15/23 16:31:32.773
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:31:32.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:31:32.845
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     11/15/23 16:31:32.857
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     11/15/23 16:31:32.857
    STEP: creating a pod to probe DNS 11/15/23 16:31:32.858
    STEP: submitting the pod to kubernetes 11/15/23 16:31:32.859
    Nov 15 16:31:32.894: INFO: Waiting up to 15m0s for pod "dns-test-a45b2d97-4bf5-44bf-b207-db3c9c2c7cff" in namespace "dns-5711" to be "running"
    Nov 15 16:31:32.908: INFO: Pod "dns-test-a45b2d97-4bf5-44bf-b207-db3c9c2c7cff": Phase="Pending", Reason="", readiness=false. Elapsed: 14.59176ms
    Nov 15 16:31:34.925: INFO: Pod "dns-test-a45b2d97-4bf5-44bf-b207-db3c9c2c7cff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030854302s
    Nov 15 16:31:36.926: INFO: Pod "dns-test-a45b2d97-4bf5-44bf-b207-db3c9c2c7cff": Phase="Running", Reason="", readiness=true. Elapsed: 4.031899572s
    Nov 15 16:31:36.926: INFO: Pod "dns-test-a45b2d97-4bf5-44bf-b207-db3c9c2c7cff" satisfied condition "running"
    STEP: retrieving the pod 11/15/23 16:31:36.926
    STEP: looking for the results for each expected name from probers 11/15/23 16:31:36.939
    Nov 15 16:31:37.098: INFO: DNS probes using dns-5711/dns-test-a45b2d97-4bf5-44bf-b207-db3c9c2c7cff succeeded

    STEP: deleting the pod 11/15/23 16:31:37.098
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:31:37.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5711" for this suite. 11/15/23 16:31:37.166
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:31:37.198
Nov 15 16:31:37.198: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename kubectl 11/15/23 16:31:37.2
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:31:37.25
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:31:37.262
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 11/15/23 16:31:37.274
Nov 15 16:31:37.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 create -f -'
Nov 15 16:31:37.648: INFO: stderr: ""
Nov 15 16:31:37.648: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 11/15/23 16:31:37.648
Nov 15 16:31:37.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 15 16:31:37.785: INFO: stderr: ""
Nov 15 16:31:37.785: INFO: stdout: "update-demo-nautilus-6r2df update-demo-nautilus-vjbgx "
Nov 15 16:31:37.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods update-demo-nautilus-6r2df -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 15 16:31:37.933: INFO: stderr: ""
Nov 15 16:31:37.933: INFO: stdout: ""
Nov 15 16:31:37.933: INFO: update-demo-nautilus-6r2df is created but not running
Nov 15 16:31:42.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 15 16:31:43.085: INFO: stderr: ""
Nov 15 16:31:43.085: INFO: stdout: "update-demo-nautilus-6r2df update-demo-nautilus-vjbgx "
Nov 15 16:31:43.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods update-demo-nautilus-6r2df -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 15 16:31:43.223: INFO: stderr: ""
Nov 15 16:31:43.223: INFO: stdout: "true"
Nov 15 16:31:43.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods update-demo-nautilus-6r2df -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 15 16:31:43.355: INFO: stderr: ""
Nov 15 16:31:43.355: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Nov 15 16:31:43.355: INFO: validating pod update-demo-nautilus-6r2df
Nov 15 16:31:43.445: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 15 16:31:43.445: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 15 16:31:43.445: INFO: update-demo-nautilus-6r2df is verified up and running
Nov 15 16:31:43.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods update-demo-nautilus-vjbgx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 15 16:31:43.582: INFO: stderr: ""
Nov 15 16:31:43.582: INFO: stdout: "true"
Nov 15 16:31:43.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods update-demo-nautilus-vjbgx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 15 16:31:43.732: INFO: stderr: ""
Nov 15 16:31:43.732: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Nov 15 16:31:43.732: INFO: validating pod update-demo-nautilus-vjbgx
Nov 15 16:31:43.797: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 15 16:31:43.797: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 15 16:31:43.797: INFO: update-demo-nautilus-vjbgx is verified up and running
STEP: scaling down the replication controller 11/15/23 16:31:43.797
Nov 15 16:31:43.803: INFO: scanned /root for discovery docs: <nil>
Nov 15 16:31:43.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Nov 15 16:31:44.986: INFO: stderr: ""
Nov 15 16:31:44.986: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 11/15/23 16:31:44.986
Nov 15 16:31:44.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 15 16:31:45.146: INFO: stderr: ""
Nov 15 16:31:45.146: INFO: stdout: "update-demo-nautilus-6r2df update-demo-nautilus-vjbgx "
STEP: Replicas for name=update-demo: expected=1 actual=2 11/15/23 16:31:45.146
Nov 15 16:31:50.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 15 16:31:50.346: INFO: stderr: ""
Nov 15 16:31:50.347: INFO: stdout: "update-demo-nautilus-vjbgx "
Nov 15 16:31:50.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods update-demo-nautilus-vjbgx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 15 16:31:50.478: INFO: stderr: ""
Nov 15 16:31:50.478: INFO: stdout: "true"
Nov 15 16:31:50.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods update-demo-nautilus-vjbgx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 15 16:31:50.634: INFO: stderr: ""
Nov 15 16:31:50.634: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Nov 15 16:31:50.634: INFO: validating pod update-demo-nautilus-vjbgx
Nov 15 16:31:50.656: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 15 16:31:50.656: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 15 16:31:50.656: INFO: update-demo-nautilus-vjbgx is verified up and running
STEP: scaling up the replication controller 11/15/23 16:31:50.656
Nov 15 16:31:50.661: INFO: scanned /root for discovery docs: <nil>
Nov 15 16:31:50.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Nov 15 16:31:51.869: INFO: stderr: ""
Nov 15 16:31:51.869: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 11/15/23 16:31:51.869
Nov 15 16:31:51.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 15 16:31:52.028: INFO: stderr: ""
Nov 15 16:31:52.029: INFO: stdout: "update-demo-nautilus-qzpkq update-demo-nautilus-vjbgx "
Nov 15 16:31:52.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods update-demo-nautilus-qzpkq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 15 16:31:52.164: INFO: stderr: ""
Nov 15 16:31:52.164: INFO: stdout: ""
Nov 15 16:31:52.164: INFO: update-demo-nautilus-qzpkq is created but not running
Nov 15 16:31:57.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 15 16:31:57.338: INFO: stderr: ""
Nov 15 16:31:57.338: INFO: stdout: "update-demo-nautilus-qzpkq update-demo-nautilus-vjbgx "
Nov 15 16:31:57.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods update-demo-nautilus-qzpkq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 15 16:31:57.472: INFO: stderr: ""
Nov 15 16:31:57.472: INFO: stdout: "true"
Nov 15 16:31:57.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods update-demo-nautilus-qzpkq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 15 16:31:57.619: INFO: stderr: ""
Nov 15 16:31:57.619: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Nov 15 16:31:57.619: INFO: validating pod update-demo-nautilus-qzpkq
Nov 15 16:31:57.698: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 15 16:31:57.698: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 15 16:31:57.698: INFO: update-demo-nautilus-qzpkq is verified up and running
Nov 15 16:31:57.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods update-demo-nautilus-vjbgx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 15 16:31:57.849: INFO: stderr: ""
Nov 15 16:31:57.849: INFO: stdout: "true"
Nov 15 16:31:57.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods update-demo-nautilus-vjbgx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 15 16:31:58.010: INFO: stderr: ""
Nov 15 16:31:58.010: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Nov 15 16:31:58.010: INFO: validating pod update-demo-nautilus-vjbgx
Nov 15 16:31:58.032: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 15 16:31:58.032: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 15 16:31:58.032: INFO: update-demo-nautilus-vjbgx is verified up and running
STEP: using delete to clean up resources 11/15/23 16:31:58.033
Nov 15 16:31:58.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 delete --grace-period=0 --force -f -'
Nov 15 16:31:58.224: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 15 16:31:58.224: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov 15 16:31:58.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get rc,svc -l name=update-demo --no-headers'
Nov 15 16:31:58.417: INFO: stderr: "No resources found in kubectl-4082 namespace.\n"
Nov 15 16:31:58.417: INFO: stdout: ""
Nov 15 16:31:58.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 15 16:31:58.566: INFO: stderr: ""
Nov 15 16:31:58.566: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 15 16:31:58.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4082" for this suite. 11/15/23 16:31:58.585
------------------------------
• [SLOW TEST] [21.411 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:31:37.198
    Nov 15 16:31:37.198: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename kubectl 11/15/23 16:31:37.2
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:31:37.25
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:31:37.262
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 11/15/23 16:31:37.274
    Nov 15 16:31:37.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 create -f -'
    Nov 15 16:31:37.648: INFO: stderr: ""
    Nov 15 16:31:37.648: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 11/15/23 16:31:37.648
    Nov 15 16:31:37.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Nov 15 16:31:37.785: INFO: stderr: ""
    Nov 15 16:31:37.785: INFO: stdout: "update-demo-nautilus-6r2df update-demo-nautilus-vjbgx "
    Nov 15 16:31:37.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods update-demo-nautilus-6r2df -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 15 16:31:37.933: INFO: stderr: ""
    Nov 15 16:31:37.933: INFO: stdout: ""
    Nov 15 16:31:37.933: INFO: update-demo-nautilus-6r2df is created but not running
    Nov 15 16:31:42.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Nov 15 16:31:43.085: INFO: stderr: ""
    Nov 15 16:31:43.085: INFO: stdout: "update-demo-nautilus-6r2df update-demo-nautilus-vjbgx "
    Nov 15 16:31:43.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods update-demo-nautilus-6r2df -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 15 16:31:43.223: INFO: stderr: ""
    Nov 15 16:31:43.223: INFO: stdout: "true"
    Nov 15 16:31:43.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods update-demo-nautilus-6r2df -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Nov 15 16:31:43.355: INFO: stderr: ""
    Nov 15 16:31:43.355: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Nov 15 16:31:43.355: INFO: validating pod update-demo-nautilus-6r2df
    Nov 15 16:31:43.445: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Nov 15 16:31:43.445: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Nov 15 16:31:43.445: INFO: update-demo-nautilus-6r2df is verified up and running
    Nov 15 16:31:43.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods update-demo-nautilus-vjbgx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 15 16:31:43.582: INFO: stderr: ""
    Nov 15 16:31:43.582: INFO: stdout: "true"
    Nov 15 16:31:43.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods update-demo-nautilus-vjbgx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Nov 15 16:31:43.732: INFO: stderr: ""
    Nov 15 16:31:43.732: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Nov 15 16:31:43.732: INFO: validating pod update-demo-nautilus-vjbgx
    Nov 15 16:31:43.797: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Nov 15 16:31:43.797: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Nov 15 16:31:43.797: INFO: update-demo-nautilus-vjbgx is verified up and running
    STEP: scaling down the replication controller 11/15/23 16:31:43.797
    Nov 15 16:31:43.803: INFO: scanned /root for discovery docs: <nil>
    Nov 15 16:31:43.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Nov 15 16:31:44.986: INFO: stderr: ""
    Nov 15 16:31:44.986: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 11/15/23 16:31:44.986
    Nov 15 16:31:44.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Nov 15 16:31:45.146: INFO: stderr: ""
    Nov 15 16:31:45.146: INFO: stdout: "update-demo-nautilus-6r2df update-demo-nautilus-vjbgx "
    STEP: Replicas for name=update-demo: expected=1 actual=2 11/15/23 16:31:45.146
    Nov 15 16:31:50.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Nov 15 16:31:50.346: INFO: stderr: ""
    Nov 15 16:31:50.347: INFO: stdout: "update-demo-nautilus-vjbgx "
    Nov 15 16:31:50.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods update-demo-nautilus-vjbgx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 15 16:31:50.478: INFO: stderr: ""
    Nov 15 16:31:50.478: INFO: stdout: "true"
    Nov 15 16:31:50.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods update-demo-nautilus-vjbgx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Nov 15 16:31:50.634: INFO: stderr: ""
    Nov 15 16:31:50.634: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Nov 15 16:31:50.634: INFO: validating pod update-demo-nautilus-vjbgx
    Nov 15 16:31:50.656: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Nov 15 16:31:50.656: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Nov 15 16:31:50.656: INFO: update-demo-nautilus-vjbgx is verified up and running
    STEP: scaling up the replication controller 11/15/23 16:31:50.656
    Nov 15 16:31:50.661: INFO: scanned /root for discovery docs: <nil>
    Nov 15 16:31:50.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Nov 15 16:31:51.869: INFO: stderr: ""
    Nov 15 16:31:51.869: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 11/15/23 16:31:51.869
    Nov 15 16:31:51.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Nov 15 16:31:52.028: INFO: stderr: ""
    Nov 15 16:31:52.029: INFO: stdout: "update-demo-nautilus-qzpkq update-demo-nautilus-vjbgx "
    Nov 15 16:31:52.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods update-demo-nautilus-qzpkq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 15 16:31:52.164: INFO: stderr: ""
    Nov 15 16:31:52.164: INFO: stdout: ""
    Nov 15 16:31:52.164: INFO: update-demo-nautilus-qzpkq is created but not running
    Nov 15 16:31:57.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Nov 15 16:31:57.338: INFO: stderr: ""
    Nov 15 16:31:57.338: INFO: stdout: "update-demo-nautilus-qzpkq update-demo-nautilus-vjbgx "
    Nov 15 16:31:57.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods update-demo-nautilus-qzpkq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 15 16:31:57.472: INFO: stderr: ""
    Nov 15 16:31:57.472: INFO: stdout: "true"
    Nov 15 16:31:57.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods update-demo-nautilus-qzpkq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Nov 15 16:31:57.619: INFO: stderr: ""
    Nov 15 16:31:57.619: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Nov 15 16:31:57.619: INFO: validating pod update-demo-nautilus-qzpkq
    Nov 15 16:31:57.698: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Nov 15 16:31:57.698: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Nov 15 16:31:57.698: INFO: update-demo-nautilus-qzpkq is verified up and running
    Nov 15 16:31:57.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods update-demo-nautilus-vjbgx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 15 16:31:57.849: INFO: stderr: ""
    Nov 15 16:31:57.849: INFO: stdout: "true"
    Nov 15 16:31:57.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods update-demo-nautilus-vjbgx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Nov 15 16:31:58.010: INFO: stderr: ""
    Nov 15 16:31:58.010: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Nov 15 16:31:58.010: INFO: validating pod update-demo-nautilus-vjbgx
    Nov 15 16:31:58.032: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Nov 15 16:31:58.032: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Nov 15 16:31:58.032: INFO: update-demo-nautilus-vjbgx is verified up and running
    STEP: using delete to clean up resources 11/15/23 16:31:58.033
    Nov 15 16:31:58.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 delete --grace-period=0 --force -f -'
    Nov 15 16:31:58.224: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Nov 15 16:31:58.224: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Nov 15 16:31:58.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get rc,svc -l name=update-demo --no-headers'
    Nov 15 16:31:58.417: INFO: stderr: "No resources found in kubectl-4082 namespace.\n"
    Nov 15 16:31:58.417: INFO: stdout: ""
    Nov 15 16:31:58.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-4082 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Nov 15 16:31:58.566: INFO: stderr: ""
    Nov 15 16:31:58.566: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:31:58.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4082" for this suite. 11/15/23 16:31:58.585
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:31:58.61
Nov 15 16:31:58.610: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename statefulset 11/15/23 16:31:58.614
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:31:58.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:31:58.684
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7084 11/15/23 16:31:58.715
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 11/15/23 16:31:58.738
Nov 15 16:31:58.780: INFO: Found 0 stateful pods, waiting for 3
Nov 15 16:32:08.800: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 16:32:08.800: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 16:32:08.800: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 16:32:08.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-7084 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 15 16:32:09.338: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 15 16:32:09.338: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 15 16:32:09.338: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 11/15/23 16:32:19.428
Nov 15 16:32:19.476: INFO: Updating stateful set ss2
STEP: Creating a new revision 11/15/23 16:32:19.476
STEP: Updating Pods in reverse ordinal order 11/15/23 16:32:29.555
Nov 15 16:32:29.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-7084 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 15 16:32:29.984: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 15 16:32:29.984: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 15 16:32:29.984: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 15 16:32:40.075: INFO: Waiting for StatefulSet statefulset-7084/ss2 to complete update
Nov 15 16:32:40.075: INFO: Waiting for Pod statefulset-7084/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Nov 15 16:32:40.075: INFO: Waiting for Pod statefulset-7084/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Nov 15 16:32:50.140: INFO: Waiting for StatefulSet statefulset-7084/ss2 to complete update
Nov 15 16:32:50.140: INFO: Waiting for Pod statefulset-7084/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Rolling back to a previous revision 11/15/23 16:33:00.112
Nov 15 16:33:00.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-7084 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 15 16:33:00.547: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 15 16:33:00.547: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 15 16:33:00.547: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 15 16:33:10.665: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 11/15/23 16:33:20.763
Nov 15 16:33:20.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-7084 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 15 16:33:21.300: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 15 16:33:21.300: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 15 16:33:21.300: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Nov 15 16:33:31.424: INFO: Deleting all statefulset in ns statefulset-7084
Nov 15 16:33:31.442: INFO: Scaling statefulset ss2 to 0
Nov 15 16:33:41.518: INFO: Waiting for statefulset status.replicas updated to 0
Nov 15 16:33:41.534: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Nov 15 16:33:41.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7084" for this suite. 11/15/23 16:33:41.617
------------------------------
• [SLOW TEST] [103.038 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:31:58.61
    Nov 15 16:31:58.610: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename statefulset 11/15/23 16:31:58.614
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:31:58.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:31:58.684
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7084 11/15/23 16:31:58.715
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 11/15/23 16:31:58.738
    Nov 15 16:31:58.780: INFO: Found 0 stateful pods, waiting for 3
    Nov 15 16:32:08.800: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Nov 15 16:32:08.800: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Nov 15 16:32:08.800: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Nov 15 16:32:08.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-7084 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 15 16:32:09.338: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 15 16:32:09.338: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 15 16:32:09.338: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 11/15/23 16:32:19.428
    Nov 15 16:32:19.476: INFO: Updating stateful set ss2
    STEP: Creating a new revision 11/15/23 16:32:19.476
    STEP: Updating Pods in reverse ordinal order 11/15/23 16:32:29.555
    Nov 15 16:32:29.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-7084 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Nov 15 16:32:29.984: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Nov 15 16:32:29.984: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Nov 15 16:32:29.984: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Nov 15 16:32:40.075: INFO: Waiting for StatefulSet statefulset-7084/ss2 to complete update
    Nov 15 16:32:40.075: INFO: Waiting for Pod statefulset-7084/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Nov 15 16:32:40.075: INFO: Waiting for Pod statefulset-7084/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Nov 15 16:32:50.140: INFO: Waiting for StatefulSet statefulset-7084/ss2 to complete update
    Nov 15 16:32:50.140: INFO: Waiting for Pod statefulset-7084/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Rolling back to a previous revision 11/15/23 16:33:00.112
    Nov 15 16:33:00.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-7084 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 15 16:33:00.547: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 15 16:33:00.547: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 15 16:33:00.547: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Nov 15 16:33:10.665: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 11/15/23 16:33:20.763
    Nov 15 16:33:20.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=statefulset-7084 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Nov 15 16:33:21.300: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Nov 15 16:33:21.300: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Nov 15 16:33:21.300: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Nov 15 16:33:31.424: INFO: Deleting all statefulset in ns statefulset-7084
    Nov 15 16:33:31.442: INFO: Scaling statefulset ss2 to 0
    Nov 15 16:33:41.518: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 15 16:33:41.534: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:33:41.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7084" for this suite. 11/15/23 16:33:41.617
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:33:41.653
Nov 15 16:33:41.654: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename container-probe 11/15/23 16:33:41.658
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:33:41.715
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:33:41.725
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-902d4a3d-9e90-4d2a-af40-9ac9381c5af6 in namespace container-probe-4592 11/15/23 16:33:41.739
Nov 15 16:33:41.770: INFO: Waiting up to 5m0s for pod "busybox-902d4a3d-9e90-4d2a-af40-9ac9381c5af6" in namespace "container-probe-4592" to be "not pending"
Nov 15 16:33:41.783: INFO: Pod "busybox-902d4a3d-9e90-4d2a-af40-9ac9381c5af6": Phase="Pending", Reason="", readiness=false. Elapsed: 13.005425ms
Nov 15 16:33:43.801: INFO: Pod "busybox-902d4a3d-9e90-4d2a-af40-9ac9381c5af6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030450284s
Nov 15 16:33:45.803: INFO: Pod "busybox-902d4a3d-9e90-4d2a-af40-9ac9381c5af6": Phase="Running", Reason="", readiness=true. Elapsed: 4.033100098s
Nov 15 16:33:45.803: INFO: Pod "busybox-902d4a3d-9e90-4d2a-af40-9ac9381c5af6" satisfied condition "not pending"
Nov 15 16:33:45.803: INFO: Started pod busybox-902d4a3d-9e90-4d2a-af40-9ac9381c5af6 in namespace container-probe-4592
STEP: checking the pod's current state and verifying that restartCount is present 11/15/23 16:33:45.803
Nov 15 16:33:45.816: INFO: Initial restart count of pod busybox-902d4a3d-9e90-4d2a-af40-9ac9381c5af6 is 0
Nov 15 16:34:34.228: INFO: Restart count of pod container-probe-4592/busybox-902d4a3d-9e90-4d2a-af40-9ac9381c5af6 is now 1 (48.411012126s elapsed)
STEP: deleting the pod 11/15/23 16:34:34.228
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Nov 15 16:34:34.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-4592" for this suite. 11/15/23 16:34:34.297
------------------------------
• [SLOW TEST] [52.669 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:33:41.653
    Nov 15 16:33:41.654: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename container-probe 11/15/23 16:33:41.658
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:33:41.715
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:33:41.725
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-902d4a3d-9e90-4d2a-af40-9ac9381c5af6 in namespace container-probe-4592 11/15/23 16:33:41.739
    Nov 15 16:33:41.770: INFO: Waiting up to 5m0s for pod "busybox-902d4a3d-9e90-4d2a-af40-9ac9381c5af6" in namespace "container-probe-4592" to be "not pending"
    Nov 15 16:33:41.783: INFO: Pod "busybox-902d4a3d-9e90-4d2a-af40-9ac9381c5af6": Phase="Pending", Reason="", readiness=false. Elapsed: 13.005425ms
    Nov 15 16:33:43.801: INFO: Pod "busybox-902d4a3d-9e90-4d2a-af40-9ac9381c5af6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030450284s
    Nov 15 16:33:45.803: INFO: Pod "busybox-902d4a3d-9e90-4d2a-af40-9ac9381c5af6": Phase="Running", Reason="", readiness=true. Elapsed: 4.033100098s
    Nov 15 16:33:45.803: INFO: Pod "busybox-902d4a3d-9e90-4d2a-af40-9ac9381c5af6" satisfied condition "not pending"
    Nov 15 16:33:45.803: INFO: Started pod busybox-902d4a3d-9e90-4d2a-af40-9ac9381c5af6 in namespace container-probe-4592
    STEP: checking the pod's current state and verifying that restartCount is present 11/15/23 16:33:45.803
    Nov 15 16:33:45.816: INFO: Initial restart count of pod busybox-902d4a3d-9e90-4d2a-af40-9ac9381c5af6 is 0
    Nov 15 16:34:34.228: INFO: Restart count of pod container-probe-4592/busybox-902d4a3d-9e90-4d2a-af40-9ac9381c5af6 is now 1 (48.411012126s elapsed)
    STEP: deleting the pod 11/15/23 16:34:34.228
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:34:34.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-4592" for this suite. 11/15/23 16:34:34.297
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:34:34.324
Nov 15 16:34:34.324: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename proxy 11/15/23 16:34:34.326
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:34:34.379
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:34:34.39
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Nov 15 16:34:34.400: INFO: Creating pod...
Nov 15 16:34:34.429: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-4777" to be "running"
Nov 15 16:34:34.442: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 13.026493ms
Nov 15 16:34:36.463: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.033939515s
Nov 15 16:34:36.463: INFO: Pod "agnhost" satisfied condition "running"
Nov 15 16:34:36.463: INFO: Creating service...
Nov 15 16:34:36.510: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4777/pods/agnhost/proxy?method=DELETE
Nov 15 16:34:36.600: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Nov 15 16:34:36.600: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4777/pods/agnhost/proxy?method=OPTIONS
Nov 15 16:34:36.626: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Nov 15 16:34:36.626: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4777/pods/agnhost/proxy?method=PATCH
Nov 15 16:34:36.662: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Nov 15 16:34:36.662: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4777/pods/agnhost/proxy?method=POST
Nov 15 16:34:36.688: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Nov 15 16:34:36.688: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4777/pods/agnhost/proxy?method=PUT
Nov 15 16:34:36.713: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Nov 15 16:34:36.713: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4777/services/e2e-proxy-test-service/proxy?method=DELETE
Nov 15 16:34:36.750: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Nov 15 16:34:36.750: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4777/services/e2e-proxy-test-service/proxy?method=OPTIONS
Nov 15 16:34:36.789: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Nov 15 16:34:36.789: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4777/services/e2e-proxy-test-service/proxy?method=PATCH
Nov 15 16:34:36.844: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Nov 15 16:34:36.844: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4777/services/e2e-proxy-test-service/proxy?method=POST
Nov 15 16:34:36.883: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Nov 15 16:34:36.883: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4777/services/e2e-proxy-test-service/proxy?method=PUT
Nov 15 16:34:36.922: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Nov 15 16:34:36.922: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4777/pods/agnhost/proxy?method=GET
Nov 15 16:34:36.934: INFO: http.Client request:GET StatusCode:301
Nov 15 16:34:36.934: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4777/services/e2e-proxy-test-service/proxy?method=GET
Nov 15 16:34:36.956: INFO: http.Client request:GET StatusCode:301
Nov 15 16:34:36.956: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4777/pods/agnhost/proxy?method=HEAD
Nov 15 16:34:36.980: INFO: http.Client request:HEAD StatusCode:301
Nov 15 16:34:36.980: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4777/services/e2e-proxy-test-service/proxy?method=HEAD
Nov 15 16:34:37.003: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Nov 15 16:34:37.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-4777" for this suite. 11/15/23 16:34:37.025
------------------------------
• [2.726 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:34:34.324
    Nov 15 16:34:34.324: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename proxy 11/15/23 16:34:34.326
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:34:34.379
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:34:34.39
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Nov 15 16:34:34.400: INFO: Creating pod...
    Nov 15 16:34:34.429: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-4777" to be "running"
    Nov 15 16:34:34.442: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 13.026493ms
    Nov 15 16:34:36.463: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.033939515s
    Nov 15 16:34:36.463: INFO: Pod "agnhost" satisfied condition "running"
    Nov 15 16:34:36.463: INFO: Creating service...
    Nov 15 16:34:36.510: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4777/pods/agnhost/proxy?method=DELETE
    Nov 15 16:34:36.600: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Nov 15 16:34:36.600: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4777/pods/agnhost/proxy?method=OPTIONS
    Nov 15 16:34:36.626: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Nov 15 16:34:36.626: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4777/pods/agnhost/proxy?method=PATCH
    Nov 15 16:34:36.662: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Nov 15 16:34:36.662: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4777/pods/agnhost/proxy?method=POST
    Nov 15 16:34:36.688: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Nov 15 16:34:36.688: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4777/pods/agnhost/proxy?method=PUT
    Nov 15 16:34:36.713: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Nov 15 16:34:36.713: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4777/services/e2e-proxy-test-service/proxy?method=DELETE
    Nov 15 16:34:36.750: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Nov 15 16:34:36.750: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4777/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Nov 15 16:34:36.789: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Nov 15 16:34:36.789: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4777/services/e2e-proxy-test-service/proxy?method=PATCH
    Nov 15 16:34:36.844: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Nov 15 16:34:36.844: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4777/services/e2e-proxy-test-service/proxy?method=POST
    Nov 15 16:34:36.883: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Nov 15 16:34:36.883: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4777/services/e2e-proxy-test-service/proxy?method=PUT
    Nov 15 16:34:36.922: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Nov 15 16:34:36.922: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4777/pods/agnhost/proxy?method=GET
    Nov 15 16:34:36.934: INFO: http.Client request:GET StatusCode:301
    Nov 15 16:34:36.934: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4777/services/e2e-proxy-test-service/proxy?method=GET
    Nov 15 16:34:36.956: INFO: http.Client request:GET StatusCode:301
    Nov 15 16:34:36.956: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4777/pods/agnhost/proxy?method=HEAD
    Nov 15 16:34:36.980: INFO: http.Client request:HEAD StatusCode:301
    Nov 15 16:34:36.980: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4777/services/e2e-proxy-test-service/proxy?method=HEAD
    Nov 15 16:34:37.003: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:34:37.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-4777" for this suite. 11/15/23 16:34:37.025
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:34:37.057
Nov 15 16:34:37.058: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename ingressclass 11/15/23 16:34:37.06
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:34:37.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:34:37.125
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 11/15/23 16:34:37.139
STEP: getting /apis/networking.k8s.io 11/15/23 16:34:37.15
STEP: getting /apis/networking.k8s.iov1 11/15/23 16:34:37.155
STEP: creating 11/15/23 16:34:37.161
STEP: getting 11/15/23 16:34:37.205
STEP: listing 11/15/23 16:34:37.219
STEP: watching 11/15/23 16:34:37.232
Nov 15 16:34:37.232: INFO: starting watch
STEP: patching 11/15/23 16:34:37.237
STEP: updating 11/15/23 16:34:37.254
Nov 15 16:34:37.271: INFO: waiting for watch events with expected annotations
Nov 15 16:34:37.271: INFO: saw patched and updated annotations
STEP: deleting 11/15/23 16:34:37.272
STEP: deleting a collection 11/15/23 16:34:37.314
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Nov 15 16:34:37.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-1017" for this suite. 11/15/23 16:34:37.413
------------------------------
• [0.381 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:34:37.057
    Nov 15 16:34:37.058: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename ingressclass 11/15/23 16:34:37.06
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:34:37.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:34:37.125
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 11/15/23 16:34:37.139
    STEP: getting /apis/networking.k8s.io 11/15/23 16:34:37.15
    STEP: getting /apis/networking.k8s.iov1 11/15/23 16:34:37.155
    STEP: creating 11/15/23 16:34:37.161
    STEP: getting 11/15/23 16:34:37.205
    STEP: listing 11/15/23 16:34:37.219
    STEP: watching 11/15/23 16:34:37.232
    Nov 15 16:34:37.232: INFO: starting watch
    STEP: patching 11/15/23 16:34:37.237
    STEP: updating 11/15/23 16:34:37.254
    Nov 15 16:34:37.271: INFO: waiting for watch events with expected annotations
    Nov 15 16:34:37.271: INFO: saw patched and updated annotations
    STEP: deleting 11/15/23 16:34:37.272
    STEP: deleting a collection 11/15/23 16:34:37.314
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:34:37.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-1017" for this suite. 11/15/23 16:34:37.413
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:34:37.442
Nov 15 16:34:37.442: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename svcaccounts 11/15/23 16:34:37.444
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:34:37.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:34:37.509
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Nov 15 16:34:37.532: INFO: Got root ca configmap in namespace "svcaccounts-3615"
Nov 15 16:34:37.555: INFO: Deleted root ca configmap in namespace "svcaccounts-3615"
STEP: waiting for a new root ca configmap created 11/15/23 16:34:38.057
Nov 15 16:34:38.071: INFO: Recreated root ca configmap in namespace "svcaccounts-3615"
Nov 15 16:34:38.087: INFO: Updated root ca configmap in namespace "svcaccounts-3615"
STEP: waiting for the root ca configmap reconciled 11/15/23 16:34:38.587
Nov 15 16:34:38.606: INFO: Reconciled root ca configmap in namespace "svcaccounts-3615"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Nov 15 16:34:38.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-3615" for this suite. 11/15/23 16:34:38.625
------------------------------
• [1.208 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:34:37.442
    Nov 15 16:34:37.442: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename svcaccounts 11/15/23 16:34:37.444
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:34:37.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:34:37.509
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Nov 15 16:34:37.532: INFO: Got root ca configmap in namespace "svcaccounts-3615"
    Nov 15 16:34:37.555: INFO: Deleted root ca configmap in namespace "svcaccounts-3615"
    STEP: waiting for a new root ca configmap created 11/15/23 16:34:38.057
    Nov 15 16:34:38.071: INFO: Recreated root ca configmap in namespace "svcaccounts-3615"
    Nov 15 16:34:38.087: INFO: Updated root ca configmap in namespace "svcaccounts-3615"
    STEP: waiting for the root ca configmap reconciled 11/15/23 16:34:38.587
    Nov 15 16:34:38.606: INFO: Reconciled root ca configmap in namespace "svcaccounts-3615"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:34:38.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-3615" for this suite. 11/15/23 16:34:38.625
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:34:38.658
Nov 15 16:34:38.658: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename statefulset 11/15/23 16:34:38.66
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:34:38.713
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:34:38.723
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-6044 11/15/23 16:34:38.735
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 11/15/23 16:34:38.759
Nov 15 16:34:38.798: INFO: Found 0 stateful pods, waiting for 3
Nov 15 16:34:48.814: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 16:34:48.814: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 16:34:48.815: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 11/15/23 16:34:48.866
Nov 15 16:34:48.914: INFO: Updating stateful set ss2
STEP: Creating a new revision 11/15/23 16:34:48.914
STEP: Not applying an update when the partition is greater than the number of replicas 11/15/23 16:34:58.99
STEP: Performing a canary update 11/15/23 16:34:58.99
Nov 15 16:34:59.042: INFO: Updating stateful set ss2
Nov 15 16:34:59.095: INFO: Waiting for Pod statefulset-6044/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 11/15/23 16:35:09.134
Nov 15 16:35:09.238: INFO: Found 1 stateful pods, waiting for 3
Nov 15 16:35:19.260: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 16:35:19.260: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 15 16:35:19.260: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 11/15/23 16:35:19.292
Nov 15 16:35:19.343: INFO: Updating stateful set ss2
Nov 15 16:35:19.374: INFO: Waiting for Pod statefulset-6044/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Nov 15 16:35:29.466: INFO: Updating stateful set ss2
Nov 15 16:35:29.498: INFO: Waiting for StatefulSet statefulset-6044/ss2 to complete update
Nov 15 16:35:29.498: INFO: Waiting for Pod statefulset-6044/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Nov 15 16:35:39.536: INFO: Deleting all statefulset in ns statefulset-6044
Nov 15 16:35:39.555: INFO: Scaling statefulset ss2 to 0
Nov 15 16:35:49.631: INFO: Waiting for statefulset status.replicas updated to 0
Nov 15 16:35:49.649: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Nov 15 16:35:49.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-6044" for this suite. 11/15/23 16:35:49.729
------------------------------
• [SLOW TEST] [71.101 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:34:38.658
    Nov 15 16:34:38.658: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename statefulset 11/15/23 16:34:38.66
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:34:38.713
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:34:38.723
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-6044 11/15/23 16:34:38.735
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 11/15/23 16:34:38.759
    Nov 15 16:34:38.798: INFO: Found 0 stateful pods, waiting for 3
    Nov 15 16:34:48.814: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Nov 15 16:34:48.814: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Nov 15 16:34:48.815: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 11/15/23 16:34:48.866
    Nov 15 16:34:48.914: INFO: Updating stateful set ss2
    STEP: Creating a new revision 11/15/23 16:34:48.914
    STEP: Not applying an update when the partition is greater than the number of replicas 11/15/23 16:34:58.99
    STEP: Performing a canary update 11/15/23 16:34:58.99
    Nov 15 16:34:59.042: INFO: Updating stateful set ss2
    Nov 15 16:34:59.095: INFO: Waiting for Pod statefulset-6044/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 11/15/23 16:35:09.134
    Nov 15 16:35:09.238: INFO: Found 1 stateful pods, waiting for 3
    Nov 15 16:35:19.260: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Nov 15 16:35:19.260: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Nov 15 16:35:19.260: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 11/15/23 16:35:19.292
    Nov 15 16:35:19.343: INFO: Updating stateful set ss2
    Nov 15 16:35:19.374: INFO: Waiting for Pod statefulset-6044/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Nov 15 16:35:29.466: INFO: Updating stateful set ss2
    Nov 15 16:35:29.498: INFO: Waiting for StatefulSet statefulset-6044/ss2 to complete update
    Nov 15 16:35:29.498: INFO: Waiting for Pod statefulset-6044/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Nov 15 16:35:39.536: INFO: Deleting all statefulset in ns statefulset-6044
    Nov 15 16:35:39.555: INFO: Scaling statefulset ss2 to 0
    Nov 15 16:35:49.631: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 15 16:35:49.649: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:35:49.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-6044" for this suite. 11/15/23 16:35:49.729
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:35:49.779
Nov 15 16:35:49.780: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename crd-publish-openapi 11/15/23 16:35:49.781
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:35:49.842
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:35:49.861
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 11/15/23 16:35:49.903
Nov 15 16:35:49.907: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 11/15/23 16:35:58.838
Nov 15 16:35:58.840: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
Nov 15 16:36:01.663: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 16:36:11.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8518" for this suite. 11/15/23 16:36:11.832
------------------------------
• [SLOW TEST] [22.083 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:35:49.779
    Nov 15 16:35:49.780: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename crd-publish-openapi 11/15/23 16:35:49.781
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:35:49.842
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:35:49.861
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 11/15/23 16:35:49.903
    Nov 15 16:35:49.907: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 11/15/23 16:35:58.838
    Nov 15 16:35:58.840: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    Nov 15 16:36:01.663: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:36:11.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8518" for this suite. 11/15/23 16:36:11.832
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:36:11.878
Nov 15 16:36:11.879: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename svcaccounts 11/15/23 16:36:11.88
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:36:11.938
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:36:11.951
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Nov 15 16:36:12.028: INFO: created pod pod-service-account-defaultsa
Nov 15 16:36:12.029: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Nov 15 16:36:12.047: INFO: created pod pod-service-account-mountsa
Nov 15 16:36:12.047: INFO: pod pod-service-account-mountsa service account token volume mount: true
Nov 15 16:36:12.065: INFO: created pod pod-service-account-nomountsa
Nov 15 16:36:12.065: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Nov 15 16:36:12.101: INFO: created pod pod-service-account-defaultsa-mountspec
Nov 15 16:36:12.101: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Nov 15 16:36:12.131: INFO: created pod pod-service-account-mountsa-mountspec
Nov 15 16:36:12.131: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Nov 15 16:36:12.150: INFO: created pod pod-service-account-nomountsa-mountspec
Nov 15 16:36:12.150: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Nov 15 16:36:12.170: INFO: created pod pod-service-account-defaultsa-nomountspec
Nov 15 16:36:12.170: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Nov 15 16:36:12.190: INFO: created pod pod-service-account-mountsa-nomountspec
Nov 15 16:36:12.190: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Nov 15 16:36:12.207: INFO: created pod pod-service-account-nomountsa-nomountspec
Nov 15 16:36:12.207: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Nov 15 16:36:12.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8871" for this suite. 11/15/23 16:36:12.228
------------------------------
• [0.374 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:36:11.878
    Nov 15 16:36:11.879: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename svcaccounts 11/15/23 16:36:11.88
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:36:11.938
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:36:11.951
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Nov 15 16:36:12.028: INFO: created pod pod-service-account-defaultsa
    Nov 15 16:36:12.029: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Nov 15 16:36:12.047: INFO: created pod pod-service-account-mountsa
    Nov 15 16:36:12.047: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Nov 15 16:36:12.065: INFO: created pod pod-service-account-nomountsa
    Nov 15 16:36:12.065: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Nov 15 16:36:12.101: INFO: created pod pod-service-account-defaultsa-mountspec
    Nov 15 16:36:12.101: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Nov 15 16:36:12.131: INFO: created pod pod-service-account-mountsa-mountspec
    Nov 15 16:36:12.131: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Nov 15 16:36:12.150: INFO: created pod pod-service-account-nomountsa-mountspec
    Nov 15 16:36:12.150: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Nov 15 16:36:12.170: INFO: created pod pod-service-account-defaultsa-nomountspec
    Nov 15 16:36:12.170: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Nov 15 16:36:12.190: INFO: created pod pod-service-account-mountsa-nomountspec
    Nov 15 16:36:12.190: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Nov 15 16:36:12.207: INFO: created pod pod-service-account-nomountsa-nomountspec
    Nov 15 16:36:12.207: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:36:12.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8871" for this suite. 11/15/23 16:36:12.228
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:36:12.266
Nov 15 16:36:12.266: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename limitrange 11/15/23 16:36:12.267
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:36:12.326
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:36:12.337
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 11/15/23 16:36:12.348
STEP: Setting up watch 11/15/23 16:36:12.348
STEP: Submitting a LimitRange 11/15/23 16:36:12.471
STEP: Verifying LimitRange creation was observed 11/15/23 16:36:12.489
STEP: Fetching the LimitRange to ensure it has proper values 11/15/23 16:36:12.49
Nov 15 16:36:12.508: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Nov 15 16:36:12.508: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 11/15/23 16:36:12.508
STEP: Ensuring Pod has resource requirements applied from LimitRange 11/15/23 16:36:12.525
Nov 15 16:36:12.539: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Nov 15 16:36:12.539: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 11/15/23 16:36:12.539
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 11/15/23 16:36:12.558
Nov 15 16:36:12.573: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Nov 15 16:36:12.573: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 11/15/23 16:36:12.573
STEP: Failing to create a Pod with more than max resources 11/15/23 16:36:12.581
STEP: Updating a LimitRange 11/15/23 16:36:12.588
STEP: Verifying LimitRange updating is effective 11/15/23 16:36:12.608
STEP: Creating a Pod with less than former min resources 11/15/23 16:36:14.623
STEP: Failing to create a Pod with more than max resources 11/15/23 16:36:14.652
STEP: Deleting a LimitRange 11/15/23 16:36:14.659
STEP: Verifying the LimitRange was deleted 11/15/23 16:36:14.689
Nov 15 16:36:19.707: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 11/15/23 16:36:19.707
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Nov 15 16:36:19.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-2069" for this suite. 11/15/23 16:36:19.761
------------------------------
• [SLOW TEST] [7.521 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:36:12.266
    Nov 15 16:36:12.266: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename limitrange 11/15/23 16:36:12.267
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:36:12.326
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:36:12.337
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 11/15/23 16:36:12.348
    STEP: Setting up watch 11/15/23 16:36:12.348
    STEP: Submitting a LimitRange 11/15/23 16:36:12.471
    STEP: Verifying LimitRange creation was observed 11/15/23 16:36:12.489
    STEP: Fetching the LimitRange to ensure it has proper values 11/15/23 16:36:12.49
    Nov 15 16:36:12.508: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Nov 15 16:36:12.508: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 11/15/23 16:36:12.508
    STEP: Ensuring Pod has resource requirements applied from LimitRange 11/15/23 16:36:12.525
    Nov 15 16:36:12.539: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Nov 15 16:36:12.539: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 11/15/23 16:36:12.539
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 11/15/23 16:36:12.558
    Nov 15 16:36:12.573: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Nov 15 16:36:12.573: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 11/15/23 16:36:12.573
    STEP: Failing to create a Pod with more than max resources 11/15/23 16:36:12.581
    STEP: Updating a LimitRange 11/15/23 16:36:12.588
    STEP: Verifying LimitRange updating is effective 11/15/23 16:36:12.608
    STEP: Creating a Pod with less than former min resources 11/15/23 16:36:14.623
    STEP: Failing to create a Pod with more than max resources 11/15/23 16:36:14.652
    STEP: Deleting a LimitRange 11/15/23 16:36:14.659
    STEP: Verifying the LimitRange was deleted 11/15/23 16:36:14.689
    Nov 15 16:36:19.707: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 11/15/23 16:36:19.707
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:36:19.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-2069" for this suite. 11/15/23 16:36:19.761
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:36:19.792
Nov 15 16:36:19.792: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename certificates 11/15/23 16:36:19.795
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:36:19.852
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:36:19.861
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 11/15/23 16:36:20.75
STEP: getting /apis/certificates.k8s.io 11/15/23 16:36:20.763
STEP: getting /apis/certificates.k8s.io/v1 11/15/23 16:36:20.766
STEP: creating 11/15/23 16:36:20.771
STEP: getting 11/15/23 16:36:20.821
STEP: listing 11/15/23 16:36:20.836
STEP: watching 11/15/23 16:36:20.853
Nov 15 16:36:20.853: INFO: starting watch
STEP: patching 11/15/23 16:36:20.861
STEP: updating 11/15/23 16:36:20.882
Nov 15 16:36:20.900: INFO: waiting for watch events with expected annotations
Nov 15 16:36:20.900: INFO: saw patched and updated annotations
STEP: getting /approval 11/15/23 16:36:20.9
STEP: patching /approval 11/15/23 16:36:20.93
STEP: updating /approval 11/15/23 16:36:20.949
STEP: getting /status 11/15/23 16:36:20.969
STEP: patching /status 11/15/23 16:36:20.984
STEP: updating /status 11/15/23 16:36:21.008
STEP: deleting 11/15/23 16:36:21.031
STEP: deleting a collection 11/15/23 16:36:21.083
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 16:36:21.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-4668" for this suite. 11/15/23 16:36:21.171
------------------------------
• [1.405 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:36:19.792
    Nov 15 16:36:19.792: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename certificates 11/15/23 16:36:19.795
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:36:19.852
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:36:19.861
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 11/15/23 16:36:20.75
    STEP: getting /apis/certificates.k8s.io 11/15/23 16:36:20.763
    STEP: getting /apis/certificates.k8s.io/v1 11/15/23 16:36:20.766
    STEP: creating 11/15/23 16:36:20.771
    STEP: getting 11/15/23 16:36:20.821
    STEP: listing 11/15/23 16:36:20.836
    STEP: watching 11/15/23 16:36:20.853
    Nov 15 16:36:20.853: INFO: starting watch
    STEP: patching 11/15/23 16:36:20.861
    STEP: updating 11/15/23 16:36:20.882
    Nov 15 16:36:20.900: INFO: waiting for watch events with expected annotations
    Nov 15 16:36:20.900: INFO: saw patched and updated annotations
    STEP: getting /approval 11/15/23 16:36:20.9
    STEP: patching /approval 11/15/23 16:36:20.93
    STEP: updating /approval 11/15/23 16:36:20.949
    STEP: getting /status 11/15/23 16:36:20.969
    STEP: patching /status 11/15/23 16:36:20.984
    STEP: updating /status 11/15/23 16:36:21.008
    STEP: deleting 11/15/23 16:36:21.031
    STEP: deleting a collection 11/15/23 16:36:21.083
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:36:21.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-4668" for this suite. 11/15/23 16:36:21.171
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:36:21.2
Nov 15 16:36:21.200: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename watch 11/15/23 16:36:21.203
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:36:21.263
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:36:21.277
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 11/15/23 16:36:21.287
STEP: creating a watch on configmaps with label B 11/15/23 16:36:21.292
STEP: creating a watch on configmaps with label A or B 11/15/23 16:36:21.298
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 11/15/23 16:36:21.308
Nov 15 16:36:21.323: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7420  d2378e2b-3c1c-4428-bdaf-e06280fa40d2 40774 0 2023-11-15 16:36:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-15 16:36:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 15 16:36:21.323: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7420  d2378e2b-3c1c-4428-bdaf-e06280fa40d2 40774 0 2023-11-15 16:36:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-15 16:36:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 11/15/23 16:36:21.324
Nov 15 16:36:21.349: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7420  d2378e2b-3c1c-4428-bdaf-e06280fa40d2 40775 0 2023-11-15 16:36:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-15 16:36:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 15 16:36:21.349: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7420  d2378e2b-3c1c-4428-bdaf-e06280fa40d2 40775 0 2023-11-15 16:36:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-15 16:36:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 11/15/23 16:36:21.349
Nov 15 16:36:21.386: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7420  d2378e2b-3c1c-4428-bdaf-e06280fa40d2 40776 0 2023-11-15 16:36:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-15 16:36:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 15 16:36:21.387: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7420  d2378e2b-3c1c-4428-bdaf-e06280fa40d2 40776 0 2023-11-15 16:36:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-15 16:36:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 11/15/23 16:36:21.387
Nov 15 16:36:21.409: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7420  d2378e2b-3c1c-4428-bdaf-e06280fa40d2 40777 0 2023-11-15 16:36:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-15 16:36:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 15 16:36:21.410: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7420  d2378e2b-3c1c-4428-bdaf-e06280fa40d2 40777 0 2023-11-15 16:36:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-15 16:36:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 11/15/23 16:36:21.41
Nov 15 16:36:21.426: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7420  dcc97718-c962-4471-b753-9e79d380e572 40778 0 2023-11-15 16:36:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-11-15 16:36:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 15 16:36:21.426: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7420  dcc97718-c962-4471-b753-9e79d380e572 40778 0 2023-11-15 16:36:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-11-15 16:36:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 11/15/23 16:36:31.427
Nov 15 16:36:31.453: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7420  dcc97718-c962-4471-b753-9e79d380e572 40822 0 2023-11-15 16:36:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-11-15 16:36:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 15 16:36:31.454: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7420  dcc97718-c962-4471-b753-9e79d380e572 40822 0 2023-11-15 16:36:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-11-15 16:36:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Nov 15 16:36:41.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-7420" for this suite. 11/15/23 16:36:41.474
------------------------------
• [SLOW TEST] [20.298 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:36:21.2
    Nov 15 16:36:21.200: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename watch 11/15/23 16:36:21.203
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:36:21.263
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:36:21.277
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 11/15/23 16:36:21.287
    STEP: creating a watch on configmaps with label B 11/15/23 16:36:21.292
    STEP: creating a watch on configmaps with label A or B 11/15/23 16:36:21.298
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 11/15/23 16:36:21.308
    Nov 15 16:36:21.323: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7420  d2378e2b-3c1c-4428-bdaf-e06280fa40d2 40774 0 2023-11-15 16:36:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-15 16:36:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 15 16:36:21.323: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7420  d2378e2b-3c1c-4428-bdaf-e06280fa40d2 40774 0 2023-11-15 16:36:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-15 16:36:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 11/15/23 16:36:21.324
    Nov 15 16:36:21.349: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7420  d2378e2b-3c1c-4428-bdaf-e06280fa40d2 40775 0 2023-11-15 16:36:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-15 16:36:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 15 16:36:21.349: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7420  d2378e2b-3c1c-4428-bdaf-e06280fa40d2 40775 0 2023-11-15 16:36:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-15 16:36:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 11/15/23 16:36:21.349
    Nov 15 16:36:21.386: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7420  d2378e2b-3c1c-4428-bdaf-e06280fa40d2 40776 0 2023-11-15 16:36:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-15 16:36:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 15 16:36:21.387: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7420  d2378e2b-3c1c-4428-bdaf-e06280fa40d2 40776 0 2023-11-15 16:36:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-15 16:36:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 11/15/23 16:36:21.387
    Nov 15 16:36:21.409: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7420  d2378e2b-3c1c-4428-bdaf-e06280fa40d2 40777 0 2023-11-15 16:36:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-15 16:36:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 15 16:36:21.410: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7420  d2378e2b-3c1c-4428-bdaf-e06280fa40d2 40777 0 2023-11-15 16:36:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-15 16:36:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 11/15/23 16:36:21.41
    Nov 15 16:36:21.426: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7420  dcc97718-c962-4471-b753-9e79d380e572 40778 0 2023-11-15 16:36:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-11-15 16:36:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 15 16:36:21.426: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7420  dcc97718-c962-4471-b753-9e79d380e572 40778 0 2023-11-15 16:36:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-11-15 16:36:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 11/15/23 16:36:31.427
    Nov 15 16:36:31.453: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7420  dcc97718-c962-4471-b753-9e79d380e572 40822 0 2023-11-15 16:36:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-11-15 16:36:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 15 16:36:31.454: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7420  dcc97718-c962-4471-b753-9e79d380e572 40822 0 2023-11-15 16:36:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-11-15 16:36:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:36:41.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-7420" for this suite. 11/15/23 16:36:41.474
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:36:41.508
Nov 15 16:36:41.509: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename dns 11/15/23 16:36:41.51
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:36:41.569
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:36:41.581
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 11/15/23 16:36:41.592
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3839.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3839.svc.cluster.local;sleep 1; done
 11/15/23 16:36:41.62
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3839.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3839.svc.cluster.local;sleep 1; done
 11/15/23 16:36:41.62
STEP: creating a pod to probe DNS 11/15/23 16:36:41.621
STEP: submitting the pod to kubernetes 11/15/23 16:36:41.621
Nov 15 16:36:41.655: INFO: Waiting up to 15m0s for pod "dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77" in namespace "dns-3839" to be "running"
Nov 15 16:36:41.669: INFO: Pod "dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77": Phase="Pending", Reason="", readiness=false. Elapsed: 14.204336ms
Nov 15 16:36:43.686: INFO: Pod "dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031017021s
Nov 15 16:36:45.686: INFO: Pod "dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77": Phase="Running", Reason="", readiness=true. Elapsed: 4.030889406s
Nov 15 16:36:45.686: INFO: Pod "dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77" satisfied condition "running"
STEP: retrieving the pod 11/15/23 16:36:45.686
STEP: looking for the results for each expected name from probers 11/15/23 16:36:45.7
Nov 15 16:36:45.784: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
Nov 15 16:36:45.811: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
Nov 15 16:36:45.836: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
Nov 15 16:36:45.891: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
Nov 15 16:36:45.917: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
Nov 15 16:36:45.947: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
Nov 15 16:36:45.971: INFO: Lookups using dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3839.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local jessie_udp@dns-test-service-2.dns-3839.svc.cluster.local]

Nov 15 16:36:50.996: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
Nov 15 16:36:51.024: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
Nov 15 16:36:51.099: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
Nov 15 16:36:51.122: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
Nov 15 16:36:51.177: INFO: Lookups using dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local]

Nov 15 16:36:55.998: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
Nov 15 16:36:56.023: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
Nov 15 16:36:56.094: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
Nov 15 16:36:56.118: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
Nov 15 16:36:56.166: INFO: Lookups using dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local]

Nov 15 16:37:00.998: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
Nov 15 16:37:01.021: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
Nov 15 16:37:01.130: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
Nov 15 16:37:01.164: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
Nov 15 16:37:01.230: INFO: Lookups using dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local]

Nov 15 16:37:06.005: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
Nov 15 16:37:06.029: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
Nov 15 16:37:06.100: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
Nov 15 16:37:06.124: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
Nov 15 16:37:06.171: INFO: Lookups using dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local]

Nov 15 16:37:11.012: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
Nov 15 16:37:11.043: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
Nov 15 16:37:11.170: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
Nov 15 16:37:11.205: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
Nov 15 16:37:11.286: INFO: Lookups using dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local]

Nov 15 16:37:16.185: INFO: DNS probes using dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77 succeeded

STEP: deleting the pod 11/15/23 16:37:16.186
STEP: deleting the test headless service 11/15/23 16:37:16.224
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Nov 15 16:37:16.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3839" for this suite. 11/15/23 16:37:16.3
------------------------------
• [SLOW TEST] [34.821 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:36:41.508
    Nov 15 16:36:41.509: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename dns 11/15/23 16:36:41.51
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:36:41.569
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:36:41.581
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 11/15/23 16:36:41.592
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3839.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3839.svc.cluster.local;sleep 1; done
     11/15/23 16:36:41.62
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3839.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3839.svc.cluster.local;sleep 1; done
     11/15/23 16:36:41.62
    STEP: creating a pod to probe DNS 11/15/23 16:36:41.621
    STEP: submitting the pod to kubernetes 11/15/23 16:36:41.621
    Nov 15 16:36:41.655: INFO: Waiting up to 15m0s for pod "dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77" in namespace "dns-3839" to be "running"
    Nov 15 16:36:41.669: INFO: Pod "dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77": Phase="Pending", Reason="", readiness=false. Elapsed: 14.204336ms
    Nov 15 16:36:43.686: INFO: Pod "dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031017021s
    Nov 15 16:36:45.686: INFO: Pod "dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77": Phase="Running", Reason="", readiness=true. Elapsed: 4.030889406s
    Nov 15 16:36:45.686: INFO: Pod "dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77" satisfied condition "running"
    STEP: retrieving the pod 11/15/23 16:36:45.686
    STEP: looking for the results for each expected name from probers 11/15/23 16:36:45.7
    Nov 15 16:36:45.784: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
    Nov 15 16:36:45.811: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
    Nov 15 16:36:45.836: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
    Nov 15 16:36:45.891: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
    Nov 15 16:36:45.917: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
    Nov 15 16:36:45.947: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
    Nov 15 16:36:45.971: INFO: Lookups using dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3839.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local jessie_udp@dns-test-service-2.dns-3839.svc.cluster.local]

    Nov 15 16:36:50.996: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
    Nov 15 16:36:51.024: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
    Nov 15 16:36:51.099: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
    Nov 15 16:36:51.122: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
    Nov 15 16:36:51.177: INFO: Lookups using dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local]

    Nov 15 16:36:55.998: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
    Nov 15 16:36:56.023: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
    Nov 15 16:36:56.094: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
    Nov 15 16:36:56.118: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
    Nov 15 16:36:56.166: INFO: Lookups using dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local]

    Nov 15 16:37:00.998: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
    Nov 15 16:37:01.021: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
    Nov 15 16:37:01.130: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
    Nov 15 16:37:01.164: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
    Nov 15 16:37:01.230: INFO: Lookups using dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local]

    Nov 15 16:37:06.005: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
    Nov 15 16:37:06.029: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
    Nov 15 16:37:06.100: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
    Nov 15 16:37:06.124: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
    Nov 15 16:37:06.171: INFO: Lookups using dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local]

    Nov 15 16:37:11.012: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
    Nov 15 16:37:11.043: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
    Nov 15 16:37:11.170: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
    Nov 15 16:37:11.205: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local from pod dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77: the server could not find the requested resource (get pods dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77)
    Nov 15 16:37:11.286: INFO: Lookups using dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3839.svc.cluster.local]

    Nov 15 16:37:16.185: INFO: DNS probes using dns-3839/dns-test-a878ed9c-a6c4-441b-ac50-bab051ce9c77 succeeded

    STEP: deleting the pod 11/15/23 16:37:16.186
    STEP: deleting the test headless service 11/15/23 16:37:16.224
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:37:16.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3839" for this suite. 11/15/23 16:37:16.3
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:37:16.339
Nov 15 16:37:16.340: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename container-runtime 11/15/23 16:37:16.342
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:37:16.395
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:37:16.406
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 11/15/23 16:37:16.418
STEP: wait for the container to reach Failed 11/15/23 16:37:16.451
STEP: get the container status 11/15/23 16:37:21.541
STEP: the container should be terminated 11/15/23 16:37:21.556
STEP: the termination message should be set 11/15/23 16:37:21.556
Nov 15 16:37:21.557: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 11/15/23 16:37:21.557
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Nov 15 16:37:21.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-4031" for this suite. 11/15/23 16:37:21.644
------------------------------
• [SLOW TEST] [5.334 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:37:16.339
    Nov 15 16:37:16.340: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename container-runtime 11/15/23 16:37:16.342
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:37:16.395
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:37:16.406
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 11/15/23 16:37:16.418
    STEP: wait for the container to reach Failed 11/15/23 16:37:16.451
    STEP: get the container status 11/15/23 16:37:21.541
    STEP: the container should be terminated 11/15/23 16:37:21.556
    STEP: the termination message should be set 11/15/23 16:37:21.556
    Nov 15 16:37:21.557: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 11/15/23 16:37:21.557
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:37:21.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-4031" for this suite. 11/15/23 16:37:21.644
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:37:21.674
Nov 15 16:37:21.674: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename container-probe 11/15/23 16:37:21.676
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:37:21.729
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:37:21.74
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-123a07fb-61a4-4b41-b5a9-63b543d0e63d in namespace container-probe-1364 11/15/23 16:37:21.752
Nov 15 16:37:21.786: INFO: Waiting up to 5m0s for pod "liveness-123a07fb-61a4-4b41-b5a9-63b543d0e63d" in namespace "container-probe-1364" to be "not pending"
Nov 15 16:37:21.802: INFO: Pod "liveness-123a07fb-61a4-4b41-b5a9-63b543d0e63d": Phase="Pending", Reason="", readiness=false. Elapsed: 16.201792ms
Nov 15 16:37:23.818: INFO: Pod "liveness-123a07fb-61a4-4b41-b5a9-63b543d0e63d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031964109s
Nov 15 16:37:25.819: INFO: Pod "liveness-123a07fb-61a4-4b41-b5a9-63b543d0e63d": Phase="Running", Reason="", readiness=true. Elapsed: 4.032292477s
Nov 15 16:37:25.819: INFO: Pod "liveness-123a07fb-61a4-4b41-b5a9-63b543d0e63d" satisfied condition "not pending"
Nov 15 16:37:25.819: INFO: Started pod liveness-123a07fb-61a4-4b41-b5a9-63b543d0e63d in namespace container-probe-1364
STEP: checking the pod's current state and verifying that restartCount is present 11/15/23 16:37:25.819
Nov 15 16:37:25.834: INFO: Initial restart count of pod liveness-123a07fb-61a4-4b41-b5a9-63b543d0e63d is 0
STEP: deleting the pod 11/15/23 16:41:26.069
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Nov 15 16:41:26.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1364" for this suite. 11/15/23 16:41:26.142
------------------------------
• [SLOW TEST] [244.498 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:37:21.674
    Nov 15 16:37:21.674: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename container-probe 11/15/23 16:37:21.676
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:37:21.729
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:37:21.74
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-123a07fb-61a4-4b41-b5a9-63b543d0e63d in namespace container-probe-1364 11/15/23 16:37:21.752
    Nov 15 16:37:21.786: INFO: Waiting up to 5m0s for pod "liveness-123a07fb-61a4-4b41-b5a9-63b543d0e63d" in namespace "container-probe-1364" to be "not pending"
    Nov 15 16:37:21.802: INFO: Pod "liveness-123a07fb-61a4-4b41-b5a9-63b543d0e63d": Phase="Pending", Reason="", readiness=false. Elapsed: 16.201792ms
    Nov 15 16:37:23.818: INFO: Pod "liveness-123a07fb-61a4-4b41-b5a9-63b543d0e63d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031964109s
    Nov 15 16:37:25.819: INFO: Pod "liveness-123a07fb-61a4-4b41-b5a9-63b543d0e63d": Phase="Running", Reason="", readiness=true. Elapsed: 4.032292477s
    Nov 15 16:37:25.819: INFO: Pod "liveness-123a07fb-61a4-4b41-b5a9-63b543d0e63d" satisfied condition "not pending"
    Nov 15 16:37:25.819: INFO: Started pod liveness-123a07fb-61a4-4b41-b5a9-63b543d0e63d in namespace container-probe-1364
    STEP: checking the pod's current state and verifying that restartCount is present 11/15/23 16:37:25.819
    Nov 15 16:37:25.834: INFO: Initial restart count of pod liveness-123a07fb-61a4-4b41-b5a9-63b543d0e63d is 0
    STEP: deleting the pod 11/15/23 16:41:26.069
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:41:26.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1364" for this suite. 11/15/23 16:41:26.142
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:41:26.176
Nov 15 16:41:26.176: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename pod-network-test 11/15/23 16:41:26.18
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:41:26.245
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:41:26.263
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-7603 11/15/23 16:41:26.283
STEP: creating a selector 11/15/23 16:41:26.283
STEP: Creating the service pods in kubernetes 11/15/23 16:41:26.284
Nov 15 16:41:26.284: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 15 16:41:26.416: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7603" to be "running and ready"
Nov 15 16:41:26.434: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.652487ms
Nov 15 16:41:26.435: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 16:41:28.453: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.037391345s
Nov 15 16:41:28.453: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 16:41:30.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.041255385s
Nov 15 16:41:30.457: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 16:41:32.453: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.037315133s
Nov 15 16:41:32.453: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 16:41:34.454: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.0381835s
Nov 15 16:41:34.454: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 16:41:36.454: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.037973282s
Nov 15 16:41:36.454: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 16:41:38.454: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.03809691s
Nov 15 16:41:38.454: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 16:41:40.453: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.037066925s
Nov 15 16:41:40.453: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 16:41:42.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.041010373s
Nov 15 16:41:42.457: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 16:41:44.454: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.038194385s
Nov 15 16:41:44.454: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 16:41:46.453: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.037570036s
Nov 15 16:41:46.454: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 16:41:48.454: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.038076547s
Nov 15 16:41:48.454: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Nov 15 16:41:48.454: INFO: Pod "netserver-0" satisfied condition "running and ready"
Nov 15 16:41:48.472: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7603" to be "running and ready"
Nov 15 16:41:48.489: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 16.658106ms
Nov 15 16:41:48.489: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Nov 15 16:41:48.489: INFO: Pod "netserver-1" satisfied condition "running and ready"
Nov 15 16:41:48.507: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7603" to be "running and ready"
Nov 15 16:41:48.525: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 17.786418ms
Nov 15 16:41:48.525: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Nov 15 16:41:48.525: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 11/15/23 16:41:48.543
Nov 15 16:41:48.588: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7603" to be "running"
Nov 15 16:41:48.610: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 21.913023ms
Nov 15 16:41:50.629: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.041213975s
Nov 15 16:41:50.629: INFO: Pod "test-container-pod" satisfied condition "running"
Nov 15 16:41:50.646: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-7603" to be "running"
Nov 15 16:41:50.664: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 17.720339ms
Nov 15 16:41:50.664: INFO: Pod "host-test-container-pod" satisfied condition "running"
Nov 15 16:41:50.681: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Nov 15 16:41:50.682: INFO: Going to poll 172.30.191.95 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Nov 15 16:41:50.699: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.191.95:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7603 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 16:41:50.699: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
Nov 15 16:41:50.701: INFO: ExecWithOptions: Clientset creation
Nov 15 16:41:50.701: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-7603/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.191.95%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Nov 15 16:41:51.026: INFO: Found all 1 expected endpoints: [netserver-0]
Nov 15 16:41:51.026: INFO: Going to poll 172.30.205.248 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Nov 15 16:41:51.045: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.205.248:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7603 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 16:41:51.045: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
Nov 15 16:41:51.046: INFO: ExecWithOptions: Clientset creation
Nov 15 16:41:51.046: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-7603/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.205.248%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Nov 15 16:41:51.360: INFO: Found all 1 expected endpoints: [netserver-1]
Nov 15 16:41:51.360: INFO: Going to poll 172.30.164.13 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Nov 15 16:41:51.378: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.164.13:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7603 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 16:41:51.378: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
Nov 15 16:41:51.380: INFO: ExecWithOptions: Clientset creation
Nov 15 16:41:51.380: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-7603/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.164.13%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Nov 15 16:41:51.701: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Nov 15 16:41:51.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-7603" for this suite. 11/15/23 16:41:51.727
------------------------------
• [SLOW TEST] [25.581 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:41:26.176
    Nov 15 16:41:26.176: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename pod-network-test 11/15/23 16:41:26.18
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:41:26.245
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:41:26.263
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-7603 11/15/23 16:41:26.283
    STEP: creating a selector 11/15/23 16:41:26.283
    STEP: Creating the service pods in kubernetes 11/15/23 16:41:26.284
    Nov 15 16:41:26.284: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Nov 15 16:41:26.416: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7603" to be "running and ready"
    Nov 15 16:41:26.434: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.652487ms
    Nov 15 16:41:26.435: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 16:41:28.453: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.037391345s
    Nov 15 16:41:28.453: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 16:41:30.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.041255385s
    Nov 15 16:41:30.457: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 16:41:32.453: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.037315133s
    Nov 15 16:41:32.453: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 16:41:34.454: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.0381835s
    Nov 15 16:41:34.454: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 16:41:36.454: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.037973282s
    Nov 15 16:41:36.454: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 16:41:38.454: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.03809691s
    Nov 15 16:41:38.454: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 16:41:40.453: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.037066925s
    Nov 15 16:41:40.453: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 16:41:42.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.041010373s
    Nov 15 16:41:42.457: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 16:41:44.454: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.038194385s
    Nov 15 16:41:44.454: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 16:41:46.453: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.037570036s
    Nov 15 16:41:46.454: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 16:41:48.454: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.038076547s
    Nov 15 16:41:48.454: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Nov 15 16:41:48.454: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Nov 15 16:41:48.472: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7603" to be "running and ready"
    Nov 15 16:41:48.489: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 16.658106ms
    Nov 15 16:41:48.489: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Nov 15 16:41:48.489: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Nov 15 16:41:48.507: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7603" to be "running and ready"
    Nov 15 16:41:48.525: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 17.786418ms
    Nov 15 16:41:48.525: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Nov 15 16:41:48.525: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 11/15/23 16:41:48.543
    Nov 15 16:41:48.588: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7603" to be "running"
    Nov 15 16:41:48.610: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 21.913023ms
    Nov 15 16:41:50.629: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.041213975s
    Nov 15 16:41:50.629: INFO: Pod "test-container-pod" satisfied condition "running"
    Nov 15 16:41:50.646: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-7603" to be "running"
    Nov 15 16:41:50.664: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 17.720339ms
    Nov 15 16:41:50.664: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Nov 15 16:41:50.681: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Nov 15 16:41:50.682: INFO: Going to poll 172.30.191.95 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Nov 15 16:41:50.699: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.191.95:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7603 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 16:41:50.699: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    Nov 15 16:41:50.701: INFO: ExecWithOptions: Clientset creation
    Nov 15 16:41:50.701: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-7603/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.191.95%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Nov 15 16:41:51.026: INFO: Found all 1 expected endpoints: [netserver-0]
    Nov 15 16:41:51.026: INFO: Going to poll 172.30.205.248 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Nov 15 16:41:51.045: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.205.248:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7603 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 16:41:51.045: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    Nov 15 16:41:51.046: INFO: ExecWithOptions: Clientset creation
    Nov 15 16:41:51.046: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-7603/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.205.248%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Nov 15 16:41:51.360: INFO: Found all 1 expected endpoints: [netserver-1]
    Nov 15 16:41:51.360: INFO: Going to poll 172.30.164.13 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Nov 15 16:41:51.378: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.164.13:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7603 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 16:41:51.378: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    Nov 15 16:41:51.380: INFO: ExecWithOptions: Clientset creation
    Nov 15 16:41:51.380: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-7603/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.164.13%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Nov 15 16:41:51.701: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:41:51.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-7603" for this suite. 11/15/23 16:41:51.727
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:41:51.759
Nov 15 16:41:51.759: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename downward-api 11/15/23 16:41:51.763
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:41:51.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:41:51.844
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 11/15/23 16:41:51.863
Nov 15 16:41:51.899: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7a86ea1d-f208-4a6c-b477-dff1db0f5a48" in namespace "downward-api-3475" to be "Succeeded or Failed"
Nov 15 16:41:51.920: INFO: Pod "downwardapi-volume-7a86ea1d-f208-4a6c-b477-dff1db0f5a48": Phase="Pending", Reason="", readiness=false. Elapsed: 20.1571ms
Nov 15 16:41:53.944: INFO: Pod "downwardapi-volume-7a86ea1d-f208-4a6c-b477-dff1db0f5a48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044130019s
Nov 15 16:41:55.938: INFO: Pod "downwardapi-volume-7a86ea1d-f208-4a6c-b477-dff1db0f5a48": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038366521s
Nov 15 16:41:57.939: INFO: Pod "downwardapi-volume-7a86ea1d-f208-4a6c-b477-dff1db0f5a48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039733385s
STEP: Saw pod success 11/15/23 16:41:57.939
Nov 15 16:41:57.940: INFO: Pod "downwardapi-volume-7a86ea1d-f208-4a6c-b477-dff1db0f5a48" satisfied condition "Succeeded or Failed"
Nov 15 16:41:57.961: INFO: Trying to get logs from node 10.15.40.115 pod downwardapi-volume-7a86ea1d-f208-4a6c-b477-dff1db0f5a48 container client-container: <nil>
STEP: delete the pod 11/15/23 16:41:58.113
Nov 15 16:41:58.167: INFO: Waiting for pod downwardapi-volume-7a86ea1d-f208-4a6c-b477-dff1db0f5a48 to disappear
Nov 15 16:41:58.192: INFO: Pod downwardapi-volume-7a86ea1d-f208-4a6c-b477-dff1db0f5a48 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Nov 15 16:41:58.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3475" for this suite. 11/15/23 16:41:58.226
------------------------------
• [SLOW TEST] [6.496 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:41:51.759
    Nov 15 16:41:51.759: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename downward-api 11/15/23 16:41:51.763
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:41:51.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:41:51.844
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 11/15/23 16:41:51.863
    Nov 15 16:41:51.899: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7a86ea1d-f208-4a6c-b477-dff1db0f5a48" in namespace "downward-api-3475" to be "Succeeded or Failed"
    Nov 15 16:41:51.920: INFO: Pod "downwardapi-volume-7a86ea1d-f208-4a6c-b477-dff1db0f5a48": Phase="Pending", Reason="", readiness=false. Elapsed: 20.1571ms
    Nov 15 16:41:53.944: INFO: Pod "downwardapi-volume-7a86ea1d-f208-4a6c-b477-dff1db0f5a48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044130019s
    Nov 15 16:41:55.938: INFO: Pod "downwardapi-volume-7a86ea1d-f208-4a6c-b477-dff1db0f5a48": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038366521s
    Nov 15 16:41:57.939: INFO: Pod "downwardapi-volume-7a86ea1d-f208-4a6c-b477-dff1db0f5a48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039733385s
    STEP: Saw pod success 11/15/23 16:41:57.939
    Nov 15 16:41:57.940: INFO: Pod "downwardapi-volume-7a86ea1d-f208-4a6c-b477-dff1db0f5a48" satisfied condition "Succeeded or Failed"
    Nov 15 16:41:57.961: INFO: Trying to get logs from node 10.15.40.115 pod downwardapi-volume-7a86ea1d-f208-4a6c-b477-dff1db0f5a48 container client-container: <nil>
    STEP: delete the pod 11/15/23 16:41:58.113
    Nov 15 16:41:58.167: INFO: Waiting for pod downwardapi-volume-7a86ea1d-f208-4a6c-b477-dff1db0f5a48 to disappear
    Nov 15 16:41:58.192: INFO: Pod downwardapi-volume-7a86ea1d-f208-4a6c-b477-dff1db0f5a48 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:41:58.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3475" for this suite. 11/15/23 16:41:58.226
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:41:58.265
Nov 15 16:41:58.266: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename dns 11/15/23 16:41:58.269
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:41:58.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:41:58.352
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3949.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-3949.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 11/15/23 16:41:58.373
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3949.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-3949.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 11/15/23 16:41:58.373
STEP: creating a pod to probe /etc/hosts 11/15/23 16:41:58.373
STEP: submitting the pod to kubernetes 11/15/23 16:41:58.374
Nov 15 16:41:58.415: INFO: Waiting up to 15m0s for pod "dns-test-f61a48b7-afb3-4245-9de3-de2be0e66c75" in namespace "dns-3949" to be "running"
Nov 15 16:41:58.444: INFO: Pod "dns-test-f61a48b7-afb3-4245-9de3-de2be0e66c75": Phase="Pending", Reason="", readiness=false. Elapsed: 28.830204ms
Nov 15 16:42:00.463: INFO: Pod "dns-test-f61a48b7-afb3-4245-9de3-de2be0e66c75": Phase="Running", Reason="", readiness=true. Elapsed: 2.047952555s
Nov 15 16:42:00.463: INFO: Pod "dns-test-f61a48b7-afb3-4245-9de3-de2be0e66c75" satisfied condition "running"
STEP: retrieving the pod 11/15/23 16:42:00.463
STEP: looking for the results for each expected name from probers 11/15/23 16:42:00.481
Nov 15 16:42:00.672: INFO: DNS probes using dns-3949/dns-test-f61a48b7-afb3-4245-9de3-de2be0e66c75 succeeded

STEP: deleting the pod 11/15/23 16:42:00.672
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Nov 15 16:42:00.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3949" for this suite. 11/15/23 16:42:00.753
------------------------------
• [2.516 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:41:58.265
    Nov 15 16:41:58.266: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename dns 11/15/23 16:41:58.269
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:41:58.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:41:58.352
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3949.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-3949.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     11/15/23 16:41:58.373
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3949.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-3949.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     11/15/23 16:41:58.373
    STEP: creating a pod to probe /etc/hosts 11/15/23 16:41:58.373
    STEP: submitting the pod to kubernetes 11/15/23 16:41:58.374
    Nov 15 16:41:58.415: INFO: Waiting up to 15m0s for pod "dns-test-f61a48b7-afb3-4245-9de3-de2be0e66c75" in namespace "dns-3949" to be "running"
    Nov 15 16:41:58.444: INFO: Pod "dns-test-f61a48b7-afb3-4245-9de3-de2be0e66c75": Phase="Pending", Reason="", readiness=false. Elapsed: 28.830204ms
    Nov 15 16:42:00.463: INFO: Pod "dns-test-f61a48b7-afb3-4245-9de3-de2be0e66c75": Phase="Running", Reason="", readiness=true. Elapsed: 2.047952555s
    Nov 15 16:42:00.463: INFO: Pod "dns-test-f61a48b7-afb3-4245-9de3-de2be0e66c75" satisfied condition "running"
    STEP: retrieving the pod 11/15/23 16:42:00.463
    STEP: looking for the results for each expected name from probers 11/15/23 16:42:00.481
    Nov 15 16:42:00.672: INFO: DNS probes using dns-3949/dns-test-f61a48b7-afb3-4245-9de3-de2be0e66c75 succeeded

    STEP: deleting the pod 11/15/23 16:42:00.672
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:42:00.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3949" for this suite. 11/15/23 16:42:00.753
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:42:00.784
Nov 15 16:42:00.785: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename webhook 11/15/23 16:42:00.788
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:42:00.85
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:42:00.867
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/15/23 16:42:00.947
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 16:42:01.613
STEP: Deploying the webhook pod 11/15/23 16:42:01.646
STEP: Wait for the deployment to be ready 11/15/23 16:42:01.691
Nov 15 16:42:01.731: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/15/23 16:42:03.808
STEP: Verifying the service has paired with the endpoint 11/15/23 16:42:03.86
Nov 15 16:42:04.862: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 11/15/23 16:42:05.237
STEP: Creating a configMap that should be mutated 11/15/23 16:42:05.359
STEP: Deleting the collection of validation webhooks 11/15/23 16:42:05.583
STEP: Creating a configMap that should not be mutated 11/15/23 16:42:05.857
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 16:42:05.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9916" for this suite. 11/15/23 16:42:06.102
STEP: Destroying namespace "webhook-9916-markers" for this suite. 11/15/23 16:42:06.132
------------------------------
• [SLOW TEST] [5.376 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:42:00.784
    Nov 15 16:42:00.785: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename webhook 11/15/23 16:42:00.788
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:42:00.85
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:42:00.867
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/15/23 16:42:00.947
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 16:42:01.613
    STEP: Deploying the webhook pod 11/15/23 16:42:01.646
    STEP: Wait for the deployment to be ready 11/15/23 16:42:01.691
    Nov 15 16:42:01.731: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/15/23 16:42:03.808
    STEP: Verifying the service has paired with the endpoint 11/15/23 16:42:03.86
    Nov 15 16:42:04.862: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 11/15/23 16:42:05.237
    STEP: Creating a configMap that should be mutated 11/15/23 16:42:05.359
    STEP: Deleting the collection of validation webhooks 11/15/23 16:42:05.583
    STEP: Creating a configMap that should not be mutated 11/15/23 16:42:05.857
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:42:05.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9916" for this suite. 11/15/23 16:42:06.102
    STEP: Destroying namespace "webhook-9916-markers" for this suite. 11/15/23 16:42:06.132
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:42:06.171
Nov 15 16:42:06.171: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename daemonsets 11/15/23 16:42:06.172
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:42:06.24
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:42:06.258
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205
Nov 15 16:42:06.387: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 11/15/23 16:42:06.408
Nov 15 16:42:06.426: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 16:42:06.426: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 11/15/23 16:42:06.426
Nov 15 16:42:06.554: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 16:42:06.554: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
Nov 15 16:42:07.573: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 16:42:07.573: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
Nov 15 16:42:08.576: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Nov 15 16:42:08.576: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 11/15/23 16:42:08.594
Nov 15 16:42:08.677: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Nov 15 16:42:08.677: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Nov 15 16:42:09.697: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 16:42:09.697: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 11/15/23 16:42:09.697
Nov 15 16:42:09.738: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 16:42:09.738: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
Nov 15 16:42:10.757: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 16:42:10.758: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
Nov 15 16:42:11.758: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 16:42:11.758: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
Nov 15 16:42:12.757: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 16:42:12.757: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
Nov 15 16:42:13.757: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Nov 15 16:42:13.757: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 11/15/23 16:42:13.795
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2185, will wait for the garbage collector to delete the pods 11/15/23 16:42:13.795
Nov 15 16:42:13.894: INFO: Deleting DaemonSet.extensions daemon-set took: 29.803465ms
Nov 15 16:42:13.995: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.274792ms
Nov 15 16:42:16.714: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 16:42:16.714: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Nov 15 16:42:16.732: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"41720"},"items":null}

Nov 15 16:42:16.750: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"41720"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 16:42:16.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-2185" for this suite. 11/15/23 16:42:16.893
------------------------------
• [SLOW TEST] [10.751 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:42:06.171
    Nov 15 16:42:06.171: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename daemonsets 11/15/23 16:42:06.172
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:42:06.24
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:42:06.258
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:205
    Nov 15 16:42:06.387: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 11/15/23 16:42:06.408
    Nov 15 16:42:06.426: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 16:42:06.426: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 11/15/23 16:42:06.426
    Nov 15 16:42:06.554: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 16:42:06.554: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
    Nov 15 16:42:07.573: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 16:42:07.573: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
    Nov 15 16:42:08.576: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Nov 15 16:42:08.576: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 11/15/23 16:42:08.594
    Nov 15 16:42:08.677: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Nov 15 16:42:08.677: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Nov 15 16:42:09.697: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 16:42:09.697: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 11/15/23 16:42:09.697
    Nov 15 16:42:09.738: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 16:42:09.738: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
    Nov 15 16:42:10.757: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 16:42:10.758: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
    Nov 15 16:42:11.758: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 16:42:11.758: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
    Nov 15 16:42:12.757: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 16:42:12.757: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
    Nov 15 16:42:13.757: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Nov 15 16:42:13.757: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 11/15/23 16:42:13.795
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2185, will wait for the garbage collector to delete the pods 11/15/23 16:42:13.795
    Nov 15 16:42:13.894: INFO: Deleting DaemonSet.extensions daemon-set took: 29.803465ms
    Nov 15 16:42:13.995: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.274792ms
    Nov 15 16:42:16.714: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 16:42:16.714: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Nov 15 16:42:16.732: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"41720"},"items":null}

    Nov 15 16:42:16.750: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"41720"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:42:16.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-2185" for this suite. 11/15/23 16:42:16.893
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:42:16.943
Nov 15 16:42:16.943: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename projected 11/15/23 16:42:16.945
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:42:17.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:42:17.03
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-0bc107d1-ac6b-443a-bd66-2a598c68cef1 11/15/23 16:42:17.053
STEP: Creating a pod to test consume secrets 11/15/23 16:42:17.076
Nov 15 16:42:17.115: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a62b6f61-6f8c-4771-9b80-e6f7b87f503c" in namespace "projected-7661" to be "Succeeded or Failed"
Nov 15 16:42:17.133: INFO: Pod "pod-projected-secrets-a62b6f61-6f8c-4771-9b80-e6f7b87f503c": Phase="Pending", Reason="", readiness=false. Elapsed: 17.80269ms
Nov 15 16:42:19.152: INFO: Pod "pod-projected-secrets-a62b6f61-6f8c-4771-9b80-e6f7b87f503c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036722921s
Nov 15 16:42:21.152: INFO: Pod "pod-projected-secrets-a62b6f61-6f8c-4771-9b80-e6f7b87f503c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036972154s
Nov 15 16:42:23.152: INFO: Pod "pod-projected-secrets-a62b6f61-6f8c-4771-9b80-e6f7b87f503c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036662698s
STEP: Saw pod success 11/15/23 16:42:23.152
Nov 15 16:42:23.152: INFO: Pod "pod-projected-secrets-a62b6f61-6f8c-4771-9b80-e6f7b87f503c" satisfied condition "Succeeded or Failed"
Nov 15 16:42:23.170: INFO: Trying to get logs from node 10.15.40.115 pod pod-projected-secrets-a62b6f61-6f8c-4771-9b80-e6f7b87f503c container secret-volume-test: <nil>
STEP: delete the pod 11/15/23 16:42:23.211
Nov 15 16:42:23.254: INFO: Waiting for pod pod-projected-secrets-a62b6f61-6f8c-4771-9b80-e6f7b87f503c to disappear
Nov 15 16:42:23.272: INFO: Pod pod-projected-secrets-a62b6f61-6f8c-4771-9b80-e6f7b87f503c no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Nov 15 16:42:23.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7661" for this suite. 11/15/23 16:42:23.299
------------------------------
• [SLOW TEST] [6.385 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:42:16.943
    Nov 15 16:42:16.943: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename projected 11/15/23 16:42:16.945
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:42:17.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:42:17.03
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-0bc107d1-ac6b-443a-bd66-2a598c68cef1 11/15/23 16:42:17.053
    STEP: Creating a pod to test consume secrets 11/15/23 16:42:17.076
    Nov 15 16:42:17.115: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a62b6f61-6f8c-4771-9b80-e6f7b87f503c" in namespace "projected-7661" to be "Succeeded or Failed"
    Nov 15 16:42:17.133: INFO: Pod "pod-projected-secrets-a62b6f61-6f8c-4771-9b80-e6f7b87f503c": Phase="Pending", Reason="", readiness=false. Elapsed: 17.80269ms
    Nov 15 16:42:19.152: INFO: Pod "pod-projected-secrets-a62b6f61-6f8c-4771-9b80-e6f7b87f503c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036722921s
    Nov 15 16:42:21.152: INFO: Pod "pod-projected-secrets-a62b6f61-6f8c-4771-9b80-e6f7b87f503c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036972154s
    Nov 15 16:42:23.152: INFO: Pod "pod-projected-secrets-a62b6f61-6f8c-4771-9b80-e6f7b87f503c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036662698s
    STEP: Saw pod success 11/15/23 16:42:23.152
    Nov 15 16:42:23.152: INFO: Pod "pod-projected-secrets-a62b6f61-6f8c-4771-9b80-e6f7b87f503c" satisfied condition "Succeeded or Failed"
    Nov 15 16:42:23.170: INFO: Trying to get logs from node 10.15.40.115 pod pod-projected-secrets-a62b6f61-6f8c-4771-9b80-e6f7b87f503c container secret-volume-test: <nil>
    STEP: delete the pod 11/15/23 16:42:23.211
    Nov 15 16:42:23.254: INFO: Waiting for pod pod-projected-secrets-a62b6f61-6f8c-4771-9b80-e6f7b87f503c to disappear
    Nov 15 16:42:23.272: INFO: Pod pod-projected-secrets-a62b6f61-6f8c-4771-9b80-e6f7b87f503c no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:42:23.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7661" for this suite. 11/15/23 16:42:23.299
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:42:23.345
Nov 15 16:42:23.345: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename sched-preemption 11/15/23 16:42:23.346
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:42:23.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:42:23.426
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Nov 15 16:42:23.518: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 15 16:43:23.690: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 11/15/23 16:43:23.704
Nov 15 16:43:23.785: INFO: Created pod: pod0-0-sched-preemption-low-priority
Nov 15 16:43:23.805: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Nov 15 16:43:23.857: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Nov 15 16:43:23.881: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Nov 15 16:43:23.942: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Nov 15 16:43:23.964: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 11/15/23 16:43:23.964
Nov 15 16:43:23.965: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8586" to be "running"
Nov 15 16:43:23.978: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 13.468464ms
Nov 15 16:43:25.997: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031692186s
Nov 15 16:43:28.000: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.034997627s
Nov 15 16:43:28.000: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Nov 15 16:43:28.000: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8586" to be "running"
Nov 15 16:43:28.013: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 13.2646ms
Nov 15 16:43:28.013: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Nov 15 16:43:28.013: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-8586" to be "running"
Nov 15 16:43:28.029: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 15.578399ms
Nov 15 16:43:28.029: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Nov 15 16:43:28.029: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-8586" to be "running"
Nov 15 16:43:28.043: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 13.556798ms
Nov 15 16:43:28.043: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Nov 15 16:43:28.043: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-8586" to be "running"
Nov 15 16:43:28.058: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 14.892776ms
Nov 15 16:43:28.058: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Nov 15 16:43:28.058: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-8586" to be "running"
Nov 15 16:43:28.072: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 13.42183ms
Nov 15 16:43:28.072: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 11/15/23 16:43:28.072
Nov 15 16:43:28.102: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-8586" to be "running"
Nov 15 16:43:28.119: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 17.455471ms
Nov 15 16:43:30.133: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031510207s
Nov 15 16:43:32.135: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033202528s
Nov 15 16:43:34.134: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.032562966s
Nov 15 16:43:34.134: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 16:43:34.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-8586" for this suite. 11/15/23 16:43:34.468
------------------------------
• [SLOW TEST] [71.147 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:42:23.345
    Nov 15 16:42:23.345: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename sched-preemption 11/15/23 16:42:23.346
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:42:23.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:42:23.426
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Nov 15 16:42:23.518: INFO: Waiting up to 1m0s for all nodes to be ready
    Nov 15 16:43:23.690: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 11/15/23 16:43:23.704
    Nov 15 16:43:23.785: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Nov 15 16:43:23.805: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Nov 15 16:43:23.857: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Nov 15 16:43:23.881: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Nov 15 16:43:23.942: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Nov 15 16:43:23.964: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 11/15/23 16:43:23.964
    Nov 15 16:43:23.965: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8586" to be "running"
    Nov 15 16:43:23.978: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 13.468464ms
    Nov 15 16:43:25.997: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031692186s
    Nov 15 16:43:28.000: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.034997627s
    Nov 15 16:43:28.000: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Nov 15 16:43:28.000: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8586" to be "running"
    Nov 15 16:43:28.013: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 13.2646ms
    Nov 15 16:43:28.013: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Nov 15 16:43:28.013: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-8586" to be "running"
    Nov 15 16:43:28.029: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 15.578399ms
    Nov 15 16:43:28.029: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Nov 15 16:43:28.029: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-8586" to be "running"
    Nov 15 16:43:28.043: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 13.556798ms
    Nov 15 16:43:28.043: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Nov 15 16:43:28.043: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-8586" to be "running"
    Nov 15 16:43:28.058: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 14.892776ms
    Nov 15 16:43:28.058: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Nov 15 16:43:28.058: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-8586" to be "running"
    Nov 15 16:43:28.072: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 13.42183ms
    Nov 15 16:43:28.072: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 11/15/23 16:43:28.072
    Nov 15 16:43:28.102: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-8586" to be "running"
    Nov 15 16:43:28.119: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 17.455471ms
    Nov 15 16:43:30.133: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031510207s
    Nov 15 16:43:32.135: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033202528s
    Nov 15 16:43:34.134: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.032562966s
    Nov 15 16:43:34.134: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:43:34.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-8586" for this suite. 11/15/23 16:43:34.468
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:43:34.494
Nov 15 16:43:34.495: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename ingress 11/15/23 16:43:34.498
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:43:34.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:43:34.64
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 11/15/23 16:43:34.652
STEP: getting /apis/networking.k8s.io 11/15/23 16:43:34.662
STEP: getting /apis/networking.k8s.iov1 11/15/23 16:43:34.667
STEP: creating 11/15/23 16:43:34.672
STEP: getting 11/15/23 16:43:34.759
STEP: listing 11/15/23 16:43:34.778
STEP: watching 11/15/23 16:43:34.796
Nov 15 16:43:34.796: INFO: starting watch
STEP: cluster-wide listing 11/15/23 16:43:34.8
STEP: cluster-wide watching 11/15/23 16:43:34.817
Nov 15 16:43:34.817: INFO: starting watch
STEP: patching 11/15/23 16:43:34.822
STEP: updating 11/15/23 16:43:34.846
Nov 15 16:43:34.947: INFO: waiting for watch events with expected annotations
Nov 15 16:43:34.955: INFO: saw patched and updated annotations
STEP: patching /status 11/15/23 16:43:34.955
STEP: updating /status 11/15/23 16:43:34.991
STEP: get /status 11/15/23 16:43:35.031
STEP: deleting 11/15/23 16:43:35.058
STEP: deleting a collection 11/15/23 16:43:35.113
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Nov 15 16:43:35.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-1409" for this suite. 11/15/23 16:43:35.22
------------------------------
• [0.746 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:43:34.494
    Nov 15 16:43:34.495: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename ingress 11/15/23 16:43:34.498
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:43:34.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:43:34.64
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 11/15/23 16:43:34.652
    STEP: getting /apis/networking.k8s.io 11/15/23 16:43:34.662
    STEP: getting /apis/networking.k8s.iov1 11/15/23 16:43:34.667
    STEP: creating 11/15/23 16:43:34.672
    STEP: getting 11/15/23 16:43:34.759
    STEP: listing 11/15/23 16:43:34.778
    STEP: watching 11/15/23 16:43:34.796
    Nov 15 16:43:34.796: INFO: starting watch
    STEP: cluster-wide listing 11/15/23 16:43:34.8
    STEP: cluster-wide watching 11/15/23 16:43:34.817
    Nov 15 16:43:34.817: INFO: starting watch
    STEP: patching 11/15/23 16:43:34.822
    STEP: updating 11/15/23 16:43:34.846
    Nov 15 16:43:34.947: INFO: waiting for watch events with expected annotations
    Nov 15 16:43:34.955: INFO: saw patched and updated annotations
    STEP: patching /status 11/15/23 16:43:34.955
    STEP: updating /status 11/15/23 16:43:34.991
    STEP: get /status 11/15/23 16:43:35.031
    STEP: deleting 11/15/23 16:43:35.058
    STEP: deleting a collection 11/15/23 16:43:35.113
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:43:35.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-1409" for this suite. 11/15/23 16:43:35.22
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:43:35.242
Nov 15 16:43:35.242: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename projected 11/15/23 16:43:35.244
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:43:35.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:43:35.306
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 11/15/23 16:43:35.317
Nov 15 16:43:35.351: INFO: Waiting up to 5m0s for pod "downwardapi-volume-20c1254a-3f5a-4420-a45d-6b7e7f4741cb" in namespace "projected-4236" to be "Succeeded or Failed"
Nov 15 16:43:35.371: INFO: Pod "downwardapi-volume-20c1254a-3f5a-4420-a45d-6b7e7f4741cb": Phase="Pending", Reason="", readiness=false. Elapsed: 19.439077ms
Nov 15 16:43:37.386: INFO: Pod "downwardapi-volume-20c1254a-3f5a-4420-a45d-6b7e7f4741cb": Phase="Running", Reason="", readiness=true. Elapsed: 2.034878483s
Nov 15 16:43:39.391: INFO: Pod "downwardapi-volume-20c1254a-3f5a-4420-a45d-6b7e7f4741cb": Phase="Running", Reason="", readiness=false. Elapsed: 4.039541949s
Nov 15 16:43:41.393: INFO: Pod "downwardapi-volume-20c1254a-3f5a-4420-a45d-6b7e7f4741cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041865525s
STEP: Saw pod success 11/15/23 16:43:41.394
Nov 15 16:43:41.395: INFO: Pod "downwardapi-volume-20c1254a-3f5a-4420-a45d-6b7e7f4741cb" satisfied condition "Succeeded or Failed"
Nov 15 16:43:41.411: INFO: Trying to get logs from node 10.15.40.115 pod downwardapi-volume-20c1254a-3f5a-4420-a45d-6b7e7f4741cb container client-container: <nil>
STEP: delete the pod 11/15/23 16:43:41.456
Nov 15 16:43:41.502: INFO: Waiting for pod downwardapi-volume-20c1254a-3f5a-4420-a45d-6b7e7f4741cb to disappear
Nov 15 16:43:41.515: INFO: Pod downwardapi-volume-20c1254a-3f5a-4420-a45d-6b7e7f4741cb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Nov 15 16:43:41.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4236" for this suite. 11/15/23 16:43:41.536
------------------------------
• [SLOW TEST] [6.321 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:43:35.242
    Nov 15 16:43:35.242: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename projected 11/15/23 16:43:35.244
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:43:35.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:43:35.306
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 11/15/23 16:43:35.317
    Nov 15 16:43:35.351: INFO: Waiting up to 5m0s for pod "downwardapi-volume-20c1254a-3f5a-4420-a45d-6b7e7f4741cb" in namespace "projected-4236" to be "Succeeded or Failed"
    Nov 15 16:43:35.371: INFO: Pod "downwardapi-volume-20c1254a-3f5a-4420-a45d-6b7e7f4741cb": Phase="Pending", Reason="", readiness=false. Elapsed: 19.439077ms
    Nov 15 16:43:37.386: INFO: Pod "downwardapi-volume-20c1254a-3f5a-4420-a45d-6b7e7f4741cb": Phase="Running", Reason="", readiness=true. Elapsed: 2.034878483s
    Nov 15 16:43:39.391: INFO: Pod "downwardapi-volume-20c1254a-3f5a-4420-a45d-6b7e7f4741cb": Phase="Running", Reason="", readiness=false. Elapsed: 4.039541949s
    Nov 15 16:43:41.393: INFO: Pod "downwardapi-volume-20c1254a-3f5a-4420-a45d-6b7e7f4741cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041865525s
    STEP: Saw pod success 11/15/23 16:43:41.394
    Nov 15 16:43:41.395: INFO: Pod "downwardapi-volume-20c1254a-3f5a-4420-a45d-6b7e7f4741cb" satisfied condition "Succeeded or Failed"
    Nov 15 16:43:41.411: INFO: Trying to get logs from node 10.15.40.115 pod downwardapi-volume-20c1254a-3f5a-4420-a45d-6b7e7f4741cb container client-container: <nil>
    STEP: delete the pod 11/15/23 16:43:41.456
    Nov 15 16:43:41.502: INFO: Waiting for pod downwardapi-volume-20c1254a-3f5a-4420-a45d-6b7e7f4741cb to disappear
    Nov 15 16:43:41.515: INFO: Pod downwardapi-volume-20c1254a-3f5a-4420-a45d-6b7e7f4741cb no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:43:41.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4236" for this suite. 11/15/23 16:43:41.536
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:43:41.578
Nov 15 16:43:41.579: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename emptydir 11/15/23 16:43:41.58
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:43:41.631
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:43:41.642
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 11/15/23 16:43:41.654
Nov 15 16:43:41.687: INFO: Waiting up to 5m0s for pod "pod-06e55793-dd7a-463d-b5ec-2bc80b0769d6" in namespace "emptydir-8956" to be "Succeeded or Failed"
Nov 15 16:43:41.701: INFO: Pod "pod-06e55793-dd7a-463d-b5ec-2bc80b0769d6": Phase="Pending", Reason="", readiness=false. Elapsed: 14.10379ms
Nov 15 16:43:43.717: INFO: Pod "pod-06e55793-dd7a-463d-b5ec-2bc80b0769d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030236572s
Nov 15 16:43:45.718: INFO: Pod "pod-06e55793-dd7a-463d-b5ec-2bc80b0769d6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031087103s
Nov 15 16:43:47.717: INFO: Pod "pod-06e55793-dd7a-463d-b5ec-2bc80b0769d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029796156s
STEP: Saw pod success 11/15/23 16:43:47.717
Nov 15 16:43:47.717: INFO: Pod "pod-06e55793-dd7a-463d-b5ec-2bc80b0769d6" satisfied condition "Succeeded or Failed"
Nov 15 16:43:47.731: INFO: Trying to get logs from node 10.15.40.115 pod pod-06e55793-dd7a-463d-b5ec-2bc80b0769d6 container test-container: <nil>
STEP: delete the pod 11/15/23 16:43:47.773
Nov 15 16:43:47.821: INFO: Waiting for pod pod-06e55793-dd7a-463d-b5ec-2bc80b0769d6 to disappear
Nov 15 16:43:47.834: INFO: Pod pod-06e55793-dd7a-463d-b5ec-2bc80b0769d6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 15 16:43:47.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8956" for this suite. 11/15/23 16:43:47.885
------------------------------
• [SLOW TEST] [6.338 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:43:41.578
    Nov 15 16:43:41.579: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename emptydir 11/15/23 16:43:41.58
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:43:41.631
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:43:41.642
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 11/15/23 16:43:41.654
    Nov 15 16:43:41.687: INFO: Waiting up to 5m0s for pod "pod-06e55793-dd7a-463d-b5ec-2bc80b0769d6" in namespace "emptydir-8956" to be "Succeeded or Failed"
    Nov 15 16:43:41.701: INFO: Pod "pod-06e55793-dd7a-463d-b5ec-2bc80b0769d6": Phase="Pending", Reason="", readiness=false. Elapsed: 14.10379ms
    Nov 15 16:43:43.717: INFO: Pod "pod-06e55793-dd7a-463d-b5ec-2bc80b0769d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030236572s
    Nov 15 16:43:45.718: INFO: Pod "pod-06e55793-dd7a-463d-b5ec-2bc80b0769d6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031087103s
    Nov 15 16:43:47.717: INFO: Pod "pod-06e55793-dd7a-463d-b5ec-2bc80b0769d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029796156s
    STEP: Saw pod success 11/15/23 16:43:47.717
    Nov 15 16:43:47.717: INFO: Pod "pod-06e55793-dd7a-463d-b5ec-2bc80b0769d6" satisfied condition "Succeeded or Failed"
    Nov 15 16:43:47.731: INFO: Trying to get logs from node 10.15.40.115 pod pod-06e55793-dd7a-463d-b5ec-2bc80b0769d6 container test-container: <nil>
    STEP: delete the pod 11/15/23 16:43:47.773
    Nov 15 16:43:47.821: INFO: Waiting for pod pod-06e55793-dd7a-463d-b5ec-2bc80b0769d6 to disappear
    Nov 15 16:43:47.834: INFO: Pod pod-06e55793-dd7a-463d-b5ec-2bc80b0769d6 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:43:47.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8956" for this suite. 11/15/23 16:43:47.885
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:43:47.92
Nov 15 16:43:47.920: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename endpointslicemirroring 11/15/23 16:43:47.923
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:43:47.982
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:43:47.994
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 11/15/23 16:43:48.067
Nov 15 16:43:48.148: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 11/15/23 16:43:50.163
STEP: mirroring deletion of a custom Endpoint 11/15/23 16:43:50.203
Nov 15 16:43:50.249: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Nov 15 16:43:52.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-1766" for this suite. 11/15/23 16:43:52.283
------------------------------
• [4.389 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:43:47.92
    Nov 15 16:43:47.920: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename endpointslicemirroring 11/15/23 16:43:47.923
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:43:47.982
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:43:47.994
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 11/15/23 16:43:48.067
    Nov 15 16:43:48.148: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 11/15/23 16:43:50.163
    STEP: mirroring deletion of a custom Endpoint 11/15/23 16:43:50.203
    Nov 15 16:43:50.249: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:43:52.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-1766" for this suite. 11/15/23 16:43:52.283
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:43:52.313
Nov 15 16:43:52.313: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename security-context-test 11/15/23 16:43:52.318
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:43:52.375
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:43:52.387
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Nov 15 16:43:52.430: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-24d17a75-467b-4abc-b100-21d578237afd" in namespace "security-context-test-7061" to be "Succeeded or Failed"
Nov 15 16:43:52.443: INFO: Pod "alpine-nnp-false-24d17a75-467b-4abc-b100-21d578237afd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.962775ms
Nov 15 16:43:54.459: INFO: Pod "alpine-nnp-false-24d17a75-467b-4abc-b100-21d578237afd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028522517s
Nov 15 16:43:56.459: INFO: Pod "alpine-nnp-false-24d17a75-467b-4abc-b100-21d578237afd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028627689s
Nov 15 16:43:58.459: INFO: Pod "alpine-nnp-false-24d17a75-467b-4abc-b100-21d578237afd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02861011s
Nov 15 16:44:00.463: INFO: Pod "alpine-nnp-false-24d17a75-467b-4abc-b100-21d578237afd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.032449651s
Nov 15 16:44:00.463: INFO: Pod "alpine-nnp-false-24d17a75-467b-4abc-b100-21d578237afd" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Nov 15 16:44:00.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-7061" for this suite. 11/15/23 16:44:00.524
------------------------------
• [SLOW TEST] [8.238 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:43:52.313
    Nov 15 16:43:52.313: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename security-context-test 11/15/23 16:43:52.318
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:43:52.375
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:43:52.387
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Nov 15 16:43:52.430: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-24d17a75-467b-4abc-b100-21d578237afd" in namespace "security-context-test-7061" to be "Succeeded or Failed"
    Nov 15 16:43:52.443: INFO: Pod "alpine-nnp-false-24d17a75-467b-4abc-b100-21d578237afd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.962775ms
    Nov 15 16:43:54.459: INFO: Pod "alpine-nnp-false-24d17a75-467b-4abc-b100-21d578237afd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028522517s
    Nov 15 16:43:56.459: INFO: Pod "alpine-nnp-false-24d17a75-467b-4abc-b100-21d578237afd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028627689s
    Nov 15 16:43:58.459: INFO: Pod "alpine-nnp-false-24d17a75-467b-4abc-b100-21d578237afd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02861011s
    Nov 15 16:44:00.463: INFO: Pod "alpine-nnp-false-24d17a75-467b-4abc-b100-21d578237afd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.032449651s
    Nov 15 16:44:00.463: INFO: Pod "alpine-nnp-false-24d17a75-467b-4abc-b100-21d578237afd" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:44:00.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-7061" for this suite. 11/15/23 16:44:00.524
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:44:00.557
Nov 15 16:44:00.557: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename gc 11/15/23 16:44:00.559
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:44:00.632
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:44:00.644
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 11/15/23 16:44:00.655
STEP: Wait for the Deployment to create new ReplicaSet 11/15/23 16:44:00.674
STEP: delete the deployment 11/15/23 16:44:01.209
STEP: wait for all rs to be garbage collected 11/15/23 16:44:01.271
STEP: expected 0 rs, got 1 rs 11/15/23 16:44:01.305
STEP: expected 0 pods, got 2 pods 11/15/23 16:44:01.326
STEP: Gathering metrics 11/15/23 16:44:01.877
W1115 16:44:01.912959      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Nov 15 16:44:01.913: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Nov 15 16:44:01.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-3842" for this suite. 11/15/23 16:44:01.933
------------------------------
• [1.401 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:44:00.557
    Nov 15 16:44:00.557: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename gc 11/15/23 16:44:00.559
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:44:00.632
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:44:00.644
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 11/15/23 16:44:00.655
    STEP: Wait for the Deployment to create new ReplicaSet 11/15/23 16:44:00.674
    STEP: delete the deployment 11/15/23 16:44:01.209
    STEP: wait for all rs to be garbage collected 11/15/23 16:44:01.271
    STEP: expected 0 rs, got 1 rs 11/15/23 16:44:01.305
    STEP: expected 0 pods, got 2 pods 11/15/23 16:44:01.326
    STEP: Gathering metrics 11/15/23 16:44:01.877
    W1115 16:44:01.912959      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Nov 15 16:44:01.913: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:44:01.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-3842" for this suite. 11/15/23 16:44:01.933
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:44:01.977
Nov 15 16:44:01.977: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename statefulset 11/15/23 16:44:01.979
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:44:02.077
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:44:02.089
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8158 11/15/23 16:44:02.113
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 11/15/23 16:44:02.134
STEP: Creating pod with conflicting port in namespace statefulset-8158 11/15/23 16:44:02.154
STEP: Waiting until pod test-pod will start running in namespace statefulset-8158 11/15/23 16:44:02.194
Nov 15 16:44:02.194: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-8158" to be "running"
Nov 15 16:44:02.210: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 15.712961ms
Nov 15 16:44:04.224: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.029605827s
Nov 15 16:44:04.224: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-8158 11/15/23 16:44:04.224
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8158 11/15/23 16:44:04.244
Nov 15 16:44:04.287: INFO: Observed stateful pod in namespace: statefulset-8158, name: ss-0, uid: fc868b48-1375-4d5b-a3fe-2990787df179, status phase: Pending. Waiting for statefulset controller to delete.
Nov 15 16:44:04.350: INFO: Observed stateful pod in namespace: statefulset-8158, name: ss-0, uid: fc868b48-1375-4d5b-a3fe-2990787df179, status phase: Failed. Waiting for statefulset controller to delete.
Nov 15 16:44:04.377: INFO: Observed stateful pod in namespace: statefulset-8158, name: ss-0, uid: fc868b48-1375-4d5b-a3fe-2990787df179, status phase: Failed. Waiting for statefulset controller to delete.
Nov 15 16:44:04.394: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8158
STEP: Removing pod with conflicting port in namespace statefulset-8158 11/15/23 16:44:04.394
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8158 and will be in running state 11/15/23 16:44:04.451
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Nov 15 16:44:08.506: INFO: Deleting all statefulset in ns statefulset-8158
Nov 15 16:44:08.523: INFO: Scaling statefulset ss to 0
Nov 15 16:44:18.594: INFO: Waiting for statefulset status.replicas updated to 0
Nov 15 16:44:18.610: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Nov 15 16:44:18.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8158" for this suite. 11/15/23 16:44:18.689
------------------------------
• [SLOW TEST] [16.738 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:44:01.977
    Nov 15 16:44:01.977: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename statefulset 11/15/23 16:44:01.979
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:44:02.077
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:44:02.089
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8158 11/15/23 16:44:02.113
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 11/15/23 16:44:02.134
    STEP: Creating pod with conflicting port in namespace statefulset-8158 11/15/23 16:44:02.154
    STEP: Waiting until pod test-pod will start running in namespace statefulset-8158 11/15/23 16:44:02.194
    Nov 15 16:44:02.194: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-8158" to be "running"
    Nov 15 16:44:02.210: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 15.712961ms
    Nov 15 16:44:04.224: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.029605827s
    Nov 15 16:44:04.224: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-8158 11/15/23 16:44:04.224
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8158 11/15/23 16:44:04.244
    Nov 15 16:44:04.287: INFO: Observed stateful pod in namespace: statefulset-8158, name: ss-0, uid: fc868b48-1375-4d5b-a3fe-2990787df179, status phase: Pending. Waiting for statefulset controller to delete.
    Nov 15 16:44:04.350: INFO: Observed stateful pod in namespace: statefulset-8158, name: ss-0, uid: fc868b48-1375-4d5b-a3fe-2990787df179, status phase: Failed. Waiting for statefulset controller to delete.
    Nov 15 16:44:04.377: INFO: Observed stateful pod in namespace: statefulset-8158, name: ss-0, uid: fc868b48-1375-4d5b-a3fe-2990787df179, status phase: Failed. Waiting for statefulset controller to delete.
    Nov 15 16:44:04.394: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8158
    STEP: Removing pod with conflicting port in namespace statefulset-8158 11/15/23 16:44:04.394
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8158 and will be in running state 11/15/23 16:44:04.451
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Nov 15 16:44:08.506: INFO: Deleting all statefulset in ns statefulset-8158
    Nov 15 16:44:08.523: INFO: Scaling statefulset ss to 0
    Nov 15 16:44:18.594: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 15 16:44:18.610: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:44:18.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8158" for this suite. 11/15/23 16:44:18.689
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:44:18.718
Nov 15 16:44:18.718: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename gc 11/15/23 16:44:18.721
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:44:18.783
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:44:18.795
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Nov 15 16:44:18.893: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"9e2e309b-745c-4f04-87c8-d21f24a8a010", Controller:(*bool)(0xc0035b0a1e), BlockOwnerDeletion:(*bool)(0xc0035b0a1f)}}
Nov 15 16:44:18.911: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"9b1d1f3e-5c9c-43ad-bb67-fff6dfbceb5b", Controller:(*bool)(0xc000ab541a), BlockOwnerDeletion:(*bool)(0xc000ab541b)}}
Nov 15 16:44:18.933: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"44ee4373-3ed2-4976-a668-2ad5306fdce8", Controller:(*bool)(0xc0035b0c4a), BlockOwnerDeletion:(*bool)(0xc0035b0c4b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Nov 15 16:44:23.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6992" for this suite. 11/15/23 16:44:23.999
------------------------------
• [SLOW TEST] [5.310 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:44:18.718
    Nov 15 16:44:18.718: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename gc 11/15/23 16:44:18.721
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:44:18.783
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:44:18.795
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Nov 15 16:44:18.893: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"9e2e309b-745c-4f04-87c8-d21f24a8a010", Controller:(*bool)(0xc0035b0a1e), BlockOwnerDeletion:(*bool)(0xc0035b0a1f)}}
    Nov 15 16:44:18.911: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"9b1d1f3e-5c9c-43ad-bb67-fff6dfbceb5b", Controller:(*bool)(0xc000ab541a), BlockOwnerDeletion:(*bool)(0xc000ab541b)}}
    Nov 15 16:44:18.933: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"44ee4373-3ed2-4976-a668-2ad5306fdce8", Controller:(*bool)(0xc0035b0c4a), BlockOwnerDeletion:(*bool)(0xc0035b0c4b)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:44:23.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6992" for this suite. 11/15/23 16:44:23.999
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:44:24.033
Nov 15 16:44:24.033: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename custom-resource-definition 11/15/23 16:44:24.037
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:44:24.09
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:44:24.101
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Nov 15 16:44:24.112: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 16:44:24.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-4120" for this suite. 11/15/23 16:44:24.763
------------------------------
• [0.771 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:44:24.033
    Nov 15 16:44:24.033: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename custom-resource-definition 11/15/23 16:44:24.037
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:44:24.09
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:44:24.101
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Nov 15 16:44:24.112: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:44:24.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-4120" for this suite. 11/15/23 16:44:24.763
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:44:24.805
Nov 15 16:44:24.805: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename custom-resource-definition 11/15/23 16:44:24.808
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:44:24.858
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:44:24.871
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Nov 15 16:44:24.881: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 16:44:25.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-9754" for this suite. 11/15/23 16:44:25.979
------------------------------
• [1.199 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:44:24.805
    Nov 15 16:44:24.805: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename custom-resource-definition 11/15/23 16:44:24.808
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:44:24.858
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:44:24.871
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Nov 15 16:44:24.881: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:44:25.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-9754" for this suite. 11/15/23 16:44:25.979
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:44:26.009
Nov 15 16:44:26.010: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename runtimeclass 11/15/23 16:44:26.012
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:44:26.065
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:44:26.076
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Nov 15 16:44:26.134: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1528 to be scheduled
Nov 15 16:44:26.147: INFO: 1 pods are not scheduled: [runtimeclass-1528/test-runtimeclass-runtimeclass-1528-preconfigured-handler-jc892(df384cae-1b5b-4007-b807-bbcc3a820d86)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Nov 15 16:44:28.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-1528" for this suite. 11/15/23 16:44:28.233
------------------------------
• [2.249 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:44:26.009
    Nov 15 16:44:26.010: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename runtimeclass 11/15/23 16:44:26.012
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:44:26.065
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:44:26.076
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Nov 15 16:44:26.134: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1528 to be scheduled
    Nov 15 16:44:26.147: INFO: 1 pods are not scheduled: [runtimeclass-1528/test-runtimeclass-runtimeclass-1528-preconfigured-handler-jc892(df384cae-1b5b-4007-b807-bbcc3a820d86)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:44:28.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-1528" for this suite. 11/15/23 16:44:28.233
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:44:28.265
Nov 15 16:44:28.265: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename projected 11/15/23 16:44:28.267
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:44:28.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:44:28.341
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-5c66b896-4990-4ccd-9ad9-6fc5c81e91d0 11/15/23 16:44:28.353
STEP: Creating a pod to test consume configMaps 11/15/23 16:44:28.37
Nov 15 16:44:28.402: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c0e971f8-d220-4f69-9939-0c7bc725222b" in namespace "projected-1596" to be "Succeeded or Failed"
Nov 15 16:44:28.418: INFO: Pod "pod-projected-configmaps-c0e971f8-d220-4f69-9939-0c7bc725222b": Phase="Pending", Reason="", readiness=false. Elapsed: 15.618591ms
Nov 15 16:44:30.432: INFO: Pod "pod-projected-configmaps-c0e971f8-d220-4f69-9939-0c7bc725222b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030109871s
Nov 15 16:44:32.433: INFO: Pod "pod-projected-configmaps-c0e971f8-d220-4f69-9939-0c7bc725222b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031313132s
STEP: Saw pod success 11/15/23 16:44:32.434
Nov 15 16:44:32.434: INFO: Pod "pod-projected-configmaps-c0e971f8-d220-4f69-9939-0c7bc725222b" satisfied condition "Succeeded or Failed"
Nov 15 16:44:32.451: INFO: Trying to get logs from node 10.15.40.115 pod pod-projected-configmaps-c0e971f8-d220-4f69-9939-0c7bc725222b container agnhost-container: <nil>
STEP: delete the pod 11/15/23 16:44:32.492
Nov 15 16:44:32.529: INFO: Waiting for pod pod-projected-configmaps-c0e971f8-d220-4f69-9939-0c7bc725222b to disappear
Nov 15 16:44:32.542: INFO: Pod pod-projected-configmaps-c0e971f8-d220-4f69-9939-0c7bc725222b no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Nov 15 16:44:32.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1596" for this suite. 11/15/23 16:44:32.563
------------------------------
• [4.322 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:44:28.265
    Nov 15 16:44:28.265: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename projected 11/15/23 16:44:28.267
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:44:28.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:44:28.341
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-5c66b896-4990-4ccd-9ad9-6fc5c81e91d0 11/15/23 16:44:28.353
    STEP: Creating a pod to test consume configMaps 11/15/23 16:44:28.37
    Nov 15 16:44:28.402: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c0e971f8-d220-4f69-9939-0c7bc725222b" in namespace "projected-1596" to be "Succeeded or Failed"
    Nov 15 16:44:28.418: INFO: Pod "pod-projected-configmaps-c0e971f8-d220-4f69-9939-0c7bc725222b": Phase="Pending", Reason="", readiness=false. Elapsed: 15.618591ms
    Nov 15 16:44:30.432: INFO: Pod "pod-projected-configmaps-c0e971f8-d220-4f69-9939-0c7bc725222b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030109871s
    Nov 15 16:44:32.433: INFO: Pod "pod-projected-configmaps-c0e971f8-d220-4f69-9939-0c7bc725222b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031313132s
    STEP: Saw pod success 11/15/23 16:44:32.434
    Nov 15 16:44:32.434: INFO: Pod "pod-projected-configmaps-c0e971f8-d220-4f69-9939-0c7bc725222b" satisfied condition "Succeeded or Failed"
    Nov 15 16:44:32.451: INFO: Trying to get logs from node 10.15.40.115 pod pod-projected-configmaps-c0e971f8-d220-4f69-9939-0c7bc725222b container agnhost-container: <nil>
    STEP: delete the pod 11/15/23 16:44:32.492
    Nov 15 16:44:32.529: INFO: Waiting for pod pod-projected-configmaps-c0e971f8-d220-4f69-9939-0c7bc725222b to disappear
    Nov 15 16:44:32.542: INFO: Pod pod-projected-configmaps-c0e971f8-d220-4f69-9939-0c7bc725222b no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:44:32.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1596" for this suite. 11/15/23 16:44:32.563
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:44:32.59
Nov 15 16:44:32.590: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename replicaset 11/15/23 16:44:32.595
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:44:32.646
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:44:32.66
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Nov 15 16:44:32.670: INFO: Creating ReplicaSet my-hostname-basic-63290cef-1c9d-4b11-adfd-856f8e68e903
Nov 15 16:44:32.705: INFO: Pod name my-hostname-basic-63290cef-1c9d-4b11-adfd-856f8e68e903: Found 0 pods out of 1
Nov 15 16:44:37.753: INFO: Pod name my-hostname-basic-63290cef-1c9d-4b11-adfd-856f8e68e903: Found 1 pods out of 1
Nov 15 16:44:37.753: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-63290cef-1c9d-4b11-adfd-856f8e68e903" is running
Nov 15 16:44:37.754: INFO: Waiting up to 5m0s for pod "my-hostname-basic-63290cef-1c9d-4b11-adfd-856f8e68e903-bp25r" in namespace "replicaset-1931" to be "running"
Nov 15 16:44:37.773: INFO: Pod "my-hostname-basic-63290cef-1c9d-4b11-adfd-856f8e68e903-bp25r": Phase="Running", Reason="", readiness=true. Elapsed: 18.772383ms
Nov 15 16:44:37.773: INFO: Pod "my-hostname-basic-63290cef-1c9d-4b11-adfd-856f8e68e903-bp25r" satisfied condition "running"
Nov 15 16:44:37.773: INFO: Pod "my-hostname-basic-63290cef-1c9d-4b11-adfd-856f8e68e903-bp25r" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-15 16:44:32 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-15 16:44:34 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-15 16:44:34 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-15 16:44:32 +0000 UTC Reason: Message:}])
Nov 15 16:44:37.773: INFO: Trying to dial the pod
Nov 15 16:44:42.873: INFO: Controller my-hostname-basic-63290cef-1c9d-4b11-adfd-856f8e68e903: Got expected result from replica 1 [my-hostname-basic-63290cef-1c9d-4b11-adfd-856f8e68e903-bp25r]: "my-hostname-basic-63290cef-1c9d-4b11-adfd-856f8e68e903-bp25r", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Nov 15 16:44:42.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-1931" for this suite. 11/15/23 16:44:42.896
------------------------------
• [SLOW TEST] [10.331 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:44:32.59
    Nov 15 16:44:32.590: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename replicaset 11/15/23 16:44:32.595
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:44:32.646
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:44:32.66
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Nov 15 16:44:32.670: INFO: Creating ReplicaSet my-hostname-basic-63290cef-1c9d-4b11-adfd-856f8e68e903
    Nov 15 16:44:32.705: INFO: Pod name my-hostname-basic-63290cef-1c9d-4b11-adfd-856f8e68e903: Found 0 pods out of 1
    Nov 15 16:44:37.753: INFO: Pod name my-hostname-basic-63290cef-1c9d-4b11-adfd-856f8e68e903: Found 1 pods out of 1
    Nov 15 16:44:37.753: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-63290cef-1c9d-4b11-adfd-856f8e68e903" is running
    Nov 15 16:44:37.754: INFO: Waiting up to 5m0s for pod "my-hostname-basic-63290cef-1c9d-4b11-adfd-856f8e68e903-bp25r" in namespace "replicaset-1931" to be "running"
    Nov 15 16:44:37.773: INFO: Pod "my-hostname-basic-63290cef-1c9d-4b11-adfd-856f8e68e903-bp25r": Phase="Running", Reason="", readiness=true. Elapsed: 18.772383ms
    Nov 15 16:44:37.773: INFO: Pod "my-hostname-basic-63290cef-1c9d-4b11-adfd-856f8e68e903-bp25r" satisfied condition "running"
    Nov 15 16:44:37.773: INFO: Pod "my-hostname-basic-63290cef-1c9d-4b11-adfd-856f8e68e903-bp25r" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-15 16:44:32 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-15 16:44:34 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-15 16:44:34 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-15 16:44:32 +0000 UTC Reason: Message:}])
    Nov 15 16:44:37.773: INFO: Trying to dial the pod
    Nov 15 16:44:42.873: INFO: Controller my-hostname-basic-63290cef-1c9d-4b11-adfd-856f8e68e903: Got expected result from replica 1 [my-hostname-basic-63290cef-1c9d-4b11-adfd-856f8e68e903-bp25r]: "my-hostname-basic-63290cef-1c9d-4b11-adfd-856f8e68e903-bp25r", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:44:42.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-1931" for this suite. 11/15/23 16:44:42.896
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:44:42.925
Nov 15 16:44:42.925: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename gc 11/15/23 16:44:42.929
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:44:42.986
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:44:42.998
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 11/15/23 16:44:43.029
STEP: delete the rc 11/15/23 16:44:48.066
STEP: wait for the rc to be deleted 11/15/23 16:44:48.096
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 11/15/23 16:44:53.109
STEP: Gathering metrics 11/15/23 16:45:23.163
W1115 16:45:23.199279      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Nov 15 16:45:23.199: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Nov 15 16:45:23.199: INFO: Deleting pod "simpletest.rc-2kj4c" in namespace "gc-5182"
Nov 15 16:45:23.234: INFO: Deleting pod "simpletest.rc-4qf5t" in namespace "gc-5182"
Nov 15 16:45:23.274: INFO: Deleting pod "simpletest.rc-4rmz2" in namespace "gc-5182"
Nov 15 16:45:23.321: INFO: Deleting pod "simpletest.rc-5768r" in namespace "gc-5182"
Nov 15 16:45:23.357: INFO: Deleting pod "simpletest.rc-5px67" in namespace "gc-5182"
Nov 15 16:45:23.434: INFO: Deleting pod "simpletest.rc-6bnwk" in namespace "gc-5182"
Nov 15 16:45:23.472: INFO: Deleting pod "simpletest.rc-6ckk9" in namespace "gc-5182"
Nov 15 16:45:23.506: INFO: Deleting pod "simpletest.rc-6dzhd" in namespace "gc-5182"
Nov 15 16:45:23.560: INFO: Deleting pod "simpletest.rc-6f6tj" in namespace "gc-5182"
Nov 15 16:45:23.614: INFO: Deleting pod "simpletest.rc-6h5k4" in namespace "gc-5182"
Nov 15 16:45:23.663: INFO: Deleting pod "simpletest.rc-6tmqw" in namespace "gc-5182"
Nov 15 16:45:23.706: INFO: Deleting pod "simpletest.rc-76dd6" in namespace "gc-5182"
Nov 15 16:45:23.745: INFO: Deleting pod "simpletest.rc-79klc" in namespace "gc-5182"
Nov 15 16:45:23.790: INFO: Deleting pod "simpletest.rc-7hzjl" in namespace "gc-5182"
Nov 15 16:45:23.834: INFO: Deleting pod "simpletest.rc-7rlxh" in namespace "gc-5182"
Nov 15 16:45:23.876: INFO: Deleting pod "simpletest.rc-7wcjc" in namespace "gc-5182"
Nov 15 16:45:23.913: INFO: Deleting pod "simpletest.rc-7x4mm" in namespace "gc-5182"
Nov 15 16:45:23.955: INFO: Deleting pod "simpletest.rc-8b5vv" in namespace "gc-5182"
Nov 15 16:45:24.001: INFO: Deleting pod "simpletest.rc-8dn9s" in namespace "gc-5182"
Nov 15 16:45:24.039: INFO: Deleting pod "simpletest.rc-8k5h5" in namespace "gc-5182"
Nov 15 16:45:24.099: INFO: Deleting pod "simpletest.rc-8nf55" in namespace "gc-5182"
Nov 15 16:45:24.137: INFO: Deleting pod "simpletest.rc-8tjsc" in namespace "gc-5182"
Nov 15 16:45:24.183: INFO: Deleting pod "simpletest.rc-95fxs" in namespace "gc-5182"
Nov 15 16:45:24.217: INFO: Deleting pod "simpletest.rc-9cmc5" in namespace "gc-5182"
Nov 15 16:45:24.259: INFO: Deleting pod "simpletest.rc-9dzgv" in namespace "gc-5182"
Nov 15 16:45:24.299: INFO: Deleting pod "simpletest.rc-9pptf" in namespace "gc-5182"
Nov 15 16:45:24.353: INFO: Deleting pod "simpletest.rc-9t97q" in namespace "gc-5182"
Nov 15 16:45:24.389: INFO: Deleting pod "simpletest.rc-b5bmd" in namespace "gc-5182"
Nov 15 16:45:24.444: INFO: Deleting pod "simpletest.rc-b6mqx" in namespace "gc-5182"
Nov 15 16:45:24.479: INFO: Deleting pod "simpletest.rc-bfs92" in namespace "gc-5182"
Nov 15 16:45:24.521: INFO: Deleting pod "simpletest.rc-bl2sd" in namespace "gc-5182"
Nov 15 16:45:24.561: INFO: Deleting pod "simpletest.rc-bql4x" in namespace "gc-5182"
Nov 15 16:45:24.617: INFO: Deleting pod "simpletest.rc-bv4h5" in namespace "gc-5182"
Nov 15 16:45:24.654: INFO: Deleting pod "simpletest.rc-bvtf9" in namespace "gc-5182"
Nov 15 16:45:24.768: INFO: Deleting pod "simpletest.rc-bzwt2" in namespace "gc-5182"
Nov 15 16:45:24.837: INFO: Deleting pod "simpletest.rc-cpm7n" in namespace "gc-5182"
Nov 15 16:45:24.872: INFO: Deleting pod "simpletest.rc-cvcg8" in namespace "gc-5182"
Nov 15 16:45:24.916: INFO: Deleting pod "simpletest.rc-d6xmh" in namespace "gc-5182"
Nov 15 16:45:24.964: INFO: Deleting pod "simpletest.rc-dt4p5" in namespace "gc-5182"
Nov 15 16:45:25.017: INFO: Deleting pod "simpletest.rc-f4p5w" in namespace "gc-5182"
Nov 15 16:45:25.053: INFO: Deleting pod "simpletest.rc-f7xqm" in namespace "gc-5182"
Nov 15 16:45:25.107: INFO: Deleting pod "simpletest.rc-fbbzg" in namespace "gc-5182"
Nov 15 16:45:25.184: INFO: Deleting pod "simpletest.rc-fkq2n" in namespace "gc-5182"
Nov 15 16:45:25.242: INFO: Deleting pod "simpletest.rc-fqhjx" in namespace "gc-5182"
Nov 15 16:45:25.306: INFO: Deleting pod "simpletest.rc-fvwvj" in namespace "gc-5182"
Nov 15 16:45:25.359: INFO: Deleting pod "simpletest.rc-gbg86" in namespace "gc-5182"
Nov 15 16:45:25.401: INFO: Deleting pod "simpletest.rc-gfs6g" in namespace "gc-5182"
Nov 15 16:45:25.445: INFO: Deleting pod "simpletest.rc-hl84r" in namespace "gc-5182"
Nov 15 16:45:25.495: INFO: Deleting pod "simpletest.rc-j9l4h" in namespace "gc-5182"
Nov 15 16:45:25.529: INFO: Deleting pod "simpletest.rc-jjddp" in namespace "gc-5182"
Nov 15 16:45:25.566: INFO: Deleting pod "simpletest.rc-jnvhz" in namespace "gc-5182"
Nov 15 16:45:25.604: INFO: Deleting pod "simpletest.rc-kc8md" in namespace "gc-5182"
Nov 15 16:45:25.640: INFO: Deleting pod "simpletest.rc-knr7p" in namespace "gc-5182"
Nov 15 16:45:25.679: INFO: Deleting pod "simpletest.rc-kr297" in namespace "gc-5182"
Nov 15 16:45:25.712: INFO: Deleting pod "simpletest.rc-l22xh" in namespace "gc-5182"
Nov 15 16:45:25.756: INFO: Deleting pod "simpletest.rc-lsbq2" in namespace "gc-5182"
Nov 15 16:45:25.797: INFO: Deleting pod "simpletest.rc-mfqtw" in namespace "gc-5182"
Nov 15 16:45:25.851: INFO: Deleting pod "simpletest.rc-mg8nj" in namespace "gc-5182"
Nov 15 16:45:25.899: INFO: Deleting pod "simpletest.rc-mndx7" in namespace "gc-5182"
Nov 15 16:45:25.937: INFO: Deleting pod "simpletest.rc-n2rlh" in namespace "gc-5182"
Nov 15 16:45:25.983: INFO: Deleting pod "simpletest.rc-nfmzr" in namespace "gc-5182"
Nov 15 16:45:26.018: INFO: Deleting pod "simpletest.rc-nm2t8" in namespace "gc-5182"
Nov 15 16:45:26.067: INFO: Deleting pod "simpletest.rc-nmn9r" in namespace "gc-5182"
Nov 15 16:45:26.117: INFO: Deleting pod "simpletest.rc-nxllp" in namespace "gc-5182"
Nov 15 16:45:26.176: INFO: Deleting pod "simpletest.rc-nz9bf" in namespace "gc-5182"
Nov 15 16:45:26.222: INFO: Deleting pod "simpletest.rc-p75rd" in namespace "gc-5182"
Nov 15 16:45:26.273: INFO: Deleting pod "simpletest.rc-pdx49" in namespace "gc-5182"
Nov 15 16:45:26.306: INFO: Deleting pod "simpletest.rc-pr6w8" in namespace "gc-5182"
Nov 15 16:45:26.358: INFO: Deleting pod "simpletest.rc-qdl8l" in namespace "gc-5182"
Nov 15 16:45:26.408: INFO: Deleting pod "simpletest.rc-qf9ks" in namespace "gc-5182"
Nov 15 16:45:26.455: INFO: Deleting pod "simpletest.rc-qk2lg" in namespace "gc-5182"
Nov 15 16:45:26.496: INFO: Deleting pod "simpletest.rc-qnh46" in namespace "gc-5182"
Nov 15 16:45:26.541: INFO: Deleting pod "simpletest.rc-r55kp" in namespace "gc-5182"
Nov 15 16:45:26.577: INFO: Deleting pod "simpletest.rc-r9vmj" in namespace "gc-5182"
Nov 15 16:45:26.627: INFO: Deleting pod "simpletest.rc-rb69s" in namespace "gc-5182"
Nov 15 16:45:26.672: INFO: Deleting pod "simpletest.rc-rhh8l" in namespace "gc-5182"
Nov 15 16:45:26.717: INFO: Deleting pod "simpletest.rc-rm8hg" in namespace "gc-5182"
Nov 15 16:45:26.757: INFO: Deleting pod "simpletest.rc-rrsw9" in namespace "gc-5182"
Nov 15 16:45:26.807: INFO: Deleting pod "simpletest.rc-s76dr" in namespace "gc-5182"
Nov 15 16:45:26.852: INFO: Deleting pod "simpletest.rc-sn5cz" in namespace "gc-5182"
Nov 15 16:45:26.888: INFO: Deleting pod "simpletest.rc-tgc4c" in namespace "gc-5182"
Nov 15 16:45:26.927: INFO: Deleting pod "simpletest.rc-th9f9" in namespace "gc-5182"
Nov 15 16:45:26.978: INFO: Deleting pod "simpletest.rc-tsbhd" in namespace "gc-5182"
Nov 15 16:45:27.023: INFO: Deleting pod "simpletest.rc-vg5zm" in namespace "gc-5182"
Nov 15 16:45:27.071: INFO: Deleting pod "simpletest.rc-vhhs4" in namespace "gc-5182"
Nov 15 16:45:27.118: INFO: Deleting pod "simpletest.rc-w8x9w" in namespace "gc-5182"
Nov 15 16:45:27.167: INFO: Deleting pod "simpletest.rc-wl4j8" in namespace "gc-5182"
Nov 15 16:45:27.231: INFO: Deleting pod "simpletest.rc-wmwnh" in namespace "gc-5182"
Nov 15 16:45:27.282: INFO: Deleting pod "simpletest.rc-wxc2k" in namespace "gc-5182"
Nov 15 16:45:27.322: INFO: Deleting pod "simpletest.rc-wxhj2" in namespace "gc-5182"
Nov 15 16:45:27.366: INFO: Deleting pod "simpletest.rc-wxrmt" in namespace "gc-5182"
Nov 15 16:45:27.439: INFO: Deleting pod "simpletest.rc-wxz9j" in namespace "gc-5182"
Nov 15 16:45:27.492: INFO: Deleting pod "simpletest.rc-wzwck" in namespace "gc-5182"
Nov 15 16:45:27.529: INFO: Deleting pod "simpletest.rc-x7ks9" in namespace "gc-5182"
Nov 15 16:45:27.566: INFO: Deleting pod "simpletest.rc-x7p6z" in namespace "gc-5182"
Nov 15 16:45:27.607: INFO: Deleting pod "simpletest.rc-xfgsc" in namespace "gc-5182"
Nov 15 16:45:27.651: INFO: Deleting pod "simpletest.rc-z8pg5" in namespace "gc-5182"
Nov 15 16:45:27.690: INFO: Deleting pod "simpletest.rc-zltwg" in namespace "gc-5182"
Nov 15 16:45:27.742: INFO: Deleting pod "simpletest.rc-zslzd" in namespace "gc-5182"
Nov 15 16:45:27.789: INFO: Deleting pod "simpletest.rc-zxtdv" in namespace "gc-5182"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Nov 15 16:45:27.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-5182" for this suite. 11/15/23 16:45:27.852
------------------------------
• [SLOW TEST] [44.953 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:44:42.925
    Nov 15 16:44:42.925: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename gc 11/15/23 16:44:42.929
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:44:42.986
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:44:42.998
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 11/15/23 16:44:43.029
    STEP: delete the rc 11/15/23 16:44:48.066
    STEP: wait for the rc to be deleted 11/15/23 16:44:48.096
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 11/15/23 16:44:53.109
    STEP: Gathering metrics 11/15/23 16:45:23.163
    W1115 16:45:23.199279      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Nov 15 16:45:23.199: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Nov 15 16:45:23.199: INFO: Deleting pod "simpletest.rc-2kj4c" in namespace "gc-5182"
    Nov 15 16:45:23.234: INFO: Deleting pod "simpletest.rc-4qf5t" in namespace "gc-5182"
    Nov 15 16:45:23.274: INFO: Deleting pod "simpletest.rc-4rmz2" in namespace "gc-5182"
    Nov 15 16:45:23.321: INFO: Deleting pod "simpletest.rc-5768r" in namespace "gc-5182"
    Nov 15 16:45:23.357: INFO: Deleting pod "simpletest.rc-5px67" in namespace "gc-5182"
    Nov 15 16:45:23.434: INFO: Deleting pod "simpletest.rc-6bnwk" in namespace "gc-5182"
    Nov 15 16:45:23.472: INFO: Deleting pod "simpletest.rc-6ckk9" in namespace "gc-5182"
    Nov 15 16:45:23.506: INFO: Deleting pod "simpletest.rc-6dzhd" in namespace "gc-5182"
    Nov 15 16:45:23.560: INFO: Deleting pod "simpletest.rc-6f6tj" in namespace "gc-5182"
    Nov 15 16:45:23.614: INFO: Deleting pod "simpletest.rc-6h5k4" in namespace "gc-5182"
    Nov 15 16:45:23.663: INFO: Deleting pod "simpletest.rc-6tmqw" in namespace "gc-5182"
    Nov 15 16:45:23.706: INFO: Deleting pod "simpletest.rc-76dd6" in namespace "gc-5182"
    Nov 15 16:45:23.745: INFO: Deleting pod "simpletest.rc-79klc" in namespace "gc-5182"
    Nov 15 16:45:23.790: INFO: Deleting pod "simpletest.rc-7hzjl" in namespace "gc-5182"
    Nov 15 16:45:23.834: INFO: Deleting pod "simpletest.rc-7rlxh" in namespace "gc-5182"
    Nov 15 16:45:23.876: INFO: Deleting pod "simpletest.rc-7wcjc" in namespace "gc-5182"
    Nov 15 16:45:23.913: INFO: Deleting pod "simpletest.rc-7x4mm" in namespace "gc-5182"
    Nov 15 16:45:23.955: INFO: Deleting pod "simpletest.rc-8b5vv" in namespace "gc-5182"
    Nov 15 16:45:24.001: INFO: Deleting pod "simpletest.rc-8dn9s" in namespace "gc-5182"
    Nov 15 16:45:24.039: INFO: Deleting pod "simpletest.rc-8k5h5" in namespace "gc-5182"
    Nov 15 16:45:24.099: INFO: Deleting pod "simpletest.rc-8nf55" in namespace "gc-5182"
    Nov 15 16:45:24.137: INFO: Deleting pod "simpletest.rc-8tjsc" in namespace "gc-5182"
    Nov 15 16:45:24.183: INFO: Deleting pod "simpletest.rc-95fxs" in namespace "gc-5182"
    Nov 15 16:45:24.217: INFO: Deleting pod "simpletest.rc-9cmc5" in namespace "gc-5182"
    Nov 15 16:45:24.259: INFO: Deleting pod "simpletest.rc-9dzgv" in namespace "gc-5182"
    Nov 15 16:45:24.299: INFO: Deleting pod "simpletest.rc-9pptf" in namespace "gc-5182"
    Nov 15 16:45:24.353: INFO: Deleting pod "simpletest.rc-9t97q" in namespace "gc-5182"
    Nov 15 16:45:24.389: INFO: Deleting pod "simpletest.rc-b5bmd" in namespace "gc-5182"
    Nov 15 16:45:24.444: INFO: Deleting pod "simpletest.rc-b6mqx" in namespace "gc-5182"
    Nov 15 16:45:24.479: INFO: Deleting pod "simpletest.rc-bfs92" in namespace "gc-5182"
    Nov 15 16:45:24.521: INFO: Deleting pod "simpletest.rc-bl2sd" in namespace "gc-5182"
    Nov 15 16:45:24.561: INFO: Deleting pod "simpletest.rc-bql4x" in namespace "gc-5182"
    Nov 15 16:45:24.617: INFO: Deleting pod "simpletest.rc-bv4h5" in namespace "gc-5182"
    Nov 15 16:45:24.654: INFO: Deleting pod "simpletest.rc-bvtf9" in namespace "gc-5182"
    Nov 15 16:45:24.768: INFO: Deleting pod "simpletest.rc-bzwt2" in namespace "gc-5182"
    Nov 15 16:45:24.837: INFO: Deleting pod "simpletest.rc-cpm7n" in namespace "gc-5182"
    Nov 15 16:45:24.872: INFO: Deleting pod "simpletest.rc-cvcg8" in namespace "gc-5182"
    Nov 15 16:45:24.916: INFO: Deleting pod "simpletest.rc-d6xmh" in namespace "gc-5182"
    Nov 15 16:45:24.964: INFO: Deleting pod "simpletest.rc-dt4p5" in namespace "gc-5182"
    Nov 15 16:45:25.017: INFO: Deleting pod "simpletest.rc-f4p5w" in namespace "gc-5182"
    Nov 15 16:45:25.053: INFO: Deleting pod "simpletest.rc-f7xqm" in namespace "gc-5182"
    Nov 15 16:45:25.107: INFO: Deleting pod "simpletest.rc-fbbzg" in namespace "gc-5182"
    Nov 15 16:45:25.184: INFO: Deleting pod "simpletest.rc-fkq2n" in namespace "gc-5182"
    Nov 15 16:45:25.242: INFO: Deleting pod "simpletest.rc-fqhjx" in namespace "gc-5182"
    Nov 15 16:45:25.306: INFO: Deleting pod "simpletest.rc-fvwvj" in namespace "gc-5182"
    Nov 15 16:45:25.359: INFO: Deleting pod "simpletest.rc-gbg86" in namespace "gc-5182"
    Nov 15 16:45:25.401: INFO: Deleting pod "simpletest.rc-gfs6g" in namespace "gc-5182"
    Nov 15 16:45:25.445: INFO: Deleting pod "simpletest.rc-hl84r" in namespace "gc-5182"
    Nov 15 16:45:25.495: INFO: Deleting pod "simpletest.rc-j9l4h" in namespace "gc-5182"
    Nov 15 16:45:25.529: INFO: Deleting pod "simpletest.rc-jjddp" in namespace "gc-5182"
    Nov 15 16:45:25.566: INFO: Deleting pod "simpletest.rc-jnvhz" in namespace "gc-5182"
    Nov 15 16:45:25.604: INFO: Deleting pod "simpletest.rc-kc8md" in namespace "gc-5182"
    Nov 15 16:45:25.640: INFO: Deleting pod "simpletest.rc-knr7p" in namespace "gc-5182"
    Nov 15 16:45:25.679: INFO: Deleting pod "simpletest.rc-kr297" in namespace "gc-5182"
    Nov 15 16:45:25.712: INFO: Deleting pod "simpletest.rc-l22xh" in namespace "gc-5182"
    Nov 15 16:45:25.756: INFO: Deleting pod "simpletest.rc-lsbq2" in namespace "gc-5182"
    Nov 15 16:45:25.797: INFO: Deleting pod "simpletest.rc-mfqtw" in namespace "gc-5182"
    Nov 15 16:45:25.851: INFO: Deleting pod "simpletest.rc-mg8nj" in namespace "gc-5182"
    Nov 15 16:45:25.899: INFO: Deleting pod "simpletest.rc-mndx7" in namespace "gc-5182"
    Nov 15 16:45:25.937: INFO: Deleting pod "simpletest.rc-n2rlh" in namespace "gc-5182"
    Nov 15 16:45:25.983: INFO: Deleting pod "simpletest.rc-nfmzr" in namespace "gc-5182"
    Nov 15 16:45:26.018: INFO: Deleting pod "simpletest.rc-nm2t8" in namespace "gc-5182"
    Nov 15 16:45:26.067: INFO: Deleting pod "simpletest.rc-nmn9r" in namespace "gc-5182"
    Nov 15 16:45:26.117: INFO: Deleting pod "simpletest.rc-nxllp" in namespace "gc-5182"
    Nov 15 16:45:26.176: INFO: Deleting pod "simpletest.rc-nz9bf" in namespace "gc-5182"
    Nov 15 16:45:26.222: INFO: Deleting pod "simpletest.rc-p75rd" in namespace "gc-5182"
    Nov 15 16:45:26.273: INFO: Deleting pod "simpletest.rc-pdx49" in namespace "gc-5182"
    Nov 15 16:45:26.306: INFO: Deleting pod "simpletest.rc-pr6w8" in namespace "gc-5182"
    Nov 15 16:45:26.358: INFO: Deleting pod "simpletest.rc-qdl8l" in namespace "gc-5182"
    Nov 15 16:45:26.408: INFO: Deleting pod "simpletest.rc-qf9ks" in namespace "gc-5182"
    Nov 15 16:45:26.455: INFO: Deleting pod "simpletest.rc-qk2lg" in namespace "gc-5182"
    Nov 15 16:45:26.496: INFO: Deleting pod "simpletest.rc-qnh46" in namespace "gc-5182"
    Nov 15 16:45:26.541: INFO: Deleting pod "simpletest.rc-r55kp" in namespace "gc-5182"
    Nov 15 16:45:26.577: INFO: Deleting pod "simpletest.rc-r9vmj" in namespace "gc-5182"
    Nov 15 16:45:26.627: INFO: Deleting pod "simpletest.rc-rb69s" in namespace "gc-5182"
    Nov 15 16:45:26.672: INFO: Deleting pod "simpletest.rc-rhh8l" in namespace "gc-5182"
    Nov 15 16:45:26.717: INFO: Deleting pod "simpletest.rc-rm8hg" in namespace "gc-5182"
    Nov 15 16:45:26.757: INFO: Deleting pod "simpletest.rc-rrsw9" in namespace "gc-5182"
    Nov 15 16:45:26.807: INFO: Deleting pod "simpletest.rc-s76dr" in namespace "gc-5182"
    Nov 15 16:45:26.852: INFO: Deleting pod "simpletest.rc-sn5cz" in namespace "gc-5182"
    Nov 15 16:45:26.888: INFO: Deleting pod "simpletest.rc-tgc4c" in namespace "gc-5182"
    Nov 15 16:45:26.927: INFO: Deleting pod "simpletest.rc-th9f9" in namespace "gc-5182"
    Nov 15 16:45:26.978: INFO: Deleting pod "simpletest.rc-tsbhd" in namespace "gc-5182"
    Nov 15 16:45:27.023: INFO: Deleting pod "simpletest.rc-vg5zm" in namespace "gc-5182"
    Nov 15 16:45:27.071: INFO: Deleting pod "simpletest.rc-vhhs4" in namespace "gc-5182"
    Nov 15 16:45:27.118: INFO: Deleting pod "simpletest.rc-w8x9w" in namespace "gc-5182"
    Nov 15 16:45:27.167: INFO: Deleting pod "simpletest.rc-wl4j8" in namespace "gc-5182"
    Nov 15 16:45:27.231: INFO: Deleting pod "simpletest.rc-wmwnh" in namespace "gc-5182"
    Nov 15 16:45:27.282: INFO: Deleting pod "simpletest.rc-wxc2k" in namespace "gc-5182"
    Nov 15 16:45:27.322: INFO: Deleting pod "simpletest.rc-wxhj2" in namespace "gc-5182"
    Nov 15 16:45:27.366: INFO: Deleting pod "simpletest.rc-wxrmt" in namespace "gc-5182"
    Nov 15 16:45:27.439: INFO: Deleting pod "simpletest.rc-wxz9j" in namespace "gc-5182"
    Nov 15 16:45:27.492: INFO: Deleting pod "simpletest.rc-wzwck" in namespace "gc-5182"
    Nov 15 16:45:27.529: INFO: Deleting pod "simpletest.rc-x7ks9" in namespace "gc-5182"
    Nov 15 16:45:27.566: INFO: Deleting pod "simpletest.rc-x7p6z" in namespace "gc-5182"
    Nov 15 16:45:27.607: INFO: Deleting pod "simpletest.rc-xfgsc" in namespace "gc-5182"
    Nov 15 16:45:27.651: INFO: Deleting pod "simpletest.rc-z8pg5" in namespace "gc-5182"
    Nov 15 16:45:27.690: INFO: Deleting pod "simpletest.rc-zltwg" in namespace "gc-5182"
    Nov 15 16:45:27.742: INFO: Deleting pod "simpletest.rc-zslzd" in namespace "gc-5182"
    Nov 15 16:45:27.789: INFO: Deleting pod "simpletest.rc-zxtdv" in namespace "gc-5182"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:45:27.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-5182" for this suite. 11/15/23 16:45:27.852
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:45:27.878
Nov 15 16:45:27.879: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename services 11/15/23 16:45:27.88
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:45:27.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:45:27.944
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 11/15/23 16:45:27.978
STEP: watching for the Service to be added 11/15/23 16:45:28.027
Nov 15 16:45:28.032: INFO: Found Service test-service-4xzhf in namespace services-4869 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Nov 15 16:45:28.032: INFO: Service test-service-4xzhf created
STEP: Getting /status 11/15/23 16:45:28.032
Nov 15 16:45:28.052: INFO: Service test-service-4xzhf has LoadBalancer: {[]}
STEP: patching the ServiceStatus 11/15/23 16:45:28.052
STEP: watching for the Service to be patched 11/15/23 16:45:28.075
Nov 15 16:45:28.080: INFO: observed Service test-service-4xzhf in namespace services-4869 with annotations: map[] & LoadBalancer: {[]}
Nov 15 16:45:28.080: INFO: Found Service test-service-4xzhf in namespace services-4869 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Nov 15 16:45:28.080: INFO: Service test-service-4xzhf has service status patched
STEP: updating the ServiceStatus 11/15/23 16:45:28.08
Nov 15 16:45:28.124: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 11/15/23 16:45:28.124
Nov 15 16:45:28.132: INFO: Observed Service test-service-4xzhf in namespace services-4869 with annotations: map[] & Conditions: {[]}
Nov 15 16:45:28.132: INFO: Observed event: &Service{ObjectMeta:{test-service-4xzhf  services-4869  0a622f8d-863f-43eb-ab05-936c27561bad 44300 0 2023-11-15 16:45:27 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-11-15 16:45:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-11-15 16:45:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.21.59.2,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.21.59.2],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Nov 15 16:45:28.132: INFO: Found Service test-service-4xzhf in namespace services-4869 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Nov 15 16:45:28.132: INFO: Service test-service-4xzhf has service status updated
STEP: patching the service 11/15/23 16:45:28.132
STEP: watching for the Service to be patched 11/15/23 16:45:28.152
Nov 15 16:45:28.162: INFO: observed Service test-service-4xzhf in namespace services-4869 with labels: map[test-service-static:true]
Nov 15 16:45:28.163: INFO: observed Service test-service-4xzhf in namespace services-4869 with labels: map[test-service-static:true]
Nov 15 16:45:28.163: INFO: observed Service test-service-4xzhf in namespace services-4869 with labels: map[test-service-static:true]
Nov 15 16:45:28.163: INFO: Found Service test-service-4xzhf in namespace services-4869 with labels: map[test-service:patched test-service-static:true]
Nov 15 16:45:28.163: INFO: Service test-service-4xzhf patched
STEP: deleting the service 11/15/23 16:45:28.163
STEP: watching for the Service to be deleted 11/15/23 16:45:28.225
Nov 15 16:45:28.229: INFO: Observed event: ADDED
Nov 15 16:45:28.230: INFO: Observed event: MODIFIED
Nov 15 16:45:28.230: INFO: Observed event: MODIFIED
Nov 15 16:45:28.230: INFO: Observed event: MODIFIED
Nov 15 16:45:28.230: INFO: Found Service test-service-4xzhf in namespace services-4869 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Nov 15 16:45:28.230: INFO: Service test-service-4xzhf deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 15 16:45:28.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4869" for this suite. 11/15/23 16:45:28.247
------------------------------
• [0.393 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:45:27.878
    Nov 15 16:45:27.879: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename services 11/15/23 16:45:27.88
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:45:27.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:45:27.944
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 11/15/23 16:45:27.978
    STEP: watching for the Service to be added 11/15/23 16:45:28.027
    Nov 15 16:45:28.032: INFO: Found Service test-service-4xzhf in namespace services-4869 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Nov 15 16:45:28.032: INFO: Service test-service-4xzhf created
    STEP: Getting /status 11/15/23 16:45:28.032
    Nov 15 16:45:28.052: INFO: Service test-service-4xzhf has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 11/15/23 16:45:28.052
    STEP: watching for the Service to be patched 11/15/23 16:45:28.075
    Nov 15 16:45:28.080: INFO: observed Service test-service-4xzhf in namespace services-4869 with annotations: map[] & LoadBalancer: {[]}
    Nov 15 16:45:28.080: INFO: Found Service test-service-4xzhf in namespace services-4869 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Nov 15 16:45:28.080: INFO: Service test-service-4xzhf has service status patched
    STEP: updating the ServiceStatus 11/15/23 16:45:28.08
    Nov 15 16:45:28.124: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 11/15/23 16:45:28.124
    Nov 15 16:45:28.132: INFO: Observed Service test-service-4xzhf in namespace services-4869 with annotations: map[] & Conditions: {[]}
    Nov 15 16:45:28.132: INFO: Observed event: &Service{ObjectMeta:{test-service-4xzhf  services-4869  0a622f8d-863f-43eb-ab05-936c27561bad 44300 0 2023-11-15 16:45:27 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-11-15 16:45:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-11-15 16:45:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.21.59.2,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.21.59.2],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Nov 15 16:45:28.132: INFO: Found Service test-service-4xzhf in namespace services-4869 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Nov 15 16:45:28.132: INFO: Service test-service-4xzhf has service status updated
    STEP: patching the service 11/15/23 16:45:28.132
    STEP: watching for the Service to be patched 11/15/23 16:45:28.152
    Nov 15 16:45:28.162: INFO: observed Service test-service-4xzhf in namespace services-4869 with labels: map[test-service-static:true]
    Nov 15 16:45:28.163: INFO: observed Service test-service-4xzhf in namespace services-4869 with labels: map[test-service-static:true]
    Nov 15 16:45:28.163: INFO: observed Service test-service-4xzhf in namespace services-4869 with labels: map[test-service-static:true]
    Nov 15 16:45:28.163: INFO: Found Service test-service-4xzhf in namespace services-4869 with labels: map[test-service:patched test-service-static:true]
    Nov 15 16:45:28.163: INFO: Service test-service-4xzhf patched
    STEP: deleting the service 11/15/23 16:45:28.163
    STEP: watching for the Service to be deleted 11/15/23 16:45:28.225
    Nov 15 16:45:28.229: INFO: Observed event: ADDED
    Nov 15 16:45:28.230: INFO: Observed event: MODIFIED
    Nov 15 16:45:28.230: INFO: Observed event: MODIFIED
    Nov 15 16:45:28.230: INFO: Observed event: MODIFIED
    Nov 15 16:45:28.230: INFO: Found Service test-service-4xzhf in namespace services-4869 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Nov 15 16:45:28.230: INFO: Service test-service-4xzhf deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:45:28.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4869" for this suite. 11/15/23 16:45:28.247
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:45:28.272
Nov 15 16:45:28.273: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename events 11/15/23 16:45:28.287
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:45:28.341
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:45:28.349
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 11/15/23 16:45:28.363
STEP: listing all events in all namespaces 11/15/23 16:45:28.382
STEP: patching the test event 11/15/23 16:45:28.428
STEP: fetching the test event 11/15/23 16:45:28.451
STEP: updating the test event 11/15/23 16:45:28.463
STEP: getting the test event 11/15/23 16:45:28.498
STEP: deleting the test event 11/15/23 16:45:28.509
STEP: listing all events in all namespaces 11/15/23 16:45:28.549
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Nov 15 16:45:28.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-5715" for this suite. 11/15/23 16:45:28.596
------------------------------
• [0.352 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:45:28.272
    Nov 15 16:45:28.273: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename events 11/15/23 16:45:28.287
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:45:28.341
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:45:28.349
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 11/15/23 16:45:28.363
    STEP: listing all events in all namespaces 11/15/23 16:45:28.382
    STEP: patching the test event 11/15/23 16:45:28.428
    STEP: fetching the test event 11/15/23 16:45:28.451
    STEP: updating the test event 11/15/23 16:45:28.463
    STEP: getting the test event 11/15/23 16:45:28.498
    STEP: deleting the test event 11/15/23 16:45:28.509
    STEP: listing all events in all namespaces 11/15/23 16:45:28.549
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:45:28.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-5715" for this suite. 11/15/23 16:45:28.596
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:45:28.625
Nov 15 16:45:28.625: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename custom-resource-definition 11/15/23 16:45:28.627
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:45:28.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:45:28.693
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 11/15/23 16:45:28.701
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 11/15/23 16:45:28.707
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 11/15/23 16:45:28.708
STEP: fetching the /apis/apiextensions.k8s.io discovery document 11/15/23 16:45:28.708
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 11/15/23 16:45:28.712
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 11/15/23 16:45:28.712
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 11/15/23 16:45:28.716
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 16:45:28.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-6449" for this suite. 11/15/23 16:45:28.734
------------------------------
• [0.133 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:45:28.625
    Nov 15 16:45:28.625: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename custom-resource-definition 11/15/23 16:45:28.627
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:45:28.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:45:28.693
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 11/15/23 16:45:28.701
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 11/15/23 16:45:28.707
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 11/15/23 16:45:28.708
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 11/15/23 16:45:28.708
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 11/15/23 16:45:28.712
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 11/15/23 16:45:28.712
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 11/15/23 16:45:28.716
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:45:28.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-6449" for this suite. 11/15/23 16:45:28.734
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:45:28.758
Nov 15 16:45:28.758: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename endpointslice 11/15/23 16:45:28.759
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:45:28.814
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:45:28.824
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 11/15/23 16:45:39.101
STEP: referencing matching pods with named port 11/15/23 16:45:44.148
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 11/15/23 16:45:49.187
STEP: recreating EndpointSlices after they've been deleted 11/15/23 16:45:54.228
Nov 15 16:45:54.309: INFO: EndpointSlice for Service endpointslice-8773/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Nov 15 16:46:04.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-8773" for this suite. 11/15/23 16:46:04.369
------------------------------
• [SLOW TEST] [35.635 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:45:28.758
    Nov 15 16:45:28.758: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename endpointslice 11/15/23 16:45:28.759
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:45:28.814
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:45:28.824
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 11/15/23 16:45:39.101
    STEP: referencing matching pods with named port 11/15/23 16:45:44.148
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 11/15/23 16:45:49.187
    STEP: recreating EndpointSlices after they've been deleted 11/15/23 16:45:54.228
    Nov 15 16:45:54.309: INFO: EndpointSlice for Service endpointslice-8773/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:46:04.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-8773" for this suite. 11/15/23 16:46:04.369
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:46:04.403
Nov 15 16:46:04.404: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename projected 11/15/23 16:46:04.406
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:46:04.459
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:46:04.468
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 11/15/23 16:46:04.489
Nov 15 16:46:04.523: INFO: Waiting up to 5m0s for pod "labelsupdatef326db26-3f2f-4249-ad42-cd4a980b3276" in namespace "projected-6506" to be "running and ready"
Nov 15 16:46:04.536: INFO: Pod "labelsupdatef326db26-3f2f-4249-ad42-cd4a980b3276": Phase="Pending", Reason="", readiness=false. Elapsed: 13.210991ms
Nov 15 16:46:04.536: INFO: The phase of Pod labelsupdatef326db26-3f2f-4249-ad42-cd4a980b3276 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 16:46:06.552: INFO: Pod "labelsupdatef326db26-3f2f-4249-ad42-cd4a980b3276": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02926641s
Nov 15 16:46:06.552: INFO: The phase of Pod labelsupdatef326db26-3f2f-4249-ad42-cd4a980b3276 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 16:46:08.551: INFO: Pod "labelsupdatef326db26-3f2f-4249-ad42-cd4a980b3276": Phase="Running", Reason="", readiness=true. Elapsed: 4.028458821s
Nov 15 16:46:08.551: INFO: The phase of Pod labelsupdatef326db26-3f2f-4249-ad42-cd4a980b3276 is Running (Ready = true)
Nov 15 16:46:08.551: INFO: Pod "labelsupdatef326db26-3f2f-4249-ad42-cd4a980b3276" satisfied condition "running and ready"
Nov 15 16:46:09.235: INFO: Successfully updated pod "labelsupdatef326db26-3f2f-4249-ad42-cd4a980b3276"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Nov 15 16:46:11.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6506" for this suite. 11/15/23 16:46:11.358
------------------------------
• [SLOW TEST] [6.981 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:46:04.403
    Nov 15 16:46:04.404: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename projected 11/15/23 16:46:04.406
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:46:04.459
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:46:04.468
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 11/15/23 16:46:04.489
    Nov 15 16:46:04.523: INFO: Waiting up to 5m0s for pod "labelsupdatef326db26-3f2f-4249-ad42-cd4a980b3276" in namespace "projected-6506" to be "running and ready"
    Nov 15 16:46:04.536: INFO: Pod "labelsupdatef326db26-3f2f-4249-ad42-cd4a980b3276": Phase="Pending", Reason="", readiness=false. Elapsed: 13.210991ms
    Nov 15 16:46:04.536: INFO: The phase of Pod labelsupdatef326db26-3f2f-4249-ad42-cd4a980b3276 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 16:46:06.552: INFO: Pod "labelsupdatef326db26-3f2f-4249-ad42-cd4a980b3276": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02926641s
    Nov 15 16:46:06.552: INFO: The phase of Pod labelsupdatef326db26-3f2f-4249-ad42-cd4a980b3276 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 16:46:08.551: INFO: Pod "labelsupdatef326db26-3f2f-4249-ad42-cd4a980b3276": Phase="Running", Reason="", readiness=true. Elapsed: 4.028458821s
    Nov 15 16:46:08.551: INFO: The phase of Pod labelsupdatef326db26-3f2f-4249-ad42-cd4a980b3276 is Running (Ready = true)
    Nov 15 16:46:08.551: INFO: Pod "labelsupdatef326db26-3f2f-4249-ad42-cd4a980b3276" satisfied condition "running and ready"
    Nov 15 16:46:09.235: INFO: Successfully updated pod "labelsupdatef326db26-3f2f-4249-ad42-cd4a980b3276"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:46:11.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6506" for this suite. 11/15/23 16:46:11.358
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:46:11.39
Nov 15 16:46:11.391: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename podtemplate 11/15/23 16:46:11.393
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:46:11.448
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:46:11.458
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 11/15/23 16:46:11.467
STEP: Replace a pod template 11/15/23 16:46:11.485
Nov 15 16:46:11.521: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Nov 15 16:46:11.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-3566" for this suite. 11/15/23 16:46:11.539
------------------------------
• [0.172 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:46:11.39
    Nov 15 16:46:11.391: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename podtemplate 11/15/23 16:46:11.393
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:46:11.448
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:46:11.458
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 11/15/23 16:46:11.467
    STEP: Replace a pod template 11/15/23 16:46:11.485
    Nov 15 16:46:11.521: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:46:11.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-3566" for this suite. 11/15/23 16:46:11.539
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:46:11.568
Nov 15 16:46:11.568: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename cronjob 11/15/23 16:46:11.569
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:46:11.614
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:46:11.626
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 11/15/23 16:46:11.637
STEP: creating 11/15/23 16:46:11.637
STEP: getting 11/15/23 16:46:11.657
STEP: listing 11/15/23 16:46:11.673
STEP: watching 11/15/23 16:46:11.694
Nov 15 16:46:11.695: INFO: starting watch
STEP: cluster-wide listing 11/15/23 16:46:11.699
STEP: cluster-wide watching 11/15/23 16:46:11.716
Nov 15 16:46:11.716: INFO: starting watch
STEP: patching 11/15/23 16:46:11.72
STEP: updating 11/15/23 16:46:11.744
Nov 15 16:46:11.781: INFO: waiting for watch events with expected annotations
Nov 15 16:46:11.781: INFO: saw patched and updated annotations
STEP: patching /status 11/15/23 16:46:11.782
STEP: updating /status 11/15/23 16:46:11.82
STEP: get /status 11/15/23 16:46:11.856
STEP: deleting 11/15/23 16:46:11.875
STEP: deleting a collection 11/15/23 16:46:11.955
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Nov 15 16:46:12.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-1512" for this suite. 11/15/23 16:46:12.047
------------------------------
• [0.506 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:46:11.568
    Nov 15 16:46:11.568: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename cronjob 11/15/23 16:46:11.569
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:46:11.614
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:46:11.626
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 11/15/23 16:46:11.637
    STEP: creating 11/15/23 16:46:11.637
    STEP: getting 11/15/23 16:46:11.657
    STEP: listing 11/15/23 16:46:11.673
    STEP: watching 11/15/23 16:46:11.694
    Nov 15 16:46:11.695: INFO: starting watch
    STEP: cluster-wide listing 11/15/23 16:46:11.699
    STEP: cluster-wide watching 11/15/23 16:46:11.716
    Nov 15 16:46:11.716: INFO: starting watch
    STEP: patching 11/15/23 16:46:11.72
    STEP: updating 11/15/23 16:46:11.744
    Nov 15 16:46:11.781: INFO: waiting for watch events with expected annotations
    Nov 15 16:46:11.781: INFO: saw patched and updated annotations
    STEP: patching /status 11/15/23 16:46:11.782
    STEP: updating /status 11/15/23 16:46:11.82
    STEP: get /status 11/15/23 16:46:11.856
    STEP: deleting 11/15/23 16:46:11.875
    STEP: deleting a collection 11/15/23 16:46:11.955
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:46:12.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-1512" for this suite. 11/15/23 16:46:12.047
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:46:12.077
Nov 15 16:46:12.077: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename webhook 11/15/23 16:46:12.08
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:46:12.136
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:46:12.151
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/15/23 16:46:12.214
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 16:46:12.812
STEP: Deploying the webhook pod 11/15/23 16:46:12.84
STEP: Wait for the deployment to be ready 11/15/23 16:46:12.879
Nov 15 16:46:12.912: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/15/23 16:46:14.988
STEP: Verifying the service has paired with the endpoint 11/15/23 16:46:15.045
Nov 15 16:46:16.045: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Nov 15 16:46:16.062: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7201-crds.webhook.example.com via the AdmissionRegistration API 11/15/23 16:46:16.592
STEP: Creating a custom resource that should be mutated by the webhook 11/15/23 16:46:16.721
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 16:46:19.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8173" for this suite. 11/15/23 16:46:19.665
STEP: Destroying namespace "webhook-8173-markers" for this suite. 11/15/23 16:46:19.688
------------------------------
• [SLOW TEST] [7.635 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:46:12.077
    Nov 15 16:46:12.077: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename webhook 11/15/23 16:46:12.08
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:46:12.136
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:46:12.151
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/15/23 16:46:12.214
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 16:46:12.812
    STEP: Deploying the webhook pod 11/15/23 16:46:12.84
    STEP: Wait for the deployment to be ready 11/15/23 16:46:12.879
    Nov 15 16:46:12.912: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/15/23 16:46:14.988
    STEP: Verifying the service has paired with the endpoint 11/15/23 16:46:15.045
    Nov 15 16:46:16.045: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Nov 15 16:46:16.062: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7201-crds.webhook.example.com via the AdmissionRegistration API 11/15/23 16:46:16.592
    STEP: Creating a custom resource that should be mutated by the webhook 11/15/23 16:46:16.721
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:46:19.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8173" for this suite. 11/15/23 16:46:19.665
    STEP: Destroying namespace "webhook-8173-markers" for this suite. 11/15/23 16:46:19.688
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:46:19.713
Nov 15 16:46:19.713: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename kubectl 11/15/23 16:46:19.714
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:46:19.769
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:46:19.776
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 11/15/23 16:46:19.785
Nov 15 16:46:19.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-3942 create -f -'
Nov 15 16:46:20.819: INFO: stderr: ""
Nov 15 16:46:20.819: INFO: stdout: "pod/pause created\n"
Nov 15 16:46:20.819: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Nov 15 16:46:20.819: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3942" to be "running and ready"
Nov 15 16:46:20.832: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 13.168659ms
Nov 15 16:46:20.832: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.15.40.115' to be 'Running' but was 'Pending'
Nov 15 16:46:22.847: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027868341s
Nov 15 16:46:22.847: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.15.40.115' to be 'Running' but was 'Pending'
Nov 15 16:46:24.851: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.031803731s
Nov 15 16:46:24.851: INFO: Pod "pause" satisfied condition "running and ready"
Nov 15 16:46:24.851: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 11/15/23 16:46:24.851
Nov 15 16:46:24.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-3942 label pods pause testing-label=testing-label-value'
Nov 15 16:46:25.033: INFO: stderr: ""
Nov 15 16:46:25.034: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 11/15/23 16:46:25.034
Nov 15 16:46:25.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-3942 get pod pause -L testing-label'
Nov 15 16:46:25.189: INFO: stderr: ""
Nov 15 16:46:25.189: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod 11/15/23 16:46:25.189
Nov 15 16:46:25.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-3942 label pods pause testing-label-'
Nov 15 16:46:25.359: INFO: stderr: ""
Nov 15 16:46:25.359: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 11/15/23 16:46:25.359
Nov 15 16:46:25.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-3942 get pod pause -L testing-label'
Nov 15 16:46:25.498: INFO: stderr: ""
Nov 15 16:46:25.498: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 11/15/23 16:46:25.499
Nov 15 16:46:25.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-3942 delete --grace-period=0 --force -f -'
Nov 15 16:46:25.681: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 15 16:46:25.681: INFO: stdout: "pod \"pause\" force deleted\n"
Nov 15 16:46:25.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-3942 get rc,svc -l name=pause --no-headers'
Nov 15 16:46:25.843: INFO: stderr: "No resources found in kubectl-3942 namespace.\n"
Nov 15 16:46:25.843: INFO: stdout: ""
Nov 15 16:46:25.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-3942 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 15 16:46:25.982: INFO: stderr: ""
Nov 15 16:46:25.982: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 15 16:46:25.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3942" for this suite. 11/15/23 16:46:26
------------------------------
• [SLOW TEST] [6.312 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:46:19.713
    Nov 15 16:46:19.713: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename kubectl 11/15/23 16:46:19.714
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:46:19.769
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:46:19.776
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 11/15/23 16:46:19.785
    Nov 15 16:46:19.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-3942 create -f -'
    Nov 15 16:46:20.819: INFO: stderr: ""
    Nov 15 16:46:20.819: INFO: stdout: "pod/pause created\n"
    Nov 15 16:46:20.819: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Nov 15 16:46:20.819: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3942" to be "running and ready"
    Nov 15 16:46:20.832: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 13.168659ms
    Nov 15 16:46:20.832: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.15.40.115' to be 'Running' but was 'Pending'
    Nov 15 16:46:22.847: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027868341s
    Nov 15 16:46:22.847: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.15.40.115' to be 'Running' but was 'Pending'
    Nov 15 16:46:24.851: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.031803731s
    Nov 15 16:46:24.851: INFO: Pod "pause" satisfied condition "running and ready"
    Nov 15 16:46:24.851: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 11/15/23 16:46:24.851
    Nov 15 16:46:24.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-3942 label pods pause testing-label=testing-label-value'
    Nov 15 16:46:25.033: INFO: stderr: ""
    Nov 15 16:46:25.034: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 11/15/23 16:46:25.034
    Nov 15 16:46:25.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-3942 get pod pause -L testing-label'
    Nov 15 16:46:25.189: INFO: stderr: ""
    Nov 15 16:46:25.189: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 11/15/23 16:46:25.189
    Nov 15 16:46:25.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-3942 label pods pause testing-label-'
    Nov 15 16:46:25.359: INFO: stderr: ""
    Nov 15 16:46:25.359: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 11/15/23 16:46:25.359
    Nov 15 16:46:25.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-3942 get pod pause -L testing-label'
    Nov 15 16:46:25.498: INFO: stderr: ""
    Nov 15 16:46:25.498: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 11/15/23 16:46:25.499
    Nov 15 16:46:25.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-3942 delete --grace-period=0 --force -f -'
    Nov 15 16:46:25.681: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Nov 15 16:46:25.681: INFO: stdout: "pod \"pause\" force deleted\n"
    Nov 15 16:46:25.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-3942 get rc,svc -l name=pause --no-headers'
    Nov 15 16:46:25.843: INFO: stderr: "No resources found in kubectl-3942 namespace.\n"
    Nov 15 16:46:25.843: INFO: stdout: ""
    Nov 15 16:46:25.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=kubectl-3942 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Nov 15 16:46:25.982: INFO: stderr: ""
    Nov 15 16:46:25.982: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:46:25.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3942" for this suite. 11/15/23 16:46:26
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:46:26.029
Nov 15 16:46:26.029: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename secrets 11/15/23 16:46:26.034
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:46:26.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:46:26.097
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-e910c04e-ebaa-4886-9926-aa05de35b20b 11/15/23 16:46:26.105
STEP: Creating a pod to test consume secrets 11/15/23 16:46:26.121
Nov 15 16:46:26.153: INFO: Waiting up to 5m0s for pod "pod-secrets-a22d3aee-1ed5-4071-987e-6b3b877bf6bc" in namespace "secrets-4727" to be "Succeeded or Failed"
Nov 15 16:46:26.172: INFO: Pod "pod-secrets-a22d3aee-1ed5-4071-987e-6b3b877bf6bc": Phase="Pending", Reason="", readiness=false. Elapsed: 19.066174ms
Nov 15 16:46:28.187: INFO: Pod "pod-secrets-a22d3aee-1ed5-4071-987e-6b3b877bf6bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034311464s
Nov 15 16:46:30.196: INFO: Pod "pod-secrets-a22d3aee-1ed5-4071-987e-6b3b877bf6bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042763144s
STEP: Saw pod success 11/15/23 16:46:30.196
Nov 15 16:46:30.197: INFO: Pod "pod-secrets-a22d3aee-1ed5-4071-987e-6b3b877bf6bc" satisfied condition "Succeeded or Failed"
Nov 15 16:46:30.223: INFO: Trying to get logs from node 10.15.40.115 pod pod-secrets-a22d3aee-1ed5-4071-987e-6b3b877bf6bc container secret-volume-test: <nil>
STEP: delete the pod 11/15/23 16:46:30.267
Nov 15 16:46:30.313: INFO: Waiting for pod pod-secrets-a22d3aee-1ed5-4071-987e-6b3b877bf6bc to disappear
Nov 15 16:46:30.325: INFO: Pod pod-secrets-a22d3aee-1ed5-4071-987e-6b3b877bf6bc no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 15 16:46:30.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4727" for this suite. 11/15/23 16:46:30.348
------------------------------
• [4.344 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:46:26.029
    Nov 15 16:46:26.029: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename secrets 11/15/23 16:46:26.034
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:46:26.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:46:26.097
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-e910c04e-ebaa-4886-9926-aa05de35b20b 11/15/23 16:46:26.105
    STEP: Creating a pod to test consume secrets 11/15/23 16:46:26.121
    Nov 15 16:46:26.153: INFO: Waiting up to 5m0s for pod "pod-secrets-a22d3aee-1ed5-4071-987e-6b3b877bf6bc" in namespace "secrets-4727" to be "Succeeded or Failed"
    Nov 15 16:46:26.172: INFO: Pod "pod-secrets-a22d3aee-1ed5-4071-987e-6b3b877bf6bc": Phase="Pending", Reason="", readiness=false. Elapsed: 19.066174ms
    Nov 15 16:46:28.187: INFO: Pod "pod-secrets-a22d3aee-1ed5-4071-987e-6b3b877bf6bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034311464s
    Nov 15 16:46:30.196: INFO: Pod "pod-secrets-a22d3aee-1ed5-4071-987e-6b3b877bf6bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042763144s
    STEP: Saw pod success 11/15/23 16:46:30.196
    Nov 15 16:46:30.197: INFO: Pod "pod-secrets-a22d3aee-1ed5-4071-987e-6b3b877bf6bc" satisfied condition "Succeeded or Failed"
    Nov 15 16:46:30.223: INFO: Trying to get logs from node 10.15.40.115 pod pod-secrets-a22d3aee-1ed5-4071-987e-6b3b877bf6bc container secret-volume-test: <nil>
    STEP: delete the pod 11/15/23 16:46:30.267
    Nov 15 16:46:30.313: INFO: Waiting for pod pod-secrets-a22d3aee-1ed5-4071-987e-6b3b877bf6bc to disappear
    Nov 15 16:46:30.325: INFO: Pod pod-secrets-a22d3aee-1ed5-4071-987e-6b3b877bf6bc no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:46:30.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4727" for this suite. 11/15/23 16:46:30.348
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:46:30.379
Nov 15 16:46:30.379: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename security-context 11/15/23 16:46:30.381
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:46:30.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:46:30.45
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 11/15/23 16:46:30.461
Nov 15 16:46:30.492: INFO: Waiting up to 5m0s for pod "security-context-11044b70-a04f-4016-96ae-005fe330aea2" in namespace "security-context-7013" to be "Succeeded or Failed"
Nov 15 16:46:30.506: INFO: Pod "security-context-11044b70-a04f-4016-96ae-005fe330aea2": Phase="Pending", Reason="", readiness=false. Elapsed: 13.829053ms
Nov 15 16:46:32.525: INFO: Pod "security-context-11044b70-a04f-4016-96ae-005fe330aea2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032890062s
Nov 15 16:46:34.537: INFO: Pod "security-context-11044b70-a04f-4016-96ae-005fe330aea2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044642603s
STEP: Saw pod success 11/15/23 16:46:34.537
Nov 15 16:46:34.538: INFO: Pod "security-context-11044b70-a04f-4016-96ae-005fe330aea2" satisfied condition "Succeeded or Failed"
Nov 15 16:46:34.554: INFO: Trying to get logs from node 10.15.40.115 pod security-context-11044b70-a04f-4016-96ae-005fe330aea2 container test-container: <nil>
STEP: delete the pod 11/15/23 16:46:34.6
Nov 15 16:46:34.634: INFO: Waiting for pod security-context-11044b70-a04f-4016-96ae-005fe330aea2 to disappear
Nov 15 16:46:34.656: INFO: Pod security-context-11044b70-a04f-4016-96ae-005fe330aea2 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Nov 15 16:46:34.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-7013" for this suite. 11/15/23 16:46:34.677
------------------------------
• [4.324 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:46:30.379
    Nov 15 16:46:30.379: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename security-context 11/15/23 16:46:30.381
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:46:30.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:46:30.45
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 11/15/23 16:46:30.461
    Nov 15 16:46:30.492: INFO: Waiting up to 5m0s for pod "security-context-11044b70-a04f-4016-96ae-005fe330aea2" in namespace "security-context-7013" to be "Succeeded or Failed"
    Nov 15 16:46:30.506: INFO: Pod "security-context-11044b70-a04f-4016-96ae-005fe330aea2": Phase="Pending", Reason="", readiness=false. Elapsed: 13.829053ms
    Nov 15 16:46:32.525: INFO: Pod "security-context-11044b70-a04f-4016-96ae-005fe330aea2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032890062s
    Nov 15 16:46:34.537: INFO: Pod "security-context-11044b70-a04f-4016-96ae-005fe330aea2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044642603s
    STEP: Saw pod success 11/15/23 16:46:34.537
    Nov 15 16:46:34.538: INFO: Pod "security-context-11044b70-a04f-4016-96ae-005fe330aea2" satisfied condition "Succeeded or Failed"
    Nov 15 16:46:34.554: INFO: Trying to get logs from node 10.15.40.115 pod security-context-11044b70-a04f-4016-96ae-005fe330aea2 container test-container: <nil>
    STEP: delete the pod 11/15/23 16:46:34.6
    Nov 15 16:46:34.634: INFO: Waiting for pod security-context-11044b70-a04f-4016-96ae-005fe330aea2 to disappear
    Nov 15 16:46:34.656: INFO: Pod security-context-11044b70-a04f-4016-96ae-005fe330aea2 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:46:34.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-7013" for this suite. 11/15/23 16:46:34.677
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:46:34.707
Nov 15 16:46:34.707: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename server-version 11/15/23 16:46:34.709
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:46:34.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:46:34.769
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 11/15/23 16:46:34.78
STEP: Confirm major version 11/15/23 16:46:34.786
Nov 15 16:46:34.786: INFO: Major version: 1
STEP: Confirm minor version 11/15/23 16:46:34.786
Nov 15 16:46:34.787: INFO: cleanMinorVersion: 26
Nov 15 16:46:34.787: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Nov 15 16:46:34.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-7142" for this suite. 11/15/23 16:46:34.806
------------------------------
• [0.125 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:46:34.707
    Nov 15 16:46:34.707: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename server-version 11/15/23 16:46:34.709
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:46:34.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:46:34.769
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 11/15/23 16:46:34.78
    STEP: Confirm major version 11/15/23 16:46:34.786
    Nov 15 16:46:34.786: INFO: Major version: 1
    STEP: Confirm minor version 11/15/23 16:46:34.786
    Nov 15 16:46:34.787: INFO: cleanMinorVersion: 26
    Nov 15 16:46:34.787: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:46:34.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-7142" for this suite. 11/15/23 16:46:34.806
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:46:34.837
Nov 15 16:46:34.837: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename var-expansion 11/15/23 16:46:34.839
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:46:34.895
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:46:34.911
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 11/15/23 16:46:34.921
STEP: waiting for pod running 11/15/23 16:46:34.954
Nov 15 16:46:34.955: INFO: Waiting up to 2m0s for pod "var-expansion-e2970d09-f187-4ca6-b603-9cb2b9356fef" in namespace "var-expansion-9847" to be "running"
Nov 15 16:46:34.969: INFO: Pod "var-expansion-e2970d09-f187-4ca6-b603-9cb2b9356fef": Phase="Pending", Reason="", readiness=false. Elapsed: 13.754878ms
Nov 15 16:46:36.985: INFO: Pod "var-expansion-e2970d09-f187-4ca6-b603-9cb2b9356fef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029494353s
Nov 15 16:46:38.983: INFO: Pod "var-expansion-e2970d09-f187-4ca6-b603-9cb2b9356fef": Phase="Running", Reason="", readiness=true. Elapsed: 4.028038866s
Nov 15 16:46:38.983: INFO: Pod "var-expansion-e2970d09-f187-4ca6-b603-9cb2b9356fef" satisfied condition "running"
STEP: creating a file in subpath 11/15/23 16:46:38.983
Nov 15 16:46:38.998: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-9847 PodName:var-expansion-e2970d09-f187-4ca6-b603-9cb2b9356fef ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 16:46:38.998: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
Nov 15 16:46:39.001: INFO: ExecWithOptions: Clientset creation
Nov 15 16:46:39.002: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-9847/pods/var-expansion-e2970d09-f187-4ca6-b603-9cb2b9356fef/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 11/15/23 16:46:39.299
Nov 15 16:46:39.314: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-9847 PodName:var-expansion-e2970d09-f187-4ca6-b603-9cb2b9356fef ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 16:46:39.314: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
Nov 15 16:46:39.316: INFO: ExecWithOptions: Clientset creation
Nov 15 16:46:39.316: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-9847/pods/var-expansion-e2970d09-f187-4ca6-b603-9cb2b9356fef/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 11/15/23 16:46:39.591
Nov 15 16:46:40.133: INFO: Successfully updated pod "var-expansion-e2970d09-f187-4ca6-b603-9cb2b9356fef"
STEP: waiting for annotated pod running 11/15/23 16:46:40.133
Nov 15 16:46:40.133: INFO: Waiting up to 2m0s for pod "var-expansion-e2970d09-f187-4ca6-b603-9cb2b9356fef" in namespace "var-expansion-9847" to be "running"
Nov 15 16:46:40.153: INFO: Pod "var-expansion-e2970d09-f187-4ca6-b603-9cb2b9356fef": Phase="Running", Reason="", readiness=true. Elapsed: 19.198096ms
Nov 15 16:46:40.153: INFO: Pod "var-expansion-e2970d09-f187-4ca6-b603-9cb2b9356fef" satisfied condition "running"
STEP: deleting the pod gracefully 11/15/23 16:46:40.153
Nov 15 16:46:40.154: INFO: Deleting pod "var-expansion-e2970d09-f187-4ca6-b603-9cb2b9356fef" in namespace "var-expansion-9847"
Nov 15 16:46:40.182: INFO: Wait up to 5m0s for pod "var-expansion-e2970d09-f187-4ca6-b603-9cb2b9356fef" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Nov 15 16:47:14.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9847" for this suite. 11/15/23 16:47:14.243
------------------------------
• [SLOW TEST] [39.431 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:46:34.837
    Nov 15 16:46:34.837: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename var-expansion 11/15/23 16:46:34.839
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:46:34.895
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:46:34.911
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 11/15/23 16:46:34.921
    STEP: waiting for pod running 11/15/23 16:46:34.954
    Nov 15 16:46:34.955: INFO: Waiting up to 2m0s for pod "var-expansion-e2970d09-f187-4ca6-b603-9cb2b9356fef" in namespace "var-expansion-9847" to be "running"
    Nov 15 16:46:34.969: INFO: Pod "var-expansion-e2970d09-f187-4ca6-b603-9cb2b9356fef": Phase="Pending", Reason="", readiness=false. Elapsed: 13.754878ms
    Nov 15 16:46:36.985: INFO: Pod "var-expansion-e2970d09-f187-4ca6-b603-9cb2b9356fef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029494353s
    Nov 15 16:46:38.983: INFO: Pod "var-expansion-e2970d09-f187-4ca6-b603-9cb2b9356fef": Phase="Running", Reason="", readiness=true. Elapsed: 4.028038866s
    Nov 15 16:46:38.983: INFO: Pod "var-expansion-e2970d09-f187-4ca6-b603-9cb2b9356fef" satisfied condition "running"
    STEP: creating a file in subpath 11/15/23 16:46:38.983
    Nov 15 16:46:38.998: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-9847 PodName:var-expansion-e2970d09-f187-4ca6-b603-9cb2b9356fef ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 16:46:38.998: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    Nov 15 16:46:39.001: INFO: ExecWithOptions: Clientset creation
    Nov 15 16:46:39.002: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-9847/pods/var-expansion-e2970d09-f187-4ca6-b603-9cb2b9356fef/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 11/15/23 16:46:39.299
    Nov 15 16:46:39.314: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-9847 PodName:var-expansion-e2970d09-f187-4ca6-b603-9cb2b9356fef ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 16:46:39.314: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    Nov 15 16:46:39.316: INFO: ExecWithOptions: Clientset creation
    Nov 15 16:46:39.316: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-9847/pods/var-expansion-e2970d09-f187-4ca6-b603-9cb2b9356fef/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 11/15/23 16:46:39.591
    Nov 15 16:46:40.133: INFO: Successfully updated pod "var-expansion-e2970d09-f187-4ca6-b603-9cb2b9356fef"
    STEP: waiting for annotated pod running 11/15/23 16:46:40.133
    Nov 15 16:46:40.133: INFO: Waiting up to 2m0s for pod "var-expansion-e2970d09-f187-4ca6-b603-9cb2b9356fef" in namespace "var-expansion-9847" to be "running"
    Nov 15 16:46:40.153: INFO: Pod "var-expansion-e2970d09-f187-4ca6-b603-9cb2b9356fef": Phase="Running", Reason="", readiness=true. Elapsed: 19.198096ms
    Nov 15 16:46:40.153: INFO: Pod "var-expansion-e2970d09-f187-4ca6-b603-9cb2b9356fef" satisfied condition "running"
    STEP: deleting the pod gracefully 11/15/23 16:46:40.153
    Nov 15 16:46:40.154: INFO: Deleting pod "var-expansion-e2970d09-f187-4ca6-b603-9cb2b9356fef" in namespace "var-expansion-9847"
    Nov 15 16:46:40.182: INFO: Wait up to 5m0s for pod "var-expansion-e2970d09-f187-4ca6-b603-9cb2b9356fef" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:47:14.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9847" for this suite. 11/15/23 16:47:14.243
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:47:14.277
Nov 15 16:47:14.278: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename configmap 11/15/23 16:47:14.279
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:47:14.345
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:47:14.357
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-30f2f4fc-ecd5-40de-8c0b-d2f632b53abb 11/15/23 16:47:14.409
STEP: Creating configMap with name cm-test-opt-upd-b5d18f82-d8af-46eb-92e8-6689f96487c2 11/15/23 16:47:14.428
STEP: Creating the pod 11/15/23 16:47:14.44
Nov 15 16:47:14.471: INFO: Waiting up to 5m0s for pod "pod-configmaps-1ea03b6f-99e2-4d14-bfe4-d782fefbd233" in namespace "configmap-2286" to be "running and ready"
Nov 15 16:47:14.486: INFO: Pod "pod-configmaps-1ea03b6f-99e2-4d14-bfe4-d782fefbd233": Phase="Pending", Reason="", readiness=false. Elapsed: 14.210038ms
Nov 15 16:47:14.486: INFO: The phase of Pod pod-configmaps-1ea03b6f-99e2-4d14-bfe4-d782fefbd233 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 16:47:16.503: INFO: Pod "pod-configmaps-1ea03b6f-99e2-4d14-bfe4-d782fefbd233": Phase="Running", Reason="", readiness=true. Elapsed: 2.031605486s
Nov 15 16:47:16.503: INFO: The phase of Pod pod-configmaps-1ea03b6f-99e2-4d14-bfe4-d782fefbd233 is Running (Ready = true)
Nov 15 16:47:16.503: INFO: Pod "pod-configmaps-1ea03b6f-99e2-4d14-bfe4-d782fefbd233" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-30f2f4fc-ecd5-40de-8c0b-d2f632b53abb 11/15/23 16:47:16.701
STEP: Updating configmap cm-test-opt-upd-b5d18f82-d8af-46eb-92e8-6689f96487c2 11/15/23 16:47:16.725
STEP: Creating configMap with name cm-test-opt-create-244e624a-307e-45a6-9e36-9f581c31c4b7 11/15/23 16:47:16.741
STEP: waiting to observe update in volume 11/15/23 16:47:16.758
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 15 16:47:18.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2286" for this suite. 11/15/23 16:47:18.963
------------------------------
• [4.711 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:47:14.277
    Nov 15 16:47:14.278: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename configmap 11/15/23 16:47:14.279
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:47:14.345
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:47:14.357
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-30f2f4fc-ecd5-40de-8c0b-d2f632b53abb 11/15/23 16:47:14.409
    STEP: Creating configMap with name cm-test-opt-upd-b5d18f82-d8af-46eb-92e8-6689f96487c2 11/15/23 16:47:14.428
    STEP: Creating the pod 11/15/23 16:47:14.44
    Nov 15 16:47:14.471: INFO: Waiting up to 5m0s for pod "pod-configmaps-1ea03b6f-99e2-4d14-bfe4-d782fefbd233" in namespace "configmap-2286" to be "running and ready"
    Nov 15 16:47:14.486: INFO: Pod "pod-configmaps-1ea03b6f-99e2-4d14-bfe4-d782fefbd233": Phase="Pending", Reason="", readiness=false. Elapsed: 14.210038ms
    Nov 15 16:47:14.486: INFO: The phase of Pod pod-configmaps-1ea03b6f-99e2-4d14-bfe4-d782fefbd233 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 16:47:16.503: INFO: Pod "pod-configmaps-1ea03b6f-99e2-4d14-bfe4-d782fefbd233": Phase="Running", Reason="", readiness=true. Elapsed: 2.031605486s
    Nov 15 16:47:16.503: INFO: The phase of Pod pod-configmaps-1ea03b6f-99e2-4d14-bfe4-d782fefbd233 is Running (Ready = true)
    Nov 15 16:47:16.503: INFO: Pod "pod-configmaps-1ea03b6f-99e2-4d14-bfe4-d782fefbd233" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-30f2f4fc-ecd5-40de-8c0b-d2f632b53abb 11/15/23 16:47:16.701
    STEP: Updating configmap cm-test-opt-upd-b5d18f82-d8af-46eb-92e8-6689f96487c2 11/15/23 16:47:16.725
    STEP: Creating configMap with name cm-test-opt-create-244e624a-307e-45a6-9e36-9f581c31c4b7 11/15/23 16:47:16.741
    STEP: waiting to observe update in volume 11/15/23 16:47:16.758
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:47:18.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2286" for this suite. 11/15/23 16:47:18.963
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:47:18.99
Nov 15 16:47:18.990: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename configmap 11/15/23 16:47:18.992
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:47:19.051
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:47:19.064
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-1695/configmap-test-e08b5b84-fece-4ab3-a05b-ad2a27702a24 11/15/23 16:47:19.076
STEP: Creating a pod to test consume configMaps 11/15/23 16:47:19.09
Nov 15 16:47:19.122: INFO: Waiting up to 5m0s for pod "pod-configmaps-23432604-fd78-4085-9450-c1ec9e4ca180" in namespace "configmap-1695" to be "Succeeded or Failed"
Nov 15 16:47:19.135: INFO: Pod "pod-configmaps-23432604-fd78-4085-9450-c1ec9e4ca180": Phase="Pending", Reason="", readiness=false. Elapsed: 13.398832ms
Nov 15 16:47:21.150: INFO: Pod "pod-configmaps-23432604-fd78-4085-9450-c1ec9e4ca180": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028041324s
Nov 15 16:47:23.151: INFO: Pod "pod-configmaps-23432604-fd78-4085-9450-c1ec9e4ca180": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029241703s
Nov 15 16:47:25.150: INFO: Pod "pod-configmaps-23432604-fd78-4085-9450-c1ec9e4ca180": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028244074s
STEP: Saw pod success 11/15/23 16:47:25.15
Nov 15 16:47:25.151: INFO: Pod "pod-configmaps-23432604-fd78-4085-9450-c1ec9e4ca180" satisfied condition "Succeeded or Failed"
Nov 15 16:47:25.165: INFO: Trying to get logs from node 10.15.40.115 pod pod-configmaps-23432604-fd78-4085-9450-c1ec9e4ca180 container env-test: <nil>
STEP: delete the pod 11/15/23 16:47:25.21
Nov 15 16:47:25.259: INFO: Waiting for pod pod-configmaps-23432604-fd78-4085-9450-c1ec9e4ca180 to disappear
Nov 15 16:47:25.271: INFO: Pod pod-configmaps-23432604-fd78-4085-9450-c1ec9e4ca180 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 15 16:47:25.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1695" for this suite. 11/15/23 16:47:25.289
------------------------------
• [SLOW TEST] [6.323 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:47:18.99
    Nov 15 16:47:18.990: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename configmap 11/15/23 16:47:18.992
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:47:19.051
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:47:19.064
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-1695/configmap-test-e08b5b84-fece-4ab3-a05b-ad2a27702a24 11/15/23 16:47:19.076
    STEP: Creating a pod to test consume configMaps 11/15/23 16:47:19.09
    Nov 15 16:47:19.122: INFO: Waiting up to 5m0s for pod "pod-configmaps-23432604-fd78-4085-9450-c1ec9e4ca180" in namespace "configmap-1695" to be "Succeeded or Failed"
    Nov 15 16:47:19.135: INFO: Pod "pod-configmaps-23432604-fd78-4085-9450-c1ec9e4ca180": Phase="Pending", Reason="", readiness=false. Elapsed: 13.398832ms
    Nov 15 16:47:21.150: INFO: Pod "pod-configmaps-23432604-fd78-4085-9450-c1ec9e4ca180": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028041324s
    Nov 15 16:47:23.151: INFO: Pod "pod-configmaps-23432604-fd78-4085-9450-c1ec9e4ca180": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029241703s
    Nov 15 16:47:25.150: INFO: Pod "pod-configmaps-23432604-fd78-4085-9450-c1ec9e4ca180": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028244074s
    STEP: Saw pod success 11/15/23 16:47:25.15
    Nov 15 16:47:25.151: INFO: Pod "pod-configmaps-23432604-fd78-4085-9450-c1ec9e4ca180" satisfied condition "Succeeded or Failed"
    Nov 15 16:47:25.165: INFO: Trying to get logs from node 10.15.40.115 pod pod-configmaps-23432604-fd78-4085-9450-c1ec9e4ca180 container env-test: <nil>
    STEP: delete the pod 11/15/23 16:47:25.21
    Nov 15 16:47:25.259: INFO: Waiting for pod pod-configmaps-23432604-fd78-4085-9450-c1ec9e4ca180 to disappear
    Nov 15 16:47:25.271: INFO: Pod pod-configmaps-23432604-fd78-4085-9450-c1ec9e4ca180 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:47:25.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1695" for this suite. 11/15/23 16:47:25.289
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:47:25.319
Nov 15 16:47:25.320: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename projected 11/15/23 16:47:25.321
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:47:25.388
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:47:25.401
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-653de34f-6d66-4ffa-97ee-618975d64101 11/15/23 16:47:25.423
STEP: Creating a pod to test consume secrets 11/15/23 16:47:25.44
Nov 15 16:47:25.472: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ac28c123-1723-4390-84d5-755d9c729c4c" in namespace "projected-9590" to be "Succeeded or Failed"
Nov 15 16:47:25.485: INFO: Pod "pod-projected-secrets-ac28c123-1723-4390-84d5-755d9c729c4c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.600995ms
Nov 15 16:47:27.500: INFO: Pod "pod-projected-secrets-ac28c123-1723-4390-84d5-755d9c729c4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027605786s
Nov 15 16:47:29.502: INFO: Pod "pod-projected-secrets-ac28c123-1723-4390-84d5-755d9c729c4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02931301s
STEP: Saw pod success 11/15/23 16:47:29.502
Nov 15 16:47:29.503: INFO: Pod "pod-projected-secrets-ac28c123-1723-4390-84d5-755d9c729c4c" satisfied condition "Succeeded or Failed"
Nov 15 16:47:29.519: INFO: Trying to get logs from node 10.15.40.115 pod pod-projected-secrets-ac28c123-1723-4390-84d5-755d9c729c4c container projected-secret-volume-test: <nil>
STEP: delete the pod 11/15/23 16:47:29.562
Nov 15 16:47:29.609: INFO: Waiting for pod pod-projected-secrets-ac28c123-1723-4390-84d5-755d9c729c4c to disappear
Nov 15 16:47:29.623: INFO: Pod pod-projected-secrets-ac28c123-1723-4390-84d5-755d9c729c4c no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Nov 15 16:47:29.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9590" for this suite. 11/15/23 16:47:29.644
------------------------------
• [4.349 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:47:25.319
    Nov 15 16:47:25.320: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename projected 11/15/23 16:47:25.321
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:47:25.388
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:47:25.401
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-653de34f-6d66-4ffa-97ee-618975d64101 11/15/23 16:47:25.423
    STEP: Creating a pod to test consume secrets 11/15/23 16:47:25.44
    Nov 15 16:47:25.472: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ac28c123-1723-4390-84d5-755d9c729c4c" in namespace "projected-9590" to be "Succeeded or Failed"
    Nov 15 16:47:25.485: INFO: Pod "pod-projected-secrets-ac28c123-1723-4390-84d5-755d9c729c4c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.600995ms
    Nov 15 16:47:27.500: INFO: Pod "pod-projected-secrets-ac28c123-1723-4390-84d5-755d9c729c4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027605786s
    Nov 15 16:47:29.502: INFO: Pod "pod-projected-secrets-ac28c123-1723-4390-84d5-755d9c729c4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02931301s
    STEP: Saw pod success 11/15/23 16:47:29.502
    Nov 15 16:47:29.503: INFO: Pod "pod-projected-secrets-ac28c123-1723-4390-84d5-755d9c729c4c" satisfied condition "Succeeded or Failed"
    Nov 15 16:47:29.519: INFO: Trying to get logs from node 10.15.40.115 pod pod-projected-secrets-ac28c123-1723-4390-84d5-755d9c729c4c container projected-secret-volume-test: <nil>
    STEP: delete the pod 11/15/23 16:47:29.562
    Nov 15 16:47:29.609: INFO: Waiting for pod pod-projected-secrets-ac28c123-1723-4390-84d5-755d9c729c4c to disappear
    Nov 15 16:47:29.623: INFO: Pod pod-projected-secrets-ac28c123-1723-4390-84d5-755d9c729c4c no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:47:29.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9590" for this suite. 11/15/23 16:47:29.644
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:47:29.671
Nov 15 16:47:29.671: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename container-runtime 11/15/23 16:47:29.673
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:47:29.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:47:29.741
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 11/15/23 16:47:29.752
STEP: wait for the container to reach Succeeded 11/15/23 16:47:29.784
STEP: get the container status 11/15/23 16:47:33.86
STEP: the container should be terminated 11/15/23 16:47:33.873
STEP: the termination message should be set 11/15/23 16:47:33.873
Nov 15 16:47:33.873: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 11/15/23 16:47:33.873
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Nov 15 16:47:33.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-2288" for this suite. 11/15/23 16:47:33.945
------------------------------
• [4.298 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:47:29.671
    Nov 15 16:47:29.671: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename container-runtime 11/15/23 16:47:29.673
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:47:29.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:47:29.741
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 11/15/23 16:47:29.752
    STEP: wait for the container to reach Succeeded 11/15/23 16:47:29.784
    STEP: get the container status 11/15/23 16:47:33.86
    STEP: the container should be terminated 11/15/23 16:47:33.873
    STEP: the termination message should be set 11/15/23 16:47:33.873
    Nov 15 16:47:33.873: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 11/15/23 16:47:33.873
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:47:33.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-2288" for this suite. 11/15/23 16:47:33.945
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:47:33.97
Nov 15 16:47:33.970: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename webhook 11/15/23 16:47:33.971
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:47:34.027
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:47:34.04
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/15/23 16:47:34.103
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 16:47:34.506
STEP: Deploying the webhook pod 11/15/23 16:47:34.534
STEP: Wait for the deployment to be ready 11/15/23 16:47:34.573
Nov 15 16:47:34.607: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/15/23 16:47:36.687
STEP: Verifying the service has paired with the endpoint 11/15/23 16:47:36.732
Nov 15 16:47:37.733: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Nov 15 16:47:37.749: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6880-crds.webhook.example.com via the AdmissionRegistration API 11/15/23 16:47:38.302
STEP: Creating a custom resource that should be mutated by the webhook 11/15/23 16:47:38.428
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 16:47:41.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4247" for this suite. 11/15/23 16:47:41.392
STEP: Destroying namespace "webhook-4247-markers" for this suite. 11/15/23 16:47:41.414
------------------------------
• [SLOW TEST] [7.467 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:47:33.97
    Nov 15 16:47:33.970: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename webhook 11/15/23 16:47:33.971
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:47:34.027
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:47:34.04
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/15/23 16:47:34.103
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 16:47:34.506
    STEP: Deploying the webhook pod 11/15/23 16:47:34.534
    STEP: Wait for the deployment to be ready 11/15/23 16:47:34.573
    Nov 15 16:47:34.607: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/15/23 16:47:36.687
    STEP: Verifying the service has paired with the endpoint 11/15/23 16:47:36.732
    Nov 15 16:47:37.733: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Nov 15 16:47:37.749: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6880-crds.webhook.example.com via the AdmissionRegistration API 11/15/23 16:47:38.302
    STEP: Creating a custom resource that should be mutated by the webhook 11/15/23 16:47:38.428
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:47:41.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4247" for this suite. 11/15/23 16:47:41.392
    STEP: Destroying namespace "webhook-4247-markers" for this suite. 11/15/23 16:47:41.414
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:47:41.46
Nov 15 16:47:41.461: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename webhook 11/15/23 16:47:41.462
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:47:41.507
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:47:41.524
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/15/23 16:47:41.585
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 16:47:42.073
STEP: Deploying the webhook pod 11/15/23 16:47:42.125
STEP: Wait for the deployment to be ready 11/15/23 16:47:42.177
Nov 15 16:47:42.210: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 15 16:47:44.262: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 16, 47, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 16, 47, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 16, 47, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 16, 47, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 11/15/23 16:47:46.277
STEP: Verifying the service has paired with the endpoint 11/15/23 16:47:46.331
Nov 15 16:47:47.333: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 11/15/23 16:47:47.358
STEP: create a pod 11/15/23 16:47:47.478
Nov 15 16:47:47.500: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-6329" to be "running"
Nov 15 16:47:47.532: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 31.39052ms
Nov 15 16:47:49.552: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051583475s
Nov 15 16:47:51.553: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.052562836s
Nov 15 16:47:51.553: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 11/15/23 16:47:51.553
Nov 15 16:47:51.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=webhook-6329 attach --namespace=webhook-6329 to-be-attached-pod -i -c=container1'
Nov 15 16:47:51.794: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 16:47:51.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6329" for this suite. 11/15/23 16:47:51.984
STEP: Destroying namespace "webhook-6329-markers" for this suite. 11/15/23 16:47:52.006
------------------------------
• [SLOW TEST] [10.570 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:47:41.46
    Nov 15 16:47:41.461: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename webhook 11/15/23 16:47:41.462
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:47:41.507
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:47:41.524
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/15/23 16:47:41.585
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 16:47:42.073
    STEP: Deploying the webhook pod 11/15/23 16:47:42.125
    STEP: Wait for the deployment to be ready 11/15/23 16:47:42.177
    Nov 15 16:47:42.210: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Nov 15 16:47:44.262: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 16, 47, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 16, 47, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 16, 47, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 16, 47, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 11/15/23 16:47:46.277
    STEP: Verifying the service has paired with the endpoint 11/15/23 16:47:46.331
    Nov 15 16:47:47.333: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 11/15/23 16:47:47.358
    STEP: create a pod 11/15/23 16:47:47.478
    Nov 15 16:47:47.500: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-6329" to be "running"
    Nov 15 16:47:47.532: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 31.39052ms
    Nov 15 16:47:49.552: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051583475s
    Nov 15 16:47:51.553: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.052562836s
    Nov 15 16:47:51.553: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 11/15/23 16:47:51.553
    Nov 15 16:47:51.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=webhook-6329 attach --namespace=webhook-6329 to-be-attached-pod -i -c=container1'
    Nov 15 16:47:51.794: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:47:51.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6329" for this suite. 11/15/23 16:47:51.984
    STEP: Destroying namespace "webhook-6329-markers" for this suite. 11/15/23 16:47:52.006
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:47:52.036
Nov 15 16:47:52.037: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename watch 11/15/23 16:47:52.039
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:47:52.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:47:52.103
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 11/15/23 16:47:52.122
STEP: creating a new configmap 11/15/23 16:47:52.131
STEP: modifying the configmap once 11/15/23 16:47:52.153
STEP: changing the label value of the configmap 11/15/23 16:47:52.209
STEP: Expecting to observe a delete notification for the watched object 11/15/23 16:47:52.254
Nov 15 16:47:52.254: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1252  859f2a39-b463-4a15-bef0-2fb7acb01953 45879 0 2023-11-15 16:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-11-15 16:47:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 15 16:47:52.255: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1252  859f2a39-b463-4a15-bef0-2fb7acb01953 45880 0 2023-11-15 16:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-11-15 16:47:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 15 16:47:52.256: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1252  859f2a39-b463-4a15-bef0-2fb7acb01953 45881 0 2023-11-15 16:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-11-15 16:47:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 11/15/23 16:47:52.256
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 11/15/23 16:47:52.313
STEP: changing the label value of the configmap back 11/15/23 16:48:02.314
STEP: modifying the configmap a third time 11/15/23 16:48:02.363
STEP: deleting the configmap 11/15/23 16:48:02.404
STEP: Expecting to observe an add notification for the watched object when the label value was restored 11/15/23 16:48:02.434
Nov 15 16:48:02.435: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1252  859f2a39-b463-4a15-bef0-2fb7acb01953 45925 0 2023-11-15 16:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-11-15 16:48:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 15 16:48:02.435: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1252  859f2a39-b463-4a15-bef0-2fb7acb01953 45926 0 2023-11-15 16:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-11-15 16:48:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 15 16:48:02.436: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1252  859f2a39-b463-4a15-bef0-2fb7acb01953 45927 0 2023-11-15 16:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-11-15 16:48:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Nov 15 16:48:02.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-1252" for this suite. 11/15/23 16:48:02.459
------------------------------
• [SLOW TEST] [10.445 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:47:52.036
    Nov 15 16:47:52.037: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename watch 11/15/23 16:47:52.039
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:47:52.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:47:52.103
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 11/15/23 16:47:52.122
    STEP: creating a new configmap 11/15/23 16:47:52.131
    STEP: modifying the configmap once 11/15/23 16:47:52.153
    STEP: changing the label value of the configmap 11/15/23 16:47:52.209
    STEP: Expecting to observe a delete notification for the watched object 11/15/23 16:47:52.254
    Nov 15 16:47:52.254: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1252  859f2a39-b463-4a15-bef0-2fb7acb01953 45879 0 2023-11-15 16:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-11-15 16:47:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 15 16:47:52.255: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1252  859f2a39-b463-4a15-bef0-2fb7acb01953 45880 0 2023-11-15 16:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-11-15 16:47:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 15 16:47:52.256: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1252  859f2a39-b463-4a15-bef0-2fb7acb01953 45881 0 2023-11-15 16:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-11-15 16:47:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 11/15/23 16:47:52.256
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 11/15/23 16:47:52.313
    STEP: changing the label value of the configmap back 11/15/23 16:48:02.314
    STEP: modifying the configmap a third time 11/15/23 16:48:02.363
    STEP: deleting the configmap 11/15/23 16:48:02.404
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 11/15/23 16:48:02.434
    Nov 15 16:48:02.435: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1252  859f2a39-b463-4a15-bef0-2fb7acb01953 45925 0 2023-11-15 16:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-11-15 16:48:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 15 16:48:02.435: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1252  859f2a39-b463-4a15-bef0-2fb7acb01953 45926 0 2023-11-15 16:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-11-15 16:48:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 15 16:48:02.436: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1252  859f2a39-b463-4a15-bef0-2fb7acb01953 45927 0 2023-11-15 16:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-11-15 16:48:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:48:02.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-1252" for this suite. 11/15/23 16:48:02.459
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:48:02.491
Nov 15 16:48:02.491: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename downward-api 11/15/23 16:48:02.493
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:48:02.548
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:48:02.581
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 11/15/23 16:48:02.599
Nov 15 16:48:02.635: INFO: Waiting up to 5m0s for pod "labelsupdate9c9f4610-9e78-4f0b-90a8-6b4c5f795e52" in namespace "downward-api-6559" to be "running and ready"
Nov 15 16:48:02.656: INFO: Pod "labelsupdate9c9f4610-9e78-4f0b-90a8-6b4c5f795e52": Phase="Pending", Reason="", readiness=false. Elapsed: 20.647921ms
Nov 15 16:48:02.656: INFO: The phase of Pod labelsupdate9c9f4610-9e78-4f0b-90a8-6b4c5f795e52 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 16:48:04.676: INFO: Pod "labelsupdate9c9f4610-9e78-4f0b-90a8-6b4c5f795e52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040641228s
Nov 15 16:48:04.676: INFO: The phase of Pod labelsupdate9c9f4610-9e78-4f0b-90a8-6b4c5f795e52 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 16:48:06.677: INFO: Pod "labelsupdate9c9f4610-9e78-4f0b-90a8-6b4c5f795e52": Phase="Running", Reason="", readiness=true. Elapsed: 4.042016615s
Nov 15 16:48:06.677: INFO: The phase of Pod labelsupdate9c9f4610-9e78-4f0b-90a8-6b4c5f795e52 is Running (Ready = true)
Nov 15 16:48:06.677: INFO: Pod "labelsupdate9c9f4610-9e78-4f0b-90a8-6b4c5f795e52" satisfied condition "running and ready"
Nov 15 16:48:07.396: INFO: Successfully updated pod "labelsupdate9c9f4610-9e78-4f0b-90a8-6b4c5f795e52"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Nov 15 16:48:09.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6559" for this suite. 11/15/23 16:48:09.5
------------------------------
• [SLOW TEST] [7.031 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:48:02.491
    Nov 15 16:48:02.491: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename downward-api 11/15/23 16:48:02.493
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:48:02.548
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:48:02.581
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 11/15/23 16:48:02.599
    Nov 15 16:48:02.635: INFO: Waiting up to 5m0s for pod "labelsupdate9c9f4610-9e78-4f0b-90a8-6b4c5f795e52" in namespace "downward-api-6559" to be "running and ready"
    Nov 15 16:48:02.656: INFO: Pod "labelsupdate9c9f4610-9e78-4f0b-90a8-6b4c5f795e52": Phase="Pending", Reason="", readiness=false. Elapsed: 20.647921ms
    Nov 15 16:48:02.656: INFO: The phase of Pod labelsupdate9c9f4610-9e78-4f0b-90a8-6b4c5f795e52 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 16:48:04.676: INFO: Pod "labelsupdate9c9f4610-9e78-4f0b-90a8-6b4c5f795e52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040641228s
    Nov 15 16:48:04.676: INFO: The phase of Pod labelsupdate9c9f4610-9e78-4f0b-90a8-6b4c5f795e52 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 16:48:06.677: INFO: Pod "labelsupdate9c9f4610-9e78-4f0b-90a8-6b4c5f795e52": Phase="Running", Reason="", readiness=true. Elapsed: 4.042016615s
    Nov 15 16:48:06.677: INFO: The phase of Pod labelsupdate9c9f4610-9e78-4f0b-90a8-6b4c5f795e52 is Running (Ready = true)
    Nov 15 16:48:06.677: INFO: Pod "labelsupdate9c9f4610-9e78-4f0b-90a8-6b4c5f795e52" satisfied condition "running and ready"
    Nov 15 16:48:07.396: INFO: Successfully updated pod "labelsupdate9c9f4610-9e78-4f0b-90a8-6b4c5f795e52"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:48:09.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6559" for this suite. 11/15/23 16:48:09.5
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:48:09.537
Nov 15 16:48:09.537: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename webhook 11/15/23 16:48:09.539
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:48:09.584
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:48:09.602
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/15/23 16:48:09.666
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 16:48:10.556
STEP: Deploying the webhook pod 11/15/23 16:48:10.578
STEP: Wait for the deployment to be ready 11/15/23 16:48:10.619
Nov 15 16:48:10.648: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 15 16:48:12.697: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 16, 48, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 16, 48, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 16, 48, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 16, 48, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 11/15/23 16:48:14.717
STEP: Verifying the service has paired with the endpoint 11/15/23 16:48:14.765
Nov 15 16:48:15.768: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 11/15/23 16:48:15.785
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 11/15/23 16:48:15.914
STEP: Creating a dummy validating-webhook-configuration object 11/15/23 16:48:16.041
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 11/15/23 16:48:16.086
STEP: Creating a dummy mutating-webhook-configuration object 11/15/23 16:48:16.12
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 11/15/23 16:48:16.164
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 16:48:16.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1636" for this suite. 11/15/23 16:48:16.446
STEP: Destroying namespace "webhook-1636-markers" for this suite. 11/15/23 16:48:16.467
------------------------------
• [SLOW TEST] [6.954 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:48:09.537
    Nov 15 16:48:09.537: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename webhook 11/15/23 16:48:09.539
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:48:09.584
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:48:09.602
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/15/23 16:48:09.666
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 16:48:10.556
    STEP: Deploying the webhook pod 11/15/23 16:48:10.578
    STEP: Wait for the deployment to be ready 11/15/23 16:48:10.619
    Nov 15 16:48:10.648: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Nov 15 16:48:12.697: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 16, 48, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 16, 48, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 16, 48, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 16, 48, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 11/15/23 16:48:14.717
    STEP: Verifying the service has paired with the endpoint 11/15/23 16:48:14.765
    Nov 15 16:48:15.768: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 11/15/23 16:48:15.785
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 11/15/23 16:48:15.914
    STEP: Creating a dummy validating-webhook-configuration object 11/15/23 16:48:16.041
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 11/15/23 16:48:16.086
    STEP: Creating a dummy mutating-webhook-configuration object 11/15/23 16:48:16.12
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 11/15/23 16:48:16.164
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:48:16.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1636" for this suite. 11/15/23 16:48:16.446
    STEP: Destroying namespace "webhook-1636-markers" for this suite. 11/15/23 16:48:16.467
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:48:16.497
Nov 15 16:48:16.498: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename svcaccounts 11/15/23 16:48:16.5
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:48:16.545
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:48:16.564
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-rs849"  11/15/23 16:48:16.581
Nov 15 16:48:16.603: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-rs849"  11/15/23 16:48:16.604
Nov 15 16:48:16.642: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Nov 15 16:48:16.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-494" for this suite. 11/15/23 16:48:16.66
------------------------------
• [0.186 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:48:16.497
    Nov 15 16:48:16.498: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename svcaccounts 11/15/23 16:48:16.5
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:48:16.545
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:48:16.564
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-rs849"  11/15/23 16:48:16.581
    Nov 15 16:48:16.603: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-rs849"  11/15/23 16:48:16.604
    Nov 15 16:48:16.642: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:48:16.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-494" for this suite. 11/15/23 16:48:16.66
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:48:16.684
Nov 15 16:48:16.684: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename daemonsets 11/15/23 16:48:16.689
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:48:16.736
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:48:16.755
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305
STEP: Creating a simple DaemonSet "daemon-set" 11/15/23 16:48:16.855
STEP: Check that daemon pods launch on every node of the cluster. 11/15/23 16:48:16.874
Nov 15 16:48:16.928: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 16:48:16.928: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
Nov 15 16:48:17.993: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 16:48:17.993: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
Nov 15 16:48:18.971: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 15 16:48:18.971: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 11/15/23 16:48:18.988
Nov 15 16:48:19.073: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 15 16:48:19.074: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
Nov 15 16:48:20.116: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 15 16:48:20.116: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
Nov 15 16:48:21.116: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 15 16:48:21.116: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 11/15/23 16:48:21.116
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 11/15/23 16:48:21.15
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3265, will wait for the garbage collector to delete the pods 11/15/23 16:48:21.151
Nov 15 16:48:21.238: INFO: Deleting DaemonSet.extensions daemon-set took: 21.356716ms
Nov 15 16:48:21.339: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.488384ms
Nov 15 16:48:24.063: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 16:48:24.063: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Nov 15 16:48:24.078: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"46180"},"items":null}

Nov 15 16:48:24.097: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"46180"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 16:48:24.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3265" for this suite. 11/15/23 16:48:24.186
------------------------------
• [SLOW TEST] [7.525 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:48:16.684
    Nov 15 16:48:16.684: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename daemonsets 11/15/23 16:48:16.689
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:48:16.736
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:48:16.755
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:305
    STEP: Creating a simple DaemonSet "daemon-set" 11/15/23 16:48:16.855
    STEP: Check that daemon pods launch on every node of the cluster. 11/15/23 16:48:16.874
    Nov 15 16:48:16.928: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 16:48:16.928: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
    Nov 15 16:48:17.993: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 16:48:17.993: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
    Nov 15 16:48:18.971: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Nov 15 16:48:18.971: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 11/15/23 16:48:18.988
    Nov 15 16:48:19.073: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Nov 15 16:48:19.074: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
    Nov 15 16:48:20.116: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Nov 15 16:48:20.116: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
    Nov 15 16:48:21.116: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Nov 15 16:48:21.116: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 11/15/23 16:48:21.116
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 11/15/23 16:48:21.15
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3265, will wait for the garbage collector to delete the pods 11/15/23 16:48:21.151
    Nov 15 16:48:21.238: INFO: Deleting DaemonSet.extensions daemon-set took: 21.356716ms
    Nov 15 16:48:21.339: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.488384ms
    Nov 15 16:48:24.063: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 16:48:24.063: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Nov 15 16:48:24.078: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"46180"},"items":null}

    Nov 15 16:48:24.097: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"46180"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:48:24.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3265" for this suite. 11/15/23 16:48:24.186
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:48:24.218
Nov 15 16:48:24.219: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename sched-preemption 11/15/23 16:48:24.22
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:48:24.264
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:48:24.283
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Nov 15 16:48:24.353: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 15 16:49:24.500: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:49:24.518
Nov 15 16:49:24.518: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename sched-preemption-path 11/15/23 16:49:24.521
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:49:24.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:49:24.587
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
Nov 15 16:49:24.662: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Nov 15 16:49:24.677: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Nov 15 16:49:24.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 16:49:24.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-7965" for this suite. 11/15/23 16:49:25.002
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-3154" for this suite. 11/15/23 16:49:25.024
------------------------------
• [SLOW TEST] [60.827 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:48:24.218
    Nov 15 16:48:24.219: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename sched-preemption 11/15/23 16:48:24.22
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:48:24.264
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:48:24.283
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Nov 15 16:48:24.353: INFO: Waiting up to 1m0s for all nodes to be ready
    Nov 15 16:49:24.500: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:49:24.518
    Nov 15 16:49:24.518: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename sched-preemption-path 11/15/23 16:49:24.521
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:49:24.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:49:24.587
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    Nov 15 16:49:24.662: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Nov 15 16:49:24.677: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:49:24.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:49:24.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-7965" for this suite. 11/15/23 16:49:25.002
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-3154" for this suite. 11/15/23 16:49:25.024
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:49:25.057
Nov 15 16:49:25.058: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename projected 11/15/23 16:49:25.059
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:49:25.101
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:49:25.119
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 11/15/23 16:49:25.138
Nov 15 16:49:25.169: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c30e8439-b4ee-451b-907b-46304677f87a" in namespace "projected-9930" to be "Succeeded or Failed"
Nov 15 16:49:25.191: INFO: Pod "downwardapi-volume-c30e8439-b4ee-451b-907b-46304677f87a": Phase="Pending", Reason="", readiness=false. Elapsed: 21.943896ms
Nov 15 16:49:27.213: INFO: Pod "downwardapi-volume-c30e8439-b4ee-451b-907b-46304677f87a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044110956s
Nov 15 16:49:29.217: INFO: Pod "downwardapi-volume-c30e8439-b4ee-451b-907b-46304677f87a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047938173s
STEP: Saw pod success 11/15/23 16:49:29.217
Nov 15 16:49:29.219: INFO: Pod "downwardapi-volume-c30e8439-b4ee-451b-907b-46304677f87a" satisfied condition "Succeeded or Failed"
Nov 15 16:49:29.256: INFO: Trying to get logs from node 10.15.40.115 pod downwardapi-volume-c30e8439-b4ee-451b-907b-46304677f87a container client-container: <nil>
STEP: delete the pod 11/15/23 16:49:29.323
Nov 15 16:49:29.368: INFO: Waiting for pod downwardapi-volume-c30e8439-b4ee-451b-907b-46304677f87a to disappear
Nov 15 16:49:29.389: INFO: Pod downwardapi-volume-c30e8439-b4ee-451b-907b-46304677f87a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Nov 15 16:49:29.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9930" for this suite. 11/15/23 16:49:29.414
------------------------------
• [4.377 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:49:25.057
    Nov 15 16:49:25.058: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename projected 11/15/23 16:49:25.059
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:49:25.101
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:49:25.119
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 11/15/23 16:49:25.138
    Nov 15 16:49:25.169: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c30e8439-b4ee-451b-907b-46304677f87a" in namespace "projected-9930" to be "Succeeded or Failed"
    Nov 15 16:49:25.191: INFO: Pod "downwardapi-volume-c30e8439-b4ee-451b-907b-46304677f87a": Phase="Pending", Reason="", readiness=false. Elapsed: 21.943896ms
    Nov 15 16:49:27.213: INFO: Pod "downwardapi-volume-c30e8439-b4ee-451b-907b-46304677f87a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044110956s
    Nov 15 16:49:29.217: INFO: Pod "downwardapi-volume-c30e8439-b4ee-451b-907b-46304677f87a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047938173s
    STEP: Saw pod success 11/15/23 16:49:29.217
    Nov 15 16:49:29.219: INFO: Pod "downwardapi-volume-c30e8439-b4ee-451b-907b-46304677f87a" satisfied condition "Succeeded or Failed"
    Nov 15 16:49:29.256: INFO: Trying to get logs from node 10.15.40.115 pod downwardapi-volume-c30e8439-b4ee-451b-907b-46304677f87a container client-container: <nil>
    STEP: delete the pod 11/15/23 16:49:29.323
    Nov 15 16:49:29.368: INFO: Waiting for pod downwardapi-volume-c30e8439-b4ee-451b-907b-46304677f87a to disappear
    Nov 15 16:49:29.389: INFO: Pod downwardapi-volume-c30e8439-b4ee-451b-907b-46304677f87a no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:49:29.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9930" for this suite. 11/15/23 16:49:29.414
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:49:29.449
Nov 15 16:49:29.449: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename replicaset 11/15/23 16:49:29.451
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:49:29.494
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:49:29.511
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 11/15/23 16:49:29.53
STEP: Verify that the required pods have come up 11/15/23 16:49:29.548
Nov 15 16:49:29.565: INFO: Pod name sample-pod: Found 0 pods out of 3
Nov 15 16:49:34.591: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 11/15/23 16:49:34.592
Nov 15 16:49:34.604: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 11/15/23 16:49:34.604
STEP: DeleteCollection of the ReplicaSets 11/15/23 16:49:34.619
STEP: After DeleteCollection verify that ReplicaSets have been deleted 11/15/23 16:49:34.667
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Nov 15 16:49:34.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-9372" for this suite. 11/15/23 16:49:34.744
------------------------------
• [SLOW TEST] [5.316 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:49:29.449
    Nov 15 16:49:29.449: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename replicaset 11/15/23 16:49:29.451
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:49:29.494
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:49:29.511
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 11/15/23 16:49:29.53
    STEP: Verify that the required pods have come up 11/15/23 16:49:29.548
    Nov 15 16:49:29.565: INFO: Pod name sample-pod: Found 0 pods out of 3
    Nov 15 16:49:34.591: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 11/15/23 16:49:34.592
    Nov 15 16:49:34.604: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 11/15/23 16:49:34.604
    STEP: DeleteCollection of the ReplicaSets 11/15/23 16:49:34.619
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 11/15/23 16:49:34.667
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:49:34.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-9372" for this suite. 11/15/23 16:49:34.744
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:49:34.765
Nov 15 16:49:34.766: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename watch 11/15/23 16:49:34.768
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:49:34.816
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:49:34.842
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 11/15/23 16:49:34.862
STEP: starting a background goroutine to produce watch events 11/15/23 16:49:34.89
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 11/15/23 16:49:34.89
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Nov 15 16:49:37.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-1851" for this suite. 11/15/23 16:49:37.687
------------------------------
• [2.946 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:49:34.765
    Nov 15 16:49:34.766: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename watch 11/15/23 16:49:34.768
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:49:34.816
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:49:34.842
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 11/15/23 16:49:34.862
    STEP: starting a background goroutine to produce watch events 11/15/23 16:49:34.89
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 11/15/23 16:49:34.89
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:49:37.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-1851" for this suite. 11/15/23 16:49:37.687
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:49:37.722
Nov 15 16:49:37.722: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename secrets 11/15/23 16:49:37.724
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:49:37.767
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:49:37.787
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-8d3a7688-fe8f-4c4e-9487-7f551d955c3e 11/15/23 16:49:37.815
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 15 16:49:37.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9808" for this suite. 11/15/23 16:49:37.846
------------------------------
• [0.157 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:49:37.722
    Nov 15 16:49:37.722: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename secrets 11/15/23 16:49:37.724
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:49:37.767
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:49:37.787
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-8d3a7688-fe8f-4c4e-9487-7f551d955c3e 11/15/23 16:49:37.815
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:49:37.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9808" for this suite. 11/15/23 16:49:37.846
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:49:37.885
Nov 15 16:49:37.886: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename emptydir 11/15/23 16:49:37.887
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:49:37.929
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:49:37.951
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 11/15/23 16:49:37.971
Nov 15 16:49:38.009: INFO: Waiting up to 5m0s for pod "pod-5489f293-9bb1-4c32-b643-2d58ff2f2c6f" in namespace "emptydir-218" to be "Succeeded or Failed"
Nov 15 16:49:38.032: INFO: Pod "pod-5489f293-9bb1-4c32-b643-2d58ff2f2c6f": Phase="Pending", Reason="", readiness=false. Elapsed: 22.585093ms
Nov 15 16:49:40.054: INFO: Pod "pod-5489f293-9bb1-4c32-b643-2d58ff2f2c6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045209458s
Nov 15 16:49:42.054: INFO: Pod "pod-5489f293-9bb1-4c32-b643-2d58ff2f2c6f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044411634s
Nov 15 16:49:44.066: INFO: Pod "pod-5489f293-9bb1-4c32-b643-2d58ff2f2c6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.057352563s
STEP: Saw pod success 11/15/23 16:49:44.067
Nov 15 16:49:44.067: INFO: Pod "pod-5489f293-9bb1-4c32-b643-2d58ff2f2c6f" satisfied condition "Succeeded or Failed"
Nov 15 16:49:44.087: INFO: Trying to get logs from node 10.15.40.115 pod pod-5489f293-9bb1-4c32-b643-2d58ff2f2c6f container test-container: <nil>
STEP: delete the pod 11/15/23 16:49:44.127
Nov 15 16:49:44.179: INFO: Waiting for pod pod-5489f293-9bb1-4c32-b643-2d58ff2f2c6f to disappear
Nov 15 16:49:44.199: INFO: Pod pod-5489f293-9bb1-4c32-b643-2d58ff2f2c6f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 15 16:49:44.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-218" for this suite. 11/15/23 16:49:44.231
------------------------------
• [SLOW TEST] [6.367 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:49:37.885
    Nov 15 16:49:37.886: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename emptydir 11/15/23 16:49:37.887
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:49:37.929
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:49:37.951
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 11/15/23 16:49:37.971
    Nov 15 16:49:38.009: INFO: Waiting up to 5m0s for pod "pod-5489f293-9bb1-4c32-b643-2d58ff2f2c6f" in namespace "emptydir-218" to be "Succeeded or Failed"
    Nov 15 16:49:38.032: INFO: Pod "pod-5489f293-9bb1-4c32-b643-2d58ff2f2c6f": Phase="Pending", Reason="", readiness=false. Elapsed: 22.585093ms
    Nov 15 16:49:40.054: INFO: Pod "pod-5489f293-9bb1-4c32-b643-2d58ff2f2c6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045209458s
    Nov 15 16:49:42.054: INFO: Pod "pod-5489f293-9bb1-4c32-b643-2d58ff2f2c6f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044411634s
    Nov 15 16:49:44.066: INFO: Pod "pod-5489f293-9bb1-4c32-b643-2d58ff2f2c6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.057352563s
    STEP: Saw pod success 11/15/23 16:49:44.067
    Nov 15 16:49:44.067: INFO: Pod "pod-5489f293-9bb1-4c32-b643-2d58ff2f2c6f" satisfied condition "Succeeded or Failed"
    Nov 15 16:49:44.087: INFO: Trying to get logs from node 10.15.40.115 pod pod-5489f293-9bb1-4c32-b643-2d58ff2f2c6f container test-container: <nil>
    STEP: delete the pod 11/15/23 16:49:44.127
    Nov 15 16:49:44.179: INFO: Waiting for pod pod-5489f293-9bb1-4c32-b643-2d58ff2f2c6f to disappear
    Nov 15 16:49:44.199: INFO: Pod pod-5489f293-9bb1-4c32-b643-2d58ff2f2c6f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:49:44.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-218" for this suite. 11/15/23 16:49:44.231
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:49:44.26
Nov 15 16:49:44.260: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename services 11/15/23 16:49:44.263
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:49:44.307
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:49:44.323
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-2207 11/15/23 16:49:44.346
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 11/15/23 16:49:44.393
STEP: creating service externalsvc in namespace services-2207 11/15/23 16:49:44.393
STEP: creating replication controller externalsvc in namespace services-2207 11/15/23 16:49:44.437
I1115 16:49:44.454788      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2207, replica count: 2
I1115 16:49:47.506061      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 11/15/23 16:49:47.531
Nov 15 16:49:47.588: INFO: Creating new exec pod
Nov 15 16:49:47.613: INFO: Waiting up to 5m0s for pod "execpodwn4mp" in namespace "services-2207" to be "running"
Nov 15 16:49:47.637: INFO: Pod "execpodwn4mp": Phase="Pending", Reason="", readiness=false. Elapsed: 24.390304ms
Nov 15 16:49:49.659: INFO: Pod "execpodwn4mp": Phase="Running", Reason="", readiness=true. Elapsed: 2.046451711s
Nov 15 16:49:49.659: INFO: Pod "execpodwn4mp" satisfied condition "running"
Nov 15 16:49:49.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-2207 exec execpodwn4mp -- /bin/sh -x -c nslookup clusterip-service.services-2207.svc.cluster.local'
Nov 15 16:49:50.175: INFO: stderr: "+ nslookup clusterip-service.services-2207.svc.cluster.local\n"
Nov 15 16:49:50.175: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-2207.svc.cluster.local\tcanonical name = externalsvc.services-2207.svc.cluster.local.\nName:\texternalsvc.services-2207.svc.cluster.local\nAddress: 172.21.50.106\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2207, will wait for the garbage collector to delete the pods 11/15/23 16:49:50.175
Nov 15 16:49:50.268: INFO: Deleting ReplicationController externalsvc took: 20.153265ms
Nov 15 16:49:50.370: INFO: Terminating ReplicationController externalsvc pods took: 101.862944ms
Nov 15 16:49:53.132: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 15 16:49:53.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2207" for this suite. 11/15/23 16:49:53.19
------------------------------
• [SLOW TEST] [8.952 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:49:44.26
    Nov 15 16:49:44.260: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename services 11/15/23 16:49:44.263
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:49:44.307
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:49:44.323
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-2207 11/15/23 16:49:44.346
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 11/15/23 16:49:44.393
    STEP: creating service externalsvc in namespace services-2207 11/15/23 16:49:44.393
    STEP: creating replication controller externalsvc in namespace services-2207 11/15/23 16:49:44.437
    I1115 16:49:44.454788      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2207, replica count: 2
    I1115 16:49:47.506061      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 11/15/23 16:49:47.531
    Nov 15 16:49:47.588: INFO: Creating new exec pod
    Nov 15 16:49:47.613: INFO: Waiting up to 5m0s for pod "execpodwn4mp" in namespace "services-2207" to be "running"
    Nov 15 16:49:47.637: INFO: Pod "execpodwn4mp": Phase="Pending", Reason="", readiness=false. Elapsed: 24.390304ms
    Nov 15 16:49:49.659: INFO: Pod "execpodwn4mp": Phase="Running", Reason="", readiness=true. Elapsed: 2.046451711s
    Nov 15 16:49:49.659: INFO: Pod "execpodwn4mp" satisfied condition "running"
    Nov 15 16:49:49.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-2207 exec execpodwn4mp -- /bin/sh -x -c nslookup clusterip-service.services-2207.svc.cluster.local'
    Nov 15 16:49:50.175: INFO: stderr: "+ nslookup clusterip-service.services-2207.svc.cluster.local\n"
    Nov 15 16:49:50.175: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-2207.svc.cluster.local\tcanonical name = externalsvc.services-2207.svc.cluster.local.\nName:\texternalsvc.services-2207.svc.cluster.local\nAddress: 172.21.50.106\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-2207, will wait for the garbage collector to delete the pods 11/15/23 16:49:50.175
    Nov 15 16:49:50.268: INFO: Deleting ReplicationController externalsvc took: 20.153265ms
    Nov 15 16:49:50.370: INFO: Terminating ReplicationController externalsvc pods took: 101.862944ms
    Nov 15 16:49:53.132: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:49:53.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2207" for this suite. 11/15/23 16:49:53.19
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:49:53.216
Nov 15 16:49:53.217: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename services 11/15/23 16:49:53.219
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:49:53.271
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:49:53.288
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-6703 11/15/23 16:49:53.312
STEP: creating service affinity-clusterip-transition in namespace services-6703 11/15/23 16:49:53.312
STEP: creating replication controller affinity-clusterip-transition in namespace services-6703 11/15/23 16:49:53.353
I1115 16:49:53.391958      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-6703, replica count: 3
I1115 16:49:56.444559      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 15 16:49:56.472: INFO: Creating new exec pod
Nov 15 16:49:56.494: INFO: Waiting up to 5m0s for pod "execpod-affinity8hnrh" in namespace "services-6703" to be "running"
Nov 15 16:49:56.514: INFO: Pod "execpod-affinity8hnrh": Phase="Pending", Reason="", readiness=false. Elapsed: 19.460706ms
Nov 15 16:49:58.534: INFO: Pod "execpod-affinity8hnrh": Phase="Running", Reason="", readiness=true. Elapsed: 2.039510914s
Nov 15 16:49:58.534: INFO: Pod "execpod-affinity8hnrh" satisfied condition "running"
Nov 15 16:49:59.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-6703 exec execpod-affinity8hnrh -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Nov 15 16:50:01.010: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Nov 15 16:50:01.010: INFO: stdout: ""
Nov 15 16:50:01.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-6703 exec execpod-affinity8hnrh -- /bin/sh -x -c nc -v -z -w 2 172.21.215.37 80'
Nov 15 16:50:01.419: INFO: stderr: "+ nc -v -z -w 2 172.21.215.37 80\nConnection to 172.21.215.37 80 port [tcp/http] succeeded!\n"
Nov 15 16:50:01.419: INFO: stdout: ""
Nov 15 16:50:01.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-6703 exec execpod-affinity8hnrh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.215.37:80/ ; done'
Nov 15 16:50:02.128: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n"
Nov 15 16:50:02.128: INFO: stdout: "\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd"
Nov 15 16:50:02.128: INFO: Received response from host: affinity-clusterip-transition-tbfjd
Nov 15 16:50:02.128: INFO: Received response from host: affinity-clusterip-transition-tbfjd
Nov 15 16:50:02.128: INFO: Received response from host: affinity-clusterip-transition-tbfjd
Nov 15 16:50:02.128: INFO: Received response from host: affinity-clusterip-transition-tbfjd
Nov 15 16:50:02.128: INFO: Received response from host: affinity-clusterip-transition-tbfjd
Nov 15 16:50:02.128: INFO: Received response from host: affinity-clusterip-transition-tbfjd
Nov 15 16:50:02.128: INFO: Received response from host: affinity-clusterip-transition-tbfjd
Nov 15 16:50:02.128: INFO: Received response from host: affinity-clusterip-transition-tbfjd
Nov 15 16:50:02.128: INFO: Received response from host: affinity-clusterip-transition-tbfjd
Nov 15 16:50:02.128: INFO: Received response from host: affinity-clusterip-transition-tbfjd
Nov 15 16:50:02.129: INFO: Received response from host: affinity-clusterip-transition-tbfjd
Nov 15 16:50:02.129: INFO: Received response from host: affinity-clusterip-transition-tbfjd
Nov 15 16:50:02.129: INFO: Received response from host: affinity-clusterip-transition-tbfjd
Nov 15 16:50:02.129: INFO: Received response from host: affinity-clusterip-transition-tbfjd
Nov 15 16:50:02.129: INFO: Received response from host: affinity-clusterip-transition-tbfjd
Nov 15 16:50:02.129: INFO: Received response from host: affinity-clusterip-transition-tbfjd
Nov 15 16:50:32.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-6703 exec execpod-affinity8hnrh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.215.37:80/ ; done'
Nov 15 16:50:32.758: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n"
Nov 15 16:50:32.758: INFO: stdout: "\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-b4x99\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-b4x99\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-b4x99\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd"
Nov 15 16:50:32.758: INFO: Received response from host: affinity-clusterip-transition-tbfjd
Nov 15 16:50:32.758: INFO: Received response from host: affinity-clusterip-transition-smgpd
Nov 15 16:50:32.758: INFO: Received response from host: affinity-clusterip-transition-tbfjd
Nov 15 16:50:32.758: INFO: Received response from host: affinity-clusterip-transition-b4x99
Nov 15 16:50:32.758: INFO: Received response from host: affinity-clusterip-transition-smgpd
Nov 15 16:50:32.758: INFO: Received response from host: affinity-clusterip-transition-b4x99
Nov 15 16:50:32.758: INFO: Received response from host: affinity-clusterip-transition-tbfjd
Nov 15 16:50:32.758: INFO: Received response from host: affinity-clusterip-transition-smgpd
Nov 15 16:50:32.758: INFO: Received response from host: affinity-clusterip-transition-smgpd
Nov 15 16:50:32.758: INFO: Received response from host: affinity-clusterip-transition-b4x99
Nov 15 16:50:32.758: INFO: Received response from host: affinity-clusterip-transition-tbfjd
Nov 15 16:50:32.758: INFO: Received response from host: affinity-clusterip-transition-tbfjd
Nov 15 16:50:32.758: INFO: Received response from host: affinity-clusterip-transition-tbfjd
Nov 15 16:50:32.758: INFO: Received response from host: affinity-clusterip-transition-tbfjd
Nov 15 16:50:32.758: INFO: Received response from host: affinity-clusterip-transition-tbfjd
Nov 15 16:50:32.758: INFO: Received response from host: affinity-clusterip-transition-tbfjd
Nov 15 16:50:32.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-6703 exec execpod-affinity8hnrh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.215.37:80/ ; done'
Nov 15 16:50:33.404: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n"
Nov 15 16:50:33.404: INFO: stdout: "\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-smgpd"
Nov 15 16:50:33.404: INFO: Received response from host: affinity-clusterip-transition-smgpd
Nov 15 16:50:33.404: INFO: Received response from host: affinity-clusterip-transition-smgpd
Nov 15 16:50:33.404: INFO: Received response from host: affinity-clusterip-transition-smgpd
Nov 15 16:50:33.404: INFO: Received response from host: affinity-clusterip-transition-smgpd
Nov 15 16:50:33.404: INFO: Received response from host: affinity-clusterip-transition-smgpd
Nov 15 16:50:33.404: INFO: Received response from host: affinity-clusterip-transition-smgpd
Nov 15 16:50:33.404: INFO: Received response from host: affinity-clusterip-transition-smgpd
Nov 15 16:50:33.404: INFO: Received response from host: affinity-clusterip-transition-smgpd
Nov 15 16:50:33.404: INFO: Received response from host: affinity-clusterip-transition-smgpd
Nov 15 16:50:33.404: INFO: Received response from host: affinity-clusterip-transition-smgpd
Nov 15 16:50:33.405: INFO: Received response from host: affinity-clusterip-transition-smgpd
Nov 15 16:50:33.405: INFO: Received response from host: affinity-clusterip-transition-smgpd
Nov 15 16:50:33.405: INFO: Received response from host: affinity-clusterip-transition-smgpd
Nov 15 16:50:33.405: INFO: Received response from host: affinity-clusterip-transition-smgpd
Nov 15 16:50:33.405: INFO: Received response from host: affinity-clusterip-transition-smgpd
Nov 15 16:50:33.405: INFO: Received response from host: affinity-clusterip-transition-smgpd
Nov 15 16:50:33.405: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-6703, will wait for the garbage collector to delete the pods 11/15/23 16:50:33.475
Nov 15 16:50:33.563: INFO: Deleting ReplicationController affinity-clusterip-transition took: 22.075394ms
Nov 15 16:50:33.763: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 200.58175ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 15 16:50:36.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6703" for this suite. 11/15/23 16:50:36.344
------------------------------
• [SLOW TEST] [43.149 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:49:53.216
    Nov 15 16:49:53.217: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename services 11/15/23 16:49:53.219
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:49:53.271
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:49:53.288
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-6703 11/15/23 16:49:53.312
    STEP: creating service affinity-clusterip-transition in namespace services-6703 11/15/23 16:49:53.312
    STEP: creating replication controller affinity-clusterip-transition in namespace services-6703 11/15/23 16:49:53.353
    I1115 16:49:53.391958      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-6703, replica count: 3
    I1115 16:49:56.444559      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 15 16:49:56.472: INFO: Creating new exec pod
    Nov 15 16:49:56.494: INFO: Waiting up to 5m0s for pod "execpod-affinity8hnrh" in namespace "services-6703" to be "running"
    Nov 15 16:49:56.514: INFO: Pod "execpod-affinity8hnrh": Phase="Pending", Reason="", readiness=false. Elapsed: 19.460706ms
    Nov 15 16:49:58.534: INFO: Pod "execpod-affinity8hnrh": Phase="Running", Reason="", readiness=true. Elapsed: 2.039510914s
    Nov 15 16:49:58.534: INFO: Pod "execpod-affinity8hnrh" satisfied condition "running"
    Nov 15 16:49:59.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-6703 exec execpod-affinity8hnrh -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Nov 15 16:50:01.010: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Nov 15 16:50:01.010: INFO: stdout: ""
    Nov 15 16:50:01.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-6703 exec execpod-affinity8hnrh -- /bin/sh -x -c nc -v -z -w 2 172.21.215.37 80'
    Nov 15 16:50:01.419: INFO: stderr: "+ nc -v -z -w 2 172.21.215.37 80\nConnection to 172.21.215.37 80 port [tcp/http] succeeded!\n"
    Nov 15 16:50:01.419: INFO: stdout: ""
    Nov 15 16:50:01.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-6703 exec execpod-affinity8hnrh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.215.37:80/ ; done'
    Nov 15 16:50:02.128: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n"
    Nov 15 16:50:02.128: INFO: stdout: "\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd"
    Nov 15 16:50:02.128: INFO: Received response from host: affinity-clusterip-transition-tbfjd
    Nov 15 16:50:02.128: INFO: Received response from host: affinity-clusterip-transition-tbfjd
    Nov 15 16:50:02.128: INFO: Received response from host: affinity-clusterip-transition-tbfjd
    Nov 15 16:50:02.128: INFO: Received response from host: affinity-clusterip-transition-tbfjd
    Nov 15 16:50:02.128: INFO: Received response from host: affinity-clusterip-transition-tbfjd
    Nov 15 16:50:02.128: INFO: Received response from host: affinity-clusterip-transition-tbfjd
    Nov 15 16:50:02.128: INFO: Received response from host: affinity-clusterip-transition-tbfjd
    Nov 15 16:50:02.128: INFO: Received response from host: affinity-clusterip-transition-tbfjd
    Nov 15 16:50:02.128: INFO: Received response from host: affinity-clusterip-transition-tbfjd
    Nov 15 16:50:02.128: INFO: Received response from host: affinity-clusterip-transition-tbfjd
    Nov 15 16:50:02.129: INFO: Received response from host: affinity-clusterip-transition-tbfjd
    Nov 15 16:50:02.129: INFO: Received response from host: affinity-clusterip-transition-tbfjd
    Nov 15 16:50:02.129: INFO: Received response from host: affinity-clusterip-transition-tbfjd
    Nov 15 16:50:02.129: INFO: Received response from host: affinity-clusterip-transition-tbfjd
    Nov 15 16:50:02.129: INFO: Received response from host: affinity-clusterip-transition-tbfjd
    Nov 15 16:50:02.129: INFO: Received response from host: affinity-clusterip-transition-tbfjd
    Nov 15 16:50:32.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-6703 exec execpod-affinity8hnrh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.215.37:80/ ; done'
    Nov 15 16:50:32.758: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n"
    Nov 15 16:50:32.758: INFO: stdout: "\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-b4x99\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-b4x99\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-b4x99\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd\naffinity-clusterip-transition-tbfjd"
    Nov 15 16:50:32.758: INFO: Received response from host: affinity-clusterip-transition-tbfjd
    Nov 15 16:50:32.758: INFO: Received response from host: affinity-clusterip-transition-smgpd
    Nov 15 16:50:32.758: INFO: Received response from host: affinity-clusterip-transition-tbfjd
    Nov 15 16:50:32.758: INFO: Received response from host: affinity-clusterip-transition-b4x99
    Nov 15 16:50:32.758: INFO: Received response from host: affinity-clusterip-transition-smgpd
    Nov 15 16:50:32.758: INFO: Received response from host: affinity-clusterip-transition-b4x99
    Nov 15 16:50:32.758: INFO: Received response from host: affinity-clusterip-transition-tbfjd
    Nov 15 16:50:32.758: INFO: Received response from host: affinity-clusterip-transition-smgpd
    Nov 15 16:50:32.758: INFO: Received response from host: affinity-clusterip-transition-smgpd
    Nov 15 16:50:32.758: INFO: Received response from host: affinity-clusterip-transition-b4x99
    Nov 15 16:50:32.758: INFO: Received response from host: affinity-clusterip-transition-tbfjd
    Nov 15 16:50:32.758: INFO: Received response from host: affinity-clusterip-transition-tbfjd
    Nov 15 16:50:32.758: INFO: Received response from host: affinity-clusterip-transition-tbfjd
    Nov 15 16:50:32.758: INFO: Received response from host: affinity-clusterip-transition-tbfjd
    Nov 15 16:50:32.758: INFO: Received response from host: affinity-clusterip-transition-tbfjd
    Nov 15 16:50:32.758: INFO: Received response from host: affinity-clusterip-transition-tbfjd
    Nov 15 16:50:32.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-6703 exec execpod-affinity8hnrh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.215.37:80/ ; done'
    Nov 15 16:50:33.404: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.215.37:80/\n"
    Nov 15 16:50:33.404: INFO: stdout: "\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-smgpd\naffinity-clusterip-transition-smgpd"
    Nov 15 16:50:33.404: INFO: Received response from host: affinity-clusterip-transition-smgpd
    Nov 15 16:50:33.404: INFO: Received response from host: affinity-clusterip-transition-smgpd
    Nov 15 16:50:33.404: INFO: Received response from host: affinity-clusterip-transition-smgpd
    Nov 15 16:50:33.404: INFO: Received response from host: affinity-clusterip-transition-smgpd
    Nov 15 16:50:33.404: INFO: Received response from host: affinity-clusterip-transition-smgpd
    Nov 15 16:50:33.404: INFO: Received response from host: affinity-clusterip-transition-smgpd
    Nov 15 16:50:33.404: INFO: Received response from host: affinity-clusterip-transition-smgpd
    Nov 15 16:50:33.404: INFO: Received response from host: affinity-clusterip-transition-smgpd
    Nov 15 16:50:33.404: INFO: Received response from host: affinity-clusterip-transition-smgpd
    Nov 15 16:50:33.404: INFO: Received response from host: affinity-clusterip-transition-smgpd
    Nov 15 16:50:33.405: INFO: Received response from host: affinity-clusterip-transition-smgpd
    Nov 15 16:50:33.405: INFO: Received response from host: affinity-clusterip-transition-smgpd
    Nov 15 16:50:33.405: INFO: Received response from host: affinity-clusterip-transition-smgpd
    Nov 15 16:50:33.405: INFO: Received response from host: affinity-clusterip-transition-smgpd
    Nov 15 16:50:33.405: INFO: Received response from host: affinity-clusterip-transition-smgpd
    Nov 15 16:50:33.405: INFO: Received response from host: affinity-clusterip-transition-smgpd
    Nov 15 16:50:33.405: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-6703, will wait for the garbage collector to delete the pods 11/15/23 16:50:33.475
    Nov 15 16:50:33.563: INFO: Deleting ReplicationController affinity-clusterip-transition took: 22.075394ms
    Nov 15 16:50:33.763: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 200.58175ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:50:36.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6703" for this suite. 11/15/23 16:50:36.344
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:50:36.368
Nov 15 16:50:36.368: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename events 11/15/23 16:50:36.372
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:50:36.423
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:50:36.441
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 11/15/23 16:50:36.46
STEP: listing events in all namespaces 11/15/23 16:50:36.509
STEP: listing events in test namespace 11/15/23 16:50:36.531
STEP: listing events with field selection filtering on source 11/15/23 16:50:36.551
STEP: listing events with field selection filtering on reportingController 11/15/23 16:50:36.565
STEP: getting the test event 11/15/23 16:50:36.585
STEP: patching the test event 11/15/23 16:50:36.608
STEP: getting the test event 11/15/23 16:50:36.658
STEP: updating the test event 11/15/23 16:50:36.677
STEP: getting the test event 11/15/23 16:50:36.715
STEP: deleting the test event 11/15/23 16:50:36.734
STEP: listing events in all namespaces 11/15/23 16:50:36.773
STEP: listing events in test namespace 11/15/23 16:50:36.794
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Nov 15 16:50:36.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-3874" for this suite. 11/15/23 16:50:36.83
------------------------------
• [0.485 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:50:36.368
    Nov 15 16:50:36.368: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename events 11/15/23 16:50:36.372
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:50:36.423
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:50:36.441
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 11/15/23 16:50:36.46
    STEP: listing events in all namespaces 11/15/23 16:50:36.509
    STEP: listing events in test namespace 11/15/23 16:50:36.531
    STEP: listing events with field selection filtering on source 11/15/23 16:50:36.551
    STEP: listing events with field selection filtering on reportingController 11/15/23 16:50:36.565
    STEP: getting the test event 11/15/23 16:50:36.585
    STEP: patching the test event 11/15/23 16:50:36.608
    STEP: getting the test event 11/15/23 16:50:36.658
    STEP: updating the test event 11/15/23 16:50:36.677
    STEP: getting the test event 11/15/23 16:50:36.715
    STEP: deleting the test event 11/15/23 16:50:36.734
    STEP: listing events in all namespaces 11/15/23 16:50:36.773
    STEP: listing events in test namespace 11/15/23 16:50:36.794
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:50:36.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-3874" for this suite. 11/15/23 16:50:36.83
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:50:36.855
Nov 15 16:50:36.856: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename resourcequota 11/15/23 16:50:36.858
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:50:36.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:50:36.92
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 11/15/23 16:50:36.938
STEP: Creating a ResourceQuota 11/15/23 16:50:41.951
STEP: Ensuring resource quota status is calculated 11/15/23 16:50:41.967
STEP: Creating a Service 11/15/23 16:50:43.986
STEP: Creating a NodePort Service 11/15/23 16:50:44.05
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 11/15/23 16:50:44.136
STEP: Ensuring resource quota status captures service creation 11/15/23 16:50:44.253
STEP: Deleting Services 11/15/23 16:50:46.269
STEP: Ensuring resource quota status released usage 11/15/23 16:50:46.395
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Nov 15 16:50:48.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4871" for this suite. 11/15/23 16:50:48.437
------------------------------
• [SLOW TEST] [11.605 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:50:36.855
    Nov 15 16:50:36.856: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename resourcequota 11/15/23 16:50:36.858
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:50:36.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:50:36.92
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 11/15/23 16:50:36.938
    STEP: Creating a ResourceQuota 11/15/23 16:50:41.951
    STEP: Ensuring resource quota status is calculated 11/15/23 16:50:41.967
    STEP: Creating a Service 11/15/23 16:50:43.986
    STEP: Creating a NodePort Service 11/15/23 16:50:44.05
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 11/15/23 16:50:44.136
    STEP: Ensuring resource quota status captures service creation 11/15/23 16:50:44.253
    STEP: Deleting Services 11/15/23 16:50:46.269
    STEP: Ensuring resource quota status released usage 11/15/23 16:50:46.395
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:50:48.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4871" for this suite. 11/15/23 16:50:48.437
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:50:48.46
Nov 15 16:50:48.461: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename services 11/15/23 16:50:48.464
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:50:48.51
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:50:48.526
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-6236 11/15/23 16:50:48.543
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 11/15/23 16:50:48.631
STEP: creating service externalsvc in namespace services-6236 11/15/23 16:50:48.631
STEP: creating replication controller externalsvc in namespace services-6236 11/15/23 16:50:48.682
I1115 16:50:48.702315      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-6236, replica count: 2
I1115 16:50:51.753020      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 11/15/23 16:50:51.768
Nov 15 16:50:51.848: INFO: Creating new exec pod
Nov 15 16:50:51.873: INFO: Waiting up to 5m0s for pod "execpodkfgzk" in namespace "services-6236" to be "running"
Nov 15 16:50:51.896: INFO: Pod "execpodkfgzk": Phase="Pending", Reason="", readiness=false. Elapsed: 23.523541ms
Nov 15 16:50:53.917: INFO: Pod "execpodkfgzk": Phase="Running", Reason="", readiness=true. Elapsed: 2.0443557s
Nov 15 16:50:53.917: INFO: Pod "execpodkfgzk" satisfied condition "running"
Nov 15 16:50:53.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-6236 exec execpodkfgzk -- /bin/sh -x -c nslookup nodeport-service.services-6236.svc.cluster.local'
Nov 15 16:50:54.402: INFO: stderr: "+ nslookup nodeport-service.services-6236.svc.cluster.local\n"
Nov 15 16:50:54.402: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-6236.svc.cluster.local\tcanonical name = externalsvc.services-6236.svc.cluster.local.\nName:\texternalsvc.services-6236.svc.cluster.local\nAddress: 172.21.253.190\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6236, will wait for the garbage collector to delete the pods 11/15/23 16:50:54.402
Nov 15 16:50:54.498: INFO: Deleting ReplicationController externalsvc took: 22.521603ms
Nov 15 16:50:54.599: INFO: Terminating ReplicationController externalsvc pods took: 100.988434ms
Nov 15 16:50:57.464: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 15 16:50:57.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6236" for this suite. 11/15/23 16:50:57.55
------------------------------
• [SLOW TEST] [9.117 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:50:48.46
    Nov 15 16:50:48.461: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename services 11/15/23 16:50:48.464
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:50:48.51
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:50:48.526
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-6236 11/15/23 16:50:48.543
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 11/15/23 16:50:48.631
    STEP: creating service externalsvc in namespace services-6236 11/15/23 16:50:48.631
    STEP: creating replication controller externalsvc in namespace services-6236 11/15/23 16:50:48.682
    I1115 16:50:48.702315      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-6236, replica count: 2
    I1115 16:50:51.753020      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 11/15/23 16:50:51.768
    Nov 15 16:50:51.848: INFO: Creating new exec pod
    Nov 15 16:50:51.873: INFO: Waiting up to 5m0s for pod "execpodkfgzk" in namespace "services-6236" to be "running"
    Nov 15 16:50:51.896: INFO: Pod "execpodkfgzk": Phase="Pending", Reason="", readiness=false. Elapsed: 23.523541ms
    Nov 15 16:50:53.917: INFO: Pod "execpodkfgzk": Phase="Running", Reason="", readiness=true. Elapsed: 2.0443557s
    Nov 15 16:50:53.917: INFO: Pod "execpodkfgzk" satisfied condition "running"
    Nov 15 16:50:53.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-6236 exec execpodkfgzk -- /bin/sh -x -c nslookup nodeport-service.services-6236.svc.cluster.local'
    Nov 15 16:50:54.402: INFO: stderr: "+ nslookup nodeport-service.services-6236.svc.cluster.local\n"
    Nov 15 16:50:54.402: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-6236.svc.cluster.local\tcanonical name = externalsvc.services-6236.svc.cluster.local.\nName:\texternalsvc.services-6236.svc.cluster.local\nAddress: 172.21.253.190\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-6236, will wait for the garbage collector to delete the pods 11/15/23 16:50:54.402
    Nov 15 16:50:54.498: INFO: Deleting ReplicationController externalsvc took: 22.521603ms
    Nov 15 16:50:54.599: INFO: Terminating ReplicationController externalsvc pods took: 100.988434ms
    Nov 15 16:50:57.464: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:50:57.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6236" for this suite. 11/15/23 16:50:57.55
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:50:57.593
Nov 15 16:50:57.593: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename downward-api 11/15/23 16:50:57.595
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:50:57.636
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:50:57.653
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 11/15/23 16:50:57.673
Nov 15 16:50:57.707: INFO: Waiting up to 5m0s for pod "downward-api-14e81ff0-f922-4d56-8ffc-9d0507287072" in namespace "downward-api-3866" to be "Succeeded or Failed"
Nov 15 16:50:57.750: INFO: Pod "downward-api-14e81ff0-f922-4d56-8ffc-9d0507287072": Phase="Pending", Reason="", readiness=false. Elapsed: 43.457867ms
Nov 15 16:50:59.773: INFO: Pod "downward-api-14e81ff0-f922-4d56-8ffc-9d0507287072": Phase="Pending", Reason="", readiness=false. Elapsed: 2.066330122s
Nov 15 16:51:01.771: INFO: Pod "downward-api-14e81ff0-f922-4d56-8ffc-9d0507287072": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064243388s
STEP: Saw pod success 11/15/23 16:51:01.771
Nov 15 16:51:01.772: INFO: Pod "downward-api-14e81ff0-f922-4d56-8ffc-9d0507287072" satisfied condition "Succeeded or Failed"
Nov 15 16:51:01.791: INFO: Trying to get logs from node 10.15.40.115 pod downward-api-14e81ff0-f922-4d56-8ffc-9d0507287072 container dapi-container: <nil>
STEP: delete the pod 11/15/23 16:51:01.846
Nov 15 16:51:01.906: INFO: Waiting for pod downward-api-14e81ff0-f922-4d56-8ffc-9d0507287072 to disappear
Nov 15 16:51:01.924: INFO: Pod downward-api-14e81ff0-f922-4d56-8ffc-9d0507287072 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Nov 15 16:51:01.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3866" for this suite. 11/15/23 16:51:01.961
------------------------------
• [4.388 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:50:57.593
    Nov 15 16:50:57.593: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename downward-api 11/15/23 16:50:57.595
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:50:57.636
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:50:57.653
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 11/15/23 16:50:57.673
    Nov 15 16:50:57.707: INFO: Waiting up to 5m0s for pod "downward-api-14e81ff0-f922-4d56-8ffc-9d0507287072" in namespace "downward-api-3866" to be "Succeeded or Failed"
    Nov 15 16:50:57.750: INFO: Pod "downward-api-14e81ff0-f922-4d56-8ffc-9d0507287072": Phase="Pending", Reason="", readiness=false. Elapsed: 43.457867ms
    Nov 15 16:50:59.773: INFO: Pod "downward-api-14e81ff0-f922-4d56-8ffc-9d0507287072": Phase="Pending", Reason="", readiness=false. Elapsed: 2.066330122s
    Nov 15 16:51:01.771: INFO: Pod "downward-api-14e81ff0-f922-4d56-8ffc-9d0507287072": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064243388s
    STEP: Saw pod success 11/15/23 16:51:01.771
    Nov 15 16:51:01.772: INFO: Pod "downward-api-14e81ff0-f922-4d56-8ffc-9d0507287072" satisfied condition "Succeeded or Failed"
    Nov 15 16:51:01.791: INFO: Trying to get logs from node 10.15.40.115 pod downward-api-14e81ff0-f922-4d56-8ffc-9d0507287072 container dapi-container: <nil>
    STEP: delete the pod 11/15/23 16:51:01.846
    Nov 15 16:51:01.906: INFO: Waiting for pod downward-api-14e81ff0-f922-4d56-8ffc-9d0507287072 to disappear
    Nov 15 16:51:01.924: INFO: Pod downward-api-14e81ff0-f922-4d56-8ffc-9d0507287072 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:51:01.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3866" for this suite. 11/15/23 16:51:01.961
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:51:01.985
Nov 15 16:51:01.985: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename secrets 11/15/23 16:51:01.988
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:51:02.052
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:51:02.072
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-ec74bddd-4379-4c56-87b2-29c2ba83de00 11/15/23 16:51:02.201
STEP: Creating a pod to test consume secrets 11/15/23 16:51:02.217
Nov 15 16:51:02.259: INFO: Waiting up to 5m0s for pod "pod-secrets-36e27251-4efb-4471-b554-c55187e87fff" in namespace "secrets-1516" to be "Succeeded or Failed"
Nov 15 16:51:02.279: INFO: Pod "pod-secrets-36e27251-4efb-4471-b554-c55187e87fff": Phase="Pending", Reason="", readiness=false. Elapsed: 20.33124ms
Nov 15 16:51:04.298: INFO: Pod "pod-secrets-36e27251-4efb-4471-b554-c55187e87fff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038851325s
Nov 15 16:51:06.307: INFO: Pod "pod-secrets-36e27251-4efb-4471-b554-c55187e87fff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048608782s
Nov 15 16:51:08.301: INFO: Pod "pod-secrets-36e27251-4efb-4471-b554-c55187e87fff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041734011s
STEP: Saw pod success 11/15/23 16:51:08.301
Nov 15 16:51:08.302: INFO: Pod "pod-secrets-36e27251-4efb-4471-b554-c55187e87fff" satisfied condition "Succeeded or Failed"
Nov 15 16:51:08.319: INFO: Trying to get logs from node 10.15.40.115 pod pod-secrets-36e27251-4efb-4471-b554-c55187e87fff container secret-volume-test: <nil>
STEP: delete the pod 11/15/23 16:51:08.359
Nov 15 16:51:08.407: INFO: Waiting for pod pod-secrets-36e27251-4efb-4471-b554-c55187e87fff to disappear
Nov 15 16:51:08.425: INFO: Pod pod-secrets-36e27251-4efb-4471-b554-c55187e87fff no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 15 16:51:08.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1516" for this suite. 11/15/23 16:51:08.445
STEP: Destroying namespace "secret-namespace-6202" for this suite. 11/15/23 16:51:08.468
------------------------------
• [SLOW TEST] [6.504 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:51:01.985
    Nov 15 16:51:01.985: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename secrets 11/15/23 16:51:01.988
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:51:02.052
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:51:02.072
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-ec74bddd-4379-4c56-87b2-29c2ba83de00 11/15/23 16:51:02.201
    STEP: Creating a pod to test consume secrets 11/15/23 16:51:02.217
    Nov 15 16:51:02.259: INFO: Waiting up to 5m0s for pod "pod-secrets-36e27251-4efb-4471-b554-c55187e87fff" in namespace "secrets-1516" to be "Succeeded or Failed"
    Nov 15 16:51:02.279: INFO: Pod "pod-secrets-36e27251-4efb-4471-b554-c55187e87fff": Phase="Pending", Reason="", readiness=false. Elapsed: 20.33124ms
    Nov 15 16:51:04.298: INFO: Pod "pod-secrets-36e27251-4efb-4471-b554-c55187e87fff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038851325s
    Nov 15 16:51:06.307: INFO: Pod "pod-secrets-36e27251-4efb-4471-b554-c55187e87fff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048608782s
    Nov 15 16:51:08.301: INFO: Pod "pod-secrets-36e27251-4efb-4471-b554-c55187e87fff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041734011s
    STEP: Saw pod success 11/15/23 16:51:08.301
    Nov 15 16:51:08.302: INFO: Pod "pod-secrets-36e27251-4efb-4471-b554-c55187e87fff" satisfied condition "Succeeded or Failed"
    Nov 15 16:51:08.319: INFO: Trying to get logs from node 10.15.40.115 pod pod-secrets-36e27251-4efb-4471-b554-c55187e87fff container secret-volume-test: <nil>
    STEP: delete the pod 11/15/23 16:51:08.359
    Nov 15 16:51:08.407: INFO: Waiting for pod pod-secrets-36e27251-4efb-4471-b554-c55187e87fff to disappear
    Nov 15 16:51:08.425: INFO: Pod pod-secrets-36e27251-4efb-4471-b554-c55187e87fff no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:51:08.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1516" for this suite. 11/15/23 16:51:08.445
    STEP: Destroying namespace "secret-namespace-6202" for this suite. 11/15/23 16:51:08.468
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:51:08.491
Nov 15 16:51:08.491: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename namespaces 11/15/23 16:51:08.493
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:51:08.537
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:51:08.55
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 11/15/23 16:51:08.565
Nov 15 16:51:08.579: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 11/15/23 16:51:08.58
Nov 15 16:51:08.596: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 11/15/23 16:51:08.597
Nov 15 16:51:08.626: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 16:51:08.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-3176" for this suite. 11/15/23 16:51:08.646
------------------------------
• [0.174 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:51:08.491
    Nov 15 16:51:08.491: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename namespaces 11/15/23 16:51:08.493
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:51:08.537
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:51:08.55
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 11/15/23 16:51:08.565
    Nov 15 16:51:08.579: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 11/15/23 16:51:08.58
    Nov 15 16:51:08.596: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 11/15/23 16:51:08.597
    Nov 15 16:51:08.626: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:51:08.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-3176" for this suite. 11/15/23 16:51:08.646
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:51:08.67
Nov 15 16:51:08.671: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename secrets 11/15/23 16:51:08.675
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:51:08.717
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:51:08.731
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 15 16:51:08.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6142" for this suite. 11/15/23 16:51:08.926
------------------------------
• [0.283 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:51:08.67
    Nov 15 16:51:08.671: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename secrets 11/15/23 16:51:08.675
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:51:08.717
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:51:08.731
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:51:08.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6142" for this suite. 11/15/23 16:51:08.926
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:51:08.953
Nov 15 16:51:08.953: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename crd-publish-openapi 11/15/23 16:51:08.954
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:51:09.017
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:51:09.029
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 11/15/23 16:51:09.042
Nov 15 16:51:09.044: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
Nov 15 16:51:11.365: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 16:51:20.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2575" for this suite. 11/15/23 16:51:21.016
------------------------------
• [SLOW TEST] [12.088 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:51:08.953
    Nov 15 16:51:08.953: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename crd-publish-openapi 11/15/23 16:51:08.954
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:51:09.017
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:51:09.029
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 11/15/23 16:51:09.042
    Nov 15 16:51:09.044: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    Nov 15 16:51:11.365: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:51:20.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2575" for this suite. 11/15/23 16:51:21.016
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:51:21.048
Nov 15 16:51:21.049: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename resourcequota 11/15/23 16:51:21.051
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:51:21.102
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:51:21.119
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 11/15/23 16:51:21.133
STEP: Getting a ResourceQuota 11/15/23 16:51:21.151
STEP: Updating a ResourceQuota 11/15/23 16:51:21.168
STEP: Verifying a ResourceQuota was modified 11/15/23 16:51:21.185
STEP: Deleting a ResourceQuota 11/15/23 16:51:21.201
STEP: Verifying the deleted ResourceQuota 11/15/23 16:51:21.228
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Nov 15 16:51:21.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6250" for this suite. 11/15/23 16:51:21.295
------------------------------
• [0.269 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:51:21.048
    Nov 15 16:51:21.049: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename resourcequota 11/15/23 16:51:21.051
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:51:21.102
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:51:21.119
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 11/15/23 16:51:21.133
    STEP: Getting a ResourceQuota 11/15/23 16:51:21.151
    STEP: Updating a ResourceQuota 11/15/23 16:51:21.168
    STEP: Verifying a ResourceQuota was modified 11/15/23 16:51:21.185
    STEP: Deleting a ResourceQuota 11/15/23 16:51:21.201
    STEP: Verifying the deleted ResourceQuota 11/15/23 16:51:21.228
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:51:21.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6250" for this suite. 11/15/23 16:51:21.295
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:51:21.318
Nov 15 16:51:21.318: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename secrets 11/15/23 16:51:21.32
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:51:21.375
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:51:21.39
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 11/15/23 16:51:21.405
STEP: listing secrets in all namespaces to ensure that there are more than zero 11/15/23 16:51:21.428
STEP: patching the secret 11/15/23 16:51:21.46
STEP: deleting the secret using a LabelSelector 11/15/23 16:51:21.504
STEP: listing secrets in all namespaces, searching for label name and value in patch 11/15/23 16:51:21.545
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 15 16:51:21.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6380" for this suite. 11/15/23 16:51:21.596
------------------------------
• [0.303 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:51:21.318
    Nov 15 16:51:21.318: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename secrets 11/15/23 16:51:21.32
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:51:21.375
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:51:21.39
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 11/15/23 16:51:21.405
    STEP: listing secrets in all namespaces to ensure that there are more than zero 11/15/23 16:51:21.428
    STEP: patching the secret 11/15/23 16:51:21.46
    STEP: deleting the secret using a LabelSelector 11/15/23 16:51:21.504
    STEP: listing secrets in all namespaces, searching for label name and value in patch 11/15/23 16:51:21.545
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:51:21.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6380" for this suite. 11/15/23 16:51:21.596
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:51:21.626
Nov 15 16:51:21.627: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename container-lifecycle-hook 11/15/23 16:51:21.63
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:51:21.68
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:51:21.692
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 11/15/23 16:51:21.729
Nov 15 16:51:21.768: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1671" to be "running and ready"
Nov 15 16:51:21.782: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 14.460881ms
Nov 15 16:51:21.783: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 15 16:51:23.807: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038888004s
Nov 15 16:51:23.807: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 15 16:51:25.799: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.030866142s
Nov 15 16:51:25.799: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Nov 15 16:51:25.799: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 11/15/23 16:51:25.814
Nov 15 16:51:25.834: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-1671" to be "running and ready"
Nov 15 16:51:25.855: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 21.295574ms
Nov 15 16:51:25.855: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 15 16:51:27.868: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034600673s
Nov 15 16:51:27.868: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 15 16:51:29.870: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.036817676s
Nov 15 16:51:29.870: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Nov 15 16:51:29.870: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 11/15/23 16:51:29.887
STEP: delete the pod with lifecycle hook 11/15/23 16:51:30.031
Nov 15 16:51:30.056: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 15 16:51:30.076: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 15 16:51:32.076: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 15 16:51:32.126: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Nov 15 16:51:32.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-1671" for this suite. 11/15/23 16:51:32.208
------------------------------
• [SLOW TEST] [10.612 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:51:21.626
    Nov 15 16:51:21.627: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename container-lifecycle-hook 11/15/23 16:51:21.63
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:51:21.68
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:51:21.692
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 11/15/23 16:51:21.729
    Nov 15 16:51:21.768: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1671" to be "running and ready"
    Nov 15 16:51:21.782: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 14.460881ms
    Nov 15 16:51:21.783: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 16:51:23.807: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038888004s
    Nov 15 16:51:23.807: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 16:51:25.799: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.030866142s
    Nov 15 16:51:25.799: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Nov 15 16:51:25.799: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 11/15/23 16:51:25.814
    Nov 15 16:51:25.834: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-1671" to be "running and ready"
    Nov 15 16:51:25.855: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 21.295574ms
    Nov 15 16:51:25.855: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 16:51:27.868: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034600673s
    Nov 15 16:51:27.868: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 16:51:29.870: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.036817676s
    Nov 15 16:51:29.870: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Nov 15 16:51:29.870: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 11/15/23 16:51:29.887
    STEP: delete the pod with lifecycle hook 11/15/23 16:51:30.031
    Nov 15 16:51:30.056: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Nov 15 16:51:30.076: INFO: Pod pod-with-poststart-exec-hook still exists
    Nov 15 16:51:32.076: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Nov 15 16:51:32.126: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:51:32.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-1671" for this suite. 11/15/23 16:51:32.208
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:51:32.24
Nov 15 16:51:32.240: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename init-container 11/15/23 16:51:32.242
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:51:32.296
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:51:32.311
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 11/15/23 16:51:32.327
Nov 15 16:51:32.328: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Nov 15 16:51:36.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-1867" for this suite. 11/15/23 16:51:36.995
------------------------------
• [4.779 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:51:32.24
    Nov 15 16:51:32.240: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename init-container 11/15/23 16:51:32.242
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:51:32.296
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:51:32.311
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 11/15/23 16:51:32.327
    Nov 15 16:51:32.328: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:51:36.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-1867" for this suite. 11/15/23 16:51:36.995
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:51:37.03
Nov 15 16:51:37.030: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename projected 11/15/23 16:51:37.033
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:51:37.09
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:51:37.106
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-eff7c55e-5086-4d11-a877-4b7f81a828ac 11/15/23 16:51:37.121
STEP: Creating a pod to test consume configMaps 11/15/23 16:51:37.142
Nov 15 16:51:37.174: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9a307529-c0d5-4c75-9150-e1795f785e85" in namespace "projected-6962" to be "Succeeded or Failed"
Nov 15 16:51:37.192: INFO: Pod "pod-projected-configmaps-9a307529-c0d5-4c75-9150-e1795f785e85": Phase="Pending", Reason="", readiness=false. Elapsed: 18.140524ms
Nov 15 16:51:39.207: INFO: Pod "pod-projected-configmaps-9a307529-c0d5-4c75-9150-e1795f785e85": Phase="Running", Reason="", readiness=true. Elapsed: 2.032830153s
Nov 15 16:51:41.208: INFO: Pod "pod-projected-configmaps-9a307529-c0d5-4c75-9150-e1795f785e85": Phase="Running", Reason="", readiness=false. Elapsed: 4.033652365s
Nov 15 16:51:43.212: INFO: Pod "pod-projected-configmaps-9a307529-c0d5-4c75-9150-e1795f785e85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038146304s
STEP: Saw pod success 11/15/23 16:51:43.212
Nov 15 16:51:43.213: INFO: Pod "pod-projected-configmaps-9a307529-c0d5-4c75-9150-e1795f785e85" satisfied condition "Succeeded or Failed"
Nov 15 16:51:43.227: INFO: Trying to get logs from node 10.15.40.115 pod pod-projected-configmaps-9a307529-c0d5-4c75-9150-e1795f785e85 container agnhost-container: <nil>
STEP: delete the pod 11/15/23 16:51:43.354
Nov 15 16:51:43.398: INFO: Waiting for pod pod-projected-configmaps-9a307529-c0d5-4c75-9150-e1795f785e85 to disappear
Nov 15 16:51:43.412: INFO: Pod pod-projected-configmaps-9a307529-c0d5-4c75-9150-e1795f785e85 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Nov 15 16:51:43.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6962" for this suite. 11/15/23 16:51:43.438
------------------------------
• [SLOW TEST] [6.435 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:51:37.03
    Nov 15 16:51:37.030: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename projected 11/15/23 16:51:37.033
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:51:37.09
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:51:37.106
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-eff7c55e-5086-4d11-a877-4b7f81a828ac 11/15/23 16:51:37.121
    STEP: Creating a pod to test consume configMaps 11/15/23 16:51:37.142
    Nov 15 16:51:37.174: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9a307529-c0d5-4c75-9150-e1795f785e85" in namespace "projected-6962" to be "Succeeded or Failed"
    Nov 15 16:51:37.192: INFO: Pod "pod-projected-configmaps-9a307529-c0d5-4c75-9150-e1795f785e85": Phase="Pending", Reason="", readiness=false. Elapsed: 18.140524ms
    Nov 15 16:51:39.207: INFO: Pod "pod-projected-configmaps-9a307529-c0d5-4c75-9150-e1795f785e85": Phase="Running", Reason="", readiness=true. Elapsed: 2.032830153s
    Nov 15 16:51:41.208: INFO: Pod "pod-projected-configmaps-9a307529-c0d5-4c75-9150-e1795f785e85": Phase="Running", Reason="", readiness=false. Elapsed: 4.033652365s
    Nov 15 16:51:43.212: INFO: Pod "pod-projected-configmaps-9a307529-c0d5-4c75-9150-e1795f785e85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038146304s
    STEP: Saw pod success 11/15/23 16:51:43.212
    Nov 15 16:51:43.213: INFO: Pod "pod-projected-configmaps-9a307529-c0d5-4c75-9150-e1795f785e85" satisfied condition "Succeeded or Failed"
    Nov 15 16:51:43.227: INFO: Trying to get logs from node 10.15.40.115 pod pod-projected-configmaps-9a307529-c0d5-4c75-9150-e1795f785e85 container agnhost-container: <nil>
    STEP: delete the pod 11/15/23 16:51:43.354
    Nov 15 16:51:43.398: INFO: Waiting for pod pod-projected-configmaps-9a307529-c0d5-4c75-9150-e1795f785e85 to disappear
    Nov 15 16:51:43.412: INFO: Pod pod-projected-configmaps-9a307529-c0d5-4c75-9150-e1795f785e85 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:51:43.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6962" for this suite. 11/15/23 16:51:43.438
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:51:43.476
Nov 15 16:51:43.477: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename csistoragecapacity 11/15/23 16:51:43.479
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:51:43.528
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:51:43.542
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 11/15/23 16:51:43.557
STEP: getting /apis/storage.k8s.io 11/15/23 16:51:43.571
STEP: getting /apis/storage.k8s.io/v1 11/15/23 16:51:43.577
STEP: creating 11/15/23 16:51:43.584
STEP: watching 11/15/23 16:51:43.65
Nov 15 16:51:43.651: INFO: starting watch
STEP: getting 11/15/23 16:51:43.681
STEP: listing in namespace 11/15/23 16:51:43.699
STEP: listing across namespaces 11/15/23 16:51:43.714
STEP: patching 11/15/23 16:51:43.732
STEP: updating 11/15/23 16:51:43.753
Nov 15 16:51:43.773: INFO: waiting for watch events with expected annotations in namespace
Nov 15 16:51:43.774: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 11/15/23 16:51:43.774
STEP: deleting a collection 11/15/23 16:51:43.834
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Nov 15 16:51:43.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-2908" for this suite. 11/15/23 16:51:43.942
------------------------------
• [0.491 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:51:43.476
    Nov 15 16:51:43.477: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename csistoragecapacity 11/15/23 16:51:43.479
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:51:43.528
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:51:43.542
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 11/15/23 16:51:43.557
    STEP: getting /apis/storage.k8s.io 11/15/23 16:51:43.571
    STEP: getting /apis/storage.k8s.io/v1 11/15/23 16:51:43.577
    STEP: creating 11/15/23 16:51:43.584
    STEP: watching 11/15/23 16:51:43.65
    Nov 15 16:51:43.651: INFO: starting watch
    STEP: getting 11/15/23 16:51:43.681
    STEP: listing in namespace 11/15/23 16:51:43.699
    STEP: listing across namespaces 11/15/23 16:51:43.714
    STEP: patching 11/15/23 16:51:43.732
    STEP: updating 11/15/23 16:51:43.753
    Nov 15 16:51:43.773: INFO: waiting for watch events with expected annotations in namespace
    Nov 15 16:51:43.774: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 11/15/23 16:51:43.774
    STEP: deleting a collection 11/15/23 16:51:43.834
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:51:43.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-2908" for this suite. 11/15/23 16:51:43.942
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:51:43.973
Nov 15 16:51:43.973: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename custom-resource-definition 11/15/23 16:51:43.976
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:51:44.032
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:51:44.067
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Nov 15 16:51:44.085: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 16:51:47.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-7912" for this suite. 11/15/23 16:51:47.551
------------------------------
• [3.604 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:51:43.973
    Nov 15 16:51:43.973: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename custom-resource-definition 11/15/23 16:51:43.976
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:51:44.032
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:51:44.067
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Nov 15 16:51:44.085: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:51:47.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-7912" for this suite. 11/15/23 16:51:47.551
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:51:47.581
Nov 15 16:51:47.581: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename downward-api 11/15/23 16:51:47.584
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:51:47.634
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:51:47.654
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 11/15/23 16:51:47.669
Nov 15 16:51:47.702: INFO: Waiting up to 5m0s for pod "downward-api-b2d452c5-501d-4dd4-aee6-6d0fa268d185" in namespace "downward-api-3730" to be "Succeeded or Failed"
Nov 15 16:51:47.716: INFO: Pod "downward-api-b2d452c5-501d-4dd4-aee6-6d0fa268d185": Phase="Pending", Reason="", readiness=false. Elapsed: 14.208316ms
Nov 15 16:51:49.732: INFO: Pod "downward-api-b2d452c5-501d-4dd4-aee6-6d0fa268d185": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030338674s
Nov 15 16:51:51.734: INFO: Pod "downward-api-b2d452c5-501d-4dd4-aee6-6d0fa268d185": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032366135s
Nov 15 16:51:53.737: INFO: Pod "downward-api-b2d452c5-501d-4dd4-aee6-6d0fa268d185": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034819015s
STEP: Saw pod success 11/15/23 16:51:53.737
Nov 15 16:51:53.737: INFO: Pod "downward-api-b2d452c5-501d-4dd4-aee6-6d0fa268d185" satisfied condition "Succeeded or Failed"
Nov 15 16:51:53.752: INFO: Trying to get logs from node 10.15.40.115 pod downward-api-b2d452c5-501d-4dd4-aee6-6d0fa268d185 container dapi-container: <nil>
STEP: delete the pod 11/15/23 16:51:53.795
Nov 15 16:51:53.842: INFO: Waiting for pod downward-api-b2d452c5-501d-4dd4-aee6-6d0fa268d185 to disappear
Nov 15 16:51:53.857: INFO: Pod downward-api-b2d452c5-501d-4dd4-aee6-6d0fa268d185 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Nov 15 16:51:53.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3730" for this suite. 11/15/23 16:51:53.882
------------------------------
• [SLOW TEST] [6.330 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:51:47.581
    Nov 15 16:51:47.581: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename downward-api 11/15/23 16:51:47.584
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:51:47.634
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:51:47.654
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 11/15/23 16:51:47.669
    Nov 15 16:51:47.702: INFO: Waiting up to 5m0s for pod "downward-api-b2d452c5-501d-4dd4-aee6-6d0fa268d185" in namespace "downward-api-3730" to be "Succeeded or Failed"
    Nov 15 16:51:47.716: INFO: Pod "downward-api-b2d452c5-501d-4dd4-aee6-6d0fa268d185": Phase="Pending", Reason="", readiness=false. Elapsed: 14.208316ms
    Nov 15 16:51:49.732: INFO: Pod "downward-api-b2d452c5-501d-4dd4-aee6-6d0fa268d185": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030338674s
    Nov 15 16:51:51.734: INFO: Pod "downward-api-b2d452c5-501d-4dd4-aee6-6d0fa268d185": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032366135s
    Nov 15 16:51:53.737: INFO: Pod "downward-api-b2d452c5-501d-4dd4-aee6-6d0fa268d185": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034819015s
    STEP: Saw pod success 11/15/23 16:51:53.737
    Nov 15 16:51:53.737: INFO: Pod "downward-api-b2d452c5-501d-4dd4-aee6-6d0fa268d185" satisfied condition "Succeeded or Failed"
    Nov 15 16:51:53.752: INFO: Trying to get logs from node 10.15.40.115 pod downward-api-b2d452c5-501d-4dd4-aee6-6d0fa268d185 container dapi-container: <nil>
    STEP: delete the pod 11/15/23 16:51:53.795
    Nov 15 16:51:53.842: INFO: Waiting for pod downward-api-b2d452c5-501d-4dd4-aee6-6d0fa268d185 to disappear
    Nov 15 16:51:53.857: INFO: Pod downward-api-b2d452c5-501d-4dd4-aee6-6d0fa268d185 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:51:53.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3730" for this suite. 11/15/23 16:51:53.882
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:51:53.928
Nov 15 16:51:53.928: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename configmap 11/15/23 16:51:53.929
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:51:53.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:51:53.995
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-e5a04f44-5211-4f91-86eb-55ef339df119 11/15/23 16:51:54.011
STEP: Creating a pod to test consume configMaps 11/15/23 16:51:54.029
Nov 15 16:51:54.061: INFO: Waiting up to 5m0s for pod "pod-configmaps-2eb29f32-8ba5-4084-981f-62f9264216c7" in namespace "configmap-5435" to be "Succeeded or Failed"
Nov 15 16:51:54.076: INFO: Pod "pod-configmaps-2eb29f32-8ba5-4084-981f-62f9264216c7": Phase="Pending", Reason="", readiness=false. Elapsed: 14.652478ms
Nov 15 16:51:56.092: INFO: Pod "pod-configmaps-2eb29f32-8ba5-4084-981f-62f9264216c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030469517s
Nov 15 16:51:58.092: INFO: Pod "pod-configmaps-2eb29f32-8ba5-4084-981f-62f9264216c7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031081384s
Nov 15 16:52:00.091: INFO: Pod "pod-configmaps-2eb29f32-8ba5-4084-981f-62f9264216c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029993501s
STEP: Saw pod success 11/15/23 16:52:00.091
Nov 15 16:52:00.092: INFO: Pod "pod-configmaps-2eb29f32-8ba5-4084-981f-62f9264216c7" satisfied condition "Succeeded or Failed"
Nov 15 16:52:00.108: INFO: Trying to get logs from node 10.15.40.115 pod pod-configmaps-2eb29f32-8ba5-4084-981f-62f9264216c7 container agnhost-container: <nil>
STEP: delete the pod 11/15/23 16:52:00.144
Nov 15 16:52:00.187: INFO: Waiting for pod pod-configmaps-2eb29f32-8ba5-4084-981f-62f9264216c7 to disappear
Nov 15 16:52:00.200: INFO: Pod pod-configmaps-2eb29f32-8ba5-4084-981f-62f9264216c7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 15 16:52:00.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5435" for this suite. 11/15/23 16:52:00.224
------------------------------
• [SLOW TEST] [6.321 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:51:53.928
    Nov 15 16:51:53.928: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename configmap 11/15/23 16:51:53.929
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:51:53.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:51:53.995
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-e5a04f44-5211-4f91-86eb-55ef339df119 11/15/23 16:51:54.011
    STEP: Creating a pod to test consume configMaps 11/15/23 16:51:54.029
    Nov 15 16:51:54.061: INFO: Waiting up to 5m0s for pod "pod-configmaps-2eb29f32-8ba5-4084-981f-62f9264216c7" in namespace "configmap-5435" to be "Succeeded or Failed"
    Nov 15 16:51:54.076: INFO: Pod "pod-configmaps-2eb29f32-8ba5-4084-981f-62f9264216c7": Phase="Pending", Reason="", readiness=false. Elapsed: 14.652478ms
    Nov 15 16:51:56.092: INFO: Pod "pod-configmaps-2eb29f32-8ba5-4084-981f-62f9264216c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030469517s
    Nov 15 16:51:58.092: INFO: Pod "pod-configmaps-2eb29f32-8ba5-4084-981f-62f9264216c7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031081384s
    Nov 15 16:52:00.091: INFO: Pod "pod-configmaps-2eb29f32-8ba5-4084-981f-62f9264216c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029993501s
    STEP: Saw pod success 11/15/23 16:52:00.091
    Nov 15 16:52:00.092: INFO: Pod "pod-configmaps-2eb29f32-8ba5-4084-981f-62f9264216c7" satisfied condition "Succeeded or Failed"
    Nov 15 16:52:00.108: INFO: Trying to get logs from node 10.15.40.115 pod pod-configmaps-2eb29f32-8ba5-4084-981f-62f9264216c7 container agnhost-container: <nil>
    STEP: delete the pod 11/15/23 16:52:00.144
    Nov 15 16:52:00.187: INFO: Waiting for pod pod-configmaps-2eb29f32-8ba5-4084-981f-62f9264216c7 to disappear
    Nov 15 16:52:00.200: INFO: Pod pod-configmaps-2eb29f32-8ba5-4084-981f-62f9264216c7 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:52:00.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5435" for this suite. 11/15/23 16:52:00.224
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:52:00.249
Nov 15 16:52:00.250: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename resourcequota 11/15/23 16:52:00.252
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:52:00.305
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:52:00.319
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 11/15/23 16:52:00.332
STEP: Creating a ResourceQuota 11/15/23 16:52:05.349
STEP: Ensuring resource quota status is calculated 11/15/23 16:52:05.374
STEP: Creating a Pod that fits quota 11/15/23 16:52:07.392
STEP: Ensuring ResourceQuota status captures the pod usage 11/15/23 16:52:07.447
STEP: Not allowing a pod to be created that exceeds remaining quota 11/15/23 16:52:09.464
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 11/15/23 16:52:09.473
STEP: Ensuring a pod cannot update its resource requirements 11/15/23 16:52:09.482
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 11/15/23 16:52:09.497
STEP: Deleting the pod 11/15/23 16:52:11.514
STEP: Ensuring resource quota status released the pod usage 11/15/23 16:52:11.592
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Nov 15 16:52:13.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9212" for this suite. 11/15/23 16:52:13.645
------------------------------
• [SLOW TEST] [13.420 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:52:00.249
    Nov 15 16:52:00.250: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename resourcequota 11/15/23 16:52:00.252
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:52:00.305
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:52:00.319
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 11/15/23 16:52:00.332
    STEP: Creating a ResourceQuota 11/15/23 16:52:05.349
    STEP: Ensuring resource quota status is calculated 11/15/23 16:52:05.374
    STEP: Creating a Pod that fits quota 11/15/23 16:52:07.392
    STEP: Ensuring ResourceQuota status captures the pod usage 11/15/23 16:52:07.447
    STEP: Not allowing a pod to be created that exceeds remaining quota 11/15/23 16:52:09.464
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 11/15/23 16:52:09.473
    STEP: Ensuring a pod cannot update its resource requirements 11/15/23 16:52:09.482
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 11/15/23 16:52:09.497
    STEP: Deleting the pod 11/15/23 16:52:11.514
    STEP: Ensuring resource quota status released the pod usage 11/15/23 16:52:11.592
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:52:13.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9212" for this suite. 11/15/23 16:52:13.645
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:52:13.676
Nov 15 16:52:13.676: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename resourcequota 11/15/23 16:52:13.679
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:52:13.736
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:52:13.75
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 11/15/23 16:52:13.766
STEP: Creating a ResourceQuota 11/15/23 16:52:18.785
STEP: Ensuring resource quota status is calculated 11/15/23 16:52:18.805
STEP: Creating a ReplicationController 11/15/23 16:52:20.824
STEP: Ensuring resource quota status captures replication controller creation 11/15/23 16:52:20.864
STEP: Deleting a ReplicationController 11/15/23 16:52:22.882
STEP: Ensuring resource quota status released usage 11/15/23 16:52:22.908
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Nov 15 16:52:24.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2844" for this suite. 11/15/23 16:52:24.954
------------------------------
• [SLOW TEST] [11.303 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:52:13.676
    Nov 15 16:52:13.676: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename resourcequota 11/15/23 16:52:13.679
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:52:13.736
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:52:13.75
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 11/15/23 16:52:13.766
    STEP: Creating a ResourceQuota 11/15/23 16:52:18.785
    STEP: Ensuring resource quota status is calculated 11/15/23 16:52:18.805
    STEP: Creating a ReplicationController 11/15/23 16:52:20.824
    STEP: Ensuring resource quota status captures replication controller creation 11/15/23 16:52:20.864
    STEP: Deleting a ReplicationController 11/15/23 16:52:22.882
    STEP: Ensuring resource quota status released usage 11/15/23 16:52:22.908
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:52:24.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2844" for this suite. 11/15/23 16:52:24.954
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:52:24.983
Nov 15 16:52:24.983: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename deployment 11/15/23 16:52:24.985
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:52:25.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:52:25.053
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 11/15/23 16:52:25.097
Nov 15 16:52:25.097: INFO: Creating simple deployment test-deployment-4n28x
Nov 15 16:52:25.155: INFO: deployment "test-deployment-4n28x" doesn't have the required revision set
Nov 15 16:52:27.205: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 16, 52, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 16, 52, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 16, 52, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 16, 52, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-4n28x-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status 11/15/23 16:52:29.24
Nov 15 16:52:29.271: INFO: Deployment test-deployment-4n28x has Conditions: [{Available True 2023-11-15 16:52:27 +0000 UTC 2023-11-15 16:52:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-11-15 16:52:27 +0000 UTC 2023-11-15 16:52:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-4n28x-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 11/15/23 16:52:29.271
Nov 15 16:52:29.317: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 16, 52, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 16, 52, 27, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 16, 52, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 16, 52, 25, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-4n28x-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 11/15/23 16:52:29.317
Nov 15 16:52:29.326: INFO: Observed &Deployment event: ADDED
Nov 15 16:52:29.327: INFO: Observed Deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 16:52:25 +0000 UTC 2023-11-15 16:52:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-4n28x-54bc444df"}
Nov 15 16:52:29.328: INFO: Observed &Deployment event: MODIFIED
Nov 15 16:52:29.328: INFO: Observed Deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 16:52:25 +0000 UTC 2023-11-15 16:52:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-4n28x-54bc444df"}
Nov 15 16:52:29.328: INFO: Observed Deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-11-15 16:52:25 +0000 UTC 2023-11-15 16:52:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Nov 15 16:52:29.329: INFO: Observed &Deployment event: MODIFIED
Nov 15 16:52:29.329: INFO: Observed Deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-11-15 16:52:25 +0000 UTC 2023-11-15 16:52:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Nov 15 16:52:29.329: INFO: Observed Deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 16:52:25 +0000 UTC 2023-11-15 16:52:25 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-4n28x-54bc444df" is progressing.}
Nov 15 16:52:29.330: INFO: Observed &Deployment event: MODIFIED
Nov 15 16:52:29.330: INFO: Observed Deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-11-15 16:52:27 +0000 UTC 2023-11-15 16:52:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Nov 15 16:52:29.330: INFO: Observed Deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 16:52:27 +0000 UTC 2023-11-15 16:52:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-4n28x-54bc444df" has successfully progressed.}
Nov 15 16:52:29.331: INFO: Observed &Deployment event: MODIFIED
Nov 15 16:52:29.331: INFO: Observed Deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-11-15 16:52:27 +0000 UTC 2023-11-15 16:52:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Nov 15 16:52:29.331: INFO: Observed Deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 16:52:27 +0000 UTC 2023-11-15 16:52:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-4n28x-54bc444df" has successfully progressed.}
Nov 15 16:52:29.331: INFO: Found Deployment test-deployment-4n28x in namespace deployment-1148 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Nov 15 16:52:29.331: INFO: Deployment test-deployment-4n28x has an updated status
STEP: patching the Statefulset Status 11/15/23 16:52:29.331
Nov 15 16:52:29.332: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Nov 15 16:52:29.358: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 11/15/23 16:52:29.358
Nov 15 16:52:29.368: INFO: Observed &Deployment event: ADDED
Nov 15 16:52:29.368: INFO: Observed deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 16:52:25 +0000 UTC 2023-11-15 16:52:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-4n28x-54bc444df"}
Nov 15 16:52:29.369: INFO: Observed &Deployment event: MODIFIED
Nov 15 16:52:29.369: INFO: Observed deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 16:52:25 +0000 UTC 2023-11-15 16:52:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-4n28x-54bc444df"}
Nov 15 16:52:29.369: INFO: Observed deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-11-15 16:52:25 +0000 UTC 2023-11-15 16:52:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Nov 15 16:52:29.370: INFO: Observed &Deployment event: MODIFIED
Nov 15 16:52:29.370: INFO: Observed deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-11-15 16:52:25 +0000 UTC 2023-11-15 16:52:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Nov 15 16:52:29.371: INFO: Observed deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 16:52:25 +0000 UTC 2023-11-15 16:52:25 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-4n28x-54bc444df" is progressing.}
Nov 15 16:52:29.372: INFO: Observed &Deployment event: MODIFIED
Nov 15 16:52:29.372: INFO: Observed deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-11-15 16:52:27 +0000 UTC 2023-11-15 16:52:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Nov 15 16:52:29.372: INFO: Observed deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 16:52:27 +0000 UTC 2023-11-15 16:52:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-4n28x-54bc444df" has successfully progressed.}
Nov 15 16:52:29.373: INFO: Observed &Deployment event: MODIFIED
Nov 15 16:52:29.373: INFO: Observed deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-11-15 16:52:27 +0000 UTC 2023-11-15 16:52:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Nov 15 16:52:29.373: INFO: Observed deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 16:52:27 +0000 UTC 2023-11-15 16:52:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-4n28x-54bc444df" has successfully progressed.}
Nov 15 16:52:29.374: INFO: Observed deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Nov 15 16:52:29.374: INFO: Observed &Deployment event: MODIFIED
Nov 15 16:52:29.374: INFO: Found deployment test-deployment-4n28x in namespace deployment-1148 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Nov 15 16:52:29.374: INFO: Deployment test-deployment-4n28x has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Nov 15 16:52:29.394: INFO: Deployment "test-deployment-4n28x":
&Deployment{ObjectMeta:{test-deployment-4n28x  deployment-1148  27492dbf-2217-47e2-8376-579ad923c61e 47679 1 2023-11-15 16:52:25 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-11-15 16:52:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-11-15 16:52:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-11-15 16:52:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005c81f38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-4n28x-54bc444df",LastUpdateTime:2023-11-15 16:52:29 +0000 UTC,LastTransitionTime:2023-11-15 16:52:29 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 15 16:52:29.411: INFO: New ReplicaSet "test-deployment-4n28x-54bc444df" of Deployment "test-deployment-4n28x":
&ReplicaSet{ObjectMeta:{test-deployment-4n28x-54bc444df  deployment-1148  7981a46d-e376-4ae1-93a0-870ac3271aac 47671 1 2023-11-15 16:52:25 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-4n28x 27492dbf-2217-47e2-8376-579ad923c61e 0xc004bb4287 0xc004bb4288}] [] [{kube-controller-manager Update apps/v1 2023-11-15 16:52:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27492dbf-2217-47e2-8376-579ad923c61e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 16:52:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004bb4338 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 15 16:52:29.426: INFO: Pod "test-deployment-4n28x-54bc444df-j7rsf" is available:
&Pod{ObjectMeta:{test-deployment-4n28x-54bc444df-j7rsf test-deployment-4n28x-54bc444df- deployment-1148  be4777ef-4056-4bab-a1fd-18fa0390f59e 47670 0 2023-11-15 16:52:25 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:10264402ad7389e15a1cb9062d0074a75060f3d7cf5664794e08b59c38270467 cni.projectcalico.org/podIP:172.30.164.8/32 cni.projectcalico.org/podIPs:172.30.164.8/32] [{apps/v1 ReplicaSet test-deployment-4n28x-54bc444df 7981a46d-e376-4ae1-93a0-870ac3271aac 0xc0052b0580 0xc0052b0581}] [] [{kube-controller-manager Update v1 2023-11-15 16:52:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7981a46d-e376-4ae1-93a0-870ac3271aac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 16:52:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 16:52:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.164.8\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tbxmp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tbxmp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:52:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:52:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:52:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:52:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:172.30.164.8,StartTime:2023-11-15 16:52:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 16:52:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1d082be7c565d75fcd2d9197c0008f267fe7c8c8a891cc3e519def3fe18affda,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.164.8,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Nov 15 16:52:29.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1148" for this suite. 11/15/23 16:52:29.448
------------------------------
• [4.491 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:52:24.983
    Nov 15 16:52:24.983: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename deployment 11/15/23 16:52:24.985
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:52:25.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:52:25.053
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 11/15/23 16:52:25.097
    Nov 15 16:52:25.097: INFO: Creating simple deployment test-deployment-4n28x
    Nov 15 16:52:25.155: INFO: deployment "test-deployment-4n28x" doesn't have the required revision set
    Nov 15 16:52:27.205: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 16, 52, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 16, 52, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 16, 52, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 16, 52, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-4n28x-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Getting /status 11/15/23 16:52:29.24
    Nov 15 16:52:29.271: INFO: Deployment test-deployment-4n28x has Conditions: [{Available True 2023-11-15 16:52:27 +0000 UTC 2023-11-15 16:52:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-11-15 16:52:27 +0000 UTC 2023-11-15 16:52:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-4n28x-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 11/15/23 16:52:29.271
    Nov 15 16:52:29.317: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 16, 52, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 16, 52, 27, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 16, 52, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 16, 52, 25, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-4n28x-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 11/15/23 16:52:29.317
    Nov 15 16:52:29.326: INFO: Observed &Deployment event: ADDED
    Nov 15 16:52:29.327: INFO: Observed Deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 16:52:25 +0000 UTC 2023-11-15 16:52:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-4n28x-54bc444df"}
    Nov 15 16:52:29.328: INFO: Observed &Deployment event: MODIFIED
    Nov 15 16:52:29.328: INFO: Observed Deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 16:52:25 +0000 UTC 2023-11-15 16:52:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-4n28x-54bc444df"}
    Nov 15 16:52:29.328: INFO: Observed Deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-11-15 16:52:25 +0000 UTC 2023-11-15 16:52:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Nov 15 16:52:29.329: INFO: Observed &Deployment event: MODIFIED
    Nov 15 16:52:29.329: INFO: Observed Deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-11-15 16:52:25 +0000 UTC 2023-11-15 16:52:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Nov 15 16:52:29.329: INFO: Observed Deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 16:52:25 +0000 UTC 2023-11-15 16:52:25 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-4n28x-54bc444df" is progressing.}
    Nov 15 16:52:29.330: INFO: Observed &Deployment event: MODIFIED
    Nov 15 16:52:29.330: INFO: Observed Deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-11-15 16:52:27 +0000 UTC 2023-11-15 16:52:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Nov 15 16:52:29.330: INFO: Observed Deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 16:52:27 +0000 UTC 2023-11-15 16:52:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-4n28x-54bc444df" has successfully progressed.}
    Nov 15 16:52:29.331: INFO: Observed &Deployment event: MODIFIED
    Nov 15 16:52:29.331: INFO: Observed Deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-11-15 16:52:27 +0000 UTC 2023-11-15 16:52:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Nov 15 16:52:29.331: INFO: Observed Deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 16:52:27 +0000 UTC 2023-11-15 16:52:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-4n28x-54bc444df" has successfully progressed.}
    Nov 15 16:52:29.331: INFO: Found Deployment test-deployment-4n28x in namespace deployment-1148 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Nov 15 16:52:29.331: INFO: Deployment test-deployment-4n28x has an updated status
    STEP: patching the Statefulset Status 11/15/23 16:52:29.331
    Nov 15 16:52:29.332: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Nov 15 16:52:29.358: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 11/15/23 16:52:29.358
    Nov 15 16:52:29.368: INFO: Observed &Deployment event: ADDED
    Nov 15 16:52:29.368: INFO: Observed deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 16:52:25 +0000 UTC 2023-11-15 16:52:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-4n28x-54bc444df"}
    Nov 15 16:52:29.369: INFO: Observed &Deployment event: MODIFIED
    Nov 15 16:52:29.369: INFO: Observed deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 16:52:25 +0000 UTC 2023-11-15 16:52:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-4n28x-54bc444df"}
    Nov 15 16:52:29.369: INFO: Observed deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-11-15 16:52:25 +0000 UTC 2023-11-15 16:52:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Nov 15 16:52:29.370: INFO: Observed &Deployment event: MODIFIED
    Nov 15 16:52:29.370: INFO: Observed deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-11-15 16:52:25 +0000 UTC 2023-11-15 16:52:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Nov 15 16:52:29.371: INFO: Observed deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 16:52:25 +0000 UTC 2023-11-15 16:52:25 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-4n28x-54bc444df" is progressing.}
    Nov 15 16:52:29.372: INFO: Observed &Deployment event: MODIFIED
    Nov 15 16:52:29.372: INFO: Observed deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-11-15 16:52:27 +0000 UTC 2023-11-15 16:52:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Nov 15 16:52:29.372: INFO: Observed deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 16:52:27 +0000 UTC 2023-11-15 16:52:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-4n28x-54bc444df" has successfully progressed.}
    Nov 15 16:52:29.373: INFO: Observed &Deployment event: MODIFIED
    Nov 15 16:52:29.373: INFO: Observed deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-11-15 16:52:27 +0000 UTC 2023-11-15 16:52:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Nov 15 16:52:29.373: INFO: Observed deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-15 16:52:27 +0000 UTC 2023-11-15 16:52:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-4n28x-54bc444df" has successfully progressed.}
    Nov 15 16:52:29.374: INFO: Observed deployment test-deployment-4n28x in namespace deployment-1148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Nov 15 16:52:29.374: INFO: Observed &Deployment event: MODIFIED
    Nov 15 16:52:29.374: INFO: Found deployment test-deployment-4n28x in namespace deployment-1148 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Nov 15 16:52:29.374: INFO: Deployment test-deployment-4n28x has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Nov 15 16:52:29.394: INFO: Deployment "test-deployment-4n28x":
    &Deployment{ObjectMeta:{test-deployment-4n28x  deployment-1148  27492dbf-2217-47e2-8376-579ad923c61e 47679 1 2023-11-15 16:52:25 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-11-15 16:52:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-11-15 16:52:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-11-15 16:52:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005c81f38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-4n28x-54bc444df",LastUpdateTime:2023-11-15 16:52:29 +0000 UTC,LastTransitionTime:2023-11-15 16:52:29 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Nov 15 16:52:29.411: INFO: New ReplicaSet "test-deployment-4n28x-54bc444df" of Deployment "test-deployment-4n28x":
    &ReplicaSet{ObjectMeta:{test-deployment-4n28x-54bc444df  deployment-1148  7981a46d-e376-4ae1-93a0-870ac3271aac 47671 1 2023-11-15 16:52:25 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-4n28x 27492dbf-2217-47e2-8376-579ad923c61e 0xc004bb4287 0xc004bb4288}] [] [{kube-controller-manager Update apps/v1 2023-11-15 16:52:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27492dbf-2217-47e2-8376-579ad923c61e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 16:52:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004bb4338 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Nov 15 16:52:29.426: INFO: Pod "test-deployment-4n28x-54bc444df-j7rsf" is available:
    &Pod{ObjectMeta:{test-deployment-4n28x-54bc444df-j7rsf test-deployment-4n28x-54bc444df- deployment-1148  be4777ef-4056-4bab-a1fd-18fa0390f59e 47670 0 2023-11-15 16:52:25 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:10264402ad7389e15a1cb9062d0074a75060f3d7cf5664794e08b59c38270467 cni.projectcalico.org/podIP:172.30.164.8/32 cni.projectcalico.org/podIPs:172.30.164.8/32] [{apps/v1 ReplicaSet test-deployment-4n28x-54bc444df 7981a46d-e376-4ae1-93a0-870ac3271aac 0xc0052b0580 0xc0052b0581}] [] [{kube-controller-manager Update v1 2023-11-15 16:52:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7981a46d-e376-4ae1-93a0-870ac3271aac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 16:52:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 16:52:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.164.8\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tbxmp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tbxmp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:52:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:52:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:52:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 16:52:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:172.30.164.8,StartTime:2023-11-15 16:52:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 16:52:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1d082be7c565d75fcd2d9197c0008f267fe7c8c8a891cc3e519def3fe18affda,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.164.8,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:52:29.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1148" for this suite. 11/15/23 16:52:29.448
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:52:29.486
Nov 15 16:52:29.486: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename containers 11/15/23 16:52:29.497
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:52:29.546
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:52:29.562
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 11/15/23 16:52:29.576
Nov 15 16:52:29.610: INFO: Waiting up to 5m0s for pod "client-containers-2adc5ee5-fd99-4174-8adb-09816a915d70" in namespace "containers-7193" to be "Succeeded or Failed"
Nov 15 16:52:29.625: INFO: Pod "client-containers-2adc5ee5-fd99-4174-8adb-09816a915d70": Phase="Pending", Reason="", readiness=false. Elapsed: 14.182303ms
Nov 15 16:52:31.642: INFO: Pod "client-containers-2adc5ee5-fd99-4174-8adb-09816a915d70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03131074s
Nov 15 16:52:33.643: INFO: Pod "client-containers-2adc5ee5-fd99-4174-8adb-09816a915d70": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03260455s
Nov 15 16:52:35.647: INFO: Pod "client-containers-2adc5ee5-fd99-4174-8adb-09816a915d70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036597516s
STEP: Saw pod success 11/15/23 16:52:35.647
Nov 15 16:52:35.648: INFO: Pod "client-containers-2adc5ee5-fd99-4174-8adb-09816a915d70" satisfied condition "Succeeded or Failed"
Nov 15 16:52:35.665: INFO: Trying to get logs from node 10.15.40.115 pod client-containers-2adc5ee5-fd99-4174-8adb-09816a915d70 container agnhost-container: <nil>
STEP: delete the pod 11/15/23 16:52:35.716
Nov 15 16:52:35.753: INFO: Waiting for pod client-containers-2adc5ee5-fd99-4174-8adb-09816a915d70 to disappear
Nov 15 16:52:35.768: INFO: Pod client-containers-2adc5ee5-fd99-4174-8adb-09816a915d70 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Nov 15 16:52:35.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-7193" for this suite. 11/15/23 16:52:35.797
------------------------------
• [SLOW TEST] [6.338 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:52:29.486
    Nov 15 16:52:29.486: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename containers 11/15/23 16:52:29.497
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:52:29.546
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:52:29.562
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 11/15/23 16:52:29.576
    Nov 15 16:52:29.610: INFO: Waiting up to 5m0s for pod "client-containers-2adc5ee5-fd99-4174-8adb-09816a915d70" in namespace "containers-7193" to be "Succeeded or Failed"
    Nov 15 16:52:29.625: INFO: Pod "client-containers-2adc5ee5-fd99-4174-8adb-09816a915d70": Phase="Pending", Reason="", readiness=false. Elapsed: 14.182303ms
    Nov 15 16:52:31.642: INFO: Pod "client-containers-2adc5ee5-fd99-4174-8adb-09816a915d70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03131074s
    Nov 15 16:52:33.643: INFO: Pod "client-containers-2adc5ee5-fd99-4174-8adb-09816a915d70": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03260455s
    Nov 15 16:52:35.647: INFO: Pod "client-containers-2adc5ee5-fd99-4174-8adb-09816a915d70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036597516s
    STEP: Saw pod success 11/15/23 16:52:35.647
    Nov 15 16:52:35.648: INFO: Pod "client-containers-2adc5ee5-fd99-4174-8adb-09816a915d70" satisfied condition "Succeeded or Failed"
    Nov 15 16:52:35.665: INFO: Trying to get logs from node 10.15.40.115 pod client-containers-2adc5ee5-fd99-4174-8adb-09816a915d70 container agnhost-container: <nil>
    STEP: delete the pod 11/15/23 16:52:35.716
    Nov 15 16:52:35.753: INFO: Waiting for pod client-containers-2adc5ee5-fd99-4174-8adb-09816a915d70 to disappear
    Nov 15 16:52:35.768: INFO: Pod client-containers-2adc5ee5-fd99-4174-8adb-09816a915d70 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:52:35.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-7193" for this suite. 11/15/23 16:52:35.797
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:52:35.834
Nov 15 16:52:35.834: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename var-expansion 11/15/23 16:52:35.837
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:52:35.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:52:35.91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 11/15/23 16:52:35.924
Nov 15 16:52:35.957: INFO: Waiting up to 2m0s for pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696" in namespace "var-expansion-9186" to be "running"
Nov 15 16:52:35.972: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 14.350526ms
Nov 15 16:52:37.987: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030187699s
Nov 15 16:52:39.990: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032637896s
Nov 15 16:52:41.990: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 6.032313164s
Nov 15 16:52:43.992: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 8.034492181s
Nov 15 16:52:46.002: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 10.04502218s
Nov 15 16:52:47.989: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 12.031425647s
Nov 15 16:52:49.988: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 14.031087223s
Nov 15 16:52:51.987: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 16.029831288s
Nov 15 16:52:53.988: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 18.030927962s
Nov 15 16:52:55.988: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 20.030539558s
Nov 15 16:52:57.993: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 22.035335052s
Nov 15 16:52:59.986: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 24.029150953s
Nov 15 16:53:01.988: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 26.030428263s
Nov 15 16:53:03.996: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 28.038645636s
Nov 15 16:53:05.988: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 30.030834465s
Nov 15 16:53:07.988: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 32.030481109s
Nov 15 16:53:09.992: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 34.035097125s
Nov 15 16:53:11.987: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 36.029311066s
Nov 15 16:53:13.988: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 38.030795661s
Nov 15 16:53:15.992: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 40.034827321s
Nov 15 16:53:17.987: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 42.029813166s
Nov 15 16:53:19.988: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 44.03061735s
Nov 15 16:53:22.002: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 46.044277775s
Nov 15 16:53:23.987: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 48.029951149s
Nov 15 16:53:25.987: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 50.03005106s
Nov 15 16:53:27.989: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 52.031383387s
Nov 15 16:53:29.989: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 54.031499078s
Nov 15 16:53:31.987: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 56.029868726s
Nov 15 16:53:33.992: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 58.034807295s
Nov 15 16:53:35.988: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.030954253s
Nov 15 16:53:37.989: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.031550315s
Nov 15 16:53:39.986: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.028552237s
Nov 15 16:53:41.988: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.030352014s
Nov 15 16:53:43.986: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.029004172s
Nov 15 16:53:45.992: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.034753401s
Nov 15 16:53:47.992: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.034442025s
Nov 15 16:53:50.000: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.04229033s
Nov 15 16:53:51.989: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.031421034s
Nov 15 16:53:53.986: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.028754445s
Nov 15 16:53:55.986: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.029155931s
Nov 15 16:53:57.993: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.035696823s
Nov 15 16:53:59.998: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.040428476s
Nov 15 16:54:01.991: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.034093655s
Nov 15 16:54:03.986: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.029131653s
Nov 15 16:54:05.987: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.029911767s
Nov 15 16:54:07.988: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.030916238s
Nov 15 16:54:09.993: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.035458056s
Nov 15 16:54:11.989: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.031502771s
Nov 15 16:54:13.986: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.029068049s
Nov 15 16:54:15.987: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.030008573s
Nov 15 16:54:17.989: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.031517313s
Nov 15 16:54:19.988: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.030821652s
Nov 15 16:54:21.993: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.035459347s
Nov 15 16:54:23.988: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.03026459s
Nov 15 16:54:25.987: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.030004432s
Nov 15 16:54:27.988: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.031086384s
Nov 15 16:54:29.988: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.031124501s
Nov 15 16:54:31.991: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.03420262s
Nov 15 16:54:33.993: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.035410817s
Nov 15 16:54:35.988: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.030810448s
Nov 15 16:54:36.004: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.046671543s
STEP: updating the pod 11/15/23 16:54:36.004
Nov 15 16:54:36.548: INFO: Successfully updated pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696"
STEP: waiting for pod running 11/15/23 16:54:36.548
Nov 15 16:54:36.549: INFO: Waiting up to 2m0s for pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696" in namespace "var-expansion-9186" to be "running"
Nov 15 16:54:36.563: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 14.06031ms
Nov 15 16:54:38.579: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Running", Reason="", readiness=true. Elapsed: 2.029312151s
Nov 15 16:54:38.579: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696" satisfied condition "running"
STEP: deleting the pod gracefully 11/15/23 16:54:38.579
Nov 15 16:54:38.579: INFO: Deleting pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696" in namespace "var-expansion-9186"
Nov 15 16:54:38.606: INFO: Wait up to 5m0s for pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Nov 15 16:55:10.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9186" for this suite. 11/15/23 16:55:10.66
------------------------------
• [SLOW TEST] [154.850 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:52:35.834
    Nov 15 16:52:35.834: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename var-expansion 11/15/23 16:52:35.837
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:52:35.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:52:35.91
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 11/15/23 16:52:35.924
    Nov 15 16:52:35.957: INFO: Waiting up to 2m0s for pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696" in namespace "var-expansion-9186" to be "running"
    Nov 15 16:52:35.972: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 14.350526ms
    Nov 15 16:52:37.987: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030187699s
    Nov 15 16:52:39.990: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032637896s
    Nov 15 16:52:41.990: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 6.032313164s
    Nov 15 16:52:43.992: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 8.034492181s
    Nov 15 16:52:46.002: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 10.04502218s
    Nov 15 16:52:47.989: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 12.031425647s
    Nov 15 16:52:49.988: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 14.031087223s
    Nov 15 16:52:51.987: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 16.029831288s
    Nov 15 16:52:53.988: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 18.030927962s
    Nov 15 16:52:55.988: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 20.030539558s
    Nov 15 16:52:57.993: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 22.035335052s
    Nov 15 16:52:59.986: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 24.029150953s
    Nov 15 16:53:01.988: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 26.030428263s
    Nov 15 16:53:03.996: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 28.038645636s
    Nov 15 16:53:05.988: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 30.030834465s
    Nov 15 16:53:07.988: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 32.030481109s
    Nov 15 16:53:09.992: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 34.035097125s
    Nov 15 16:53:11.987: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 36.029311066s
    Nov 15 16:53:13.988: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 38.030795661s
    Nov 15 16:53:15.992: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 40.034827321s
    Nov 15 16:53:17.987: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 42.029813166s
    Nov 15 16:53:19.988: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 44.03061735s
    Nov 15 16:53:22.002: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 46.044277775s
    Nov 15 16:53:23.987: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 48.029951149s
    Nov 15 16:53:25.987: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 50.03005106s
    Nov 15 16:53:27.989: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 52.031383387s
    Nov 15 16:53:29.989: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 54.031499078s
    Nov 15 16:53:31.987: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 56.029868726s
    Nov 15 16:53:33.992: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 58.034807295s
    Nov 15 16:53:35.988: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.030954253s
    Nov 15 16:53:37.989: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.031550315s
    Nov 15 16:53:39.986: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.028552237s
    Nov 15 16:53:41.988: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.030352014s
    Nov 15 16:53:43.986: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.029004172s
    Nov 15 16:53:45.992: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.034753401s
    Nov 15 16:53:47.992: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.034442025s
    Nov 15 16:53:50.000: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.04229033s
    Nov 15 16:53:51.989: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.031421034s
    Nov 15 16:53:53.986: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.028754445s
    Nov 15 16:53:55.986: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.029155931s
    Nov 15 16:53:57.993: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.035696823s
    Nov 15 16:53:59.998: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.040428476s
    Nov 15 16:54:01.991: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.034093655s
    Nov 15 16:54:03.986: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.029131653s
    Nov 15 16:54:05.987: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.029911767s
    Nov 15 16:54:07.988: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.030916238s
    Nov 15 16:54:09.993: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.035458056s
    Nov 15 16:54:11.989: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.031502771s
    Nov 15 16:54:13.986: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.029068049s
    Nov 15 16:54:15.987: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.030008573s
    Nov 15 16:54:17.989: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.031517313s
    Nov 15 16:54:19.988: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.030821652s
    Nov 15 16:54:21.993: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.035459347s
    Nov 15 16:54:23.988: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.03026459s
    Nov 15 16:54:25.987: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.030004432s
    Nov 15 16:54:27.988: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.031086384s
    Nov 15 16:54:29.988: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.031124501s
    Nov 15 16:54:31.991: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.03420262s
    Nov 15 16:54:33.993: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.035410817s
    Nov 15 16:54:35.988: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.030810448s
    Nov 15 16:54:36.004: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.046671543s
    STEP: updating the pod 11/15/23 16:54:36.004
    Nov 15 16:54:36.548: INFO: Successfully updated pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696"
    STEP: waiting for pod running 11/15/23 16:54:36.548
    Nov 15 16:54:36.549: INFO: Waiting up to 2m0s for pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696" in namespace "var-expansion-9186" to be "running"
    Nov 15 16:54:36.563: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Pending", Reason="", readiness=false. Elapsed: 14.06031ms
    Nov 15 16:54:38.579: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696": Phase="Running", Reason="", readiness=true. Elapsed: 2.029312151s
    Nov 15 16:54:38.579: INFO: Pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696" satisfied condition "running"
    STEP: deleting the pod gracefully 11/15/23 16:54:38.579
    Nov 15 16:54:38.579: INFO: Deleting pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696" in namespace "var-expansion-9186"
    Nov 15 16:54:38.606: INFO: Wait up to 5m0s for pod "var-expansion-f7a6bb6d-f897-45bb-8ba6-ca60b3921696" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:55:10.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9186" for this suite. 11/15/23 16:55:10.66
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:55:10.703
Nov 15 16:55:10.703: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename replicaset 11/15/23 16:55:10.704
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:55:10.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:55:10.771
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Nov 15 16:55:10.844: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov 15 16:55:15.857: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 11/15/23 16:55:15.858
STEP: Scaling up "test-rs" replicaset  11/15/23 16:55:15.858
Nov 15 16:55:15.892: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 11/15/23 16:55:15.892
W1115 16:55:15.920233      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Nov 15 16:55:15.928: INFO: observed ReplicaSet test-rs in namespace replicaset-7801 with ReadyReplicas 1, AvailableReplicas 1
Nov 15 16:55:15.958: INFO: observed ReplicaSet test-rs in namespace replicaset-7801 with ReadyReplicas 1, AvailableReplicas 1
Nov 15 16:55:15.991: INFO: observed ReplicaSet test-rs in namespace replicaset-7801 with ReadyReplicas 1, AvailableReplicas 1
Nov 15 16:55:16.009: INFO: observed ReplicaSet test-rs in namespace replicaset-7801 with ReadyReplicas 1, AvailableReplicas 1
Nov 15 16:55:17.611: INFO: observed ReplicaSet test-rs in namespace replicaset-7801 with ReadyReplicas 2, AvailableReplicas 2
Nov 15 16:55:18.267: INFO: observed Replicaset test-rs in namespace replicaset-7801 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Nov 15 16:55:18.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-7801" for this suite. 11/15/23 16:55:18.295
------------------------------
• [SLOW TEST] [7.617 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:55:10.703
    Nov 15 16:55:10.703: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename replicaset 11/15/23 16:55:10.704
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:55:10.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:55:10.771
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Nov 15 16:55:10.844: INFO: Pod name sample-pod: Found 0 pods out of 1
    Nov 15 16:55:15.857: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 11/15/23 16:55:15.858
    STEP: Scaling up "test-rs" replicaset  11/15/23 16:55:15.858
    Nov 15 16:55:15.892: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 11/15/23 16:55:15.892
    W1115 16:55:15.920233      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Nov 15 16:55:15.928: INFO: observed ReplicaSet test-rs in namespace replicaset-7801 with ReadyReplicas 1, AvailableReplicas 1
    Nov 15 16:55:15.958: INFO: observed ReplicaSet test-rs in namespace replicaset-7801 with ReadyReplicas 1, AvailableReplicas 1
    Nov 15 16:55:15.991: INFO: observed ReplicaSet test-rs in namespace replicaset-7801 with ReadyReplicas 1, AvailableReplicas 1
    Nov 15 16:55:16.009: INFO: observed ReplicaSet test-rs in namespace replicaset-7801 with ReadyReplicas 1, AvailableReplicas 1
    Nov 15 16:55:17.611: INFO: observed ReplicaSet test-rs in namespace replicaset-7801 with ReadyReplicas 2, AvailableReplicas 2
    Nov 15 16:55:18.267: INFO: observed Replicaset test-rs in namespace replicaset-7801 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:55:18.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-7801" for this suite. 11/15/23 16:55:18.295
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:55:18.328
Nov 15 16:55:18.329: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename job 11/15/23 16:55:18.33
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:55:18.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:55:18.394
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 11/15/23 16:55:18.408
STEP: Ensuring active pods == parallelism 11/15/23 16:55:18.428
STEP: delete a job 11/15/23 16:55:20.449
STEP: deleting Job.batch foo in namespace job-7210, will wait for the garbage collector to delete the pods 11/15/23 16:55:20.449
Nov 15 16:55:20.537: INFO: Deleting Job.batch foo took: 23.26317ms
Nov 15 16:55:20.738: INFO: Terminating Job.batch foo pods took: 201.067326ms
STEP: Ensuring job was deleted 11/15/23 16:55:53.639
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Nov 15 16:55:53.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-7210" for this suite. 11/15/23 16:55:53.683
------------------------------
• [SLOW TEST] [35.380 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:55:18.328
    Nov 15 16:55:18.329: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename job 11/15/23 16:55:18.33
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:55:18.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:55:18.394
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 11/15/23 16:55:18.408
    STEP: Ensuring active pods == parallelism 11/15/23 16:55:18.428
    STEP: delete a job 11/15/23 16:55:20.449
    STEP: deleting Job.batch foo in namespace job-7210, will wait for the garbage collector to delete the pods 11/15/23 16:55:20.449
    Nov 15 16:55:20.537: INFO: Deleting Job.batch foo took: 23.26317ms
    Nov 15 16:55:20.738: INFO: Terminating Job.batch foo pods took: 201.067326ms
    STEP: Ensuring job was deleted 11/15/23 16:55:53.639
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:55:53.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-7210" for this suite. 11/15/23 16:55:53.683
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:55:53.727
Nov 15 16:55:53.728: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename emptydir 11/15/23 16:55:53.729
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:55:53.787
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:55:53.802
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 11/15/23 16:55:53.816
Nov 15 16:55:53.851: INFO: Waiting up to 5m0s for pod "pod-0482a84f-f8d4-4d44-8fff-49ff7a8a2140" in namespace "emptydir-9028" to be "Succeeded or Failed"
Nov 15 16:55:53.865: INFO: Pod "pod-0482a84f-f8d4-4d44-8fff-49ff7a8a2140": Phase="Pending", Reason="", readiness=false. Elapsed: 13.241305ms
Nov 15 16:55:55.880: INFO: Pod "pod-0482a84f-f8d4-4d44-8fff-49ff7a8a2140": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028522364s
Nov 15 16:55:57.880: INFO: Pod "pod-0482a84f-f8d4-4d44-8fff-49ff7a8a2140": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028213965s
STEP: Saw pod success 11/15/23 16:55:57.88
Nov 15 16:55:57.881: INFO: Pod "pod-0482a84f-f8d4-4d44-8fff-49ff7a8a2140" satisfied condition "Succeeded or Failed"
Nov 15 16:55:57.895: INFO: Trying to get logs from node 10.15.40.115 pod pod-0482a84f-f8d4-4d44-8fff-49ff7a8a2140 container test-container: <nil>
STEP: delete the pod 11/15/23 16:55:58.022
Nov 15 16:55:58.057: INFO: Waiting for pod pod-0482a84f-f8d4-4d44-8fff-49ff7a8a2140 to disappear
Nov 15 16:55:58.080: INFO: Pod pod-0482a84f-f8d4-4d44-8fff-49ff7a8a2140 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 15 16:55:58.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9028" for this suite. 11/15/23 16:55:58.103
------------------------------
• [4.421 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:55:53.727
    Nov 15 16:55:53.728: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename emptydir 11/15/23 16:55:53.729
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:55:53.787
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:55:53.802
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 11/15/23 16:55:53.816
    Nov 15 16:55:53.851: INFO: Waiting up to 5m0s for pod "pod-0482a84f-f8d4-4d44-8fff-49ff7a8a2140" in namespace "emptydir-9028" to be "Succeeded or Failed"
    Nov 15 16:55:53.865: INFO: Pod "pod-0482a84f-f8d4-4d44-8fff-49ff7a8a2140": Phase="Pending", Reason="", readiness=false. Elapsed: 13.241305ms
    Nov 15 16:55:55.880: INFO: Pod "pod-0482a84f-f8d4-4d44-8fff-49ff7a8a2140": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028522364s
    Nov 15 16:55:57.880: INFO: Pod "pod-0482a84f-f8d4-4d44-8fff-49ff7a8a2140": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028213965s
    STEP: Saw pod success 11/15/23 16:55:57.88
    Nov 15 16:55:57.881: INFO: Pod "pod-0482a84f-f8d4-4d44-8fff-49ff7a8a2140" satisfied condition "Succeeded or Failed"
    Nov 15 16:55:57.895: INFO: Trying to get logs from node 10.15.40.115 pod pod-0482a84f-f8d4-4d44-8fff-49ff7a8a2140 container test-container: <nil>
    STEP: delete the pod 11/15/23 16:55:58.022
    Nov 15 16:55:58.057: INFO: Waiting for pod pod-0482a84f-f8d4-4d44-8fff-49ff7a8a2140 to disappear
    Nov 15 16:55:58.080: INFO: Pod pod-0482a84f-f8d4-4d44-8fff-49ff7a8a2140 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:55:58.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9028" for this suite. 11/15/23 16:55:58.103
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:55:58.169
Nov 15 16:55:58.170: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename configmap 11/15/23 16:55:58.171
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:55:58.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:55:58.238
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-0b3eb4c2-d8a0-4d3c-a9a7-7ea8ebeb2ce7 11/15/23 16:55:58.261
STEP: Creating a pod to test consume configMaps 11/15/23 16:55:58.281
Nov 15 16:55:58.314: INFO: Waiting up to 5m0s for pod "pod-configmaps-b50b952a-cb0a-4ce3-8cc8-cc68ef489009" in namespace "configmap-8892" to be "Succeeded or Failed"
Nov 15 16:55:58.329: INFO: Pod "pod-configmaps-b50b952a-cb0a-4ce3-8cc8-cc68ef489009": Phase="Pending", Reason="", readiness=false. Elapsed: 15.010333ms
Nov 15 16:56:00.345: INFO: Pod "pod-configmaps-b50b952a-cb0a-4ce3-8cc8-cc68ef489009": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030712969s
Nov 15 16:56:02.345: INFO: Pod "pod-configmaps-b50b952a-cb0a-4ce3-8cc8-cc68ef489009": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031212551s
Nov 15 16:56:04.353: INFO: Pod "pod-configmaps-b50b952a-cb0a-4ce3-8cc8-cc68ef489009": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038595475s
STEP: Saw pod success 11/15/23 16:56:04.353
Nov 15 16:56:04.353: INFO: Pod "pod-configmaps-b50b952a-cb0a-4ce3-8cc8-cc68ef489009" satisfied condition "Succeeded or Failed"
Nov 15 16:56:04.368: INFO: Trying to get logs from node 10.15.40.115 pod pod-configmaps-b50b952a-cb0a-4ce3-8cc8-cc68ef489009 container agnhost-container: <nil>
STEP: delete the pod 11/15/23 16:56:04.402
Nov 15 16:56:04.448: INFO: Waiting for pod pod-configmaps-b50b952a-cb0a-4ce3-8cc8-cc68ef489009 to disappear
Nov 15 16:56:04.463: INFO: Pod pod-configmaps-b50b952a-cb0a-4ce3-8cc8-cc68ef489009 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 15 16:56:04.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8892" for this suite. 11/15/23 16:56:04.487
------------------------------
• [SLOW TEST] [6.346 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:55:58.169
    Nov 15 16:55:58.170: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename configmap 11/15/23 16:55:58.171
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:55:58.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:55:58.238
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-0b3eb4c2-d8a0-4d3c-a9a7-7ea8ebeb2ce7 11/15/23 16:55:58.261
    STEP: Creating a pod to test consume configMaps 11/15/23 16:55:58.281
    Nov 15 16:55:58.314: INFO: Waiting up to 5m0s for pod "pod-configmaps-b50b952a-cb0a-4ce3-8cc8-cc68ef489009" in namespace "configmap-8892" to be "Succeeded or Failed"
    Nov 15 16:55:58.329: INFO: Pod "pod-configmaps-b50b952a-cb0a-4ce3-8cc8-cc68ef489009": Phase="Pending", Reason="", readiness=false. Elapsed: 15.010333ms
    Nov 15 16:56:00.345: INFO: Pod "pod-configmaps-b50b952a-cb0a-4ce3-8cc8-cc68ef489009": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030712969s
    Nov 15 16:56:02.345: INFO: Pod "pod-configmaps-b50b952a-cb0a-4ce3-8cc8-cc68ef489009": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031212551s
    Nov 15 16:56:04.353: INFO: Pod "pod-configmaps-b50b952a-cb0a-4ce3-8cc8-cc68ef489009": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038595475s
    STEP: Saw pod success 11/15/23 16:56:04.353
    Nov 15 16:56:04.353: INFO: Pod "pod-configmaps-b50b952a-cb0a-4ce3-8cc8-cc68ef489009" satisfied condition "Succeeded or Failed"
    Nov 15 16:56:04.368: INFO: Trying to get logs from node 10.15.40.115 pod pod-configmaps-b50b952a-cb0a-4ce3-8cc8-cc68ef489009 container agnhost-container: <nil>
    STEP: delete the pod 11/15/23 16:56:04.402
    Nov 15 16:56:04.448: INFO: Waiting for pod pod-configmaps-b50b952a-cb0a-4ce3-8cc8-cc68ef489009 to disappear
    Nov 15 16:56:04.463: INFO: Pod pod-configmaps-b50b952a-cb0a-4ce3-8cc8-cc68ef489009 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:56:04.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8892" for this suite. 11/15/23 16:56:04.487
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:56:04.518
Nov 15 16:56:04.519: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename services 11/15/23 16:56:04.521
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:56:04.588
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:56:04.6
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-2651 11/15/23 16:56:04.619
STEP: creating service affinity-nodeport-transition in namespace services-2651 11/15/23 16:56:04.62
STEP: creating replication controller affinity-nodeport-transition in namespace services-2651 11/15/23 16:56:04.678
I1115 16:56:04.700752      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-2651, replica count: 3
I1115 16:56:07.753556      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 15 16:56:07.804: INFO: Creating new exec pod
Nov 15 16:56:07.838: INFO: Waiting up to 5m0s for pod "execpod-affinityqqstz" in namespace "services-2651" to be "running"
Nov 15 16:56:07.852: INFO: Pod "execpod-affinityqqstz": Phase="Pending", Reason="", readiness=false. Elapsed: 13.492939ms
Nov 15 16:56:09.867: INFO: Pod "execpod-affinityqqstz": Phase="Running", Reason="", readiness=true. Elapsed: 2.029224001s
Nov 15 16:56:09.867: INFO: Pod "execpod-affinityqqstz" satisfied condition "running"
Nov 15 16:56:10.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-2651 exec execpod-affinityqqstz -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Nov 15 16:56:11.366: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Nov 15 16:56:11.366: INFO: stdout: ""
Nov 15 16:56:11.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-2651 exec execpod-affinityqqstz -- /bin/sh -x -c nc -v -z -w 2 172.21.133.231 80'
Nov 15 16:56:11.823: INFO: stderr: "+ nc -v -z -w 2 172.21.133.231 80\nConnection to 172.21.133.231 80 port [tcp/http] succeeded!\n"
Nov 15 16:56:11.824: INFO: stdout: ""
Nov 15 16:56:11.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-2651 exec execpod-affinityqqstz -- /bin/sh -x -c nc -v -z -w 2 10.15.40.106 31481'
Nov 15 16:56:12.241: INFO: stderr: "+ nc -v -z -w 2 10.15.40.106 31481\nConnection to 10.15.40.106 31481 port [tcp/*] succeeded!\n"
Nov 15 16:56:12.241: INFO: stdout: ""
Nov 15 16:56:12.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-2651 exec execpod-affinityqqstz -- /bin/sh -x -c nc -v -z -w 2 10.15.40.115 31481'
Nov 15 16:56:12.647: INFO: stderr: "+ nc -v -z -w 2 10.15.40.115 31481\nConnection to 10.15.40.115 31481 port [tcp/*] succeeded!\n"
Nov 15 16:56:12.647: INFO: stdout: ""
Nov 15 16:56:12.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-2651 exec execpod-affinityqqstz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.15.40.106:31481/ ; done'
Nov 15 16:56:13.341: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n"
Nov 15 16:56:13.341: INFO: stdout: "\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-nngnc"
Nov 15 16:56:13.341: INFO: Received response from host: affinity-nodeport-transition-nngnc
Nov 15 16:56:13.341: INFO: Received response from host: affinity-nodeport-transition-nngnc
Nov 15 16:56:13.341: INFO: Received response from host: affinity-nodeport-transition-nngnc
Nov 15 16:56:13.341: INFO: Received response from host: affinity-nodeport-transition-nngnc
Nov 15 16:56:13.341: INFO: Received response from host: affinity-nodeport-transition-nngnc
Nov 15 16:56:13.341: INFO: Received response from host: affinity-nodeport-transition-nngnc
Nov 15 16:56:13.341: INFO: Received response from host: affinity-nodeport-transition-nngnc
Nov 15 16:56:13.341: INFO: Received response from host: affinity-nodeport-transition-nngnc
Nov 15 16:56:13.341: INFO: Received response from host: affinity-nodeport-transition-nngnc
Nov 15 16:56:13.341: INFO: Received response from host: affinity-nodeport-transition-nngnc
Nov 15 16:56:13.341: INFO: Received response from host: affinity-nodeport-transition-nngnc
Nov 15 16:56:13.341: INFO: Received response from host: affinity-nodeport-transition-nngnc
Nov 15 16:56:13.341: INFO: Received response from host: affinity-nodeport-transition-nngnc
Nov 15 16:56:13.341: INFO: Received response from host: affinity-nodeport-transition-nngnc
Nov 15 16:56:13.341: INFO: Received response from host: affinity-nodeport-transition-nngnc
Nov 15 16:56:13.341: INFO: Received response from host: affinity-nodeport-transition-nngnc
Nov 15 16:56:43.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-2651 exec execpod-affinityqqstz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.15.40.106:31481/ ; done'
Nov 15 16:56:44.020: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n"
Nov 15 16:56:44.020: INFO: stdout: "\naffinity-nodeport-transition-bwmcj\naffinity-nodeport-transition-bwmcj\naffinity-nodeport-transition-q99xd\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-bwmcj\naffinity-nodeport-transition-bwmcj\naffinity-nodeport-transition-q99xd\naffinity-nodeport-transition-bwmcj\naffinity-nodeport-transition-q99xd\naffinity-nodeport-transition-bwmcj\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-bwmcj\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-bwmcj\naffinity-nodeport-transition-q99xd"
Nov 15 16:56:44.020: INFO: Received response from host: affinity-nodeport-transition-bwmcj
Nov 15 16:56:44.020: INFO: Received response from host: affinity-nodeport-transition-bwmcj
Nov 15 16:56:44.020: INFO: Received response from host: affinity-nodeport-transition-q99xd
Nov 15 16:56:44.020: INFO: Received response from host: affinity-nodeport-transition-nngnc
Nov 15 16:56:44.020: INFO: Received response from host: affinity-nodeport-transition-bwmcj
Nov 15 16:56:44.020: INFO: Received response from host: affinity-nodeport-transition-bwmcj
Nov 15 16:56:44.020: INFO: Received response from host: affinity-nodeport-transition-q99xd
Nov 15 16:56:44.020: INFO: Received response from host: affinity-nodeport-transition-bwmcj
Nov 15 16:56:44.020: INFO: Received response from host: affinity-nodeport-transition-q99xd
Nov 15 16:56:44.020: INFO: Received response from host: affinity-nodeport-transition-bwmcj
Nov 15 16:56:44.020: INFO: Received response from host: affinity-nodeport-transition-nngnc
Nov 15 16:56:44.020: INFO: Received response from host: affinity-nodeport-transition-bwmcj
Nov 15 16:56:44.020: INFO: Received response from host: affinity-nodeport-transition-nngnc
Nov 15 16:56:44.020: INFO: Received response from host: affinity-nodeport-transition-nngnc
Nov 15 16:56:44.020: INFO: Received response from host: affinity-nodeport-transition-bwmcj
Nov 15 16:56:44.020: INFO: Received response from host: affinity-nodeport-transition-q99xd
Nov 15 16:56:44.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-2651 exec execpod-affinityqqstz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.15.40.106:31481/ ; done'
Nov 15 16:56:44.773: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n"
Nov 15 16:56:44.773: INFO: stdout: "\naffinity-nodeport-transition-q99xd\naffinity-nodeport-transition-q99xd\naffinity-nodeport-transition-q99xd\naffinity-nodeport-transition-q99xd\naffinity-nodeport-transition-q99xd\naffinity-nodeport-transition-q99xd\naffinity-nodeport-transition-q99xd\naffinity-nodeport-transition-q99xd\naffinity-nodeport-transition-q99xd\naffinity-nodeport-transition-q99xd\naffinity-nodeport-transition-q99xd\naffinity-nodeport-transition-q99xd\naffinity-nodeport-transition-q99xd\naffinity-nodeport-transition-q99xd\naffinity-nodeport-transition-q99xd\naffinity-nodeport-transition-q99xd"
Nov 15 16:56:44.773: INFO: Received response from host: affinity-nodeport-transition-q99xd
Nov 15 16:56:44.773: INFO: Received response from host: affinity-nodeport-transition-q99xd
Nov 15 16:56:44.773: INFO: Received response from host: affinity-nodeport-transition-q99xd
Nov 15 16:56:44.773: INFO: Received response from host: affinity-nodeport-transition-q99xd
Nov 15 16:56:44.774: INFO: Received response from host: affinity-nodeport-transition-q99xd
Nov 15 16:56:44.774: INFO: Received response from host: affinity-nodeport-transition-q99xd
Nov 15 16:56:44.774: INFO: Received response from host: affinity-nodeport-transition-q99xd
Nov 15 16:56:44.774: INFO: Received response from host: affinity-nodeport-transition-q99xd
Nov 15 16:56:44.774: INFO: Received response from host: affinity-nodeport-transition-q99xd
Nov 15 16:56:44.774: INFO: Received response from host: affinity-nodeport-transition-q99xd
Nov 15 16:56:44.774: INFO: Received response from host: affinity-nodeport-transition-q99xd
Nov 15 16:56:44.774: INFO: Received response from host: affinity-nodeport-transition-q99xd
Nov 15 16:56:44.774: INFO: Received response from host: affinity-nodeport-transition-q99xd
Nov 15 16:56:44.774: INFO: Received response from host: affinity-nodeport-transition-q99xd
Nov 15 16:56:44.774: INFO: Received response from host: affinity-nodeport-transition-q99xd
Nov 15 16:56:44.774: INFO: Received response from host: affinity-nodeport-transition-q99xd
Nov 15 16:56:44.774: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-2651, will wait for the garbage collector to delete the pods 11/15/23 16:56:44.81
Nov 15 16:56:44.907: INFO: Deleting ReplicationController affinity-nodeport-transition took: 28.15462ms
Nov 15 16:56:45.108: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 201.227883ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 15 16:56:48.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2651" for this suite. 11/15/23 16:56:48.037
------------------------------
• [SLOW TEST] [43.545 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:56:04.518
    Nov 15 16:56:04.519: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename services 11/15/23 16:56:04.521
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:56:04.588
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:56:04.6
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-2651 11/15/23 16:56:04.619
    STEP: creating service affinity-nodeport-transition in namespace services-2651 11/15/23 16:56:04.62
    STEP: creating replication controller affinity-nodeport-transition in namespace services-2651 11/15/23 16:56:04.678
    I1115 16:56:04.700752      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-2651, replica count: 3
    I1115 16:56:07.753556      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 15 16:56:07.804: INFO: Creating new exec pod
    Nov 15 16:56:07.838: INFO: Waiting up to 5m0s for pod "execpod-affinityqqstz" in namespace "services-2651" to be "running"
    Nov 15 16:56:07.852: INFO: Pod "execpod-affinityqqstz": Phase="Pending", Reason="", readiness=false. Elapsed: 13.492939ms
    Nov 15 16:56:09.867: INFO: Pod "execpod-affinityqqstz": Phase="Running", Reason="", readiness=true. Elapsed: 2.029224001s
    Nov 15 16:56:09.867: INFO: Pod "execpod-affinityqqstz" satisfied condition "running"
    Nov 15 16:56:10.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-2651 exec execpod-affinityqqstz -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Nov 15 16:56:11.366: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Nov 15 16:56:11.366: INFO: stdout: ""
    Nov 15 16:56:11.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-2651 exec execpod-affinityqqstz -- /bin/sh -x -c nc -v -z -w 2 172.21.133.231 80'
    Nov 15 16:56:11.823: INFO: stderr: "+ nc -v -z -w 2 172.21.133.231 80\nConnection to 172.21.133.231 80 port [tcp/http] succeeded!\n"
    Nov 15 16:56:11.824: INFO: stdout: ""
    Nov 15 16:56:11.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-2651 exec execpod-affinityqqstz -- /bin/sh -x -c nc -v -z -w 2 10.15.40.106 31481'
    Nov 15 16:56:12.241: INFO: stderr: "+ nc -v -z -w 2 10.15.40.106 31481\nConnection to 10.15.40.106 31481 port [tcp/*] succeeded!\n"
    Nov 15 16:56:12.241: INFO: stdout: ""
    Nov 15 16:56:12.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-2651 exec execpod-affinityqqstz -- /bin/sh -x -c nc -v -z -w 2 10.15.40.115 31481'
    Nov 15 16:56:12.647: INFO: stderr: "+ nc -v -z -w 2 10.15.40.115 31481\nConnection to 10.15.40.115 31481 port [tcp/*] succeeded!\n"
    Nov 15 16:56:12.647: INFO: stdout: ""
    Nov 15 16:56:12.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-2651 exec execpod-affinityqqstz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.15.40.106:31481/ ; done'
    Nov 15 16:56:13.341: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n"
    Nov 15 16:56:13.341: INFO: stdout: "\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-nngnc"
    Nov 15 16:56:13.341: INFO: Received response from host: affinity-nodeport-transition-nngnc
    Nov 15 16:56:13.341: INFO: Received response from host: affinity-nodeport-transition-nngnc
    Nov 15 16:56:13.341: INFO: Received response from host: affinity-nodeport-transition-nngnc
    Nov 15 16:56:13.341: INFO: Received response from host: affinity-nodeport-transition-nngnc
    Nov 15 16:56:13.341: INFO: Received response from host: affinity-nodeport-transition-nngnc
    Nov 15 16:56:13.341: INFO: Received response from host: affinity-nodeport-transition-nngnc
    Nov 15 16:56:13.341: INFO: Received response from host: affinity-nodeport-transition-nngnc
    Nov 15 16:56:13.341: INFO: Received response from host: affinity-nodeport-transition-nngnc
    Nov 15 16:56:13.341: INFO: Received response from host: affinity-nodeport-transition-nngnc
    Nov 15 16:56:13.341: INFO: Received response from host: affinity-nodeport-transition-nngnc
    Nov 15 16:56:13.341: INFO: Received response from host: affinity-nodeport-transition-nngnc
    Nov 15 16:56:13.341: INFO: Received response from host: affinity-nodeport-transition-nngnc
    Nov 15 16:56:13.341: INFO: Received response from host: affinity-nodeport-transition-nngnc
    Nov 15 16:56:13.341: INFO: Received response from host: affinity-nodeport-transition-nngnc
    Nov 15 16:56:13.341: INFO: Received response from host: affinity-nodeport-transition-nngnc
    Nov 15 16:56:13.341: INFO: Received response from host: affinity-nodeport-transition-nngnc
    Nov 15 16:56:43.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-2651 exec execpod-affinityqqstz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.15.40.106:31481/ ; done'
    Nov 15 16:56:44.020: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n"
    Nov 15 16:56:44.020: INFO: stdout: "\naffinity-nodeport-transition-bwmcj\naffinity-nodeport-transition-bwmcj\naffinity-nodeport-transition-q99xd\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-bwmcj\naffinity-nodeport-transition-bwmcj\naffinity-nodeport-transition-q99xd\naffinity-nodeport-transition-bwmcj\naffinity-nodeport-transition-q99xd\naffinity-nodeport-transition-bwmcj\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-bwmcj\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-nngnc\naffinity-nodeport-transition-bwmcj\naffinity-nodeport-transition-q99xd"
    Nov 15 16:56:44.020: INFO: Received response from host: affinity-nodeport-transition-bwmcj
    Nov 15 16:56:44.020: INFO: Received response from host: affinity-nodeport-transition-bwmcj
    Nov 15 16:56:44.020: INFO: Received response from host: affinity-nodeport-transition-q99xd
    Nov 15 16:56:44.020: INFO: Received response from host: affinity-nodeport-transition-nngnc
    Nov 15 16:56:44.020: INFO: Received response from host: affinity-nodeport-transition-bwmcj
    Nov 15 16:56:44.020: INFO: Received response from host: affinity-nodeport-transition-bwmcj
    Nov 15 16:56:44.020: INFO: Received response from host: affinity-nodeport-transition-q99xd
    Nov 15 16:56:44.020: INFO: Received response from host: affinity-nodeport-transition-bwmcj
    Nov 15 16:56:44.020: INFO: Received response from host: affinity-nodeport-transition-q99xd
    Nov 15 16:56:44.020: INFO: Received response from host: affinity-nodeport-transition-bwmcj
    Nov 15 16:56:44.020: INFO: Received response from host: affinity-nodeport-transition-nngnc
    Nov 15 16:56:44.020: INFO: Received response from host: affinity-nodeport-transition-bwmcj
    Nov 15 16:56:44.020: INFO: Received response from host: affinity-nodeport-transition-nngnc
    Nov 15 16:56:44.020: INFO: Received response from host: affinity-nodeport-transition-nngnc
    Nov 15 16:56:44.020: INFO: Received response from host: affinity-nodeport-transition-bwmcj
    Nov 15 16:56:44.020: INFO: Received response from host: affinity-nodeport-transition-q99xd
    Nov 15 16:56:44.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-2651 exec execpod-affinityqqstz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.15.40.106:31481/ ; done'
    Nov 15 16:56:44.773: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.40.106:31481/\n"
    Nov 15 16:56:44.773: INFO: stdout: "\naffinity-nodeport-transition-q99xd\naffinity-nodeport-transition-q99xd\naffinity-nodeport-transition-q99xd\naffinity-nodeport-transition-q99xd\naffinity-nodeport-transition-q99xd\naffinity-nodeport-transition-q99xd\naffinity-nodeport-transition-q99xd\naffinity-nodeport-transition-q99xd\naffinity-nodeport-transition-q99xd\naffinity-nodeport-transition-q99xd\naffinity-nodeport-transition-q99xd\naffinity-nodeport-transition-q99xd\naffinity-nodeport-transition-q99xd\naffinity-nodeport-transition-q99xd\naffinity-nodeport-transition-q99xd\naffinity-nodeport-transition-q99xd"
    Nov 15 16:56:44.773: INFO: Received response from host: affinity-nodeport-transition-q99xd
    Nov 15 16:56:44.773: INFO: Received response from host: affinity-nodeport-transition-q99xd
    Nov 15 16:56:44.773: INFO: Received response from host: affinity-nodeport-transition-q99xd
    Nov 15 16:56:44.773: INFO: Received response from host: affinity-nodeport-transition-q99xd
    Nov 15 16:56:44.774: INFO: Received response from host: affinity-nodeport-transition-q99xd
    Nov 15 16:56:44.774: INFO: Received response from host: affinity-nodeport-transition-q99xd
    Nov 15 16:56:44.774: INFO: Received response from host: affinity-nodeport-transition-q99xd
    Nov 15 16:56:44.774: INFO: Received response from host: affinity-nodeport-transition-q99xd
    Nov 15 16:56:44.774: INFO: Received response from host: affinity-nodeport-transition-q99xd
    Nov 15 16:56:44.774: INFO: Received response from host: affinity-nodeport-transition-q99xd
    Nov 15 16:56:44.774: INFO: Received response from host: affinity-nodeport-transition-q99xd
    Nov 15 16:56:44.774: INFO: Received response from host: affinity-nodeport-transition-q99xd
    Nov 15 16:56:44.774: INFO: Received response from host: affinity-nodeport-transition-q99xd
    Nov 15 16:56:44.774: INFO: Received response from host: affinity-nodeport-transition-q99xd
    Nov 15 16:56:44.774: INFO: Received response from host: affinity-nodeport-transition-q99xd
    Nov 15 16:56:44.774: INFO: Received response from host: affinity-nodeport-transition-q99xd
    Nov 15 16:56:44.774: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-2651, will wait for the garbage collector to delete the pods 11/15/23 16:56:44.81
    Nov 15 16:56:44.907: INFO: Deleting ReplicationController affinity-nodeport-transition took: 28.15462ms
    Nov 15 16:56:45.108: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 201.227883ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:56:48.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2651" for this suite. 11/15/23 16:56:48.037
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:56:48.066
Nov 15 16:56:48.066: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename hostport 11/15/23 16:56:48.07
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:56:48.122
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:56:48.137
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 11/15/23 16:56:48.177
Nov 15 16:56:48.211: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-8181" to be "running and ready"
Nov 15 16:56:48.224: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.041532ms
Nov 15 16:56:48.224: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 16:56:50.241: INFO: Pod "pod1": Phase="Running", Reason="", readiness=false. Elapsed: 2.029819249s
Nov 15 16:56:50.241: INFO: The phase of Pod pod1 is Running (Ready = false)
Nov 15 16:56:52.242: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.030602249s
Nov 15 16:56:52.242: INFO: The phase of Pod pod1 is Running (Ready = true)
Nov 15 16:56:52.242: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.15.40.114 on the node which pod1 resides and expect scheduled 11/15/23 16:56:52.242
Nov 15 16:56:52.264: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-8181" to be "running and ready"
Nov 15 16:56:52.282: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 17.419473ms
Nov 15 16:56:52.282: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 16:56:54.307: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.042786914s
Nov 15 16:56:54.307: INFO: The phase of Pod pod2 is Running (Ready = false)
Nov 15 16:56:56.300: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.035817774s
Nov 15 16:56:56.301: INFO: The phase of Pod pod2 is Running (Ready = true)
Nov 15 16:56:56.301: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.15.40.114 but use UDP protocol on the node which pod2 resides 11/15/23 16:56:56.301
Nov 15 16:56:56.321: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-8181" to be "running and ready"
Nov 15 16:56:56.345: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 24.229611ms
Nov 15 16:56:56.345: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 16:56:58.360: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.03941433s
Nov 15 16:56:58.360: INFO: The phase of Pod pod3 is Running (Ready = true)
Nov 15 16:56:58.360: INFO: Pod "pod3" satisfied condition "running and ready"
Nov 15 16:56:58.383: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-8181" to be "running and ready"
Nov 15 16:56:58.400: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 15.573212ms
Nov 15 16:56:58.400: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Nov 15 16:57:00.416: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.031442136s
Nov 15 16:57:00.416: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Nov 15 16:57:00.416: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 11/15/23 16:57:00.431
Nov 15 16:57:00.431: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.15.40.114 http://127.0.0.1:54323/hostname] Namespace:hostport-8181 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 16:57:00.431: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
Nov 15 16:57:00.433: INFO: ExecWithOptions: Clientset creation
Nov 15 16:57:00.433: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-8181/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.15.40.114+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.15.40.114, port: 54323 11/15/23 16:57:00.755
Nov 15 16:57:00.755: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.15.40.114:54323/hostname] Namespace:hostport-8181 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 16:57:00.755: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
Nov 15 16:57:00.756: INFO: ExecWithOptions: Clientset creation
Nov 15 16:57:00.757: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-8181/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.15.40.114%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.15.40.114, port: 54323 UDP 11/15/23 16:57:01.033
Nov 15 16:57:01.033: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.15.40.114 54323] Namespace:hostport-8181 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 16:57:01.033: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
Nov 15 16:57:01.034: INFO: ExecWithOptions: Clientset creation
Nov 15 16:57:01.034: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-8181/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.15.40.114+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Nov 15 16:57:06.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-8181" for this suite. 11/15/23 16:57:06.39
------------------------------
• [SLOW TEST] [18.348 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:56:48.066
    Nov 15 16:56:48.066: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename hostport 11/15/23 16:56:48.07
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:56:48.122
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:56:48.137
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 11/15/23 16:56:48.177
    Nov 15 16:56:48.211: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-8181" to be "running and ready"
    Nov 15 16:56:48.224: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.041532ms
    Nov 15 16:56:48.224: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 16:56:50.241: INFO: Pod "pod1": Phase="Running", Reason="", readiness=false. Elapsed: 2.029819249s
    Nov 15 16:56:50.241: INFO: The phase of Pod pod1 is Running (Ready = false)
    Nov 15 16:56:52.242: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.030602249s
    Nov 15 16:56:52.242: INFO: The phase of Pod pod1 is Running (Ready = true)
    Nov 15 16:56:52.242: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.15.40.114 on the node which pod1 resides and expect scheduled 11/15/23 16:56:52.242
    Nov 15 16:56:52.264: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-8181" to be "running and ready"
    Nov 15 16:56:52.282: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 17.419473ms
    Nov 15 16:56:52.282: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 16:56:54.307: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.042786914s
    Nov 15 16:56:54.307: INFO: The phase of Pod pod2 is Running (Ready = false)
    Nov 15 16:56:56.300: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.035817774s
    Nov 15 16:56:56.301: INFO: The phase of Pod pod2 is Running (Ready = true)
    Nov 15 16:56:56.301: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.15.40.114 but use UDP protocol on the node which pod2 resides 11/15/23 16:56:56.301
    Nov 15 16:56:56.321: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-8181" to be "running and ready"
    Nov 15 16:56:56.345: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 24.229611ms
    Nov 15 16:56:56.345: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 16:56:58.360: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.03941433s
    Nov 15 16:56:58.360: INFO: The phase of Pod pod3 is Running (Ready = true)
    Nov 15 16:56:58.360: INFO: Pod "pod3" satisfied condition "running and ready"
    Nov 15 16:56:58.383: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-8181" to be "running and ready"
    Nov 15 16:56:58.400: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 15.573212ms
    Nov 15 16:56:58.400: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 16:57:00.416: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.031442136s
    Nov 15 16:57:00.416: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Nov 15 16:57:00.416: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 11/15/23 16:57:00.431
    Nov 15 16:57:00.431: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.15.40.114 http://127.0.0.1:54323/hostname] Namespace:hostport-8181 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 16:57:00.431: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    Nov 15 16:57:00.433: INFO: ExecWithOptions: Clientset creation
    Nov 15 16:57:00.433: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-8181/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.15.40.114+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.15.40.114, port: 54323 11/15/23 16:57:00.755
    Nov 15 16:57:00.755: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.15.40.114:54323/hostname] Namespace:hostport-8181 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 16:57:00.755: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    Nov 15 16:57:00.756: INFO: ExecWithOptions: Clientset creation
    Nov 15 16:57:00.757: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-8181/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.15.40.114%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.15.40.114, port: 54323 UDP 11/15/23 16:57:01.033
    Nov 15 16:57:01.033: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.15.40.114 54323] Namespace:hostport-8181 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 16:57:01.033: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    Nov 15 16:57:01.034: INFO: ExecWithOptions: Clientset creation
    Nov 15 16:57:01.034: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-8181/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.15.40.114+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:57:06.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-8181" for this suite. 11/15/23 16:57:06.39
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:57:06.418
Nov 15 16:57:06.418: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename resourcequota 11/15/23 16:57:06.421
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:57:06.475
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:57:06.49
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 11/15/23 16:57:06.505
STEP: Ensuring ResourceQuota status is calculated 11/15/23 16:57:06.524
STEP: Creating a ResourceQuota with not best effort scope 11/15/23 16:57:08.545
STEP: Ensuring ResourceQuota status is calculated 11/15/23 16:57:08.57
STEP: Creating a best-effort pod 11/15/23 16:57:10.587
STEP: Ensuring resource quota with best effort scope captures the pod usage 11/15/23 16:57:10.634
STEP: Ensuring resource quota with not best effort ignored the pod usage 11/15/23 16:57:12.65
STEP: Deleting the pod 11/15/23 16:57:14.668
STEP: Ensuring resource quota status released the pod usage 11/15/23 16:57:14.7
STEP: Creating a not best-effort pod 11/15/23 16:57:16.721
STEP: Ensuring resource quota with not best effort scope captures the pod usage 11/15/23 16:57:16.757
STEP: Ensuring resource quota with best effort scope ignored the pod usage 11/15/23 16:57:18.776
STEP: Deleting the pod 11/15/23 16:57:20.792
STEP: Ensuring resource quota status released the pod usage 11/15/23 16:57:20.835
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Nov 15 16:57:22.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6772" for this suite. 11/15/23 16:57:22.877
------------------------------
• [SLOW TEST] [16.486 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:57:06.418
    Nov 15 16:57:06.418: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename resourcequota 11/15/23 16:57:06.421
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:57:06.475
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:57:06.49
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 11/15/23 16:57:06.505
    STEP: Ensuring ResourceQuota status is calculated 11/15/23 16:57:06.524
    STEP: Creating a ResourceQuota with not best effort scope 11/15/23 16:57:08.545
    STEP: Ensuring ResourceQuota status is calculated 11/15/23 16:57:08.57
    STEP: Creating a best-effort pod 11/15/23 16:57:10.587
    STEP: Ensuring resource quota with best effort scope captures the pod usage 11/15/23 16:57:10.634
    STEP: Ensuring resource quota with not best effort ignored the pod usage 11/15/23 16:57:12.65
    STEP: Deleting the pod 11/15/23 16:57:14.668
    STEP: Ensuring resource quota status released the pod usage 11/15/23 16:57:14.7
    STEP: Creating a not best-effort pod 11/15/23 16:57:16.721
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 11/15/23 16:57:16.757
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 11/15/23 16:57:18.776
    STEP: Deleting the pod 11/15/23 16:57:20.792
    STEP: Ensuring resource quota status released the pod usage 11/15/23 16:57:20.835
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:57:22.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6772" for this suite. 11/15/23 16:57:22.877
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:57:22.914
Nov 15 16:57:22.915: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename services 11/15/23 16:57:22.917
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:57:22.969
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:57:22.986
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7631 11/15/23 16:57:23.001
STEP: changing the ExternalName service to type=NodePort 11/15/23 16:57:23.021
STEP: creating replication controller externalname-service in namespace services-7631 11/15/23 16:57:23.101
I1115 16:57:23.133894      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7631, replica count: 2
I1115 16:57:26.185402      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 15 16:57:26.185: INFO: Creating new exec pod
Nov 15 16:57:26.220: INFO: Waiting up to 5m0s for pod "execpodqjk2k" in namespace "services-7631" to be "running"
Nov 15 16:57:26.236: INFO: Pod "execpodqjk2k": Phase="Pending", Reason="", readiness=false. Elapsed: 15.337927ms
Nov 15 16:57:28.257: INFO: Pod "execpodqjk2k": Phase="Running", Reason="", readiness=true. Elapsed: 2.036711229s
Nov 15 16:57:28.257: INFO: Pod "execpodqjk2k" satisfied condition "running"
Nov 15 16:57:29.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-7631 exec execpodqjk2k -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Nov 15 16:57:29.733: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Nov 15 16:57:29.733: INFO: stdout: ""
Nov 15 16:57:29.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-7631 exec execpodqjk2k -- /bin/sh -x -c nc -v -z -w 2 172.21.166.249 80'
Nov 15 16:57:30.223: INFO: stderr: "+ nc -v -z -w 2 172.21.166.249 80\nConnection to 172.21.166.249 80 port [tcp/http] succeeded!\n"
Nov 15 16:57:30.223: INFO: stdout: ""
Nov 15 16:57:30.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-7631 exec execpodqjk2k -- /bin/sh -x -c nc -v -z -w 2 10.15.40.115 30490'
Nov 15 16:57:30.636: INFO: stderr: "+ nc -v -z -w 2 10.15.40.115 30490\nConnection to 10.15.40.115 30490 port [tcp/*] succeeded!\n"
Nov 15 16:57:30.636: INFO: stdout: ""
Nov 15 16:57:30.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-7631 exec execpodqjk2k -- /bin/sh -x -c nc -v -z -w 2 10.15.40.114 30490'
Nov 15 16:57:31.059: INFO: stderr: "+ nc -v -z -w 2 10.15.40.114 30490\nConnection to 10.15.40.114 30490 port [tcp/*] succeeded!\n"
Nov 15 16:57:31.059: INFO: stdout: ""
Nov 15 16:57:31.059: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 15 16:57:31.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7631" for this suite. 11/15/23 16:57:31.161
------------------------------
• [SLOW TEST] [8.272 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:57:22.914
    Nov 15 16:57:22.915: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename services 11/15/23 16:57:22.917
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:57:22.969
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:57:22.986
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-7631 11/15/23 16:57:23.001
    STEP: changing the ExternalName service to type=NodePort 11/15/23 16:57:23.021
    STEP: creating replication controller externalname-service in namespace services-7631 11/15/23 16:57:23.101
    I1115 16:57:23.133894      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7631, replica count: 2
    I1115 16:57:26.185402      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 15 16:57:26.185: INFO: Creating new exec pod
    Nov 15 16:57:26.220: INFO: Waiting up to 5m0s for pod "execpodqjk2k" in namespace "services-7631" to be "running"
    Nov 15 16:57:26.236: INFO: Pod "execpodqjk2k": Phase="Pending", Reason="", readiness=false. Elapsed: 15.337927ms
    Nov 15 16:57:28.257: INFO: Pod "execpodqjk2k": Phase="Running", Reason="", readiness=true. Elapsed: 2.036711229s
    Nov 15 16:57:28.257: INFO: Pod "execpodqjk2k" satisfied condition "running"
    Nov 15 16:57:29.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-7631 exec execpodqjk2k -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Nov 15 16:57:29.733: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Nov 15 16:57:29.733: INFO: stdout: ""
    Nov 15 16:57:29.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-7631 exec execpodqjk2k -- /bin/sh -x -c nc -v -z -w 2 172.21.166.249 80'
    Nov 15 16:57:30.223: INFO: stderr: "+ nc -v -z -w 2 172.21.166.249 80\nConnection to 172.21.166.249 80 port [tcp/http] succeeded!\n"
    Nov 15 16:57:30.223: INFO: stdout: ""
    Nov 15 16:57:30.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-7631 exec execpodqjk2k -- /bin/sh -x -c nc -v -z -w 2 10.15.40.115 30490'
    Nov 15 16:57:30.636: INFO: stderr: "+ nc -v -z -w 2 10.15.40.115 30490\nConnection to 10.15.40.115 30490 port [tcp/*] succeeded!\n"
    Nov 15 16:57:30.636: INFO: stdout: ""
    Nov 15 16:57:30.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-7631 exec execpodqjk2k -- /bin/sh -x -c nc -v -z -w 2 10.15.40.114 30490'
    Nov 15 16:57:31.059: INFO: stderr: "+ nc -v -z -w 2 10.15.40.114 30490\nConnection to 10.15.40.114 30490 port [tcp/*] succeeded!\n"
    Nov 15 16:57:31.059: INFO: stdout: ""
    Nov 15 16:57:31.059: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:57:31.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7631" for this suite. 11/15/23 16:57:31.161
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:57:31.189
Nov 15 16:57:31.189: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename container-lifecycle-hook 11/15/23 16:57:31.191
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:57:31.24
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:57:31.253
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 11/15/23 16:57:31.292
Nov 15 16:57:31.324: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7680" to be "running and ready"
Nov 15 16:57:31.338: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 13.721037ms
Nov 15 16:57:31.338: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 15 16:57:33.356: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.031974496s
Nov 15 16:57:33.356: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Nov 15 16:57:33.356: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 11/15/23 16:57:33.373
Nov 15 16:57:33.395: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-7680" to be "running and ready"
Nov 15 16:57:33.410: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 14.382293ms
Nov 15 16:57:33.410: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 15 16:57:35.424: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.029245908s
Nov 15 16:57:35.425: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Nov 15 16:57:35.425: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 11/15/23 16:57:35.448
Nov 15 16:57:35.469: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 15 16:57:35.485: INFO: Pod pod-with-prestop-http-hook still exists
Nov 15 16:57:37.486: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 15 16:57:37.503: INFO: Pod pod-with-prestop-http-hook still exists
Nov 15 16:57:39.487: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 15 16:57:39.506: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 11/15/23 16:57:39.506
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Nov 15 16:57:39.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-7680" for this suite. 11/15/23 16:57:39.684
------------------------------
• [SLOW TEST] [8.529 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:57:31.189
    Nov 15 16:57:31.189: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename container-lifecycle-hook 11/15/23 16:57:31.191
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:57:31.24
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:57:31.253
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 11/15/23 16:57:31.292
    Nov 15 16:57:31.324: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7680" to be "running and ready"
    Nov 15 16:57:31.338: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 13.721037ms
    Nov 15 16:57:31.338: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 16:57:33.356: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.031974496s
    Nov 15 16:57:33.356: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Nov 15 16:57:33.356: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 11/15/23 16:57:33.373
    Nov 15 16:57:33.395: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-7680" to be "running and ready"
    Nov 15 16:57:33.410: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 14.382293ms
    Nov 15 16:57:33.410: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 16:57:35.424: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.029245908s
    Nov 15 16:57:35.425: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Nov 15 16:57:35.425: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 11/15/23 16:57:35.448
    Nov 15 16:57:35.469: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Nov 15 16:57:35.485: INFO: Pod pod-with-prestop-http-hook still exists
    Nov 15 16:57:37.486: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Nov 15 16:57:37.503: INFO: Pod pod-with-prestop-http-hook still exists
    Nov 15 16:57:39.487: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Nov 15 16:57:39.506: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 11/15/23 16:57:39.506
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:57:39.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-7680" for this suite. 11/15/23 16:57:39.684
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:57:39.719
Nov 15 16:57:39.719: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename configmap 11/15/23 16:57:39.722
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:57:39.77
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:57:39.784
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-314bb617-29aa-4cff-8558-59f226e25478 11/15/23 16:57:39.819
STEP: Creating the pod 11/15/23 16:57:39.837
Nov 15 16:57:39.872: INFO: Waiting up to 5m0s for pod "pod-configmaps-7099c501-03ae-477c-bf68-32e116fc9e39" in namespace "configmap-8636" to be "running and ready"
Nov 15 16:57:39.889: INFO: Pod "pod-configmaps-7099c501-03ae-477c-bf68-32e116fc9e39": Phase="Pending", Reason="", readiness=false. Elapsed: 16.423061ms
Nov 15 16:57:39.889: INFO: The phase of Pod pod-configmaps-7099c501-03ae-477c-bf68-32e116fc9e39 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 16:57:41.906: INFO: Pod "pod-configmaps-7099c501-03ae-477c-bf68-32e116fc9e39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03347596s
Nov 15 16:57:41.906: INFO: The phase of Pod pod-configmaps-7099c501-03ae-477c-bf68-32e116fc9e39 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 16:57:43.905: INFO: Pod "pod-configmaps-7099c501-03ae-477c-bf68-32e116fc9e39": Phase="Running", Reason="", readiness=true. Elapsed: 4.032746335s
Nov 15 16:57:43.905: INFO: The phase of Pod pod-configmaps-7099c501-03ae-477c-bf68-32e116fc9e39 is Running (Ready = true)
Nov 15 16:57:43.905: INFO: Pod "pod-configmaps-7099c501-03ae-477c-bf68-32e116fc9e39" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-314bb617-29aa-4cff-8558-59f226e25478 11/15/23 16:57:44.067
STEP: waiting to observe update in volume 11/15/23 16:57:44.089
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 15 16:58:49.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8636" for this suite. 11/15/23 16:58:49.714
------------------------------
• [SLOW TEST] [70.019 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:57:39.719
    Nov 15 16:57:39.719: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename configmap 11/15/23 16:57:39.722
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:57:39.77
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:57:39.784
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-314bb617-29aa-4cff-8558-59f226e25478 11/15/23 16:57:39.819
    STEP: Creating the pod 11/15/23 16:57:39.837
    Nov 15 16:57:39.872: INFO: Waiting up to 5m0s for pod "pod-configmaps-7099c501-03ae-477c-bf68-32e116fc9e39" in namespace "configmap-8636" to be "running and ready"
    Nov 15 16:57:39.889: INFO: Pod "pod-configmaps-7099c501-03ae-477c-bf68-32e116fc9e39": Phase="Pending", Reason="", readiness=false. Elapsed: 16.423061ms
    Nov 15 16:57:39.889: INFO: The phase of Pod pod-configmaps-7099c501-03ae-477c-bf68-32e116fc9e39 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 16:57:41.906: INFO: Pod "pod-configmaps-7099c501-03ae-477c-bf68-32e116fc9e39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03347596s
    Nov 15 16:57:41.906: INFO: The phase of Pod pod-configmaps-7099c501-03ae-477c-bf68-32e116fc9e39 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 16:57:43.905: INFO: Pod "pod-configmaps-7099c501-03ae-477c-bf68-32e116fc9e39": Phase="Running", Reason="", readiness=true. Elapsed: 4.032746335s
    Nov 15 16:57:43.905: INFO: The phase of Pod pod-configmaps-7099c501-03ae-477c-bf68-32e116fc9e39 is Running (Ready = true)
    Nov 15 16:57:43.905: INFO: Pod "pod-configmaps-7099c501-03ae-477c-bf68-32e116fc9e39" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-314bb617-29aa-4cff-8558-59f226e25478 11/15/23 16:57:44.067
    STEP: waiting to observe update in volume 11/15/23 16:57:44.089
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:58:49.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8636" for this suite. 11/15/23 16:58:49.714
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:58:49.738
Nov 15 16:58:49.738: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename projected 11/15/23 16:58:49.74
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:58:49.791
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:58:49.805
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 11/15/23 16:58:49.821
Nov 15 16:58:49.855: INFO: Waiting up to 5m0s for pod "downwardapi-volume-761a6d58-e7d0-488a-ad82-f01c38df2f4b" in namespace "projected-7575" to be "Succeeded or Failed"
Nov 15 16:58:49.870: INFO: Pod "downwardapi-volume-761a6d58-e7d0-488a-ad82-f01c38df2f4b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.680412ms
Nov 15 16:58:51.889: INFO: Pod "downwardapi-volume-761a6d58-e7d0-488a-ad82-f01c38df2f4b": Phase="Running", Reason="", readiness=true. Elapsed: 2.034257788s
Nov 15 16:58:53.885: INFO: Pod "downwardapi-volume-761a6d58-e7d0-488a-ad82-f01c38df2f4b": Phase="Running", Reason="", readiness=false. Elapsed: 4.029562832s
Nov 15 16:58:55.885: INFO: Pod "downwardapi-volume-761a6d58-e7d0-488a-ad82-f01c38df2f4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02983026s
STEP: Saw pod success 11/15/23 16:58:55.885
Nov 15 16:58:55.885: INFO: Pod "downwardapi-volume-761a6d58-e7d0-488a-ad82-f01c38df2f4b" satisfied condition "Succeeded or Failed"
Nov 15 16:58:55.903: INFO: Trying to get logs from node 10.15.40.115 pod downwardapi-volume-761a6d58-e7d0-488a-ad82-f01c38df2f4b container client-container: <nil>
STEP: delete the pod 11/15/23 16:58:55.95
Nov 15 16:58:55.999: INFO: Waiting for pod downwardapi-volume-761a6d58-e7d0-488a-ad82-f01c38df2f4b to disappear
Nov 15 16:58:56.014: INFO: Pod downwardapi-volume-761a6d58-e7d0-488a-ad82-f01c38df2f4b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Nov 15 16:58:56.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7575" for this suite. 11/15/23 16:58:56.037
------------------------------
• [SLOW TEST] [6.325 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:58:49.738
    Nov 15 16:58:49.738: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename projected 11/15/23 16:58:49.74
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:58:49.791
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:58:49.805
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 11/15/23 16:58:49.821
    Nov 15 16:58:49.855: INFO: Waiting up to 5m0s for pod "downwardapi-volume-761a6d58-e7d0-488a-ad82-f01c38df2f4b" in namespace "projected-7575" to be "Succeeded or Failed"
    Nov 15 16:58:49.870: INFO: Pod "downwardapi-volume-761a6d58-e7d0-488a-ad82-f01c38df2f4b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.680412ms
    Nov 15 16:58:51.889: INFO: Pod "downwardapi-volume-761a6d58-e7d0-488a-ad82-f01c38df2f4b": Phase="Running", Reason="", readiness=true. Elapsed: 2.034257788s
    Nov 15 16:58:53.885: INFO: Pod "downwardapi-volume-761a6d58-e7d0-488a-ad82-f01c38df2f4b": Phase="Running", Reason="", readiness=false. Elapsed: 4.029562832s
    Nov 15 16:58:55.885: INFO: Pod "downwardapi-volume-761a6d58-e7d0-488a-ad82-f01c38df2f4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02983026s
    STEP: Saw pod success 11/15/23 16:58:55.885
    Nov 15 16:58:55.885: INFO: Pod "downwardapi-volume-761a6d58-e7d0-488a-ad82-f01c38df2f4b" satisfied condition "Succeeded or Failed"
    Nov 15 16:58:55.903: INFO: Trying to get logs from node 10.15.40.115 pod downwardapi-volume-761a6d58-e7d0-488a-ad82-f01c38df2f4b container client-container: <nil>
    STEP: delete the pod 11/15/23 16:58:55.95
    Nov 15 16:58:55.999: INFO: Waiting for pod downwardapi-volume-761a6d58-e7d0-488a-ad82-f01c38df2f4b to disappear
    Nov 15 16:58:56.014: INFO: Pod downwardapi-volume-761a6d58-e7d0-488a-ad82-f01c38df2f4b no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:58:56.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7575" for this suite. 11/15/23 16:58:56.037
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:58:56.075
Nov 15 16:58:56.076: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename services 11/15/23 16:58:56.077
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:58:56.131
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:58:56.149
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-3039 11/15/23 16:58:56.166
STEP: creating replication controller nodeport-test in namespace services-3039 11/15/23 16:58:56.229
I1115 16:58:56.252989      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-3039, replica count: 2
I1115 16:58:59.305775      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 15 16:58:59.306: INFO: Creating new exec pod
Nov 15 16:58:59.339: INFO: Waiting up to 5m0s for pod "execpodbkcrf" in namespace "services-3039" to be "running"
Nov 15 16:58:59.369: INFO: Pod "execpodbkcrf": Phase="Pending", Reason="", readiness=false. Elapsed: 30.68547ms
Nov 15 16:59:01.391: INFO: Pod "execpodbkcrf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051797851s
Nov 15 16:59:03.396: INFO: Pod "execpodbkcrf": Phase="Running", Reason="", readiness=true. Elapsed: 4.057439013s
Nov 15 16:59:03.396: INFO: Pod "execpodbkcrf" satisfied condition "running"
Nov 15 16:59:04.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-3039 exec execpodbkcrf -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Nov 15 16:59:04.885: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Nov 15 16:59:04.886: INFO: stdout: ""
Nov 15 16:59:04.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-3039 exec execpodbkcrf -- /bin/sh -x -c nc -v -z -w 2 172.21.220.251 80'
Nov 15 16:59:05.346: INFO: stderr: "+ nc -v -z -w 2 172.21.220.251 80\nConnection to 172.21.220.251 80 port [tcp/http] succeeded!\n"
Nov 15 16:59:05.346: INFO: stdout: ""
Nov 15 16:59:05.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-3039 exec execpodbkcrf -- /bin/sh -x -c nc -v -z -w 2 10.15.40.115 30125'
Nov 15 16:59:05.791: INFO: stderr: "+ nc -v -z -w 2 10.15.40.115 30125\nConnection to 10.15.40.115 30125 port [tcp/*] succeeded!\n"
Nov 15 16:59:05.791: INFO: stdout: ""
Nov 15 16:59:05.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-3039 exec execpodbkcrf -- /bin/sh -x -c nc -v -z -w 2 10.15.40.114 30125'
Nov 15 16:59:06.190: INFO: stderr: "+ nc -v -z -w 2 10.15.40.114 30125\nConnection to 10.15.40.114 30125 port [tcp/*] succeeded!\n"
Nov 15 16:59:06.190: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 15 16:59:06.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3039" for this suite. 11/15/23 16:59:06.216
------------------------------
• [SLOW TEST] [10.165 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:58:56.075
    Nov 15 16:58:56.076: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename services 11/15/23 16:58:56.077
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:58:56.131
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:58:56.149
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-3039 11/15/23 16:58:56.166
    STEP: creating replication controller nodeport-test in namespace services-3039 11/15/23 16:58:56.229
    I1115 16:58:56.252989      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-3039, replica count: 2
    I1115 16:58:59.305775      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 15 16:58:59.306: INFO: Creating new exec pod
    Nov 15 16:58:59.339: INFO: Waiting up to 5m0s for pod "execpodbkcrf" in namespace "services-3039" to be "running"
    Nov 15 16:58:59.369: INFO: Pod "execpodbkcrf": Phase="Pending", Reason="", readiness=false. Elapsed: 30.68547ms
    Nov 15 16:59:01.391: INFO: Pod "execpodbkcrf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051797851s
    Nov 15 16:59:03.396: INFO: Pod "execpodbkcrf": Phase="Running", Reason="", readiness=true. Elapsed: 4.057439013s
    Nov 15 16:59:03.396: INFO: Pod "execpodbkcrf" satisfied condition "running"
    Nov 15 16:59:04.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-3039 exec execpodbkcrf -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Nov 15 16:59:04.885: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Nov 15 16:59:04.886: INFO: stdout: ""
    Nov 15 16:59:04.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-3039 exec execpodbkcrf -- /bin/sh -x -c nc -v -z -w 2 172.21.220.251 80'
    Nov 15 16:59:05.346: INFO: stderr: "+ nc -v -z -w 2 172.21.220.251 80\nConnection to 172.21.220.251 80 port [tcp/http] succeeded!\n"
    Nov 15 16:59:05.346: INFO: stdout: ""
    Nov 15 16:59:05.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-3039 exec execpodbkcrf -- /bin/sh -x -c nc -v -z -w 2 10.15.40.115 30125'
    Nov 15 16:59:05.791: INFO: stderr: "+ nc -v -z -w 2 10.15.40.115 30125\nConnection to 10.15.40.115 30125 port [tcp/*] succeeded!\n"
    Nov 15 16:59:05.791: INFO: stdout: ""
    Nov 15 16:59:05.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-3039 exec execpodbkcrf -- /bin/sh -x -c nc -v -z -w 2 10.15.40.114 30125'
    Nov 15 16:59:06.190: INFO: stderr: "+ nc -v -z -w 2 10.15.40.114 30125\nConnection to 10.15.40.114 30125 port [tcp/*] succeeded!\n"
    Nov 15 16:59:06.190: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:59:06.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3039" for this suite. 11/15/23 16:59:06.216
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:59:06.241
Nov 15 16:59:06.242: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename projected 11/15/23 16:59:06.246
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:59:06.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:59:06.313
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-3d55d848-8207-47bc-98bb-ace03ea926c9 11/15/23 16:59:06.329
STEP: Creating a pod to test consume configMaps 11/15/23 16:59:06.35
Nov 15 16:59:06.383: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a060d8f5-eb6b-4490-a1c6-6cc18516a34a" in namespace "projected-4174" to be "Succeeded or Failed"
Nov 15 16:59:06.397: INFO: Pod "pod-projected-configmaps-a060d8f5-eb6b-4490-a1c6-6cc18516a34a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.22344ms
Nov 15 16:59:08.413: INFO: Pod "pod-projected-configmaps-a060d8f5-eb6b-4490-a1c6-6cc18516a34a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030430122s
Nov 15 16:59:10.413: INFO: Pod "pod-projected-configmaps-a060d8f5-eb6b-4490-a1c6-6cc18516a34a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029994512s
Nov 15 16:59:12.413: INFO: Pod "pod-projected-configmaps-a060d8f5-eb6b-4490-a1c6-6cc18516a34a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030171391s
STEP: Saw pod success 11/15/23 16:59:12.413
Nov 15 16:59:12.414: INFO: Pod "pod-projected-configmaps-a060d8f5-eb6b-4490-a1c6-6cc18516a34a" satisfied condition "Succeeded or Failed"
Nov 15 16:59:12.429: INFO: Trying to get logs from node 10.15.40.115 pod pod-projected-configmaps-a060d8f5-eb6b-4490-a1c6-6cc18516a34a container agnhost-container: <nil>
STEP: delete the pod 11/15/23 16:59:12.47
Nov 15 16:59:12.515: INFO: Waiting for pod pod-projected-configmaps-a060d8f5-eb6b-4490-a1c6-6cc18516a34a to disappear
Nov 15 16:59:12.531: INFO: Pod pod-projected-configmaps-a060d8f5-eb6b-4490-a1c6-6cc18516a34a no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Nov 15 16:59:12.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4174" for this suite. 11/15/23 16:59:12.555
------------------------------
• [SLOW TEST] [6.337 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:59:06.241
    Nov 15 16:59:06.242: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename projected 11/15/23 16:59:06.246
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:59:06.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:59:06.313
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-3d55d848-8207-47bc-98bb-ace03ea926c9 11/15/23 16:59:06.329
    STEP: Creating a pod to test consume configMaps 11/15/23 16:59:06.35
    Nov 15 16:59:06.383: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a060d8f5-eb6b-4490-a1c6-6cc18516a34a" in namespace "projected-4174" to be "Succeeded or Failed"
    Nov 15 16:59:06.397: INFO: Pod "pod-projected-configmaps-a060d8f5-eb6b-4490-a1c6-6cc18516a34a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.22344ms
    Nov 15 16:59:08.413: INFO: Pod "pod-projected-configmaps-a060d8f5-eb6b-4490-a1c6-6cc18516a34a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030430122s
    Nov 15 16:59:10.413: INFO: Pod "pod-projected-configmaps-a060d8f5-eb6b-4490-a1c6-6cc18516a34a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029994512s
    Nov 15 16:59:12.413: INFO: Pod "pod-projected-configmaps-a060d8f5-eb6b-4490-a1c6-6cc18516a34a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030171391s
    STEP: Saw pod success 11/15/23 16:59:12.413
    Nov 15 16:59:12.414: INFO: Pod "pod-projected-configmaps-a060d8f5-eb6b-4490-a1c6-6cc18516a34a" satisfied condition "Succeeded or Failed"
    Nov 15 16:59:12.429: INFO: Trying to get logs from node 10.15.40.115 pod pod-projected-configmaps-a060d8f5-eb6b-4490-a1c6-6cc18516a34a container agnhost-container: <nil>
    STEP: delete the pod 11/15/23 16:59:12.47
    Nov 15 16:59:12.515: INFO: Waiting for pod pod-projected-configmaps-a060d8f5-eb6b-4490-a1c6-6cc18516a34a to disappear
    Nov 15 16:59:12.531: INFO: Pod pod-projected-configmaps-a060d8f5-eb6b-4490-a1c6-6cc18516a34a no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:59:12.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4174" for this suite. 11/15/23 16:59:12.555
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:59:12.594
Nov 15 16:59:12.594: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename downward-api 11/15/23 16:59:12.596
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:59:12.647
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:59:12.664
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 11/15/23 16:59:12.682
Nov 15 16:59:12.727: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f8d84ab0-8887-4018-8c82-0cc1ebbe231c" in namespace "downward-api-5964" to be "Succeeded or Failed"
Nov 15 16:59:12.744: INFO: Pod "downwardapi-volume-f8d84ab0-8887-4018-8c82-0cc1ebbe231c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.538773ms
Nov 15 16:59:14.767: INFO: Pod "downwardapi-volume-f8d84ab0-8887-4018-8c82-0cc1ebbe231c": Phase="Running", Reason="", readiness=true. Elapsed: 2.039750802s
Nov 15 16:59:16.761: INFO: Pod "downwardapi-volume-f8d84ab0-8887-4018-8c82-0cc1ebbe231c": Phase="Running", Reason="", readiness=false. Elapsed: 4.033682345s
Nov 15 16:59:18.760: INFO: Pod "downwardapi-volume-f8d84ab0-8887-4018-8c82-0cc1ebbe231c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032558535s
STEP: Saw pod success 11/15/23 16:59:18.76
Nov 15 16:59:18.761: INFO: Pod "downwardapi-volume-f8d84ab0-8887-4018-8c82-0cc1ebbe231c" satisfied condition "Succeeded or Failed"
Nov 15 16:59:18.776: INFO: Trying to get logs from node 10.15.40.115 pod downwardapi-volume-f8d84ab0-8887-4018-8c82-0cc1ebbe231c container client-container: <nil>
STEP: delete the pod 11/15/23 16:59:18.831
Nov 15 16:59:18.868: INFO: Waiting for pod downwardapi-volume-f8d84ab0-8887-4018-8c82-0cc1ebbe231c to disappear
Nov 15 16:59:18.882: INFO: Pod downwardapi-volume-f8d84ab0-8887-4018-8c82-0cc1ebbe231c no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Nov 15 16:59:18.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5964" for this suite. 11/15/23 16:59:18.904
------------------------------
• [SLOW TEST] [6.338 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:59:12.594
    Nov 15 16:59:12.594: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename downward-api 11/15/23 16:59:12.596
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:59:12.647
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:59:12.664
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 11/15/23 16:59:12.682
    Nov 15 16:59:12.727: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f8d84ab0-8887-4018-8c82-0cc1ebbe231c" in namespace "downward-api-5964" to be "Succeeded or Failed"
    Nov 15 16:59:12.744: INFO: Pod "downwardapi-volume-f8d84ab0-8887-4018-8c82-0cc1ebbe231c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.538773ms
    Nov 15 16:59:14.767: INFO: Pod "downwardapi-volume-f8d84ab0-8887-4018-8c82-0cc1ebbe231c": Phase="Running", Reason="", readiness=true. Elapsed: 2.039750802s
    Nov 15 16:59:16.761: INFO: Pod "downwardapi-volume-f8d84ab0-8887-4018-8c82-0cc1ebbe231c": Phase="Running", Reason="", readiness=false. Elapsed: 4.033682345s
    Nov 15 16:59:18.760: INFO: Pod "downwardapi-volume-f8d84ab0-8887-4018-8c82-0cc1ebbe231c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032558535s
    STEP: Saw pod success 11/15/23 16:59:18.76
    Nov 15 16:59:18.761: INFO: Pod "downwardapi-volume-f8d84ab0-8887-4018-8c82-0cc1ebbe231c" satisfied condition "Succeeded or Failed"
    Nov 15 16:59:18.776: INFO: Trying to get logs from node 10.15.40.115 pod downwardapi-volume-f8d84ab0-8887-4018-8c82-0cc1ebbe231c container client-container: <nil>
    STEP: delete the pod 11/15/23 16:59:18.831
    Nov 15 16:59:18.868: INFO: Waiting for pod downwardapi-volume-f8d84ab0-8887-4018-8c82-0cc1ebbe231c to disappear
    Nov 15 16:59:18.882: INFO: Pod downwardapi-volume-f8d84ab0-8887-4018-8c82-0cc1ebbe231c no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Nov 15 16:59:18.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5964" for this suite. 11/15/23 16:59:18.904
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 16:59:18.933
Nov 15 16:59:18.933: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename projected 11/15/23 16:59:18.935
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:59:18.986
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:59:19.002
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-9d7762fe-4c5b-4f24-8b5c-bc7a6dfaf621 11/15/23 16:59:19.043
STEP: Creating secret with name s-test-opt-upd-4b4485e2-4bf0-4583-9fb5-87d11d30aaf3 11/15/23 16:59:19.064
STEP: Creating the pod 11/15/23 16:59:19.085
Nov 15 16:59:19.121: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fbc35e84-c370-4d62-a2f9-bb5f62f43e49" in namespace "projected-8381" to be "running and ready"
Nov 15 16:59:19.137: INFO: Pod "pod-projected-secrets-fbc35e84-c370-4d62-a2f9-bb5f62f43e49": Phase="Pending", Reason="", readiness=false. Elapsed: 15.897121ms
Nov 15 16:59:19.137: INFO: The phase of Pod pod-projected-secrets-fbc35e84-c370-4d62-a2f9-bb5f62f43e49 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 16:59:21.154: INFO: Pod "pod-projected-secrets-fbc35e84-c370-4d62-a2f9-bb5f62f43e49": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033752358s
Nov 15 16:59:21.154: INFO: The phase of Pod pod-projected-secrets-fbc35e84-c370-4d62-a2f9-bb5f62f43e49 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 16:59:23.157: INFO: Pod "pod-projected-secrets-fbc35e84-c370-4d62-a2f9-bb5f62f43e49": Phase="Running", Reason="", readiness=true. Elapsed: 4.03602596s
Nov 15 16:59:23.157: INFO: The phase of Pod pod-projected-secrets-fbc35e84-c370-4d62-a2f9-bb5f62f43e49 is Running (Ready = true)
Nov 15 16:59:23.157: INFO: Pod "pod-projected-secrets-fbc35e84-c370-4d62-a2f9-bb5f62f43e49" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-9d7762fe-4c5b-4f24-8b5c-bc7a6dfaf621 11/15/23 16:59:23.291
STEP: Updating secret s-test-opt-upd-4b4485e2-4bf0-4583-9fb5-87d11d30aaf3 11/15/23 16:59:23.324
STEP: Creating secret with name s-test-opt-create-d8b6f50b-4d3d-4759-8528-9e53c1815a62 11/15/23 16:59:23.346
STEP: waiting to observe update in volume 11/15/23 16:59:23.374
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Nov 15 17:00:35.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8381" for this suite. 11/15/23 17:00:35.2
------------------------------
• [SLOW TEST] [76.291 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 16:59:18.933
    Nov 15 16:59:18.933: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename projected 11/15/23 16:59:18.935
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 16:59:18.986
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 16:59:19.002
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-9d7762fe-4c5b-4f24-8b5c-bc7a6dfaf621 11/15/23 16:59:19.043
    STEP: Creating secret with name s-test-opt-upd-4b4485e2-4bf0-4583-9fb5-87d11d30aaf3 11/15/23 16:59:19.064
    STEP: Creating the pod 11/15/23 16:59:19.085
    Nov 15 16:59:19.121: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fbc35e84-c370-4d62-a2f9-bb5f62f43e49" in namespace "projected-8381" to be "running and ready"
    Nov 15 16:59:19.137: INFO: Pod "pod-projected-secrets-fbc35e84-c370-4d62-a2f9-bb5f62f43e49": Phase="Pending", Reason="", readiness=false. Elapsed: 15.897121ms
    Nov 15 16:59:19.137: INFO: The phase of Pod pod-projected-secrets-fbc35e84-c370-4d62-a2f9-bb5f62f43e49 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 16:59:21.154: INFO: Pod "pod-projected-secrets-fbc35e84-c370-4d62-a2f9-bb5f62f43e49": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033752358s
    Nov 15 16:59:21.154: INFO: The phase of Pod pod-projected-secrets-fbc35e84-c370-4d62-a2f9-bb5f62f43e49 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 16:59:23.157: INFO: Pod "pod-projected-secrets-fbc35e84-c370-4d62-a2f9-bb5f62f43e49": Phase="Running", Reason="", readiness=true. Elapsed: 4.03602596s
    Nov 15 16:59:23.157: INFO: The phase of Pod pod-projected-secrets-fbc35e84-c370-4d62-a2f9-bb5f62f43e49 is Running (Ready = true)
    Nov 15 16:59:23.157: INFO: Pod "pod-projected-secrets-fbc35e84-c370-4d62-a2f9-bb5f62f43e49" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-9d7762fe-4c5b-4f24-8b5c-bc7a6dfaf621 11/15/23 16:59:23.291
    STEP: Updating secret s-test-opt-upd-4b4485e2-4bf0-4583-9fb5-87d11d30aaf3 11/15/23 16:59:23.324
    STEP: Creating secret with name s-test-opt-create-d8b6f50b-4d3d-4759-8528-9e53c1815a62 11/15/23 16:59:23.346
    STEP: waiting to observe update in volume 11/15/23 16:59:23.374
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:00:35.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8381" for this suite. 11/15/23 17:00:35.2
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:00:35.233
Nov 15 17:00:35.234: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename emptydir 11/15/23 17:00:35.236
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:00:35.291
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:00:35.305
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 11/15/23 17:00:35.321
Nov 15 17:00:35.354: INFO: Waiting up to 5m0s for pod "pod-5ae4f05e-78f4-4f31-9d5c-13633d9acdf8" in namespace "emptydir-49" to be "Succeeded or Failed"
Nov 15 17:00:35.368: INFO: Pod "pod-5ae4f05e-78f4-4f31-9d5c-13633d9acdf8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.21835ms
Nov 15 17:00:37.398: INFO: Pod "pod-5ae4f05e-78f4-4f31-9d5c-13633d9acdf8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043800692s
Nov 15 17:00:39.384: INFO: Pod "pod-5ae4f05e-78f4-4f31-9d5c-13633d9acdf8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029697016s
STEP: Saw pod success 11/15/23 17:00:39.384
Nov 15 17:00:39.384: INFO: Pod "pod-5ae4f05e-78f4-4f31-9d5c-13633d9acdf8" satisfied condition "Succeeded or Failed"
Nov 15 17:00:39.398: INFO: Trying to get logs from node 10.15.40.115 pod pod-5ae4f05e-78f4-4f31-9d5c-13633d9acdf8 container test-container: <nil>
STEP: delete the pod 11/15/23 17:00:39.438
Nov 15 17:00:39.473: INFO: Waiting for pod pod-5ae4f05e-78f4-4f31-9d5c-13633d9acdf8 to disappear
Nov 15 17:00:39.487: INFO: Pod pod-5ae4f05e-78f4-4f31-9d5c-13633d9acdf8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 15 17:00:39.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-49" for this suite. 11/15/23 17:00:39.509
------------------------------
• [4.300 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:00:35.233
    Nov 15 17:00:35.234: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename emptydir 11/15/23 17:00:35.236
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:00:35.291
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:00:35.305
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 11/15/23 17:00:35.321
    Nov 15 17:00:35.354: INFO: Waiting up to 5m0s for pod "pod-5ae4f05e-78f4-4f31-9d5c-13633d9acdf8" in namespace "emptydir-49" to be "Succeeded or Failed"
    Nov 15 17:00:35.368: INFO: Pod "pod-5ae4f05e-78f4-4f31-9d5c-13633d9acdf8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.21835ms
    Nov 15 17:00:37.398: INFO: Pod "pod-5ae4f05e-78f4-4f31-9d5c-13633d9acdf8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043800692s
    Nov 15 17:00:39.384: INFO: Pod "pod-5ae4f05e-78f4-4f31-9d5c-13633d9acdf8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029697016s
    STEP: Saw pod success 11/15/23 17:00:39.384
    Nov 15 17:00:39.384: INFO: Pod "pod-5ae4f05e-78f4-4f31-9d5c-13633d9acdf8" satisfied condition "Succeeded or Failed"
    Nov 15 17:00:39.398: INFO: Trying to get logs from node 10.15.40.115 pod pod-5ae4f05e-78f4-4f31-9d5c-13633d9acdf8 container test-container: <nil>
    STEP: delete the pod 11/15/23 17:00:39.438
    Nov 15 17:00:39.473: INFO: Waiting for pod pod-5ae4f05e-78f4-4f31-9d5c-13633d9acdf8 to disappear
    Nov 15 17:00:39.487: INFO: Pod pod-5ae4f05e-78f4-4f31-9d5c-13633d9acdf8 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:00:39.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-49" for this suite. 11/15/23 17:00:39.509
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:00:39.54
Nov 15 17:00:39.540: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename crd-publish-openapi 11/15/23 17:00:39.542
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:00:39.59
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:00:39.607
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Nov 15 17:00:39.624: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 11/15/23 17:00:42.135
Nov 15 17:00:42.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-1378 --namespace=crd-publish-openapi-1378 create -f -'
Nov 15 17:00:43.214: INFO: stderr: ""
Nov 15 17:00:43.214: INFO: stdout: "e2e-test-crd-publish-openapi-5257-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Nov 15 17:00:43.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-1378 --namespace=crd-publish-openapi-1378 delete e2e-test-crd-publish-openapi-5257-crds test-foo'
Nov 15 17:00:43.471: INFO: stderr: ""
Nov 15 17:00:43.471: INFO: stdout: "e2e-test-crd-publish-openapi-5257-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Nov 15 17:00:43.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-1378 --namespace=crd-publish-openapi-1378 apply -f -'
Nov 15 17:00:44.225: INFO: stderr: ""
Nov 15 17:00:44.225: INFO: stdout: "e2e-test-crd-publish-openapi-5257-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Nov 15 17:00:44.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-1378 --namespace=crd-publish-openapi-1378 delete e2e-test-crd-publish-openapi-5257-crds test-foo'
Nov 15 17:00:44.387: INFO: stderr: ""
Nov 15 17:00:44.387: INFO: stdout: "e2e-test-crd-publish-openapi-5257-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 11/15/23 17:00:44.387
Nov 15 17:00:44.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-1378 --namespace=crd-publish-openapi-1378 create -f -'
Nov 15 17:00:44.728: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 11/15/23 17:00:44.728
Nov 15 17:00:44.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-1378 --namespace=crd-publish-openapi-1378 create -f -'
Nov 15 17:00:45.533: INFO: rc: 1
Nov 15 17:00:45.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-1378 --namespace=crd-publish-openapi-1378 apply -f -'
Nov 15 17:00:45.892: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 11/15/23 17:00:45.892
Nov 15 17:00:45.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-1378 --namespace=crd-publish-openapi-1378 create -f -'
Nov 15 17:00:46.269: INFO: rc: 1
Nov 15 17:00:46.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-1378 --namespace=crd-publish-openapi-1378 apply -f -'
Nov 15 17:00:46.655: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 11/15/23 17:00:46.655
Nov 15 17:00:46.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-1378 explain e2e-test-crd-publish-openapi-5257-crds'
Nov 15 17:00:46.978: INFO: stderr: ""
Nov 15 17:00:46.978: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5257-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 11/15/23 17:00:46.978
Nov 15 17:00:46.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-1378 explain e2e-test-crd-publish-openapi-5257-crds.metadata'
Nov 15 17:00:47.273: INFO: stderr: ""
Nov 15 17:00:47.273: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5257-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Nov 15 17:00:47.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-1378 explain e2e-test-crd-publish-openapi-5257-crds.spec'
Nov 15 17:00:47.592: INFO: stderr: ""
Nov 15 17:00:47.592: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5257-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Nov 15 17:00:47.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-1378 explain e2e-test-crd-publish-openapi-5257-crds.spec.bars'
Nov 15 17:00:47.905: INFO: stderr: ""
Nov 15 17:00:47.905: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5257-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 11/15/23 17:00:47.905
Nov 15 17:00:47.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-1378 explain e2e-test-crd-publish-openapi-5257-crds.spec.bars2'
Nov 15 17:00:48.222: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 17:00:50.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1378" for this suite. 11/15/23 17:00:50.766
------------------------------
• [SLOW TEST] [11.251 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:00:39.54
    Nov 15 17:00:39.540: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename crd-publish-openapi 11/15/23 17:00:39.542
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:00:39.59
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:00:39.607
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Nov 15 17:00:39.624: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 11/15/23 17:00:42.135
    Nov 15 17:00:42.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-1378 --namespace=crd-publish-openapi-1378 create -f -'
    Nov 15 17:00:43.214: INFO: stderr: ""
    Nov 15 17:00:43.214: INFO: stdout: "e2e-test-crd-publish-openapi-5257-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Nov 15 17:00:43.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-1378 --namespace=crd-publish-openapi-1378 delete e2e-test-crd-publish-openapi-5257-crds test-foo'
    Nov 15 17:00:43.471: INFO: stderr: ""
    Nov 15 17:00:43.471: INFO: stdout: "e2e-test-crd-publish-openapi-5257-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Nov 15 17:00:43.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-1378 --namespace=crd-publish-openapi-1378 apply -f -'
    Nov 15 17:00:44.225: INFO: stderr: ""
    Nov 15 17:00:44.225: INFO: stdout: "e2e-test-crd-publish-openapi-5257-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Nov 15 17:00:44.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-1378 --namespace=crd-publish-openapi-1378 delete e2e-test-crd-publish-openapi-5257-crds test-foo'
    Nov 15 17:00:44.387: INFO: stderr: ""
    Nov 15 17:00:44.387: INFO: stdout: "e2e-test-crd-publish-openapi-5257-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 11/15/23 17:00:44.387
    Nov 15 17:00:44.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-1378 --namespace=crd-publish-openapi-1378 create -f -'
    Nov 15 17:00:44.728: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 11/15/23 17:00:44.728
    Nov 15 17:00:44.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-1378 --namespace=crd-publish-openapi-1378 create -f -'
    Nov 15 17:00:45.533: INFO: rc: 1
    Nov 15 17:00:45.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-1378 --namespace=crd-publish-openapi-1378 apply -f -'
    Nov 15 17:00:45.892: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 11/15/23 17:00:45.892
    Nov 15 17:00:45.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-1378 --namespace=crd-publish-openapi-1378 create -f -'
    Nov 15 17:00:46.269: INFO: rc: 1
    Nov 15 17:00:46.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-1378 --namespace=crd-publish-openapi-1378 apply -f -'
    Nov 15 17:00:46.655: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 11/15/23 17:00:46.655
    Nov 15 17:00:46.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-1378 explain e2e-test-crd-publish-openapi-5257-crds'
    Nov 15 17:00:46.978: INFO: stderr: ""
    Nov 15 17:00:46.978: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5257-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 11/15/23 17:00:46.978
    Nov 15 17:00:46.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-1378 explain e2e-test-crd-publish-openapi-5257-crds.metadata'
    Nov 15 17:00:47.273: INFO: stderr: ""
    Nov 15 17:00:47.273: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5257-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Nov 15 17:00:47.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-1378 explain e2e-test-crd-publish-openapi-5257-crds.spec'
    Nov 15 17:00:47.592: INFO: stderr: ""
    Nov 15 17:00:47.592: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5257-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Nov 15 17:00:47.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-1378 explain e2e-test-crd-publish-openapi-5257-crds.spec.bars'
    Nov 15 17:00:47.905: INFO: stderr: ""
    Nov 15 17:00:47.905: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5257-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 11/15/23 17:00:47.905
    Nov 15 17:00:47.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-1378 explain e2e-test-crd-publish-openapi-5257-crds.spec.bars2'
    Nov 15 17:00:48.222: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:00:50.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1378" for this suite. 11/15/23 17:00:50.766
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:00:50.794
Nov 15 17:00:50.794: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename emptydir 11/15/23 17:00:50.797
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:00:50.85
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:00:50.866
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 11/15/23 17:00:50.884
Nov 15 17:00:50.919: INFO: Waiting up to 5m0s for pod "pod-0476f261-c9ce-498c-9d8b-22235730c075" in namespace "emptydir-7181" to be "Succeeded or Failed"
Nov 15 17:00:50.934: INFO: Pod "pod-0476f261-c9ce-498c-9d8b-22235730c075": Phase="Pending", Reason="", readiness=false. Elapsed: 15.015486ms
Nov 15 17:00:52.950: INFO: Pod "pod-0476f261-c9ce-498c-9d8b-22235730c075": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03173695s
Nov 15 17:00:54.952: INFO: Pod "pod-0476f261-c9ce-498c-9d8b-22235730c075": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03317957s
Nov 15 17:00:56.953: INFO: Pod "pod-0476f261-c9ce-498c-9d8b-22235730c075": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033987898s
STEP: Saw pod success 11/15/23 17:00:56.953
Nov 15 17:00:56.954: INFO: Pod "pod-0476f261-c9ce-498c-9d8b-22235730c075" satisfied condition "Succeeded or Failed"
Nov 15 17:00:56.969: INFO: Trying to get logs from node 10.15.40.115 pod pod-0476f261-c9ce-498c-9d8b-22235730c075 container test-container: <nil>
STEP: delete the pod 11/15/23 17:00:57.009
Nov 15 17:00:57.045: INFO: Waiting for pod pod-0476f261-c9ce-498c-9d8b-22235730c075 to disappear
Nov 15 17:00:57.060: INFO: Pod pod-0476f261-c9ce-498c-9d8b-22235730c075 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 15 17:00:57.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7181" for this suite. 11/15/23 17:00:57.087
------------------------------
• [SLOW TEST] [6.320 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:00:50.794
    Nov 15 17:00:50.794: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename emptydir 11/15/23 17:00:50.797
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:00:50.85
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:00:50.866
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 11/15/23 17:00:50.884
    Nov 15 17:00:50.919: INFO: Waiting up to 5m0s for pod "pod-0476f261-c9ce-498c-9d8b-22235730c075" in namespace "emptydir-7181" to be "Succeeded or Failed"
    Nov 15 17:00:50.934: INFO: Pod "pod-0476f261-c9ce-498c-9d8b-22235730c075": Phase="Pending", Reason="", readiness=false. Elapsed: 15.015486ms
    Nov 15 17:00:52.950: INFO: Pod "pod-0476f261-c9ce-498c-9d8b-22235730c075": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03173695s
    Nov 15 17:00:54.952: INFO: Pod "pod-0476f261-c9ce-498c-9d8b-22235730c075": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03317957s
    Nov 15 17:00:56.953: INFO: Pod "pod-0476f261-c9ce-498c-9d8b-22235730c075": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033987898s
    STEP: Saw pod success 11/15/23 17:00:56.953
    Nov 15 17:00:56.954: INFO: Pod "pod-0476f261-c9ce-498c-9d8b-22235730c075" satisfied condition "Succeeded or Failed"
    Nov 15 17:00:56.969: INFO: Trying to get logs from node 10.15.40.115 pod pod-0476f261-c9ce-498c-9d8b-22235730c075 container test-container: <nil>
    STEP: delete the pod 11/15/23 17:00:57.009
    Nov 15 17:00:57.045: INFO: Waiting for pod pod-0476f261-c9ce-498c-9d8b-22235730c075 to disappear
    Nov 15 17:00:57.060: INFO: Pod pod-0476f261-c9ce-498c-9d8b-22235730c075 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:00:57.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7181" for this suite. 11/15/23 17:00:57.087
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:00:57.123
Nov 15 17:00:57.124: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename csiinlinevolumes 11/15/23 17:00:57.126
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:00:57.186
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:00:57.202
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 11/15/23 17:00:57.219
STEP: getting 11/15/23 17:00:57.296
STEP: listing 11/15/23 17:00:57.331
STEP: deleting 11/15/23 17:00:57.347
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Nov 15 17:00:57.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-8234" for this suite. 11/15/23 17:00:57.466
------------------------------
• [0.366 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:00:57.123
    Nov 15 17:00:57.124: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename csiinlinevolumes 11/15/23 17:00:57.126
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:00:57.186
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:00:57.202
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 11/15/23 17:00:57.219
    STEP: getting 11/15/23 17:00:57.296
    STEP: listing 11/15/23 17:00:57.331
    STEP: deleting 11/15/23 17:00:57.347
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:00:57.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-8234" for this suite. 11/15/23 17:00:57.466
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:00:57.497
Nov 15 17:00:57.497: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename proxy 11/15/23 17:00:57.499
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:00:57.554
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:00:57.572
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Nov 15 17:00:57.588: INFO: Creating pod...
Nov 15 17:00:57.621: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8347" to be "running"
Nov 15 17:00:57.636: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 14.698701ms
Nov 15 17:00:59.674: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.052980642s
Nov 15 17:00:59.675: INFO: Pod "agnhost" satisfied condition "running"
Nov 15 17:00:59.675: INFO: Creating service...
Nov 15 17:00:59.717: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8347/pods/agnhost/proxy/some/path/with/DELETE
Nov 15 17:00:59.822: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Nov 15 17:00:59.822: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8347/pods/agnhost/proxy/some/path/with/GET
Nov 15 17:00:59.849: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Nov 15 17:00:59.849: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8347/pods/agnhost/proxy/some/path/with/HEAD
Nov 15 17:00:59.877: INFO: http.Client request:HEAD | StatusCode:200
Nov 15 17:00:59.877: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8347/pods/agnhost/proxy/some/path/with/OPTIONS
Nov 15 17:00:59.905: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Nov 15 17:00:59.905: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8347/pods/agnhost/proxy/some/path/with/PATCH
Nov 15 17:00:59.932: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Nov 15 17:00:59.932: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8347/pods/agnhost/proxy/some/path/with/POST
Nov 15 17:00:59.958: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Nov 15 17:00:59.958: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8347/pods/agnhost/proxy/some/path/with/PUT
Nov 15 17:00:59.989: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Nov 15 17:00:59.989: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8347/services/test-service/proxy/some/path/with/DELETE
Nov 15 17:01:00.025: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Nov 15 17:01:00.025: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8347/services/test-service/proxy/some/path/with/GET
Nov 15 17:01:00.059: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Nov 15 17:01:00.059: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8347/services/test-service/proxy/some/path/with/HEAD
Nov 15 17:01:00.094: INFO: http.Client request:HEAD | StatusCode:200
Nov 15 17:01:00.094: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8347/services/test-service/proxy/some/path/with/OPTIONS
Nov 15 17:01:00.128: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Nov 15 17:01:00.129: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8347/services/test-service/proxy/some/path/with/PATCH
Nov 15 17:01:00.167: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Nov 15 17:01:00.167: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8347/services/test-service/proxy/some/path/with/POST
Nov 15 17:01:00.204: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Nov 15 17:01:00.204: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8347/services/test-service/proxy/some/path/with/PUT
Nov 15 17:01:00.239: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Nov 15 17:01:00.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-8347" for this suite. 11/15/23 17:01:00.269
------------------------------
• [2.798 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:00:57.497
    Nov 15 17:00:57.497: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename proxy 11/15/23 17:00:57.499
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:00:57.554
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:00:57.572
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Nov 15 17:00:57.588: INFO: Creating pod...
    Nov 15 17:00:57.621: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8347" to be "running"
    Nov 15 17:00:57.636: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 14.698701ms
    Nov 15 17:00:59.674: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.052980642s
    Nov 15 17:00:59.675: INFO: Pod "agnhost" satisfied condition "running"
    Nov 15 17:00:59.675: INFO: Creating service...
    Nov 15 17:00:59.717: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8347/pods/agnhost/proxy/some/path/with/DELETE
    Nov 15 17:00:59.822: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Nov 15 17:00:59.822: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8347/pods/agnhost/proxy/some/path/with/GET
    Nov 15 17:00:59.849: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Nov 15 17:00:59.849: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8347/pods/agnhost/proxy/some/path/with/HEAD
    Nov 15 17:00:59.877: INFO: http.Client request:HEAD | StatusCode:200
    Nov 15 17:00:59.877: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8347/pods/agnhost/proxy/some/path/with/OPTIONS
    Nov 15 17:00:59.905: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Nov 15 17:00:59.905: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8347/pods/agnhost/proxy/some/path/with/PATCH
    Nov 15 17:00:59.932: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Nov 15 17:00:59.932: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8347/pods/agnhost/proxy/some/path/with/POST
    Nov 15 17:00:59.958: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Nov 15 17:00:59.958: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8347/pods/agnhost/proxy/some/path/with/PUT
    Nov 15 17:00:59.989: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Nov 15 17:00:59.989: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8347/services/test-service/proxy/some/path/with/DELETE
    Nov 15 17:01:00.025: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Nov 15 17:01:00.025: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8347/services/test-service/proxy/some/path/with/GET
    Nov 15 17:01:00.059: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Nov 15 17:01:00.059: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8347/services/test-service/proxy/some/path/with/HEAD
    Nov 15 17:01:00.094: INFO: http.Client request:HEAD | StatusCode:200
    Nov 15 17:01:00.094: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8347/services/test-service/proxy/some/path/with/OPTIONS
    Nov 15 17:01:00.128: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Nov 15 17:01:00.129: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8347/services/test-service/proxy/some/path/with/PATCH
    Nov 15 17:01:00.167: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Nov 15 17:01:00.167: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8347/services/test-service/proxy/some/path/with/POST
    Nov 15 17:01:00.204: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Nov 15 17:01:00.204: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8347/services/test-service/proxy/some/path/with/PUT
    Nov 15 17:01:00.239: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:01:00.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-8347" for this suite. 11/15/23 17:01:00.269
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:01:00.303
Nov 15 17:01:00.303: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename configmap 11/15/23 17:01:00.305
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:01:00.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:01:00.376
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-7500/configmap-test-ab369178-2d9b-4d99-b7ef-82d90a7bb85c 11/15/23 17:01:00.394
STEP: Creating a pod to test consume configMaps 11/15/23 17:01:00.415
Nov 15 17:01:00.450: INFO: Waiting up to 5m0s for pod "pod-configmaps-7b85ce16-d71e-44c8-b60a-40f0cbded316" in namespace "configmap-7500" to be "Succeeded or Failed"
Nov 15 17:01:00.464: INFO: Pod "pod-configmaps-7b85ce16-d71e-44c8-b60a-40f0cbded316": Phase="Pending", Reason="", readiness=false. Elapsed: 14.535549ms
Nov 15 17:01:02.479: INFO: Pod "pod-configmaps-7b85ce16-d71e-44c8-b60a-40f0cbded316": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029774576s
Nov 15 17:01:04.483: INFO: Pod "pod-configmaps-7b85ce16-d71e-44c8-b60a-40f0cbded316": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032911278s
STEP: Saw pod success 11/15/23 17:01:04.483
Nov 15 17:01:04.483: INFO: Pod "pod-configmaps-7b85ce16-d71e-44c8-b60a-40f0cbded316" satisfied condition "Succeeded or Failed"
Nov 15 17:01:04.498: INFO: Trying to get logs from node 10.15.40.115 pod pod-configmaps-7b85ce16-d71e-44c8-b60a-40f0cbded316 container env-test: <nil>
STEP: delete the pod 11/15/23 17:01:04.546
Nov 15 17:01:04.583: INFO: Waiting for pod pod-configmaps-7b85ce16-d71e-44c8-b60a-40f0cbded316 to disappear
Nov 15 17:01:04.597: INFO: Pod pod-configmaps-7b85ce16-d71e-44c8-b60a-40f0cbded316 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 15 17:01:04.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7500" for this suite. 11/15/23 17:01:04.624
------------------------------
• [4.348 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:01:00.303
    Nov 15 17:01:00.303: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename configmap 11/15/23 17:01:00.305
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:01:00.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:01:00.376
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-7500/configmap-test-ab369178-2d9b-4d99-b7ef-82d90a7bb85c 11/15/23 17:01:00.394
    STEP: Creating a pod to test consume configMaps 11/15/23 17:01:00.415
    Nov 15 17:01:00.450: INFO: Waiting up to 5m0s for pod "pod-configmaps-7b85ce16-d71e-44c8-b60a-40f0cbded316" in namespace "configmap-7500" to be "Succeeded or Failed"
    Nov 15 17:01:00.464: INFO: Pod "pod-configmaps-7b85ce16-d71e-44c8-b60a-40f0cbded316": Phase="Pending", Reason="", readiness=false. Elapsed: 14.535549ms
    Nov 15 17:01:02.479: INFO: Pod "pod-configmaps-7b85ce16-d71e-44c8-b60a-40f0cbded316": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029774576s
    Nov 15 17:01:04.483: INFO: Pod "pod-configmaps-7b85ce16-d71e-44c8-b60a-40f0cbded316": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032911278s
    STEP: Saw pod success 11/15/23 17:01:04.483
    Nov 15 17:01:04.483: INFO: Pod "pod-configmaps-7b85ce16-d71e-44c8-b60a-40f0cbded316" satisfied condition "Succeeded or Failed"
    Nov 15 17:01:04.498: INFO: Trying to get logs from node 10.15.40.115 pod pod-configmaps-7b85ce16-d71e-44c8-b60a-40f0cbded316 container env-test: <nil>
    STEP: delete the pod 11/15/23 17:01:04.546
    Nov 15 17:01:04.583: INFO: Waiting for pod pod-configmaps-7b85ce16-d71e-44c8-b60a-40f0cbded316 to disappear
    Nov 15 17:01:04.597: INFO: Pod pod-configmaps-7b85ce16-d71e-44c8-b60a-40f0cbded316 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:01:04.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7500" for this suite. 11/15/23 17:01:04.624
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:01:04.657
Nov 15 17:01:04.657: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename namespaces 11/15/23 17:01:04.661
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:01:04.712
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:01:04.726
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-hphjk" 11/15/23 17:01:04.74
Nov 15 17:01:04.791: INFO: Namespace "e2e-ns-hphjk-5603" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-hphjk-5603" 11/15/23 17:01:04.792
Nov 15 17:01:04.827: INFO: Namespace "e2e-ns-hphjk-5603" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-hphjk-5603" 11/15/23 17:01:04.827
Nov 15 17:01:04.860: INFO: Namespace "e2e-ns-hphjk-5603" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 17:01:04.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-345" for this suite. 11/15/23 17:01:04.884
STEP: Destroying namespace "e2e-ns-hphjk-5603" for this suite. 11/15/23 17:01:04.907
------------------------------
• [0.274 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:01:04.657
    Nov 15 17:01:04.657: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename namespaces 11/15/23 17:01:04.661
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:01:04.712
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:01:04.726
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-hphjk" 11/15/23 17:01:04.74
    Nov 15 17:01:04.791: INFO: Namespace "e2e-ns-hphjk-5603" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-hphjk-5603" 11/15/23 17:01:04.792
    Nov 15 17:01:04.827: INFO: Namespace "e2e-ns-hphjk-5603" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-hphjk-5603" 11/15/23 17:01:04.827
    Nov 15 17:01:04.860: INFO: Namespace "e2e-ns-hphjk-5603" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:01:04.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-345" for this suite. 11/15/23 17:01:04.884
    STEP: Destroying namespace "e2e-ns-hphjk-5603" for this suite. 11/15/23 17:01:04.907
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:01:04.939
Nov 15 17:01:04.939: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename tables 11/15/23 17:01:04.941
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:01:04.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:01:05.01
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Nov 15 17:01:05.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-3757" for this suite. 11/15/23 17:01:05.07
------------------------------
• [0.157 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:01:04.939
    Nov 15 17:01:04.939: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename tables 11/15/23 17:01:04.941
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:01:04.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:01:05.01
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:01:05.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-3757" for this suite. 11/15/23 17:01:05.07
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:01:05.106
Nov 15 17:01:05.107: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename configmap 11/15/23 17:01:05.109
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:01:05.157
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:01:05.175
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-8f40472e-73f7-404c-8e12-7b2dfaadad78 11/15/23 17:01:05.191
STEP: Creating a pod to test consume configMaps 11/15/23 17:01:05.211
Nov 15 17:01:05.250: INFO: Waiting up to 5m0s for pod "pod-configmaps-d14d369f-444c-4055-89ba-ba1523ee0fcd" in namespace "configmap-7971" to be "Succeeded or Failed"
Nov 15 17:01:05.267: INFO: Pod "pod-configmaps-d14d369f-444c-4055-89ba-ba1523ee0fcd": Phase="Pending", Reason="", readiness=false. Elapsed: 16.425878ms
Nov 15 17:01:07.284: INFO: Pod "pod-configmaps-d14d369f-444c-4055-89ba-ba1523ee0fcd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033157848s
Nov 15 17:01:09.288: INFO: Pod "pod-configmaps-d14d369f-444c-4055-89ba-ba1523ee0fcd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036997627s
STEP: Saw pod success 11/15/23 17:01:09.288
Nov 15 17:01:09.288: INFO: Pod "pod-configmaps-d14d369f-444c-4055-89ba-ba1523ee0fcd" satisfied condition "Succeeded or Failed"
Nov 15 17:01:09.316: INFO: Trying to get logs from node 10.15.40.115 pod pod-configmaps-d14d369f-444c-4055-89ba-ba1523ee0fcd container agnhost-container: <nil>
STEP: delete the pod 11/15/23 17:01:09.373
Nov 15 17:01:09.409: INFO: Waiting for pod pod-configmaps-d14d369f-444c-4055-89ba-ba1523ee0fcd to disappear
Nov 15 17:01:09.424: INFO: Pod pod-configmaps-d14d369f-444c-4055-89ba-ba1523ee0fcd no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 15 17:01:09.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7971" for this suite. 11/15/23 17:01:09.458
------------------------------
• [4.377 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:01:05.106
    Nov 15 17:01:05.107: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename configmap 11/15/23 17:01:05.109
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:01:05.157
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:01:05.175
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-8f40472e-73f7-404c-8e12-7b2dfaadad78 11/15/23 17:01:05.191
    STEP: Creating a pod to test consume configMaps 11/15/23 17:01:05.211
    Nov 15 17:01:05.250: INFO: Waiting up to 5m0s for pod "pod-configmaps-d14d369f-444c-4055-89ba-ba1523ee0fcd" in namespace "configmap-7971" to be "Succeeded or Failed"
    Nov 15 17:01:05.267: INFO: Pod "pod-configmaps-d14d369f-444c-4055-89ba-ba1523ee0fcd": Phase="Pending", Reason="", readiness=false. Elapsed: 16.425878ms
    Nov 15 17:01:07.284: INFO: Pod "pod-configmaps-d14d369f-444c-4055-89ba-ba1523ee0fcd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033157848s
    Nov 15 17:01:09.288: INFO: Pod "pod-configmaps-d14d369f-444c-4055-89ba-ba1523ee0fcd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036997627s
    STEP: Saw pod success 11/15/23 17:01:09.288
    Nov 15 17:01:09.288: INFO: Pod "pod-configmaps-d14d369f-444c-4055-89ba-ba1523ee0fcd" satisfied condition "Succeeded or Failed"
    Nov 15 17:01:09.316: INFO: Trying to get logs from node 10.15.40.115 pod pod-configmaps-d14d369f-444c-4055-89ba-ba1523ee0fcd container agnhost-container: <nil>
    STEP: delete the pod 11/15/23 17:01:09.373
    Nov 15 17:01:09.409: INFO: Waiting for pod pod-configmaps-d14d369f-444c-4055-89ba-ba1523ee0fcd to disappear
    Nov 15 17:01:09.424: INFO: Pod pod-configmaps-d14d369f-444c-4055-89ba-ba1523ee0fcd no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:01:09.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7971" for this suite. 11/15/23 17:01:09.458
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:01:09.484
Nov 15 17:01:09.484: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename configmap 11/15/23 17:01:09.488
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:01:09.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:01:09.561
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 11/15/23 17:01:09.577
STEP: fetching the ConfigMap 11/15/23 17:01:09.598
STEP: patching the ConfigMap 11/15/23 17:01:09.614
STEP: listing all ConfigMaps in all namespaces with a label selector 11/15/23 17:01:09.637
STEP: deleting the ConfigMap by collection with a label selector 11/15/23 17:01:09.657
STEP: listing all ConfigMaps in test namespace 11/15/23 17:01:09.692
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 15 17:01:09.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9600" for this suite. 11/15/23 17:01:09.734
------------------------------
• [0.275 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:01:09.484
    Nov 15 17:01:09.484: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename configmap 11/15/23 17:01:09.488
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:01:09.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:01:09.561
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 11/15/23 17:01:09.577
    STEP: fetching the ConfigMap 11/15/23 17:01:09.598
    STEP: patching the ConfigMap 11/15/23 17:01:09.614
    STEP: listing all ConfigMaps in all namespaces with a label selector 11/15/23 17:01:09.637
    STEP: deleting the ConfigMap by collection with a label selector 11/15/23 17:01:09.657
    STEP: listing all ConfigMaps in test namespace 11/15/23 17:01:09.692
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:01:09.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9600" for this suite. 11/15/23 17:01:09.734
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:01:09.762
Nov 15 17:01:09.763: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename secrets 11/15/23 17:01:09.766
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:01:09.816
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:01:09.835
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-fd1ca56c-3488-4a83-8652-210f82ccc916 11/15/23 17:01:09.85
STEP: Creating a pod to test consume secrets 11/15/23 17:01:09.872
Nov 15 17:01:09.904: INFO: Waiting up to 5m0s for pod "pod-secrets-b1aa3701-b4e4-4564-85c9-99a34a9d8da8" in namespace "secrets-7082" to be "Succeeded or Failed"
Nov 15 17:01:09.918: INFO: Pod "pod-secrets-b1aa3701-b4e4-4564-85c9-99a34a9d8da8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.033566ms
Nov 15 17:01:11.936: INFO: Pod "pod-secrets-b1aa3701-b4e4-4564-85c9-99a34a9d8da8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031323191s
Nov 15 17:01:13.934: INFO: Pod "pod-secrets-b1aa3701-b4e4-4564-85c9-99a34a9d8da8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029976956s
Nov 15 17:01:15.936: INFO: Pod "pod-secrets-b1aa3701-b4e4-4564-85c9-99a34a9d8da8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032083427s
STEP: Saw pod success 11/15/23 17:01:15.937
Nov 15 17:01:15.937: INFO: Pod "pod-secrets-b1aa3701-b4e4-4564-85c9-99a34a9d8da8" satisfied condition "Succeeded or Failed"
Nov 15 17:01:15.953: INFO: Trying to get logs from node 10.15.40.115 pod pod-secrets-b1aa3701-b4e4-4564-85c9-99a34a9d8da8 container secret-volume-test: <nil>
STEP: delete the pod 11/15/23 17:01:16.001
Nov 15 17:01:16.038: INFO: Waiting for pod pod-secrets-b1aa3701-b4e4-4564-85c9-99a34a9d8da8 to disappear
Nov 15 17:01:16.053: INFO: Pod pod-secrets-b1aa3701-b4e4-4564-85c9-99a34a9d8da8 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 15 17:01:16.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7082" for this suite. 11/15/23 17:01:16.08
------------------------------
• [SLOW TEST] [6.343 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:01:09.762
    Nov 15 17:01:09.763: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename secrets 11/15/23 17:01:09.766
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:01:09.816
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:01:09.835
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-fd1ca56c-3488-4a83-8652-210f82ccc916 11/15/23 17:01:09.85
    STEP: Creating a pod to test consume secrets 11/15/23 17:01:09.872
    Nov 15 17:01:09.904: INFO: Waiting up to 5m0s for pod "pod-secrets-b1aa3701-b4e4-4564-85c9-99a34a9d8da8" in namespace "secrets-7082" to be "Succeeded or Failed"
    Nov 15 17:01:09.918: INFO: Pod "pod-secrets-b1aa3701-b4e4-4564-85c9-99a34a9d8da8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.033566ms
    Nov 15 17:01:11.936: INFO: Pod "pod-secrets-b1aa3701-b4e4-4564-85c9-99a34a9d8da8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031323191s
    Nov 15 17:01:13.934: INFO: Pod "pod-secrets-b1aa3701-b4e4-4564-85c9-99a34a9d8da8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029976956s
    Nov 15 17:01:15.936: INFO: Pod "pod-secrets-b1aa3701-b4e4-4564-85c9-99a34a9d8da8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032083427s
    STEP: Saw pod success 11/15/23 17:01:15.937
    Nov 15 17:01:15.937: INFO: Pod "pod-secrets-b1aa3701-b4e4-4564-85c9-99a34a9d8da8" satisfied condition "Succeeded or Failed"
    Nov 15 17:01:15.953: INFO: Trying to get logs from node 10.15.40.115 pod pod-secrets-b1aa3701-b4e4-4564-85c9-99a34a9d8da8 container secret-volume-test: <nil>
    STEP: delete the pod 11/15/23 17:01:16.001
    Nov 15 17:01:16.038: INFO: Waiting for pod pod-secrets-b1aa3701-b4e4-4564-85c9-99a34a9d8da8 to disappear
    Nov 15 17:01:16.053: INFO: Pod pod-secrets-b1aa3701-b4e4-4564-85c9-99a34a9d8da8 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:01:16.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7082" for this suite. 11/15/23 17:01:16.08
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:01:16.12
Nov 15 17:01:16.120: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename services 11/15/23 17:01:16.122
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:01:16.181
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:01:16.191
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-3393 11/15/23 17:01:16.209
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3393 to expose endpoints map[] 11/15/23 17:01:16.253
Nov 15 17:01:16.289: INFO: successfully validated that service endpoint-test2 in namespace services-3393 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3393 11/15/23 17:01:16.289
Nov 15 17:01:16.327: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3393" to be "running and ready"
Nov 15 17:01:16.341: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.593603ms
Nov 15 17:01:16.342: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 17:01:18.358: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.030740045s
Nov 15 17:01:18.358: INFO: The phase of Pod pod1 is Running (Ready = true)
Nov 15 17:01:18.358: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3393 to expose endpoints map[pod1:[80]] 11/15/23 17:01:18.374
Nov 15 17:01:18.424: INFO: successfully validated that service endpoint-test2 in namespace services-3393 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 11/15/23 17:01:18.424
Nov 15 17:01:18.425: INFO: Creating new exec pod
Nov 15 17:01:18.443: INFO: Waiting up to 5m0s for pod "execpod9gvjq" in namespace "services-3393" to be "running"
Nov 15 17:01:18.459: INFO: Pod "execpod9gvjq": Phase="Pending", Reason="", readiness=false. Elapsed: 15.415468ms
Nov 15 17:01:20.478: INFO: Pod "execpod9gvjq": Phase="Running", Reason="", readiness=true. Elapsed: 2.03472809s
Nov 15 17:01:20.478: INFO: Pod "execpod9gvjq" satisfied condition "running"
Nov 15 17:01:21.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-3393 exec execpod9gvjq -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Nov 15 17:01:21.965: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Nov 15 17:01:21.965: INFO: stdout: ""
Nov 15 17:01:21.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-3393 exec execpod9gvjq -- /bin/sh -x -c nc -v -z -w 2 172.21.32.40 80'
Nov 15 17:01:22.384: INFO: stderr: "+ nc -v -z -w 2 172.21.32.40 80\nConnection to 172.21.32.40 80 port [tcp/http] succeeded!\n"
Nov 15 17:01:22.384: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-3393 11/15/23 17:01:22.384
Nov 15 17:01:22.404: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3393" to be "running and ready"
Nov 15 17:01:22.419: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 15.109796ms
Nov 15 17:01:22.419: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 17:01:24.435: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030665575s
Nov 15 17:01:24.435: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 17:01:26.435: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.03074804s
Nov 15 17:01:26.435: INFO: The phase of Pod pod2 is Running (Ready = true)
Nov 15 17:01:26.435: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3393 to expose endpoints map[pod1:[80] pod2:[80]] 11/15/23 17:01:26.451
Nov 15 17:01:26.516: INFO: successfully validated that service endpoint-test2 in namespace services-3393 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 11/15/23 17:01:26.516
Nov 15 17:01:27.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-3393 exec execpod9gvjq -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Nov 15 17:01:27.982: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Nov 15 17:01:27.982: INFO: stdout: ""
Nov 15 17:01:27.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-3393 exec execpod9gvjq -- /bin/sh -x -c nc -v -z -w 2 172.21.32.40 80'
Nov 15 17:01:28.425: INFO: stderr: "+ nc -v -z -w 2 172.21.32.40 80\nConnection to 172.21.32.40 80 port [tcp/http] succeeded!\n"
Nov 15 17:01:28.425: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-3393 11/15/23 17:01:28.425
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3393 to expose endpoints map[pod2:[80]] 11/15/23 17:01:28.464
Nov 15 17:01:28.517: INFO: successfully validated that service endpoint-test2 in namespace services-3393 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 11/15/23 17:01:28.518
Nov 15 17:01:29.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-3393 exec execpod9gvjq -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Nov 15 17:01:31.053: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Nov 15 17:01:31.053: INFO: stdout: ""
Nov 15 17:01:31.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-3393 exec execpod9gvjq -- /bin/sh -x -c nc -v -z -w 2 172.21.32.40 80'
Nov 15 17:01:32.528: INFO: stderr: "+ nc -v -z -w 2 172.21.32.40 80\nConnection to 172.21.32.40 80 port [tcp/http] succeeded!\n"
Nov 15 17:01:32.528: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-3393 11/15/23 17:01:32.528
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3393 to expose endpoints map[] 11/15/23 17:01:32.587
Nov 15 17:01:32.631: INFO: successfully validated that service endpoint-test2 in namespace services-3393 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 15 17:01:32.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3393" for this suite. 11/15/23 17:01:32.739
------------------------------
• [SLOW TEST] [16.647 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:01:16.12
    Nov 15 17:01:16.120: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename services 11/15/23 17:01:16.122
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:01:16.181
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:01:16.191
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-3393 11/15/23 17:01:16.209
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3393 to expose endpoints map[] 11/15/23 17:01:16.253
    Nov 15 17:01:16.289: INFO: successfully validated that service endpoint-test2 in namespace services-3393 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-3393 11/15/23 17:01:16.289
    Nov 15 17:01:16.327: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3393" to be "running and ready"
    Nov 15 17:01:16.341: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.593603ms
    Nov 15 17:01:16.342: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 17:01:18.358: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.030740045s
    Nov 15 17:01:18.358: INFO: The phase of Pod pod1 is Running (Ready = true)
    Nov 15 17:01:18.358: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3393 to expose endpoints map[pod1:[80]] 11/15/23 17:01:18.374
    Nov 15 17:01:18.424: INFO: successfully validated that service endpoint-test2 in namespace services-3393 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 11/15/23 17:01:18.424
    Nov 15 17:01:18.425: INFO: Creating new exec pod
    Nov 15 17:01:18.443: INFO: Waiting up to 5m0s for pod "execpod9gvjq" in namespace "services-3393" to be "running"
    Nov 15 17:01:18.459: INFO: Pod "execpod9gvjq": Phase="Pending", Reason="", readiness=false. Elapsed: 15.415468ms
    Nov 15 17:01:20.478: INFO: Pod "execpod9gvjq": Phase="Running", Reason="", readiness=true. Elapsed: 2.03472809s
    Nov 15 17:01:20.478: INFO: Pod "execpod9gvjq" satisfied condition "running"
    Nov 15 17:01:21.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-3393 exec execpod9gvjq -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Nov 15 17:01:21.965: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Nov 15 17:01:21.965: INFO: stdout: ""
    Nov 15 17:01:21.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-3393 exec execpod9gvjq -- /bin/sh -x -c nc -v -z -w 2 172.21.32.40 80'
    Nov 15 17:01:22.384: INFO: stderr: "+ nc -v -z -w 2 172.21.32.40 80\nConnection to 172.21.32.40 80 port [tcp/http] succeeded!\n"
    Nov 15 17:01:22.384: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-3393 11/15/23 17:01:22.384
    Nov 15 17:01:22.404: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3393" to be "running and ready"
    Nov 15 17:01:22.419: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 15.109796ms
    Nov 15 17:01:22.419: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 17:01:24.435: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030665575s
    Nov 15 17:01:24.435: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 17:01:26.435: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.03074804s
    Nov 15 17:01:26.435: INFO: The phase of Pod pod2 is Running (Ready = true)
    Nov 15 17:01:26.435: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3393 to expose endpoints map[pod1:[80] pod2:[80]] 11/15/23 17:01:26.451
    Nov 15 17:01:26.516: INFO: successfully validated that service endpoint-test2 in namespace services-3393 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 11/15/23 17:01:26.516
    Nov 15 17:01:27.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-3393 exec execpod9gvjq -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Nov 15 17:01:27.982: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Nov 15 17:01:27.982: INFO: stdout: ""
    Nov 15 17:01:27.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-3393 exec execpod9gvjq -- /bin/sh -x -c nc -v -z -w 2 172.21.32.40 80'
    Nov 15 17:01:28.425: INFO: stderr: "+ nc -v -z -w 2 172.21.32.40 80\nConnection to 172.21.32.40 80 port [tcp/http] succeeded!\n"
    Nov 15 17:01:28.425: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-3393 11/15/23 17:01:28.425
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3393 to expose endpoints map[pod2:[80]] 11/15/23 17:01:28.464
    Nov 15 17:01:28.517: INFO: successfully validated that service endpoint-test2 in namespace services-3393 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 11/15/23 17:01:28.518
    Nov 15 17:01:29.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-3393 exec execpod9gvjq -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Nov 15 17:01:31.053: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Nov 15 17:01:31.053: INFO: stdout: ""
    Nov 15 17:01:31.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=services-3393 exec execpod9gvjq -- /bin/sh -x -c nc -v -z -w 2 172.21.32.40 80'
    Nov 15 17:01:32.528: INFO: stderr: "+ nc -v -z -w 2 172.21.32.40 80\nConnection to 172.21.32.40 80 port [tcp/http] succeeded!\n"
    Nov 15 17:01:32.528: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-3393 11/15/23 17:01:32.528
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3393 to expose endpoints map[] 11/15/23 17:01:32.587
    Nov 15 17:01:32.631: INFO: successfully validated that service endpoint-test2 in namespace services-3393 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:01:32.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3393" for this suite. 11/15/23 17:01:32.739
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:01:32.771
Nov 15 17:01:32.771: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename pods 11/15/23 17:01:32.772
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:01:32.827
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:01:32.843
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 11/15/23 17:01:32.9
STEP: watching for Pod to be ready 11/15/23 17:01:32.936
Nov 15 17:01:32.951: INFO: observed Pod pod-test in namespace pods-342 in phase Pending with labels: map[test-pod-static:true] & conditions []
Nov 15 17:01:32.958: INFO: observed Pod pod-test in namespace pods-342 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 17:01:32 +0000 UTC  }]
Nov 15 17:01:33.012: INFO: observed Pod pod-test in namespace pods-342 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 17:01:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 17:01:32 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 17:01:32 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 17:01:32 +0000 UTC  }]
Nov 15 17:01:33.835: INFO: observed Pod pod-test in namespace pods-342 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 17:01:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 17:01:32 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 17:01:32 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 17:01:32 +0000 UTC  }]
Nov 15 17:01:34.410: INFO: Found Pod pod-test in namespace pods-342 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 17:01:32 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 17:01:34 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 17:01:34 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 17:01:32 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 11/15/23 17:01:34.427
STEP: getting the Pod and ensuring that it's patched 11/15/23 17:01:34.467
STEP: replacing the Pod's status Ready condition to False 11/15/23 17:01:34.482
STEP: check the Pod again to ensure its Ready conditions are False 11/15/23 17:01:34.522
STEP: deleting the Pod via a Collection with a LabelSelector 11/15/23 17:01:34.523
STEP: watching for the Pod to be deleted 11/15/23 17:01:34.556
Nov 15 17:01:34.566: INFO: observed event type MODIFIED
Nov 15 17:01:36.418: INFO: observed event type MODIFIED
Nov 15 17:01:36.812: INFO: observed event type MODIFIED
Nov 15 17:01:37.433: INFO: observed event type MODIFIED
Nov 15 17:01:37.467: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Nov 15 17:01:37.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-342" for this suite. 11/15/23 17:01:37.531
------------------------------
• [4.786 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:01:32.771
    Nov 15 17:01:32.771: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename pods 11/15/23 17:01:32.772
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:01:32.827
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:01:32.843
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 11/15/23 17:01:32.9
    STEP: watching for Pod to be ready 11/15/23 17:01:32.936
    Nov 15 17:01:32.951: INFO: observed Pod pod-test in namespace pods-342 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Nov 15 17:01:32.958: INFO: observed Pod pod-test in namespace pods-342 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 17:01:32 +0000 UTC  }]
    Nov 15 17:01:33.012: INFO: observed Pod pod-test in namespace pods-342 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 17:01:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 17:01:32 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 17:01:32 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 17:01:32 +0000 UTC  }]
    Nov 15 17:01:33.835: INFO: observed Pod pod-test in namespace pods-342 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 17:01:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 17:01:32 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-15 17:01:32 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 17:01:32 +0000 UTC  }]
    Nov 15 17:01:34.410: INFO: Found Pod pod-test in namespace pods-342 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 17:01:32 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 17:01:34 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 17:01:34 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-15 17:01:32 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 11/15/23 17:01:34.427
    STEP: getting the Pod and ensuring that it's patched 11/15/23 17:01:34.467
    STEP: replacing the Pod's status Ready condition to False 11/15/23 17:01:34.482
    STEP: check the Pod again to ensure its Ready conditions are False 11/15/23 17:01:34.522
    STEP: deleting the Pod via a Collection with a LabelSelector 11/15/23 17:01:34.523
    STEP: watching for the Pod to be deleted 11/15/23 17:01:34.556
    Nov 15 17:01:34.566: INFO: observed event type MODIFIED
    Nov 15 17:01:36.418: INFO: observed event type MODIFIED
    Nov 15 17:01:36.812: INFO: observed event type MODIFIED
    Nov 15 17:01:37.433: INFO: observed event type MODIFIED
    Nov 15 17:01:37.467: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:01:37.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-342" for this suite. 11/15/23 17:01:37.531
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:01:37.56
Nov 15 17:01:37.560: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename crd-publish-openapi 11/15/23 17:01:37.561
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:01:37.61
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:01:37.627
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Nov 15 17:01:37.674: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 11/15/23 17:01:40.247
Nov 15 17:01:40.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-8151 --namespace=crd-publish-openapi-8151 create -f -'
Nov 15 17:01:41.138: INFO: stderr: ""
Nov 15 17:01:41.139: INFO: stdout: "e2e-test-crd-publish-openapi-7905-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Nov 15 17:01:41.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-8151 --namespace=crd-publish-openapi-8151 delete e2e-test-crd-publish-openapi-7905-crds test-cr'
Nov 15 17:01:41.311: INFO: stderr: ""
Nov 15 17:01:41.311: INFO: stdout: "e2e-test-crd-publish-openapi-7905-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Nov 15 17:01:41.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-8151 --namespace=crd-publish-openapi-8151 apply -f -'
Nov 15 17:01:42.125: INFO: stderr: ""
Nov 15 17:01:42.125: INFO: stdout: "e2e-test-crd-publish-openapi-7905-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Nov 15 17:01:42.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-8151 --namespace=crd-publish-openapi-8151 delete e2e-test-crd-publish-openapi-7905-crds test-cr'
Nov 15 17:01:42.300: INFO: stderr: ""
Nov 15 17:01:42.300: INFO: stdout: "e2e-test-crd-publish-openapi-7905-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 11/15/23 17:01:42.3
Nov 15 17:01:42.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-8151 explain e2e-test-crd-publish-openapi-7905-crds'
Nov 15 17:01:42.638: INFO: stderr: ""
Nov 15 17:01:42.638: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7905-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 17:01:45.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8151" for this suite. 11/15/23 17:01:45.13
------------------------------
• [SLOW TEST] [7.593 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:01:37.56
    Nov 15 17:01:37.560: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename crd-publish-openapi 11/15/23 17:01:37.561
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:01:37.61
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:01:37.627
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Nov 15 17:01:37.674: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 11/15/23 17:01:40.247
    Nov 15 17:01:40.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-8151 --namespace=crd-publish-openapi-8151 create -f -'
    Nov 15 17:01:41.138: INFO: stderr: ""
    Nov 15 17:01:41.139: INFO: stdout: "e2e-test-crd-publish-openapi-7905-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Nov 15 17:01:41.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-8151 --namespace=crd-publish-openapi-8151 delete e2e-test-crd-publish-openapi-7905-crds test-cr'
    Nov 15 17:01:41.311: INFO: stderr: ""
    Nov 15 17:01:41.311: INFO: stdout: "e2e-test-crd-publish-openapi-7905-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Nov 15 17:01:41.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-8151 --namespace=crd-publish-openapi-8151 apply -f -'
    Nov 15 17:01:42.125: INFO: stderr: ""
    Nov 15 17:01:42.125: INFO: stdout: "e2e-test-crd-publish-openapi-7905-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Nov 15 17:01:42.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-8151 --namespace=crd-publish-openapi-8151 delete e2e-test-crd-publish-openapi-7905-crds test-cr'
    Nov 15 17:01:42.300: INFO: stderr: ""
    Nov 15 17:01:42.300: INFO: stdout: "e2e-test-crd-publish-openapi-7905-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 11/15/23 17:01:42.3
    Nov 15 17:01:42.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-8151 explain e2e-test-crd-publish-openapi-7905-crds'
    Nov 15 17:01:42.638: INFO: stderr: ""
    Nov 15 17:01:42.638: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7905-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:01:45.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8151" for this suite. 11/15/23 17:01:45.13
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:01:45.17
Nov 15 17:01:45.171: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename daemonsets 11/15/23 17:01:45.173
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:01:45.214
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:01:45.232
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834
STEP: Creating simple DaemonSet "daemon-set" 11/15/23 17:01:45.351
STEP: Check that daemon pods launch on every node of the cluster. 11/15/23 17:01:45.369
Nov 15 17:01:45.405: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 17:01:45.405: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
Nov 15 17:01:46.456: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 17:01:46.456: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
Nov 15 17:01:47.447: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Nov 15 17:01:47.447: INFO: Node 10.15.40.114 is running 0 daemon pod, expected 1
Nov 15 17:01:48.446: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 15 17:01:48.447: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 11/15/23 17:01:48.461
STEP: DeleteCollection of the DaemonSets 11/15/23 17:01:48.478
STEP: Verify that ReplicaSets have been deleted 11/15/23 17:01:48.504
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
Nov 15 17:01:48.548: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"49866"},"items":null}

Nov 15 17:01:48.570: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"49869"},"items":[{"metadata":{"name":"daemon-set-qczqz","generateName":"daemon-set-","namespace":"daemonsets-1025","uid":"e313e718-1232-4571-b705-4c0058157fb3","resourceVersion":"49869","creationTimestamp":"2023-11-15T17:01:45Z","deletionTimestamp":"2023-11-15T17:02:18Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"a30476afecb46c34d4c4f06f0fafd085c5603d342da17a7b283c1b9e2ff4e71f","cni.projectcalico.org/podIP":"172.30.164.56/32","cni.projectcalico.org/podIPs":"172.30.164.56/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"2ceba867-9e40-4a2b-97b7-219bb773fc7e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-11-15T17:01:45Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2ceba867-9e40-4a2b-97b7-219bb773fc7e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-11-15T17:01:46Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-11-15T17:01:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.164.56\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-2psjf","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-2psjf","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.15.40.115","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.15.40.115"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T17:01:45Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T17:01:47Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T17:01:47Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T17:01:45Z"}],"hostIP":"10.15.40.115","podIP":"172.30.164.56","podIPs":[{"ip":"172.30.164.56"}],"startTime":"2023-11-15T17:01:45Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-11-15T17:01:46Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://46adbac4d05b44cb852527b152a63f7a8bd67866b26f0d33ccd845ea6124cc7d","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-rwpqd","generateName":"daemon-set-","namespace":"daemonsets-1025","uid":"ab5d5389-e7f7-4957-9e45-987d7efea102","resourceVersion":"49868","creationTimestamp":"2023-11-15T17:01:45Z","deletionTimestamp":"2023-11-15T17:02:18Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"f08560d71b3a0917ba2954682ccd28bcda5a2560500a252ffe8f318efafa58ac","cni.projectcalico.org/podIP":"172.30.205.255/32","cni.projectcalico.org/podIPs":"172.30.205.255/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"2ceba867-9e40-4a2b-97b7-219bb773fc7e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-11-15T17:01:45Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2ceba867-9e40-4a2b-97b7-219bb773fc7e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-11-15T17:01:46Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-11-15T17:01:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.205.255\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-5bv4l","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-5bv4l","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.15.40.114","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.15.40.114"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T17:01:45Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T17:01:47Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T17:01:47Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T17:01:45Z"}],"hostIP":"10.15.40.114","podIP":"172.30.205.255","podIPs":[{"ip":"172.30.205.255"}],"startTime":"2023-11-15T17:01:45Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-11-15T17:01:46Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://1a596e75c4fd2f097899e174eeb6c83ca0dc989a1c8b5f672672dd785c7dda99","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-xkq5l","generateName":"daemon-set-","namespace":"daemonsets-1025","uid":"0cd9005c-a8dd-4b54-9ded-83ed3beee969","resourceVersion":"49867","creationTimestamp":"2023-11-15T17:01:45Z","deletionTimestamp":"2023-11-15T17:02:18Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"d7db020c2dae4c6d84e16a3aa687a3ab643c5eda73ce64e1c4cb5fb71f5cff39","cni.projectcalico.org/podIP":"172.30.191.97/32","cni.projectcalico.org/podIPs":"172.30.191.97/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"2ceba867-9e40-4a2b-97b7-219bb773fc7e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-11-15T17:01:45Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2ceba867-9e40-4a2b-97b7-219bb773fc7e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-11-15T17:01:46Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-11-15T17:01:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.191.97\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-pqzp6","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-pqzp6","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.15.40.106","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.15.40.106"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T17:01:45Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T17:01:47Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T17:01:47Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T17:01:45Z"}],"hostIP":"10.15.40.106","podIP":"172.30.191.97","podIPs":[{"ip":"172.30.191.97"}],"startTime":"2023-11-15T17:01:45Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-11-15T17:01:46Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://4759a6292c4d253c7eb00a868b7ce2f53bef97b43e9c9bd0c8bf771e5ef9f2d0","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 17:01:48.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1025" for this suite. 11/15/23 17:01:48.713
------------------------------
• [3.572 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:01:45.17
    Nov 15 17:01:45.171: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename daemonsets 11/15/23 17:01:45.173
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:01:45.214
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:01:45.232
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:834
    STEP: Creating simple DaemonSet "daemon-set" 11/15/23 17:01:45.351
    STEP: Check that daemon pods launch on every node of the cluster. 11/15/23 17:01:45.369
    Nov 15 17:01:45.405: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 17:01:45.405: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
    Nov 15 17:01:46.456: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 17:01:46.456: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
    Nov 15 17:01:47.447: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Nov 15 17:01:47.447: INFO: Node 10.15.40.114 is running 0 daemon pod, expected 1
    Nov 15 17:01:48.446: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Nov 15 17:01:48.447: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 11/15/23 17:01:48.461
    STEP: DeleteCollection of the DaemonSets 11/15/23 17:01:48.478
    STEP: Verify that ReplicaSets have been deleted 11/15/23 17:01:48.504
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    Nov 15 17:01:48.548: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"49866"},"items":null}

    Nov 15 17:01:48.570: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"49869"},"items":[{"metadata":{"name":"daemon-set-qczqz","generateName":"daemon-set-","namespace":"daemonsets-1025","uid":"e313e718-1232-4571-b705-4c0058157fb3","resourceVersion":"49869","creationTimestamp":"2023-11-15T17:01:45Z","deletionTimestamp":"2023-11-15T17:02:18Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"a30476afecb46c34d4c4f06f0fafd085c5603d342da17a7b283c1b9e2ff4e71f","cni.projectcalico.org/podIP":"172.30.164.56/32","cni.projectcalico.org/podIPs":"172.30.164.56/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"2ceba867-9e40-4a2b-97b7-219bb773fc7e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-11-15T17:01:45Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2ceba867-9e40-4a2b-97b7-219bb773fc7e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-11-15T17:01:46Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-11-15T17:01:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.164.56\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-2psjf","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-2psjf","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.15.40.115","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.15.40.115"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T17:01:45Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T17:01:47Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T17:01:47Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T17:01:45Z"}],"hostIP":"10.15.40.115","podIP":"172.30.164.56","podIPs":[{"ip":"172.30.164.56"}],"startTime":"2023-11-15T17:01:45Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-11-15T17:01:46Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://46adbac4d05b44cb852527b152a63f7a8bd67866b26f0d33ccd845ea6124cc7d","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-rwpqd","generateName":"daemon-set-","namespace":"daemonsets-1025","uid":"ab5d5389-e7f7-4957-9e45-987d7efea102","resourceVersion":"49868","creationTimestamp":"2023-11-15T17:01:45Z","deletionTimestamp":"2023-11-15T17:02:18Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"f08560d71b3a0917ba2954682ccd28bcda5a2560500a252ffe8f318efafa58ac","cni.projectcalico.org/podIP":"172.30.205.255/32","cni.projectcalico.org/podIPs":"172.30.205.255/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"2ceba867-9e40-4a2b-97b7-219bb773fc7e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-11-15T17:01:45Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2ceba867-9e40-4a2b-97b7-219bb773fc7e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-11-15T17:01:46Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-11-15T17:01:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.205.255\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-5bv4l","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-5bv4l","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.15.40.114","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.15.40.114"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T17:01:45Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T17:01:47Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T17:01:47Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T17:01:45Z"}],"hostIP":"10.15.40.114","podIP":"172.30.205.255","podIPs":[{"ip":"172.30.205.255"}],"startTime":"2023-11-15T17:01:45Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-11-15T17:01:46Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://1a596e75c4fd2f097899e174eeb6c83ca0dc989a1c8b5f672672dd785c7dda99","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-xkq5l","generateName":"daemon-set-","namespace":"daemonsets-1025","uid":"0cd9005c-a8dd-4b54-9ded-83ed3beee969","resourceVersion":"49867","creationTimestamp":"2023-11-15T17:01:45Z","deletionTimestamp":"2023-11-15T17:02:18Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"d7db020c2dae4c6d84e16a3aa687a3ab643c5eda73ce64e1c4cb5fb71f5cff39","cni.projectcalico.org/podIP":"172.30.191.97/32","cni.projectcalico.org/podIPs":"172.30.191.97/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"2ceba867-9e40-4a2b-97b7-219bb773fc7e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-11-15T17:01:45Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2ceba867-9e40-4a2b-97b7-219bb773fc7e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-11-15T17:01:46Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-11-15T17:01:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.191.97\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-pqzp6","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-pqzp6","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.15.40.106","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.15.40.106"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T17:01:45Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T17:01:47Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T17:01:47Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-15T17:01:45Z"}],"hostIP":"10.15.40.106","podIP":"172.30.191.97","podIPs":[{"ip":"172.30.191.97"}],"startTime":"2023-11-15T17:01:45Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-11-15T17:01:46Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://4759a6292c4d253c7eb00a868b7ce2f53bef97b43e9c9bd0c8bf771e5ef9f2d0","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:01:48.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1025" for this suite. 11/15/23 17:01:48.713
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:01:48.754
Nov 15 17:01:48.754: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename webhook 11/15/23 17:01:48.755
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:01:48.814
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:01:48.834
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/15/23 17:01:48.911
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 17:01:49.325
STEP: Deploying the webhook pod 11/15/23 17:01:49.37
STEP: Wait for the deployment to be ready 11/15/23 17:01:49.423
Nov 15 17:01:49.467: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 15 17:01:51.529: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 17, 1, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 17, 1, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 17, 1, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 17, 1, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 11/15/23 17:01:53.548
STEP: Verifying the service has paired with the endpoint 11/15/23 17:01:53.602
Nov 15 17:01:54.603: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 11/15/23 17:01:54.626
STEP: Creating a configMap that does not comply to the validation webhook rules 11/15/23 17:01:54.749
STEP: Updating a validating webhook configuration's rules to not include the create operation 11/15/23 17:01:54.874
STEP: Creating a configMap that does not comply to the validation webhook rules 11/15/23 17:01:54.914
STEP: Patching a validating webhook configuration's rules to include the create operation 11/15/23 17:01:54.962
STEP: Creating a configMap that does not comply to the validation webhook rules 11/15/23 17:01:54.987
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 17:01:55.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6160" for this suite. 11/15/23 17:01:55.235
STEP: Destroying namespace "webhook-6160-markers" for this suite. 11/15/23 17:01:55.26
------------------------------
• [SLOW TEST] [6.539 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:01:48.754
    Nov 15 17:01:48.754: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename webhook 11/15/23 17:01:48.755
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:01:48.814
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:01:48.834
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/15/23 17:01:48.911
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 17:01:49.325
    STEP: Deploying the webhook pod 11/15/23 17:01:49.37
    STEP: Wait for the deployment to be ready 11/15/23 17:01:49.423
    Nov 15 17:01:49.467: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Nov 15 17:01:51.529: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 17, 1, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 17, 1, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 17, 1, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 17, 1, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 11/15/23 17:01:53.548
    STEP: Verifying the service has paired with the endpoint 11/15/23 17:01:53.602
    Nov 15 17:01:54.603: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 11/15/23 17:01:54.626
    STEP: Creating a configMap that does not comply to the validation webhook rules 11/15/23 17:01:54.749
    STEP: Updating a validating webhook configuration's rules to not include the create operation 11/15/23 17:01:54.874
    STEP: Creating a configMap that does not comply to the validation webhook rules 11/15/23 17:01:54.914
    STEP: Patching a validating webhook configuration's rules to include the create operation 11/15/23 17:01:54.962
    STEP: Creating a configMap that does not comply to the validation webhook rules 11/15/23 17:01:54.987
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:01:55.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6160" for this suite. 11/15/23 17:01:55.235
    STEP: Destroying namespace "webhook-6160-markers" for this suite. 11/15/23 17:01:55.26
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:01:55.298
Nov 15 17:01:55.298: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename services 11/15/23 17:01:55.301
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:01:55.423
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:01:55.442
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 11/15/23 17:01:55.491
STEP: waiting for available Endpoint 11/15/23 17:01:55.515
STEP: listing all Endpoints 11/15/23 17:01:55.525
STEP: updating the Endpoint 11/15/23 17:01:55.548
STEP: fetching the Endpoint 11/15/23 17:01:55.581
STEP: patching the Endpoint 11/15/23 17:01:55.602
STEP: fetching the Endpoint 11/15/23 17:01:55.643
STEP: deleting the Endpoint by Collection 11/15/23 17:01:55.663
STEP: waiting for Endpoint deletion 11/15/23 17:01:55.719
STEP: fetching the Endpoint 11/15/23 17:01:55.729
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 15 17:01:55.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5956" for this suite. 11/15/23 17:01:55.779
------------------------------
• [0.512 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:01:55.298
    Nov 15 17:01:55.298: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename services 11/15/23 17:01:55.301
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:01:55.423
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:01:55.442
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 11/15/23 17:01:55.491
    STEP: waiting for available Endpoint 11/15/23 17:01:55.515
    STEP: listing all Endpoints 11/15/23 17:01:55.525
    STEP: updating the Endpoint 11/15/23 17:01:55.548
    STEP: fetching the Endpoint 11/15/23 17:01:55.581
    STEP: patching the Endpoint 11/15/23 17:01:55.602
    STEP: fetching the Endpoint 11/15/23 17:01:55.643
    STEP: deleting the Endpoint by Collection 11/15/23 17:01:55.663
    STEP: waiting for Endpoint deletion 11/15/23 17:01:55.719
    STEP: fetching the Endpoint 11/15/23 17:01:55.729
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:01:55.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5956" for this suite. 11/15/23 17:01:55.779
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:01:55.816
Nov 15 17:01:55.816: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename configmap 11/15/23 17:01:55.818
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:01:55.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:01:55.9
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-76db1277-2883-46e2-b992-5eb638fb60a9 11/15/23 17:01:55.919
STEP: Creating a pod to test consume configMaps 11/15/23 17:01:55.94
Nov 15 17:01:55.979: INFO: Waiting up to 5m0s for pod "pod-configmaps-6c0d907f-d337-450e-b27e-9af469e154d5" in namespace "configmap-5852" to be "Succeeded or Failed"
Nov 15 17:01:56.002: INFO: Pod "pod-configmaps-6c0d907f-d337-450e-b27e-9af469e154d5": Phase="Pending", Reason="", readiness=false. Elapsed: 23.096116ms
Nov 15 17:01:58.021: INFO: Pod "pod-configmaps-6c0d907f-d337-450e-b27e-9af469e154d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041990851s
Nov 15 17:02:00.025: INFO: Pod "pod-configmaps-6c0d907f-d337-450e-b27e-9af469e154d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045471321s
STEP: Saw pod success 11/15/23 17:02:00.025
Nov 15 17:02:00.025: INFO: Pod "pod-configmaps-6c0d907f-d337-450e-b27e-9af469e154d5" satisfied condition "Succeeded or Failed"
Nov 15 17:02:00.048: INFO: Trying to get logs from node 10.15.40.115 pod pod-configmaps-6c0d907f-d337-450e-b27e-9af469e154d5 container agnhost-container: <nil>
STEP: delete the pod 11/15/23 17:02:00.172
Nov 15 17:02:00.217: INFO: Waiting for pod pod-configmaps-6c0d907f-d337-450e-b27e-9af469e154d5 to disappear
Nov 15 17:02:00.234: INFO: Pod pod-configmaps-6c0d907f-d337-450e-b27e-9af469e154d5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 15 17:02:00.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5852" for this suite. 11/15/23 17:02:00.264
------------------------------
• [4.480 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:01:55.816
    Nov 15 17:01:55.816: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename configmap 11/15/23 17:01:55.818
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:01:55.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:01:55.9
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-76db1277-2883-46e2-b992-5eb638fb60a9 11/15/23 17:01:55.919
    STEP: Creating a pod to test consume configMaps 11/15/23 17:01:55.94
    Nov 15 17:01:55.979: INFO: Waiting up to 5m0s for pod "pod-configmaps-6c0d907f-d337-450e-b27e-9af469e154d5" in namespace "configmap-5852" to be "Succeeded or Failed"
    Nov 15 17:01:56.002: INFO: Pod "pod-configmaps-6c0d907f-d337-450e-b27e-9af469e154d5": Phase="Pending", Reason="", readiness=false. Elapsed: 23.096116ms
    Nov 15 17:01:58.021: INFO: Pod "pod-configmaps-6c0d907f-d337-450e-b27e-9af469e154d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041990851s
    Nov 15 17:02:00.025: INFO: Pod "pod-configmaps-6c0d907f-d337-450e-b27e-9af469e154d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045471321s
    STEP: Saw pod success 11/15/23 17:02:00.025
    Nov 15 17:02:00.025: INFO: Pod "pod-configmaps-6c0d907f-d337-450e-b27e-9af469e154d5" satisfied condition "Succeeded or Failed"
    Nov 15 17:02:00.048: INFO: Trying to get logs from node 10.15.40.115 pod pod-configmaps-6c0d907f-d337-450e-b27e-9af469e154d5 container agnhost-container: <nil>
    STEP: delete the pod 11/15/23 17:02:00.172
    Nov 15 17:02:00.217: INFO: Waiting for pod pod-configmaps-6c0d907f-d337-450e-b27e-9af469e154d5 to disappear
    Nov 15 17:02:00.234: INFO: Pod pod-configmaps-6c0d907f-d337-450e-b27e-9af469e154d5 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:02:00.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5852" for this suite. 11/15/23 17:02:00.264
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:02:00.298
Nov 15 17:02:00.298: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename crd-publish-openapi 11/15/23 17:02:00.302
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:02:00.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:02:00.376
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Nov 15 17:02:00.398: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 11/15/23 17:02:02.511
Nov 15 17:02:02.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-2189 --namespace=crd-publish-openapi-2189 create -f -'
Nov 15 17:02:03.550: INFO: stderr: ""
Nov 15 17:02:03.550: INFO: stdout: "e2e-test-crd-publish-openapi-5084-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Nov 15 17:02:03.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-2189 --namespace=crd-publish-openapi-2189 delete e2e-test-crd-publish-openapi-5084-crds test-cr'
Nov 15 17:02:03.749: INFO: stderr: ""
Nov 15 17:02:03.750: INFO: stdout: "e2e-test-crd-publish-openapi-5084-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Nov 15 17:02:03.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-2189 --namespace=crd-publish-openapi-2189 apply -f -'
Nov 15 17:02:04.636: INFO: stderr: ""
Nov 15 17:02:04.636: INFO: stdout: "e2e-test-crd-publish-openapi-5084-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Nov 15 17:02:04.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-2189 --namespace=crd-publish-openapi-2189 delete e2e-test-crd-publish-openapi-5084-crds test-cr'
Nov 15 17:02:04.916: INFO: stderr: ""
Nov 15 17:02:04.916: INFO: stdout: "e2e-test-crd-publish-openapi-5084-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 11/15/23 17:02:04.916
Nov 15 17:02:04.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-2189 explain e2e-test-crd-publish-openapi-5084-crds'
Nov 15 17:02:05.230: INFO: stderr: ""
Nov 15 17:02:05.230: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5084-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 17:02:07.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2189" for this suite. 11/15/23 17:02:07.615
------------------------------
• [SLOW TEST] [7.343 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:02:00.298
    Nov 15 17:02:00.298: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename crd-publish-openapi 11/15/23 17:02:00.302
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:02:00.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:02:00.376
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Nov 15 17:02:00.398: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 11/15/23 17:02:02.511
    Nov 15 17:02:02.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-2189 --namespace=crd-publish-openapi-2189 create -f -'
    Nov 15 17:02:03.550: INFO: stderr: ""
    Nov 15 17:02:03.550: INFO: stdout: "e2e-test-crd-publish-openapi-5084-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Nov 15 17:02:03.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-2189 --namespace=crd-publish-openapi-2189 delete e2e-test-crd-publish-openapi-5084-crds test-cr'
    Nov 15 17:02:03.749: INFO: stderr: ""
    Nov 15 17:02:03.750: INFO: stdout: "e2e-test-crd-publish-openapi-5084-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Nov 15 17:02:03.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-2189 --namespace=crd-publish-openapi-2189 apply -f -'
    Nov 15 17:02:04.636: INFO: stderr: ""
    Nov 15 17:02:04.636: INFO: stdout: "e2e-test-crd-publish-openapi-5084-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Nov 15 17:02:04.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-2189 --namespace=crd-publish-openapi-2189 delete e2e-test-crd-publish-openapi-5084-crds test-cr'
    Nov 15 17:02:04.916: INFO: stderr: ""
    Nov 15 17:02:04.916: INFO: stdout: "e2e-test-crd-publish-openapi-5084-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 11/15/23 17:02:04.916
    Nov 15 17:02:04.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-831742819 --namespace=crd-publish-openapi-2189 explain e2e-test-crd-publish-openapi-5084-crds'
    Nov 15 17:02:05.230: INFO: stderr: ""
    Nov 15 17:02:05.230: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5084-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:02:07.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2189" for this suite. 11/15/23 17:02:07.615
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:02:07.648
Nov 15 17:02:07.648: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename events 11/15/23 17:02:07.65
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:02:07.7
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:02:07.717
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 11/15/23 17:02:07.74
STEP: get a list of Events with a label in the current namespace 11/15/23 17:02:07.804
STEP: delete a list of events 11/15/23 17:02:07.819
Nov 15 17:02:07.820: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 11/15/23 17:02:07.902
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Nov 15 17:02:07.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-4318" for this suite. 11/15/23 17:02:07.944
------------------------------
• [0.322 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:02:07.648
    Nov 15 17:02:07.648: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename events 11/15/23 17:02:07.65
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:02:07.7
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:02:07.717
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 11/15/23 17:02:07.74
    STEP: get a list of Events with a label in the current namespace 11/15/23 17:02:07.804
    STEP: delete a list of events 11/15/23 17:02:07.819
    Nov 15 17:02:07.820: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 11/15/23 17:02:07.902
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:02:07.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-4318" for this suite. 11/15/23 17:02:07.944
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:02:07.971
Nov 15 17:02:07.971: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename container-lifecycle-hook 11/15/23 17:02:07.974
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:02:08.027
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:02:08.042
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 11/15/23 17:02:08.081
Nov 15 17:02:08.115: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6093" to be "running and ready"
Nov 15 17:02:08.132: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 17.169069ms
Nov 15 17:02:08.133: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 15 17:02:10.148: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03315478s
Nov 15 17:02:10.148: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 15 17:02:12.148: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.033114177s
Nov 15 17:02:12.149: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Nov 15 17:02:12.149: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 11/15/23 17:02:12.164
Nov 15 17:02:12.185: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-6093" to be "running and ready"
Nov 15 17:02:12.200: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 14.519952ms
Nov 15 17:02:12.200: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 15 17:02:14.215: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.029738292s
Nov 15 17:02:14.215: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Nov 15 17:02:14.215: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 11/15/23 17:02:14.229
STEP: delete the pod with lifecycle hook 11/15/23 17:02:14.271
Nov 15 17:02:14.300: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 15 17:02:14.317: INFO: Pod pod-with-poststart-http-hook still exists
Nov 15 17:02:16.317: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 15 17:02:16.334: INFO: Pod pod-with-poststart-http-hook still exists
Nov 15 17:02:18.317: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 15 17:02:18.334: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Nov 15 17:02:18.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-6093" for this suite. 11/15/23 17:02:18.357
------------------------------
• [SLOW TEST] [10.413 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:02:07.971
    Nov 15 17:02:07.971: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename container-lifecycle-hook 11/15/23 17:02:07.974
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:02:08.027
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:02:08.042
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 11/15/23 17:02:08.081
    Nov 15 17:02:08.115: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6093" to be "running and ready"
    Nov 15 17:02:08.132: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 17.169069ms
    Nov 15 17:02:08.133: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 17:02:10.148: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03315478s
    Nov 15 17:02:10.148: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 17:02:12.148: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.033114177s
    Nov 15 17:02:12.149: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Nov 15 17:02:12.149: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 11/15/23 17:02:12.164
    Nov 15 17:02:12.185: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-6093" to be "running and ready"
    Nov 15 17:02:12.200: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 14.519952ms
    Nov 15 17:02:12.200: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 17:02:14.215: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.029738292s
    Nov 15 17:02:14.215: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Nov 15 17:02:14.215: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 11/15/23 17:02:14.229
    STEP: delete the pod with lifecycle hook 11/15/23 17:02:14.271
    Nov 15 17:02:14.300: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Nov 15 17:02:14.317: INFO: Pod pod-with-poststart-http-hook still exists
    Nov 15 17:02:16.317: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Nov 15 17:02:16.334: INFO: Pod pod-with-poststart-http-hook still exists
    Nov 15 17:02:18.317: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Nov 15 17:02:18.334: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:02:18.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-6093" for this suite. 11/15/23 17:02:18.357
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:02:18.386
Nov 15 17:02:18.387: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename disruption 11/15/23 17:02:18.39
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:02:18.446
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:02:18.461
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:02:18.476
Nov 15 17:02:18.477: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename disruption-2 11/15/23 17:02:18.479
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:02:18.537
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:02:18.551
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 11/15/23 17:02:18.587
STEP: Waiting for the pdb to be processed 11/15/23 17:02:20.646
STEP: Waiting for the pdb to be processed 11/15/23 17:02:22.701
STEP: listing a collection of PDBs across all namespaces 11/15/23 17:02:22.719
STEP: listing a collection of PDBs in namespace disruption-9381 11/15/23 17:02:22.737
STEP: deleting a collection of PDBs 11/15/23 17:02:22.754
STEP: Waiting for the PDB collection to be deleted 11/15/23 17:02:22.806
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Nov 15 17:02:22.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Nov 15 17:02:22.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-4236" for this suite. 11/15/23 17:02:22.869
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-9381" for this suite. 11/15/23 17:02:22.897
------------------------------
• [4.537 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:02:18.386
    Nov 15 17:02:18.387: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename disruption 11/15/23 17:02:18.39
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:02:18.446
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:02:18.461
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:02:18.476
    Nov 15 17:02:18.477: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename disruption-2 11/15/23 17:02:18.479
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:02:18.537
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:02:18.551
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 11/15/23 17:02:18.587
    STEP: Waiting for the pdb to be processed 11/15/23 17:02:20.646
    STEP: Waiting for the pdb to be processed 11/15/23 17:02:22.701
    STEP: listing a collection of PDBs across all namespaces 11/15/23 17:02:22.719
    STEP: listing a collection of PDBs in namespace disruption-9381 11/15/23 17:02:22.737
    STEP: deleting a collection of PDBs 11/15/23 17:02:22.754
    STEP: Waiting for the PDB collection to be deleted 11/15/23 17:02:22.806
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:02:22.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:02:22.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-4236" for this suite. 11/15/23 17:02:22.869
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-9381" for this suite. 11/15/23 17:02:22.897
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:02:22.931
Nov 15 17:02:22.932: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename sched-pred 11/15/23 17:02:22.933
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:02:22.981
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:02:22.998
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Nov 15 17:02:23.015: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 15 17:02:23.057: INFO: Waiting for terminating namespaces to be deleted...
Nov 15 17:02:23.074: INFO: 
Logging pods the apiserver thinks is on node 10.15.40.106 before test
Nov 15 17:02:23.119: INFO: ibm-cloud-provider-ip-163-109-74-98-5d59656977-bgllw from ibm-system started at 2023-11-15 15:39:54 +0000 UTC (1 container statuses recorded)
Nov 15 17:02:23.119: INFO: 	Container ibm-cloud-provider-ip-163-109-74-98 ready: true, restart count 0
Nov 15 17:02:23.119: INFO: calico-kube-controllers-7847f7647d-srqd4 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 17:02:23.119: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Nov 15 17:02:23.119: INFO: calico-node-67csc from kube-system started at 2023-11-15 13:13:21 +0000 UTC (1 container statuses recorded)
Nov 15 17:02:23.119: INFO: 	Container calico-node ready: true, restart count 0
Nov 15 17:02:23.119: INFO: calico-typha-5b5db87f55-nwmh4 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 17:02:23.119: INFO: 	Container calico-typha ready: true, restart count 0
Nov 15 17:02:23.119: INFO: coredns-5845f98d4-2lnrf from kube-system started at 2023-11-15 13:23:15 +0000 UTC (1 container statuses recorded)
Nov 15 17:02:23.119: INFO: 	Container coredns ready: true, restart count 0
Nov 15 17:02:23.119: INFO: coredns-autoscaler-85f4bdddf6-qwxmw from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 17:02:23.119: INFO: 	Container autoscaler ready: true, restart count 0
Nov 15 17:02:23.119: INFO: dashboard-metrics-scraper-7cf679fbdf-frv7t from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 17:02:23.119: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Nov 15 17:02:23.119: INFO: ibm-file-plugin-557f875d5f-jrfnv from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 17:02:23.119: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Nov 15 17:02:23.119: INFO: ibm-keepalived-watcher-dqknf from kube-system started at 2023-11-15 13:13:21 +0000 UTC (1 container statuses recorded)
Nov 15 17:02:23.119: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 15 17:02:23.119: INFO: ibm-master-proxy-static-10.15.40.106 from kube-system started at 2023-11-15 13:13:08 +0000 UTC (2 container statuses recorded)
Nov 15 17:02:23.119: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 15 17:02:23.119: INFO: 	Container pause ready: true, restart count 0
Nov 15 17:02:23.119: INFO: ibm-storage-watcher-7d897847f4-k64t5 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 17:02:23.119: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Nov 15 17:02:23.119: INFO: ibmcloud-block-storage-driver-f97nm from kube-system started at 2023-11-15 13:13:30 +0000 UTC (1 container statuses recorded)
Nov 15 17:02:23.119: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 15 17:02:23.119: INFO: ibmcloud-block-storage-plugin-5bd59f7b48-2dxcj from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 17:02:23.119: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Nov 15 17:02:23.119: INFO: ingress-cluster-healthcheck-5985f966bb-jgjx8 from kube-system started at 2023-11-15 15:39:54 +0000 UTC (1 container statuses recorded)
Nov 15 17:02:23.119: INFO: 	Container ingress-cluster-healthcheck ready: true, restart count 0
Nov 15 17:02:23.119: INFO: konnectivity-agent-5brpz from kube-system started at 2023-11-15 13:22:38 +0000 UTC (1 container statuses recorded)
Nov 15 17:02:23.119: INFO: 	Container konnectivity-agent ready: true, restart count 0
Nov 15 17:02:23.119: INFO: kubernetes-dashboard-5ccdc9cbb8-wbmfz from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 17:02:23.119: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Nov 15 17:02:23.119: INFO: metrics-server-7cbd9c9b48-8tr74 from kube-system started at 2023-11-15 15:39:54 +0000 UTC (3 container statuses recorded)
Nov 15 17:02:23.119: INFO: 	Container config-watcher ready: true, restart count 0
Nov 15 17:02:23.119: INFO: 	Container metrics-server ready: true, restart count 0
Nov 15 17:02:23.119: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Nov 15 17:02:23.119: INFO: public-crclac150z0u38f277tnpg-alb1-6df9445bb6-qw2bt from kube-system started at 2023-11-15 15:39:54 +0000 UTC (1 container statuses recorded)
Nov 15 17:02:23.119: INFO: 	Container nginx-ingress ready: true, restart count 0
Nov 15 17:02:23.120: INFO: snapshot-controller-6db47fc545-5nnph from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 17:02:23.120: INFO: 	Container snapshot-controller ready: true, restart count 0
Nov 15 17:02:23.120: INFO: snapshot-controller-6db47fc545-h4q49 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 17:02:23.120: INFO: 	Container snapshot-controller ready: true, restart count 0
Nov 15 17:02:23.120: INFO: snapshot-controller-6db47fc545-khkb2 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
Nov 15 17:02:23.120: INFO: 	Container snapshot-controller ready: true, restart count 0
Nov 15 17:02:23.120: INFO: sonobuoy-systemd-logs-daemon-set-eabb9aa298c743f7-zxdnv from sonobuoy started at 2023-11-15 15:24:45 +0000 UTC (2 container statuses recorded)
Nov 15 17:02:23.120: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 15 17:02:23.120: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 15 17:02:23.120: INFO: 
Logging pods the apiserver thinks is on node 10.15.40.114 before test
Nov 15 17:02:23.161: INFO: ibm-cloud-provider-ip-163-109-74-98-5d59656977-wxlx6 from ibm-system started at 2023-11-15 13:18:32 +0000 UTC (1 container statuses recorded)
Nov 15 17:02:23.162: INFO: 	Container ibm-cloud-provider-ip-163-109-74-98 ready: true, restart count 0
Nov 15 17:02:23.162: INFO: calico-node-wqvj8 from kube-system started at 2023-11-15 13:13:28 +0000 UTC (1 container statuses recorded)
Nov 15 17:02:23.162: INFO: 	Container calico-node ready: true, restart count 0
Nov 15 17:02:23.162: INFO: calico-typha-5b5db87f55-4qv4c from kube-system started at 2023-11-15 13:13:53 +0000 UTC (1 container statuses recorded)
Nov 15 17:02:23.162: INFO: 	Container calico-typha ready: true, restart count 0
Nov 15 17:02:23.162: INFO: coredns-5845f98d4-6cld8 from kube-system started at 2023-11-15 13:23:15 +0000 UTC (1 container statuses recorded)
Nov 15 17:02:23.162: INFO: 	Container coredns ready: true, restart count 0
Nov 15 17:02:23.162: INFO: ibm-keepalived-watcher-xsslz from kube-system started at 2023-11-15 13:13:28 +0000 UTC (1 container statuses recorded)
Nov 15 17:02:23.162: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 15 17:02:23.162: INFO: ibm-master-proxy-static-10.15.40.114 from kube-system started at 2023-11-15 13:13:15 +0000 UTC (2 container statuses recorded)
Nov 15 17:02:23.162: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 15 17:02:23.162: INFO: 	Container pause ready: true, restart count 0
Nov 15 17:02:23.162: INFO: ibmcloud-block-storage-driver-4txx8 from kube-system started at 2023-11-15 13:13:37 +0000 UTC (1 container statuses recorded)
Nov 15 17:02:23.162: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 15 17:02:23.162: INFO: konnectivity-agent-zr4zl from kube-system started at 2023-11-15 13:22:41 +0000 UTC (1 container statuses recorded)
Nov 15 17:02:23.162: INFO: 	Container konnectivity-agent ready: true, restart count 0
Nov 15 17:02:23.162: INFO: metrics-server-7cbd9c9b48-cnskr from kube-system started at 2023-11-15 13:56:24 +0000 UTC (3 container statuses recorded)
Nov 15 17:02:23.162: INFO: 	Container config-watcher ready: true, restart count 0
Nov 15 17:02:23.162: INFO: 	Container metrics-server ready: true, restart count 0
Nov 15 17:02:23.162: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Nov 15 17:02:23.162: INFO: public-crclac150z0u38f277tnpg-alb1-6df9445bb6-jkpcd from kube-system started at 2023-11-15 13:23:30 +0000 UTC (1 container statuses recorded)
Nov 15 17:02:23.162: INFO: 	Container nginx-ingress ready: true, restart count 0
Nov 15 17:02:23.162: INFO: sonobuoy-e2e-job-3a51e32fa56b4156 from sonobuoy started at 2023-11-15 15:24:45 +0000 UTC (2 container statuses recorded)
Nov 15 17:02:23.163: INFO: 	Container e2e ready: true, restart count 0
Nov 15 17:02:23.163: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 15 17:02:23.163: INFO: sonobuoy-systemd-logs-daemon-set-eabb9aa298c743f7-sxg5b from sonobuoy started at 2023-11-15 15:24:45 +0000 UTC (2 container statuses recorded)
Nov 15 17:02:23.163: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 15 17:02:23.163: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 15 17:02:23.163: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-11-15 13:15:59 +0000 UTC (1 container statuses recorded)
Nov 15 17:02:23.163: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Nov 15 17:02:23.163: INFO: 
Logging pods the apiserver thinks is on node 10.15.40.115 before test
Nov 15 17:02:23.194: INFO: pod-handle-http-request from container-lifecycle-hook-6093 started at 2023-11-15 17:02:08 +0000 UTC (2 container statuses recorded)
Nov 15 17:02:23.194: INFO: 	Container container-handle-http-request ready: true, restart count 0
Nov 15 17:02:23.194: INFO: 	Container container-handle-https-request ready: true, restart count 0
Nov 15 17:02:23.194: INFO: calico-node-gknnt from kube-system started at 2023-11-15 13:13:40 +0000 UTC (1 container statuses recorded)
Nov 15 17:02:23.194: INFO: 	Container calico-node ready: true, restart count 0
Nov 15 17:02:23.194: INFO: calico-typha-5b5db87f55-fchsg from kube-system started at 2023-11-15 15:40:02 +0000 UTC (1 container statuses recorded)
Nov 15 17:02:23.194: INFO: 	Container calico-typha ready: true, restart count 0
Nov 15 17:02:23.194: INFO: coredns-5845f98d4-rt4sq from kube-system started at 2023-11-15 15:39:54 +0000 UTC (1 container statuses recorded)
Nov 15 17:02:23.194: INFO: 	Container coredns ready: true, restart count 0
Nov 15 17:02:23.194: INFO: ibm-keepalived-watcher-75lpq from kube-system started at 2023-11-15 13:13:40 +0000 UTC (1 container statuses recorded)
Nov 15 17:02:23.194: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 15 17:02:23.195: INFO: ibm-master-proxy-static-10.15.40.115 from kube-system started at 2023-11-15 13:13:28 +0000 UTC (2 container statuses recorded)
Nov 15 17:02:23.195: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 15 17:02:23.195: INFO: 	Container pause ready: true, restart count 0
Nov 15 17:02:23.195: INFO: ibmcloud-block-storage-driver-gbqth from kube-system started at 2023-11-15 13:13:49 +0000 UTC (1 container statuses recorded)
Nov 15 17:02:23.195: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Nov 15 17:02:23.196: INFO: konnectivity-agent-9pkld from kube-system started at 2023-11-15 13:22:45 +0000 UTC (1 container statuses recorded)
Nov 15 17:02:23.196: INFO: 	Container konnectivity-agent ready: true, restart count 0
Nov 15 17:02:23.196: INFO: sonobuoy from sonobuoy started at 2023-11-15 15:24:39 +0000 UTC (1 container statuses recorded)
Nov 15 17:02:23.196: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 15 17:02:23.196: INFO: sonobuoy-systemd-logs-daemon-set-eabb9aa298c743f7-hmfn7 from sonobuoy started at 2023-11-15 15:24:45 +0000 UTC (2 container statuses recorded)
Nov 15 17:02:23.196: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 15 17:02:23.196: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 11/15/23 17:02:23.197
Nov 15 17:02:23.234: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-4031" to be "running"
Nov 15 17:02:23.249: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 14.239516ms
Nov 15 17:02:25.265: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030632748s
Nov 15 17:02:27.266: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.032001554s
Nov 15 17:02:27.267: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 11/15/23 17:02:27.281
STEP: Trying to apply a random label on the found node. 11/15/23 17:02:27.318
STEP: verifying the node has the label kubernetes.io/e2e-684cac58-abd7-4847-9505-8e1c0eac19e7 42 11/15/23 17:02:27.357
STEP: Trying to relaunch the pod, now with labels. 11/15/23 17:02:27.376
Nov 15 17:02:27.396: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-4031" to be "not pending"
Nov 15 17:02:27.412: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 15.525779ms
Nov 15 17:02:29.437: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040831241s
Nov 15 17:02:31.428: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 4.03114795s
Nov 15 17:02:31.428: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-684cac58-abd7-4847-9505-8e1c0eac19e7 off the node 10.15.40.115 11/15/23 17:02:31.449
STEP: verifying the node doesn't have the label kubernetes.io/e2e-684cac58-abd7-4847-9505-8e1c0eac19e7 11/15/23 17:02:31.527
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 17:02:31.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-4031" for this suite. 11/15/23 17:02:31.569
------------------------------
• [SLOW TEST] [8.663 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:02:22.931
    Nov 15 17:02:22.932: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename sched-pred 11/15/23 17:02:22.933
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:02:22.981
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:02:22.998
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Nov 15 17:02:23.015: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Nov 15 17:02:23.057: INFO: Waiting for terminating namespaces to be deleted...
    Nov 15 17:02:23.074: INFO: 
    Logging pods the apiserver thinks is on node 10.15.40.106 before test
    Nov 15 17:02:23.119: INFO: ibm-cloud-provider-ip-163-109-74-98-5d59656977-bgllw from ibm-system started at 2023-11-15 15:39:54 +0000 UTC (1 container statuses recorded)
    Nov 15 17:02:23.119: INFO: 	Container ibm-cloud-provider-ip-163-109-74-98 ready: true, restart count 0
    Nov 15 17:02:23.119: INFO: calico-kube-controllers-7847f7647d-srqd4 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 17:02:23.119: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Nov 15 17:02:23.119: INFO: calico-node-67csc from kube-system started at 2023-11-15 13:13:21 +0000 UTC (1 container statuses recorded)
    Nov 15 17:02:23.119: INFO: 	Container calico-node ready: true, restart count 0
    Nov 15 17:02:23.119: INFO: calico-typha-5b5db87f55-nwmh4 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 17:02:23.119: INFO: 	Container calico-typha ready: true, restart count 0
    Nov 15 17:02:23.119: INFO: coredns-5845f98d4-2lnrf from kube-system started at 2023-11-15 13:23:15 +0000 UTC (1 container statuses recorded)
    Nov 15 17:02:23.119: INFO: 	Container coredns ready: true, restart count 0
    Nov 15 17:02:23.119: INFO: coredns-autoscaler-85f4bdddf6-qwxmw from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 17:02:23.119: INFO: 	Container autoscaler ready: true, restart count 0
    Nov 15 17:02:23.119: INFO: dashboard-metrics-scraper-7cf679fbdf-frv7t from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 17:02:23.119: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Nov 15 17:02:23.119: INFO: ibm-file-plugin-557f875d5f-jrfnv from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 17:02:23.119: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    Nov 15 17:02:23.119: INFO: ibm-keepalived-watcher-dqknf from kube-system started at 2023-11-15 13:13:21 +0000 UTC (1 container statuses recorded)
    Nov 15 17:02:23.119: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Nov 15 17:02:23.119: INFO: ibm-master-proxy-static-10.15.40.106 from kube-system started at 2023-11-15 13:13:08 +0000 UTC (2 container statuses recorded)
    Nov 15 17:02:23.119: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Nov 15 17:02:23.119: INFO: 	Container pause ready: true, restart count 0
    Nov 15 17:02:23.119: INFO: ibm-storage-watcher-7d897847f4-k64t5 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 17:02:23.119: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    Nov 15 17:02:23.119: INFO: ibmcloud-block-storage-driver-f97nm from kube-system started at 2023-11-15 13:13:30 +0000 UTC (1 container statuses recorded)
    Nov 15 17:02:23.119: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Nov 15 17:02:23.119: INFO: ibmcloud-block-storage-plugin-5bd59f7b48-2dxcj from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 17:02:23.119: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    Nov 15 17:02:23.119: INFO: ingress-cluster-healthcheck-5985f966bb-jgjx8 from kube-system started at 2023-11-15 15:39:54 +0000 UTC (1 container statuses recorded)
    Nov 15 17:02:23.119: INFO: 	Container ingress-cluster-healthcheck ready: true, restart count 0
    Nov 15 17:02:23.119: INFO: konnectivity-agent-5brpz from kube-system started at 2023-11-15 13:22:38 +0000 UTC (1 container statuses recorded)
    Nov 15 17:02:23.119: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Nov 15 17:02:23.119: INFO: kubernetes-dashboard-5ccdc9cbb8-wbmfz from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 17:02:23.119: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Nov 15 17:02:23.119: INFO: metrics-server-7cbd9c9b48-8tr74 from kube-system started at 2023-11-15 15:39:54 +0000 UTC (3 container statuses recorded)
    Nov 15 17:02:23.119: INFO: 	Container config-watcher ready: true, restart count 0
    Nov 15 17:02:23.119: INFO: 	Container metrics-server ready: true, restart count 0
    Nov 15 17:02:23.119: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Nov 15 17:02:23.119: INFO: public-crclac150z0u38f277tnpg-alb1-6df9445bb6-qw2bt from kube-system started at 2023-11-15 15:39:54 +0000 UTC (1 container statuses recorded)
    Nov 15 17:02:23.119: INFO: 	Container nginx-ingress ready: true, restart count 0
    Nov 15 17:02:23.120: INFO: snapshot-controller-6db47fc545-5nnph from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 17:02:23.120: INFO: 	Container snapshot-controller ready: true, restart count 0
    Nov 15 17:02:23.120: INFO: snapshot-controller-6db47fc545-h4q49 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 17:02:23.120: INFO: 	Container snapshot-controller ready: true, restart count 0
    Nov 15 17:02:23.120: INFO: snapshot-controller-6db47fc545-khkb2 from kube-system started at 2023-11-15 13:13:45 +0000 UTC (1 container statuses recorded)
    Nov 15 17:02:23.120: INFO: 	Container snapshot-controller ready: true, restart count 0
    Nov 15 17:02:23.120: INFO: sonobuoy-systemd-logs-daemon-set-eabb9aa298c743f7-zxdnv from sonobuoy started at 2023-11-15 15:24:45 +0000 UTC (2 container statuses recorded)
    Nov 15 17:02:23.120: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 15 17:02:23.120: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 15 17:02:23.120: INFO: 
    Logging pods the apiserver thinks is on node 10.15.40.114 before test
    Nov 15 17:02:23.161: INFO: ibm-cloud-provider-ip-163-109-74-98-5d59656977-wxlx6 from ibm-system started at 2023-11-15 13:18:32 +0000 UTC (1 container statuses recorded)
    Nov 15 17:02:23.162: INFO: 	Container ibm-cloud-provider-ip-163-109-74-98 ready: true, restart count 0
    Nov 15 17:02:23.162: INFO: calico-node-wqvj8 from kube-system started at 2023-11-15 13:13:28 +0000 UTC (1 container statuses recorded)
    Nov 15 17:02:23.162: INFO: 	Container calico-node ready: true, restart count 0
    Nov 15 17:02:23.162: INFO: calico-typha-5b5db87f55-4qv4c from kube-system started at 2023-11-15 13:13:53 +0000 UTC (1 container statuses recorded)
    Nov 15 17:02:23.162: INFO: 	Container calico-typha ready: true, restart count 0
    Nov 15 17:02:23.162: INFO: coredns-5845f98d4-6cld8 from kube-system started at 2023-11-15 13:23:15 +0000 UTC (1 container statuses recorded)
    Nov 15 17:02:23.162: INFO: 	Container coredns ready: true, restart count 0
    Nov 15 17:02:23.162: INFO: ibm-keepalived-watcher-xsslz from kube-system started at 2023-11-15 13:13:28 +0000 UTC (1 container statuses recorded)
    Nov 15 17:02:23.162: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Nov 15 17:02:23.162: INFO: ibm-master-proxy-static-10.15.40.114 from kube-system started at 2023-11-15 13:13:15 +0000 UTC (2 container statuses recorded)
    Nov 15 17:02:23.162: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Nov 15 17:02:23.162: INFO: 	Container pause ready: true, restart count 0
    Nov 15 17:02:23.162: INFO: ibmcloud-block-storage-driver-4txx8 from kube-system started at 2023-11-15 13:13:37 +0000 UTC (1 container statuses recorded)
    Nov 15 17:02:23.162: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Nov 15 17:02:23.162: INFO: konnectivity-agent-zr4zl from kube-system started at 2023-11-15 13:22:41 +0000 UTC (1 container statuses recorded)
    Nov 15 17:02:23.162: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Nov 15 17:02:23.162: INFO: metrics-server-7cbd9c9b48-cnskr from kube-system started at 2023-11-15 13:56:24 +0000 UTC (3 container statuses recorded)
    Nov 15 17:02:23.162: INFO: 	Container config-watcher ready: true, restart count 0
    Nov 15 17:02:23.162: INFO: 	Container metrics-server ready: true, restart count 0
    Nov 15 17:02:23.162: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Nov 15 17:02:23.162: INFO: public-crclac150z0u38f277tnpg-alb1-6df9445bb6-jkpcd from kube-system started at 2023-11-15 13:23:30 +0000 UTC (1 container statuses recorded)
    Nov 15 17:02:23.162: INFO: 	Container nginx-ingress ready: true, restart count 0
    Nov 15 17:02:23.162: INFO: sonobuoy-e2e-job-3a51e32fa56b4156 from sonobuoy started at 2023-11-15 15:24:45 +0000 UTC (2 container statuses recorded)
    Nov 15 17:02:23.163: INFO: 	Container e2e ready: true, restart count 0
    Nov 15 17:02:23.163: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 15 17:02:23.163: INFO: sonobuoy-systemd-logs-daemon-set-eabb9aa298c743f7-sxg5b from sonobuoy started at 2023-11-15 15:24:45 +0000 UTC (2 container statuses recorded)
    Nov 15 17:02:23.163: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 15 17:02:23.163: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 15 17:02:23.163: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-11-15 13:15:59 +0000 UTC (1 container statuses recorded)
    Nov 15 17:02:23.163: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
    Nov 15 17:02:23.163: INFO: 
    Logging pods the apiserver thinks is on node 10.15.40.115 before test
    Nov 15 17:02:23.194: INFO: pod-handle-http-request from container-lifecycle-hook-6093 started at 2023-11-15 17:02:08 +0000 UTC (2 container statuses recorded)
    Nov 15 17:02:23.194: INFO: 	Container container-handle-http-request ready: true, restart count 0
    Nov 15 17:02:23.194: INFO: 	Container container-handle-https-request ready: true, restart count 0
    Nov 15 17:02:23.194: INFO: calico-node-gknnt from kube-system started at 2023-11-15 13:13:40 +0000 UTC (1 container statuses recorded)
    Nov 15 17:02:23.194: INFO: 	Container calico-node ready: true, restart count 0
    Nov 15 17:02:23.194: INFO: calico-typha-5b5db87f55-fchsg from kube-system started at 2023-11-15 15:40:02 +0000 UTC (1 container statuses recorded)
    Nov 15 17:02:23.194: INFO: 	Container calico-typha ready: true, restart count 0
    Nov 15 17:02:23.194: INFO: coredns-5845f98d4-rt4sq from kube-system started at 2023-11-15 15:39:54 +0000 UTC (1 container statuses recorded)
    Nov 15 17:02:23.194: INFO: 	Container coredns ready: true, restart count 0
    Nov 15 17:02:23.194: INFO: ibm-keepalived-watcher-75lpq from kube-system started at 2023-11-15 13:13:40 +0000 UTC (1 container statuses recorded)
    Nov 15 17:02:23.194: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Nov 15 17:02:23.195: INFO: ibm-master-proxy-static-10.15.40.115 from kube-system started at 2023-11-15 13:13:28 +0000 UTC (2 container statuses recorded)
    Nov 15 17:02:23.195: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Nov 15 17:02:23.195: INFO: 	Container pause ready: true, restart count 0
    Nov 15 17:02:23.195: INFO: ibmcloud-block-storage-driver-gbqth from kube-system started at 2023-11-15 13:13:49 +0000 UTC (1 container statuses recorded)
    Nov 15 17:02:23.195: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Nov 15 17:02:23.196: INFO: konnectivity-agent-9pkld from kube-system started at 2023-11-15 13:22:45 +0000 UTC (1 container statuses recorded)
    Nov 15 17:02:23.196: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Nov 15 17:02:23.196: INFO: sonobuoy from sonobuoy started at 2023-11-15 15:24:39 +0000 UTC (1 container statuses recorded)
    Nov 15 17:02:23.196: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Nov 15 17:02:23.196: INFO: sonobuoy-systemd-logs-daemon-set-eabb9aa298c743f7-hmfn7 from sonobuoy started at 2023-11-15 15:24:45 +0000 UTC (2 container statuses recorded)
    Nov 15 17:02:23.196: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 15 17:02:23.196: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 11/15/23 17:02:23.197
    Nov 15 17:02:23.234: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-4031" to be "running"
    Nov 15 17:02:23.249: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 14.239516ms
    Nov 15 17:02:25.265: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030632748s
    Nov 15 17:02:27.266: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.032001554s
    Nov 15 17:02:27.267: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 11/15/23 17:02:27.281
    STEP: Trying to apply a random label on the found node. 11/15/23 17:02:27.318
    STEP: verifying the node has the label kubernetes.io/e2e-684cac58-abd7-4847-9505-8e1c0eac19e7 42 11/15/23 17:02:27.357
    STEP: Trying to relaunch the pod, now with labels. 11/15/23 17:02:27.376
    Nov 15 17:02:27.396: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-4031" to be "not pending"
    Nov 15 17:02:27.412: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 15.525779ms
    Nov 15 17:02:29.437: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040831241s
    Nov 15 17:02:31.428: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 4.03114795s
    Nov 15 17:02:31.428: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-684cac58-abd7-4847-9505-8e1c0eac19e7 off the node 10.15.40.115 11/15/23 17:02:31.449
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-684cac58-abd7-4847-9505-8e1c0eac19e7 11/15/23 17:02:31.527
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:02:31.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-4031" for this suite. 11/15/23 17:02:31.569
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:02:31.605
Nov 15 17:02:31.605: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename deployment 11/15/23 17:02:31.608
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:02:31.666
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:02:31.683
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Nov 15 17:02:31.698: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Nov 15 17:02:31.732: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov 15 17:02:36.757: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 11/15/23 17:02:36.757
Nov 15 17:02:36.758: INFO: Creating deployment "test-rolling-update-deployment"
Nov 15 17:02:36.781: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Nov 15 17:02:36.817: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Nov 15 17:02:38.870: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Nov 15 17:02:38.889: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 17, 2, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 17, 2, 36, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 17, 2, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 17, 2, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-7549d9f46d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 15 17:02:40.909: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Nov 15 17:02:40.963: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5671  5e5bd82f-bbdd-4d06-a2a8-51bf6aaca35b 50328 1 2023-11-15 17:02:36 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-11-15 17:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 17:02:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045bc538 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-11-15 17:02:36 +0000 UTC,LastTransitionTime:2023-11-15 17:02:36 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-11-15 17:02:39 +0000 UTC,LastTransitionTime:2023-11-15 17:02:36 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 15 17:02:40.980: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-5671  ad2f53c1-1602-40df-938a-2902b3cb41ed 50318 1 2023-11-15 17:02:36 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 5e5bd82f-bbdd-4d06-a2a8-51bf6aaca35b 0xc002b2d137 0xc002b2d138}] [] [{kube-controller-manager Update apps/v1 2023-11-15 17:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e5bd82f-bbdd-4d06-a2a8-51bf6aaca35b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 17:02:38 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b2d1e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 15 17:02:40.980: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Nov 15 17:02:40.980: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5671  a4e341d2-f472-4901-86f2-d6da3d319f85 50327 2 2023-11-15 17:02:31 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 5e5bd82f-bbdd-4d06-a2a8-51bf6aaca35b 0xc002b2d007 0xc002b2d008}] [] [{e2e.test Update apps/v1 2023-11-15 17:02:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 17:02:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e5bd82f-bbdd-4d06-a2a8-51bf6aaca35b\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-11-15 17:02:39 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002b2d0c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 15 17:02:41.000: INFO: Pod "test-rolling-update-deployment-7549d9f46d-2zs9r" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-2zs9r test-rolling-update-deployment-7549d9f46d- deployment-5671  2d6c5e1d-1bc0-4f5a-b09f-e3ecd1f8926f 50317 0 2023-11-15 17:02:36 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:045a50a0da45942e4160824630f1fae22fbda5d5f0937092b4ae4db4bb8508db cni.projectcalico.org/podIP:172.30.164.4/32 cni.projectcalico.org/podIPs:172.30.164.4/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d ad2f53c1-1602-40df-938a-2902b3cb41ed 0xc002b2d6a7 0xc002b2d6a8}] [] [{kube-controller-manager Update v1 2023-11-15 17:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ad2f53c1-1602-40df-938a-2902b3cb41ed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 17:02:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 17:02:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.164.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zf7xp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zf7xp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:02:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:02:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:02:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:02:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:172.30.164.4,StartTime:2023-11-15 17:02:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 17:02:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://cfb40527ddcd89388bffc0e2c7c1c8b06753b89024ec425edd6b44c4fc624765,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.164.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Nov 15 17:02:41.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5671" for this suite. 11/15/23 17:02:41.025
------------------------------
• [SLOW TEST] [9.445 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:02:31.605
    Nov 15 17:02:31.605: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename deployment 11/15/23 17:02:31.608
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:02:31.666
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:02:31.683
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Nov 15 17:02:31.698: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Nov 15 17:02:31.732: INFO: Pod name sample-pod: Found 0 pods out of 1
    Nov 15 17:02:36.757: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 11/15/23 17:02:36.757
    Nov 15 17:02:36.758: INFO: Creating deployment "test-rolling-update-deployment"
    Nov 15 17:02:36.781: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Nov 15 17:02:36.817: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Nov 15 17:02:38.870: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Nov 15 17:02:38.889: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 17, 2, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 17, 2, 36, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 17, 2, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 17, 2, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-7549d9f46d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 15 17:02:40.909: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Nov 15 17:02:40.963: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5671  5e5bd82f-bbdd-4d06-a2a8-51bf6aaca35b 50328 1 2023-11-15 17:02:36 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-11-15 17:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 17:02:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045bc538 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-11-15 17:02:36 +0000 UTC,LastTransitionTime:2023-11-15 17:02:36 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-11-15 17:02:39 +0000 UTC,LastTransitionTime:2023-11-15 17:02:36 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Nov 15 17:02:40.980: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-5671  ad2f53c1-1602-40df-938a-2902b3cb41ed 50318 1 2023-11-15 17:02:36 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 5e5bd82f-bbdd-4d06-a2a8-51bf6aaca35b 0xc002b2d137 0xc002b2d138}] [] [{kube-controller-manager Update apps/v1 2023-11-15 17:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e5bd82f-bbdd-4d06-a2a8-51bf6aaca35b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 17:02:38 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b2d1e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Nov 15 17:02:40.980: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Nov 15 17:02:40.980: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5671  a4e341d2-f472-4901-86f2-d6da3d319f85 50327 2 2023-11-15 17:02:31 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 5e5bd82f-bbdd-4d06-a2a8-51bf6aaca35b 0xc002b2d007 0xc002b2d008}] [] [{e2e.test Update apps/v1 2023-11-15 17:02:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 17:02:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e5bd82f-bbdd-4d06-a2a8-51bf6aaca35b\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-11-15 17:02:39 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002b2d0c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Nov 15 17:02:41.000: INFO: Pod "test-rolling-update-deployment-7549d9f46d-2zs9r" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-2zs9r test-rolling-update-deployment-7549d9f46d- deployment-5671  2d6c5e1d-1bc0-4f5a-b09f-e3ecd1f8926f 50317 0 2023-11-15 17:02:36 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:045a50a0da45942e4160824630f1fae22fbda5d5f0937092b4ae4db4bb8508db cni.projectcalico.org/podIP:172.30.164.4/32 cni.projectcalico.org/podIPs:172.30.164.4/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d ad2f53c1-1602-40df-938a-2902b3cb41ed 0xc002b2d6a7 0xc002b2d6a8}] [] [{kube-controller-manager Update v1 2023-11-15 17:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ad2f53c1-1602-40df-938a-2902b3cb41ed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 17:02:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 17:02:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.164.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zf7xp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zf7xp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:02:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:02:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:02:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:02:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:172.30.164.4,StartTime:2023-11-15 17:02:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 17:02:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://cfb40527ddcd89388bffc0e2c7c1c8b06753b89024ec425edd6b44c4fc624765,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.164.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:02:41.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5671" for this suite. 11/15/23 17:02:41.025
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:02:41.057
Nov 15 17:02:41.057: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 11/15/23 17:02:41.06
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:02:41.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:02:41.129
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 11/15/23 17:02:41.144
STEP: Creating hostNetwork=false pod 11/15/23 17:02:41.144
Nov 15 17:02:41.177: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-967" to be "running and ready"
Nov 15 17:02:41.208: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 30.994379ms
Nov 15 17:02:41.209: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Nov 15 17:02:43.225: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047562639s
Nov 15 17:02:43.225: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Nov 15 17:02:45.230: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.052719365s
Nov 15 17:02:45.230: INFO: The phase of Pod test-pod is Running (Ready = true)
Nov 15 17:02:45.230: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 11/15/23 17:02:45.246
Nov 15 17:02:45.270: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-967" to be "running and ready"
Nov 15 17:02:45.284: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.287635ms
Nov 15 17:02:45.284: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Nov 15 17:02:47.301: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.030658454s
Nov 15 17:02:47.301: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Nov 15 17:02:47.301: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 11/15/23 17:02:47.317
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 11/15/23 17:02:47.317
Nov 15 17:02:47.317: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-967 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 17:02:47.317: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
Nov 15 17:02:47.319: INFO: ExecWithOptions: Clientset creation
Nov 15 17:02:47.319: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-967/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Nov 15 17:02:47.612: INFO: Exec stderr: ""
Nov 15 17:02:47.612: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-967 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 17:02:47.612: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
Nov 15 17:02:47.614: INFO: ExecWithOptions: Clientset creation
Nov 15 17:02:47.615: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-967/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Nov 15 17:02:47.950: INFO: Exec stderr: ""
Nov 15 17:02:47.950: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-967 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 17:02:47.950: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
Nov 15 17:02:47.952: INFO: ExecWithOptions: Clientset creation
Nov 15 17:02:47.952: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-967/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Nov 15 17:02:48.207: INFO: Exec stderr: ""
Nov 15 17:02:48.207: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-967 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 17:02:48.208: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
Nov 15 17:02:48.209: INFO: ExecWithOptions: Clientset creation
Nov 15 17:02:48.209: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-967/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Nov 15 17:02:48.509: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 11/15/23 17:02:48.509
Nov 15 17:02:48.510: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-967 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 17:02:48.510: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
Nov 15 17:02:48.512: INFO: ExecWithOptions: Clientset creation
Nov 15 17:02:48.512: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-967/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Nov 15 17:02:48.824: INFO: Exec stderr: ""
Nov 15 17:02:48.824: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-967 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 17:02:48.824: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
Nov 15 17:02:48.826: INFO: ExecWithOptions: Clientset creation
Nov 15 17:02:48.827: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-967/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Nov 15 17:02:49.095: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 11/15/23 17:02:49.095
Nov 15 17:02:49.096: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-967 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 17:02:49.096: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
Nov 15 17:02:49.098: INFO: ExecWithOptions: Clientset creation
Nov 15 17:02:49.098: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-967/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Nov 15 17:02:49.437: INFO: Exec stderr: ""
Nov 15 17:02:49.437: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-967 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 17:02:49.437: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
Nov 15 17:02:49.438: INFO: ExecWithOptions: Clientset creation
Nov 15 17:02:49.438: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-967/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Nov 15 17:02:49.699: INFO: Exec stderr: ""
Nov 15 17:02:49.699: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-967 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 17:02:49.699: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
Nov 15 17:02:49.700: INFO: ExecWithOptions: Clientset creation
Nov 15 17:02:49.700: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-967/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Nov 15 17:02:49.985: INFO: Exec stderr: ""
Nov 15 17:02:49.985: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-967 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 17:02:49.986: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
Nov 15 17:02:49.988: INFO: ExecWithOptions: Clientset creation
Nov 15 17:02:49.988: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-967/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Nov 15 17:02:50.280: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Nov 15 17:02:50.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-967" for this suite. 11/15/23 17:02:50.306
------------------------------
• [SLOW TEST] [9.283 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:02:41.057
    Nov 15 17:02:41.057: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 11/15/23 17:02:41.06
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:02:41.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:02:41.129
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 11/15/23 17:02:41.144
    STEP: Creating hostNetwork=false pod 11/15/23 17:02:41.144
    Nov 15 17:02:41.177: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-967" to be "running and ready"
    Nov 15 17:02:41.208: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 30.994379ms
    Nov 15 17:02:41.209: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 17:02:43.225: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047562639s
    Nov 15 17:02:43.225: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 17:02:45.230: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.052719365s
    Nov 15 17:02:45.230: INFO: The phase of Pod test-pod is Running (Ready = true)
    Nov 15 17:02:45.230: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 11/15/23 17:02:45.246
    Nov 15 17:02:45.270: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-967" to be "running and ready"
    Nov 15 17:02:45.284: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.287635ms
    Nov 15 17:02:45.284: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 17:02:47.301: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.030658454s
    Nov 15 17:02:47.301: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Nov 15 17:02:47.301: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 11/15/23 17:02:47.317
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 11/15/23 17:02:47.317
    Nov 15 17:02:47.317: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-967 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 17:02:47.317: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    Nov 15 17:02:47.319: INFO: ExecWithOptions: Clientset creation
    Nov 15 17:02:47.319: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-967/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Nov 15 17:02:47.612: INFO: Exec stderr: ""
    Nov 15 17:02:47.612: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-967 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 17:02:47.612: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    Nov 15 17:02:47.614: INFO: ExecWithOptions: Clientset creation
    Nov 15 17:02:47.615: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-967/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Nov 15 17:02:47.950: INFO: Exec stderr: ""
    Nov 15 17:02:47.950: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-967 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 17:02:47.950: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    Nov 15 17:02:47.952: INFO: ExecWithOptions: Clientset creation
    Nov 15 17:02:47.952: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-967/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Nov 15 17:02:48.207: INFO: Exec stderr: ""
    Nov 15 17:02:48.207: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-967 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 17:02:48.208: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    Nov 15 17:02:48.209: INFO: ExecWithOptions: Clientset creation
    Nov 15 17:02:48.209: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-967/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Nov 15 17:02:48.509: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 11/15/23 17:02:48.509
    Nov 15 17:02:48.510: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-967 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 17:02:48.510: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    Nov 15 17:02:48.512: INFO: ExecWithOptions: Clientset creation
    Nov 15 17:02:48.512: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-967/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Nov 15 17:02:48.824: INFO: Exec stderr: ""
    Nov 15 17:02:48.824: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-967 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 17:02:48.824: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    Nov 15 17:02:48.826: INFO: ExecWithOptions: Clientset creation
    Nov 15 17:02:48.827: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-967/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Nov 15 17:02:49.095: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 11/15/23 17:02:49.095
    Nov 15 17:02:49.096: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-967 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 17:02:49.096: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    Nov 15 17:02:49.098: INFO: ExecWithOptions: Clientset creation
    Nov 15 17:02:49.098: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-967/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Nov 15 17:02:49.437: INFO: Exec stderr: ""
    Nov 15 17:02:49.437: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-967 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 17:02:49.437: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    Nov 15 17:02:49.438: INFO: ExecWithOptions: Clientset creation
    Nov 15 17:02:49.438: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-967/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Nov 15 17:02:49.699: INFO: Exec stderr: ""
    Nov 15 17:02:49.699: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-967 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 17:02:49.699: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    Nov 15 17:02:49.700: INFO: ExecWithOptions: Clientset creation
    Nov 15 17:02:49.700: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-967/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Nov 15 17:02:49.985: INFO: Exec stderr: ""
    Nov 15 17:02:49.985: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-967 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 17:02:49.986: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    Nov 15 17:02:49.988: INFO: ExecWithOptions: Clientset creation
    Nov 15 17:02:49.988: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-967/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Nov 15 17:02:50.280: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:02:50.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-967" for this suite. 11/15/23 17:02:50.306
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:02:50.352
Nov 15 17:02:50.353: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename webhook 11/15/23 17:02:50.357
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:02:50.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:02:50.426
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/15/23 17:02:50.488
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 17:02:50.729
STEP: Deploying the webhook pod 11/15/23 17:02:50.757
STEP: Wait for the deployment to be ready 11/15/23 17:02:50.801
Nov 15 17:02:50.836: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 15 17:02:52.900: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 17, 2, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 17, 2, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 17, 2, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 17, 2, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 11/15/23 17:02:54.919
STEP: Verifying the service has paired with the endpoint 11/15/23 17:02:54.96
Nov 15 17:02:55.961: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Nov 15 17:02:55.979: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4824-crds.webhook.example.com via the AdmissionRegistration API 11/15/23 17:02:56.528
STEP: Creating a custom resource while v1 is storage version 11/15/23 17:02:56.638
STEP: Patching Custom Resource Definition to set v2 as storage 11/15/23 17:02:59.097
STEP: Patching the custom resource while v2 is storage version 11/15/23 17:02:59.122
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 17:02:59.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8365" for this suite. 11/15/23 17:03:00.076
STEP: Destroying namespace "webhook-8365-markers" for this suite. 11/15/23 17:03:00.1
------------------------------
• [SLOW TEST] [9.770 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:02:50.352
    Nov 15 17:02:50.353: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename webhook 11/15/23 17:02:50.357
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:02:50.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:02:50.426
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/15/23 17:02:50.488
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 17:02:50.729
    STEP: Deploying the webhook pod 11/15/23 17:02:50.757
    STEP: Wait for the deployment to be ready 11/15/23 17:02:50.801
    Nov 15 17:02:50.836: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Nov 15 17:02:52.900: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 17, 2, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 17, 2, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 17, 2, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 17, 2, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 11/15/23 17:02:54.919
    STEP: Verifying the service has paired with the endpoint 11/15/23 17:02:54.96
    Nov 15 17:02:55.961: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Nov 15 17:02:55.979: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4824-crds.webhook.example.com via the AdmissionRegistration API 11/15/23 17:02:56.528
    STEP: Creating a custom resource while v1 is storage version 11/15/23 17:02:56.638
    STEP: Patching Custom Resource Definition to set v2 as storage 11/15/23 17:02:59.097
    STEP: Patching the custom resource while v2 is storage version 11/15/23 17:02:59.122
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:02:59.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8365" for this suite. 11/15/23 17:03:00.076
    STEP: Destroying namespace "webhook-8365-markers" for this suite. 11/15/23 17:03:00.1
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:03:00.132
Nov 15 17:03:00.133: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename namespaces 11/15/23 17:03:00.135
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:03:00.188
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:03:00.204
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 11/15/23 17:03:00.22
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:03:00.269
STEP: Creating a service in the namespace 11/15/23 17:03:00.285
STEP: Deleting the namespace 11/15/23 17:03:00.333
STEP: Waiting for the namespace to be removed. 11/15/23 17:03:00.359
STEP: Recreating the namespace 11/15/23 17:03:06.378
STEP: Verifying there is no service in the namespace 11/15/23 17:03:06.434
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 17:03:06.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-7349" for this suite. 11/15/23 17:03:06.498
STEP: Destroying namespace "nsdeletetest-6719" for this suite. 11/15/23 17:03:06.523
Nov 15 17:03:06.539: INFO: Namespace nsdeletetest-6719 was already deleted
STEP: Destroying namespace "nsdeletetest-7960" for this suite. 11/15/23 17:03:06.539
------------------------------
• [SLOW TEST] [6.433 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:03:00.132
    Nov 15 17:03:00.133: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename namespaces 11/15/23 17:03:00.135
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:03:00.188
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:03:00.204
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 11/15/23 17:03:00.22
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:03:00.269
    STEP: Creating a service in the namespace 11/15/23 17:03:00.285
    STEP: Deleting the namespace 11/15/23 17:03:00.333
    STEP: Waiting for the namespace to be removed. 11/15/23 17:03:00.359
    STEP: Recreating the namespace 11/15/23 17:03:06.378
    STEP: Verifying there is no service in the namespace 11/15/23 17:03:06.434
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:03:06.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-7349" for this suite. 11/15/23 17:03:06.498
    STEP: Destroying namespace "nsdeletetest-6719" for this suite. 11/15/23 17:03:06.523
    Nov 15 17:03:06.539: INFO: Namespace nsdeletetest-6719 was already deleted
    STEP: Destroying namespace "nsdeletetest-7960" for this suite. 11/15/23 17:03:06.539
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:03:06.569
Nov 15 17:03:06.569: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename podtemplate 11/15/23 17:03:06.571
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:03:06.62
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:03:06.635
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 11/15/23 17:03:06.651
Nov 15 17:03:06.672: INFO: created test-podtemplate-1
Nov 15 17:03:06.692: INFO: created test-podtemplate-2
Nov 15 17:03:06.709: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 11/15/23 17:03:06.709
STEP: delete collection of pod templates 11/15/23 17:03:06.724
Nov 15 17:03:06.724: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 11/15/23 17:03:06.785
Nov 15 17:03:06.785: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Nov 15 17:03:06.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-4715" for this suite. 11/15/23 17:03:06.824
------------------------------
• [0.281 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:03:06.569
    Nov 15 17:03:06.569: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename podtemplate 11/15/23 17:03:06.571
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:03:06.62
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:03:06.635
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 11/15/23 17:03:06.651
    Nov 15 17:03:06.672: INFO: created test-podtemplate-1
    Nov 15 17:03:06.692: INFO: created test-podtemplate-2
    Nov 15 17:03:06.709: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 11/15/23 17:03:06.709
    STEP: delete collection of pod templates 11/15/23 17:03:06.724
    Nov 15 17:03:06.724: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 11/15/23 17:03:06.785
    Nov 15 17:03:06.785: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:03:06.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-4715" for this suite. 11/15/23 17:03:06.824
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:03:06.853
Nov 15 17:03:06.853: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename projected 11/15/23 17:03:06.855
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:03:06.906
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:03:06.922
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 11/15/23 17:03:06.938
Nov 15 17:03:06.975: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4a9d32db-a7c5-49dc-b954-bd55eeb2135e" in namespace "projected-7911" to be "Succeeded or Failed"
Nov 15 17:03:06.996: INFO: Pod "downwardapi-volume-4a9d32db-a7c5-49dc-b954-bd55eeb2135e": Phase="Pending", Reason="", readiness=false. Elapsed: 20.992139ms
Nov 15 17:03:09.013: INFO: Pod "downwardapi-volume-4a9d32db-a7c5-49dc-b954-bd55eeb2135e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038345455s
Nov 15 17:03:11.012: INFO: Pod "downwardapi-volume-4a9d32db-a7c5-49dc-b954-bd55eeb2135e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037399877s
Nov 15 17:03:13.013: INFO: Pod "downwardapi-volume-4a9d32db-a7c5-49dc-b954-bd55eeb2135e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03758507s
STEP: Saw pod success 11/15/23 17:03:13.013
Nov 15 17:03:13.013: INFO: Pod "downwardapi-volume-4a9d32db-a7c5-49dc-b954-bd55eeb2135e" satisfied condition "Succeeded or Failed"
Nov 15 17:03:13.029: INFO: Trying to get logs from node 10.15.40.115 pod downwardapi-volume-4a9d32db-a7c5-49dc-b954-bd55eeb2135e container client-container: <nil>
STEP: delete the pod 11/15/23 17:03:13.101
Nov 15 17:03:13.147: INFO: Waiting for pod downwardapi-volume-4a9d32db-a7c5-49dc-b954-bd55eeb2135e to disappear
Nov 15 17:03:13.162: INFO: Pod downwardapi-volume-4a9d32db-a7c5-49dc-b954-bd55eeb2135e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Nov 15 17:03:13.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7911" for this suite. 11/15/23 17:03:13.184
------------------------------
• [SLOW TEST] [6.357 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:03:06.853
    Nov 15 17:03:06.853: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename projected 11/15/23 17:03:06.855
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:03:06.906
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:03:06.922
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 11/15/23 17:03:06.938
    Nov 15 17:03:06.975: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4a9d32db-a7c5-49dc-b954-bd55eeb2135e" in namespace "projected-7911" to be "Succeeded or Failed"
    Nov 15 17:03:06.996: INFO: Pod "downwardapi-volume-4a9d32db-a7c5-49dc-b954-bd55eeb2135e": Phase="Pending", Reason="", readiness=false. Elapsed: 20.992139ms
    Nov 15 17:03:09.013: INFO: Pod "downwardapi-volume-4a9d32db-a7c5-49dc-b954-bd55eeb2135e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038345455s
    Nov 15 17:03:11.012: INFO: Pod "downwardapi-volume-4a9d32db-a7c5-49dc-b954-bd55eeb2135e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037399877s
    Nov 15 17:03:13.013: INFO: Pod "downwardapi-volume-4a9d32db-a7c5-49dc-b954-bd55eeb2135e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03758507s
    STEP: Saw pod success 11/15/23 17:03:13.013
    Nov 15 17:03:13.013: INFO: Pod "downwardapi-volume-4a9d32db-a7c5-49dc-b954-bd55eeb2135e" satisfied condition "Succeeded or Failed"
    Nov 15 17:03:13.029: INFO: Trying to get logs from node 10.15.40.115 pod downwardapi-volume-4a9d32db-a7c5-49dc-b954-bd55eeb2135e container client-container: <nil>
    STEP: delete the pod 11/15/23 17:03:13.101
    Nov 15 17:03:13.147: INFO: Waiting for pod downwardapi-volume-4a9d32db-a7c5-49dc-b954-bd55eeb2135e to disappear
    Nov 15 17:03:13.162: INFO: Pod downwardapi-volume-4a9d32db-a7c5-49dc-b954-bd55eeb2135e no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:03:13.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7911" for this suite. 11/15/23 17:03:13.184
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:03:13.212
Nov 15 17:03:13.213: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename containers 11/15/23 17:03:13.217
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:03:13.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:03:13.299
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 11/15/23 17:03:13.316
Nov 15 17:03:13.351: INFO: Waiting up to 5m0s for pod "client-containers-f3c42e9c-9427-46e9-a7e9-05718746db1f" in namespace "containers-4092" to be "Succeeded or Failed"
Nov 15 17:03:13.373: INFO: Pod "client-containers-f3c42e9c-9427-46e9-a7e9-05718746db1f": Phase="Pending", Reason="", readiness=false. Elapsed: 21.778561ms
Nov 15 17:03:15.393: INFO: Pod "client-containers-f3c42e9c-9427-46e9-a7e9-05718746db1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041919106s
Nov 15 17:03:17.393: INFO: Pod "client-containers-f3c42e9c-9427-46e9-a7e9-05718746db1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041865582s
STEP: Saw pod success 11/15/23 17:03:17.394
Nov 15 17:03:17.395: INFO: Pod "client-containers-f3c42e9c-9427-46e9-a7e9-05718746db1f" satisfied condition "Succeeded or Failed"
Nov 15 17:03:17.409: INFO: Trying to get logs from node 10.15.40.115 pod client-containers-f3c42e9c-9427-46e9-a7e9-05718746db1f container agnhost-container: <nil>
STEP: delete the pod 11/15/23 17:03:17.455
Nov 15 17:03:17.491: INFO: Waiting for pod client-containers-f3c42e9c-9427-46e9-a7e9-05718746db1f to disappear
Nov 15 17:03:17.506: INFO: Pod client-containers-f3c42e9c-9427-46e9-a7e9-05718746db1f no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Nov 15 17:03:17.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-4092" for this suite. 11/15/23 17:03:17.53
------------------------------
• [4.342 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:03:13.212
    Nov 15 17:03:13.213: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename containers 11/15/23 17:03:13.217
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:03:13.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:03:13.299
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 11/15/23 17:03:13.316
    Nov 15 17:03:13.351: INFO: Waiting up to 5m0s for pod "client-containers-f3c42e9c-9427-46e9-a7e9-05718746db1f" in namespace "containers-4092" to be "Succeeded or Failed"
    Nov 15 17:03:13.373: INFO: Pod "client-containers-f3c42e9c-9427-46e9-a7e9-05718746db1f": Phase="Pending", Reason="", readiness=false. Elapsed: 21.778561ms
    Nov 15 17:03:15.393: INFO: Pod "client-containers-f3c42e9c-9427-46e9-a7e9-05718746db1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041919106s
    Nov 15 17:03:17.393: INFO: Pod "client-containers-f3c42e9c-9427-46e9-a7e9-05718746db1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041865582s
    STEP: Saw pod success 11/15/23 17:03:17.394
    Nov 15 17:03:17.395: INFO: Pod "client-containers-f3c42e9c-9427-46e9-a7e9-05718746db1f" satisfied condition "Succeeded or Failed"
    Nov 15 17:03:17.409: INFO: Trying to get logs from node 10.15.40.115 pod client-containers-f3c42e9c-9427-46e9-a7e9-05718746db1f container agnhost-container: <nil>
    STEP: delete the pod 11/15/23 17:03:17.455
    Nov 15 17:03:17.491: INFO: Waiting for pod client-containers-f3c42e9c-9427-46e9-a7e9-05718746db1f to disappear
    Nov 15 17:03:17.506: INFO: Pod client-containers-f3c42e9c-9427-46e9-a7e9-05718746db1f no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:03:17.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-4092" for this suite. 11/15/23 17:03:17.53
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:03:17.557
Nov 15 17:03:17.557: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename secrets 11/15/23 17:03:17.56
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:03:17.612
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:03:17.625
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-58f93cdd-91ef-436a-a4ad-56fbf8cd7fad 11/15/23 17:03:17.642
STEP: Creating a pod to test consume secrets 11/15/23 17:03:17.665
Nov 15 17:03:17.708: INFO: Waiting up to 5m0s for pod "pod-secrets-8300d841-3b90-4fe2-a54b-0bf79c74827a" in namespace "secrets-2858" to be "Succeeded or Failed"
Nov 15 17:03:17.722: INFO: Pod "pod-secrets-8300d841-3b90-4fe2-a54b-0bf79c74827a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.021228ms
Nov 15 17:03:19.739: INFO: Pod "pod-secrets-8300d841-3b90-4fe2-a54b-0bf79c74827a": Phase="Running", Reason="", readiness=true. Elapsed: 2.0310485s
Nov 15 17:03:21.738: INFO: Pod "pod-secrets-8300d841-3b90-4fe2-a54b-0bf79c74827a": Phase="Running", Reason="", readiness=false. Elapsed: 4.030017061s
Nov 15 17:03:23.739: INFO: Pod "pod-secrets-8300d841-3b90-4fe2-a54b-0bf79c74827a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031471496s
STEP: Saw pod success 11/15/23 17:03:23.739
Nov 15 17:03:23.740: INFO: Pod "pod-secrets-8300d841-3b90-4fe2-a54b-0bf79c74827a" satisfied condition "Succeeded or Failed"
Nov 15 17:03:23.754: INFO: Trying to get logs from node 10.15.40.115 pod pod-secrets-8300d841-3b90-4fe2-a54b-0bf79c74827a container secret-volume-test: <nil>
STEP: delete the pod 11/15/23 17:03:23.799
Nov 15 17:03:23.833: INFO: Waiting for pod pod-secrets-8300d841-3b90-4fe2-a54b-0bf79c74827a to disappear
Nov 15 17:03:23.847: INFO: Pod pod-secrets-8300d841-3b90-4fe2-a54b-0bf79c74827a no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 15 17:03:23.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2858" for this suite. 11/15/23 17:03:23.871
------------------------------
• [SLOW TEST] [6.340 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:03:17.557
    Nov 15 17:03:17.557: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename secrets 11/15/23 17:03:17.56
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:03:17.612
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:03:17.625
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-58f93cdd-91ef-436a-a4ad-56fbf8cd7fad 11/15/23 17:03:17.642
    STEP: Creating a pod to test consume secrets 11/15/23 17:03:17.665
    Nov 15 17:03:17.708: INFO: Waiting up to 5m0s for pod "pod-secrets-8300d841-3b90-4fe2-a54b-0bf79c74827a" in namespace "secrets-2858" to be "Succeeded or Failed"
    Nov 15 17:03:17.722: INFO: Pod "pod-secrets-8300d841-3b90-4fe2-a54b-0bf79c74827a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.021228ms
    Nov 15 17:03:19.739: INFO: Pod "pod-secrets-8300d841-3b90-4fe2-a54b-0bf79c74827a": Phase="Running", Reason="", readiness=true. Elapsed: 2.0310485s
    Nov 15 17:03:21.738: INFO: Pod "pod-secrets-8300d841-3b90-4fe2-a54b-0bf79c74827a": Phase="Running", Reason="", readiness=false. Elapsed: 4.030017061s
    Nov 15 17:03:23.739: INFO: Pod "pod-secrets-8300d841-3b90-4fe2-a54b-0bf79c74827a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031471496s
    STEP: Saw pod success 11/15/23 17:03:23.739
    Nov 15 17:03:23.740: INFO: Pod "pod-secrets-8300d841-3b90-4fe2-a54b-0bf79c74827a" satisfied condition "Succeeded or Failed"
    Nov 15 17:03:23.754: INFO: Trying to get logs from node 10.15.40.115 pod pod-secrets-8300d841-3b90-4fe2-a54b-0bf79c74827a container secret-volume-test: <nil>
    STEP: delete the pod 11/15/23 17:03:23.799
    Nov 15 17:03:23.833: INFO: Waiting for pod pod-secrets-8300d841-3b90-4fe2-a54b-0bf79c74827a to disappear
    Nov 15 17:03:23.847: INFO: Pod pod-secrets-8300d841-3b90-4fe2-a54b-0bf79c74827a no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:03:23.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2858" for this suite. 11/15/23 17:03:23.871
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:03:23.908
Nov 15 17:03:23.908: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename webhook 11/15/23 17:03:23.909
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:03:23.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:03:23.979
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/15/23 17:03:24.05
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 17:03:24.532
STEP: Deploying the webhook pod 11/15/23 17:03:24.561
STEP: Wait for the deployment to be ready 11/15/23 17:03:24.619
Nov 15 17:03:24.661: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 15 17:03:26.723: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 17, 3, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 17, 3, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 17, 3, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 17, 3, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 11/15/23 17:03:28.741
STEP: Verifying the service has paired with the endpoint 11/15/23 17:03:28.778
Nov 15 17:03:29.782: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 11/15/23 17:03:29.799
STEP: Creating a custom resource definition that should be denied by the webhook 11/15/23 17:03:29.945
Nov 15 17:03:29.945: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 17:03:30.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2875" for this suite. 11/15/23 17:03:30.254
STEP: Destroying namespace "webhook-2875-markers" for this suite. 11/15/23 17:03:30.28
------------------------------
• [SLOW TEST] [6.399 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:03:23.908
    Nov 15 17:03:23.908: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename webhook 11/15/23 17:03:23.909
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:03:23.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:03:23.979
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/15/23 17:03:24.05
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 17:03:24.532
    STEP: Deploying the webhook pod 11/15/23 17:03:24.561
    STEP: Wait for the deployment to be ready 11/15/23 17:03:24.619
    Nov 15 17:03:24.661: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Nov 15 17:03:26.723: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 17, 3, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 17, 3, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 17, 3, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 17, 3, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 11/15/23 17:03:28.741
    STEP: Verifying the service has paired with the endpoint 11/15/23 17:03:28.778
    Nov 15 17:03:29.782: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 11/15/23 17:03:29.799
    STEP: Creating a custom resource definition that should be denied by the webhook 11/15/23 17:03:29.945
    Nov 15 17:03:29.945: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:03:30.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2875" for this suite. 11/15/23 17:03:30.254
    STEP: Destroying namespace "webhook-2875-markers" for this suite. 11/15/23 17:03:30.28
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:03:30.309
Nov 15 17:03:30.309: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename runtimeclass 11/15/23 17:03:30.313
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:03:30.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:03:30.384
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 11/15/23 17:03:30.4
STEP: getting /apis/node.k8s.io 11/15/23 17:03:30.416
STEP: getting /apis/node.k8s.io/v1 11/15/23 17:03:30.423
STEP: creating 11/15/23 17:03:30.43
STEP: watching 11/15/23 17:03:30.501
Nov 15 17:03:30.501: INFO: starting watch
STEP: getting 11/15/23 17:03:30.528
STEP: listing 11/15/23 17:03:30.547
STEP: patching 11/15/23 17:03:30.564
STEP: updating 11/15/23 17:03:30.588
Nov 15 17:03:30.609: INFO: waiting for watch events with expected annotations
STEP: deleting 11/15/23 17:03:30.61
STEP: deleting a collection 11/15/23 17:03:30.681
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Nov 15 17:03:30.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-1542" for this suite. 11/15/23 17:03:30.797
------------------------------
• [0.514 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:03:30.309
    Nov 15 17:03:30.309: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename runtimeclass 11/15/23 17:03:30.313
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:03:30.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:03:30.384
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 11/15/23 17:03:30.4
    STEP: getting /apis/node.k8s.io 11/15/23 17:03:30.416
    STEP: getting /apis/node.k8s.io/v1 11/15/23 17:03:30.423
    STEP: creating 11/15/23 17:03:30.43
    STEP: watching 11/15/23 17:03:30.501
    Nov 15 17:03:30.501: INFO: starting watch
    STEP: getting 11/15/23 17:03:30.528
    STEP: listing 11/15/23 17:03:30.547
    STEP: patching 11/15/23 17:03:30.564
    STEP: updating 11/15/23 17:03:30.588
    Nov 15 17:03:30.609: INFO: waiting for watch events with expected annotations
    STEP: deleting 11/15/23 17:03:30.61
    STEP: deleting a collection 11/15/23 17:03:30.681
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:03:30.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-1542" for this suite. 11/15/23 17:03:30.797
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:03:30.825
Nov 15 17:03:30.825: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename security-context-test 11/15/23 17:03:30.827
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:03:30.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:03:30.911
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Nov 15 17:03:30.966: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-fb9138c5-5a4a-441d-8dc1-402d374f1d04" in namespace "security-context-test-5461" to be "Succeeded or Failed"
Nov 15 17:03:30.981: INFO: Pod "busybox-readonly-false-fb9138c5-5a4a-441d-8dc1-402d374f1d04": Phase="Pending", Reason="", readiness=false. Elapsed: 14.289292ms
Nov 15 17:03:32.997: INFO: Pod "busybox-readonly-false-fb9138c5-5a4a-441d-8dc1-402d374f1d04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030628317s
Nov 15 17:03:34.997: INFO: Pod "busybox-readonly-false-fb9138c5-5a4a-441d-8dc1-402d374f1d04": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030743456s
Nov 15 17:03:37.002: INFO: Pod "busybox-readonly-false-fb9138c5-5a4a-441d-8dc1-402d374f1d04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036127132s
Nov 15 17:03:37.003: INFO: Pod "busybox-readonly-false-fb9138c5-5a4a-441d-8dc1-402d374f1d04" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Nov 15 17:03:37.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-5461" for this suite. 11/15/23 17:03:37.03
------------------------------
• [SLOW TEST] [6.230 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:03:30.825
    Nov 15 17:03:30.825: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename security-context-test 11/15/23 17:03:30.827
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:03:30.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:03:30.911
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Nov 15 17:03:30.966: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-fb9138c5-5a4a-441d-8dc1-402d374f1d04" in namespace "security-context-test-5461" to be "Succeeded or Failed"
    Nov 15 17:03:30.981: INFO: Pod "busybox-readonly-false-fb9138c5-5a4a-441d-8dc1-402d374f1d04": Phase="Pending", Reason="", readiness=false. Elapsed: 14.289292ms
    Nov 15 17:03:32.997: INFO: Pod "busybox-readonly-false-fb9138c5-5a4a-441d-8dc1-402d374f1d04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030628317s
    Nov 15 17:03:34.997: INFO: Pod "busybox-readonly-false-fb9138c5-5a4a-441d-8dc1-402d374f1d04": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030743456s
    Nov 15 17:03:37.002: INFO: Pod "busybox-readonly-false-fb9138c5-5a4a-441d-8dc1-402d374f1d04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036127132s
    Nov 15 17:03:37.003: INFO: Pod "busybox-readonly-false-fb9138c5-5a4a-441d-8dc1-402d374f1d04" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:03:37.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-5461" for this suite. 11/15/23 17:03:37.03
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:03:37.058
Nov 15 17:03:37.058: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename subpath 11/15/23 17:03:37.061
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:03:37.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:03:37.128
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 11/15/23 17:03:37.145
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-x4xd 11/15/23 17:03:37.19
STEP: Creating a pod to test atomic-volume-subpath 11/15/23 17:03:37.191
Nov 15 17:03:37.225: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-x4xd" in namespace "subpath-7212" to be "Succeeded or Failed"
Nov 15 17:03:37.239: INFO: Pod "pod-subpath-test-configmap-x4xd": Phase="Pending", Reason="", readiness=false. Elapsed: 14.43775ms
Nov 15 17:03:39.257: INFO: Pod "pod-subpath-test-configmap-x4xd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031864422s
Nov 15 17:03:41.267: INFO: Pod "pod-subpath-test-configmap-x4xd": Phase="Running", Reason="", readiness=true. Elapsed: 4.041809378s
Nov 15 17:03:43.257: INFO: Pod "pod-subpath-test-configmap-x4xd": Phase="Running", Reason="", readiness=true. Elapsed: 6.03155799s
Nov 15 17:03:45.257: INFO: Pod "pod-subpath-test-configmap-x4xd": Phase="Running", Reason="", readiness=true. Elapsed: 8.031801304s
Nov 15 17:03:47.261: INFO: Pod "pod-subpath-test-configmap-x4xd": Phase="Running", Reason="", readiness=true. Elapsed: 10.036069104s
Nov 15 17:03:49.266: INFO: Pod "pod-subpath-test-configmap-x4xd": Phase="Running", Reason="", readiness=true. Elapsed: 12.041195075s
Nov 15 17:03:51.257: INFO: Pod "pod-subpath-test-configmap-x4xd": Phase="Running", Reason="", readiness=true. Elapsed: 14.032118827s
Nov 15 17:03:53.260: INFO: Pod "pod-subpath-test-configmap-x4xd": Phase="Running", Reason="", readiness=true. Elapsed: 16.03450937s
Nov 15 17:03:55.258: INFO: Pod "pod-subpath-test-configmap-x4xd": Phase="Running", Reason="", readiness=true. Elapsed: 18.033409749s
Nov 15 17:03:57.260: INFO: Pod "pod-subpath-test-configmap-x4xd": Phase="Running", Reason="", readiness=true. Elapsed: 20.035416015s
Nov 15 17:03:59.261: INFO: Pod "pod-subpath-test-configmap-x4xd": Phase="Running", Reason="", readiness=true. Elapsed: 22.035543145s
Nov 15 17:04:01.261: INFO: Pod "pod-subpath-test-configmap-x4xd": Phase="Running", Reason="", readiness=false. Elapsed: 24.036409441s
Nov 15 17:04:03.255: INFO: Pod "pod-subpath-test-configmap-x4xd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.030381219s
STEP: Saw pod success 11/15/23 17:04:03.255
Nov 15 17:04:03.256: INFO: Pod "pod-subpath-test-configmap-x4xd" satisfied condition "Succeeded or Failed"
Nov 15 17:04:03.269: INFO: Trying to get logs from node 10.15.40.115 pod pod-subpath-test-configmap-x4xd container test-container-subpath-configmap-x4xd: <nil>
STEP: delete the pod 11/15/23 17:04:03.314
Nov 15 17:04:03.391: INFO: Waiting for pod pod-subpath-test-configmap-x4xd to disappear
Nov 15 17:04:03.438: INFO: Pod pod-subpath-test-configmap-x4xd no longer exists
STEP: Deleting pod pod-subpath-test-configmap-x4xd 11/15/23 17:04:03.438
Nov 15 17:04:03.438: INFO: Deleting pod "pod-subpath-test-configmap-x4xd" in namespace "subpath-7212"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Nov 15 17:04:03.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-7212" for this suite. 11/15/23 17:04:03.477
------------------------------
• [SLOW TEST] [26.445 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:03:37.058
    Nov 15 17:03:37.058: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename subpath 11/15/23 17:03:37.061
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:03:37.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:03:37.128
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 11/15/23 17:03:37.145
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-x4xd 11/15/23 17:03:37.19
    STEP: Creating a pod to test atomic-volume-subpath 11/15/23 17:03:37.191
    Nov 15 17:03:37.225: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-x4xd" in namespace "subpath-7212" to be "Succeeded or Failed"
    Nov 15 17:03:37.239: INFO: Pod "pod-subpath-test-configmap-x4xd": Phase="Pending", Reason="", readiness=false. Elapsed: 14.43775ms
    Nov 15 17:03:39.257: INFO: Pod "pod-subpath-test-configmap-x4xd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031864422s
    Nov 15 17:03:41.267: INFO: Pod "pod-subpath-test-configmap-x4xd": Phase="Running", Reason="", readiness=true. Elapsed: 4.041809378s
    Nov 15 17:03:43.257: INFO: Pod "pod-subpath-test-configmap-x4xd": Phase="Running", Reason="", readiness=true. Elapsed: 6.03155799s
    Nov 15 17:03:45.257: INFO: Pod "pod-subpath-test-configmap-x4xd": Phase="Running", Reason="", readiness=true. Elapsed: 8.031801304s
    Nov 15 17:03:47.261: INFO: Pod "pod-subpath-test-configmap-x4xd": Phase="Running", Reason="", readiness=true. Elapsed: 10.036069104s
    Nov 15 17:03:49.266: INFO: Pod "pod-subpath-test-configmap-x4xd": Phase="Running", Reason="", readiness=true. Elapsed: 12.041195075s
    Nov 15 17:03:51.257: INFO: Pod "pod-subpath-test-configmap-x4xd": Phase="Running", Reason="", readiness=true. Elapsed: 14.032118827s
    Nov 15 17:03:53.260: INFO: Pod "pod-subpath-test-configmap-x4xd": Phase="Running", Reason="", readiness=true. Elapsed: 16.03450937s
    Nov 15 17:03:55.258: INFO: Pod "pod-subpath-test-configmap-x4xd": Phase="Running", Reason="", readiness=true. Elapsed: 18.033409749s
    Nov 15 17:03:57.260: INFO: Pod "pod-subpath-test-configmap-x4xd": Phase="Running", Reason="", readiness=true. Elapsed: 20.035416015s
    Nov 15 17:03:59.261: INFO: Pod "pod-subpath-test-configmap-x4xd": Phase="Running", Reason="", readiness=true. Elapsed: 22.035543145s
    Nov 15 17:04:01.261: INFO: Pod "pod-subpath-test-configmap-x4xd": Phase="Running", Reason="", readiness=false. Elapsed: 24.036409441s
    Nov 15 17:04:03.255: INFO: Pod "pod-subpath-test-configmap-x4xd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.030381219s
    STEP: Saw pod success 11/15/23 17:04:03.255
    Nov 15 17:04:03.256: INFO: Pod "pod-subpath-test-configmap-x4xd" satisfied condition "Succeeded or Failed"
    Nov 15 17:04:03.269: INFO: Trying to get logs from node 10.15.40.115 pod pod-subpath-test-configmap-x4xd container test-container-subpath-configmap-x4xd: <nil>
    STEP: delete the pod 11/15/23 17:04:03.314
    Nov 15 17:04:03.391: INFO: Waiting for pod pod-subpath-test-configmap-x4xd to disappear
    Nov 15 17:04:03.438: INFO: Pod pod-subpath-test-configmap-x4xd no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-x4xd 11/15/23 17:04:03.438
    Nov 15 17:04:03.438: INFO: Deleting pod "pod-subpath-test-configmap-x4xd" in namespace "subpath-7212"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:04:03.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-7212" for this suite. 11/15/23 17:04:03.477
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:04:03.511
Nov 15 17:04:03.511: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename services 11/15/23 17:04:03.514
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:04:03.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:04:03.582
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 15 17:04:03.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8672" for this suite. 11/15/23 17:04:03.639
------------------------------
• [0.156 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:04:03.511
    Nov 15 17:04:03.511: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename services 11/15/23 17:04:03.514
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:04:03.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:04:03.582
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:04:03.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8672" for this suite. 11/15/23 17:04:03.639
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:04:03.674
Nov 15 17:04:03.674: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename pod-network-test 11/15/23 17:04:03.676
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:04:03.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:04:03.745
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-3976 11/15/23 17:04:03.762
STEP: creating a selector 11/15/23 17:04:03.762
STEP: Creating the service pods in kubernetes 11/15/23 17:04:03.763
Nov 15 17:04:03.764: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 15 17:04:03.870: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3976" to be "running and ready"
Nov 15 17:04:03.887: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.443724ms
Nov 15 17:04:03.887: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 17:04:05.902: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031677519s
Nov 15 17:04:05.902: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 17:04:07.902: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.0316942s
Nov 15 17:04:07.902: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 17:04:09.908: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.037250865s
Nov 15 17:04:09.908: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 17:04:11.905: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.034250934s
Nov 15 17:04:11.905: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 17:04:13.903: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.032688418s
Nov 15 17:04:13.903: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 17:04:15.903: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.032916207s
Nov 15 17:04:15.903: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 17:04:17.903: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.032322422s
Nov 15 17:04:17.903: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 17:04:19.902: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.032022393s
Nov 15 17:04:19.903: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 17:04:21.910: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.039230509s
Nov 15 17:04:21.910: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 17:04:23.904: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.033143234s
Nov 15 17:04:23.904: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 15 17:04:25.906: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.035403701s
Nov 15 17:04:25.906: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Nov 15 17:04:25.906: INFO: Pod "netserver-0" satisfied condition "running and ready"
Nov 15 17:04:25.920: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3976" to be "running and ready"
Nov 15 17:04:25.949: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 27.692061ms
Nov 15 17:04:25.949: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Nov 15 17:04:25.950: INFO: Pod "netserver-1" satisfied condition "running and ready"
Nov 15 17:04:25.964: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3976" to be "running and ready"
Nov 15 17:04:25.980: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 15.804084ms
Nov 15 17:04:25.980: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Nov 15 17:04:25.981: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 11/15/23 17:04:25.997
Nov 15 17:04:26.018: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3976" to be "running"
Nov 15 17:04:26.032: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.086749ms
Nov 15 17:04:28.049: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.031389983s
Nov 15 17:04:28.049: INFO: Pod "test-container-pod" satisfied condition "running"
Nov 15 17:04:28.063: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Nov 15 17:04:28.064: INFO: Breadth first check of 172.30.191.99 on host 10.15.40.106...
Nov 15 17:04:28.079: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.164.33:9080/dial?request=hostname&protocol=udp&host=172.30.191.99&port=8081&tries=1'] Namespace:pod-network-test-3976 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 17:04:28.079: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
Nov 15 17:04:28.081: INFO: ExecWithOptions: Clientset creation
Nov 15 17:04:28.082: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3976/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.164.33%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.191.99%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Nov 15 17:04:28.419: INFO: Waiting for responses: map[]
Nov 15 17:04:28.419: INFO: reached 172.30.191.99 after 0/1 tries
Nov 15 17:04:28.419: INFO: Breadth first check of 172.30.205.246 on host 10.15.40.114...
Nov 15 17:04:28.435: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.164.33:9080/dial?request=hostname&protocol=udp&host=172.30.205.246&port=8081&tries=1'] Namespace:pod-network-test-3976 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 17:04:28.435: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
Nov 15 17:04:28.437: INFO: ExecWithOptions: Clientset creation
Nov 15 17:04:28.437: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3976/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.164.33%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.205.246%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Nov 15 17:04:28.711: INFO: Waiting for responses: map[]
Nov 15 17:04:28.711: INFO: reached 172.30.205.246 after 0/1 tries
Nov 15 17:04:28.711: INFO: Breadth first check of 172.30.164.44 on host 10.15.40.115...
Nov 15 17:04:28.727: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.164.33:9080/dial?request=hostname&protocol=udp&host=172.30.164.44&port=8081&tries=1'] Namespace:pod-network-test-3976 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 15 17:04:28.727: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
Nov 15 17:04:28.728: INFO: ExecWithOptions: Clientset creation
Nov 15 17:04:28.729: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3976/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.164.33%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.164.44%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Nov 15 17:04:29.095: INFO: Waiting for responses: map[]
Nov 15 17:04:29.096: INFO: reached 172.30.164.44 after 0/1 tries
Nov 15 17:04:29.096: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Nov 15 17:04:29.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-3976" for this suite. 11/15/23 17:04:29.122
------------------------------
• [SLOW TEST] [25.472 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:04:03.674
    Nov 15 17:04:03.674: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename pod-network-test 11/15/23 17:04:03.676
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:04:03.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:04:03.745
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-3976 11/15/23 17:04:03.762
    STEP: creating a selector 11/15/23 17:04:03.762
    STEP: Creating the service pods in kubernetes 11/15/23 17:04:03.763
    Nov 15 17:04:03.764: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Nov 15 17:04:03.870: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3976" to be "running and ready"
    Nov 15 17:04:03.887: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.443724ms
    Nov 15 17:04:03.887: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 17:04:05.902: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031677519s
    Nov 15 17:04:05.902: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 17:04:07.902: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.0316942s
    Nov 15 17:04:07.902: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 17:04:09.908: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.037250865s
    Nov 15 17:04:09.908: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 17:04:11.905: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.034250934s
    Nov 15 17:04:11.905: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 17:04:13.903: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.032688418s
    Nov 15 17:04:13.903: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 17:04:15.903: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.032916207s
    Nov 15 17:04:15.903: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 17:04:17.903: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.032322422s
    Nov 15 17:04:17.903: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 17:04:19.902: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.032022393s
    Nov 15 17:04:19.903: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 17:04:21.910: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.039230509s
    Nov 15 17:04:21.910: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 17:04:23.904: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.033143234s
    Nov 15 17:04:23.904: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 15 17:04:25.906: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.035403701s
    Nov 15 17:04:25.906: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Nov 15 17:04:25.906: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Nov 15 17:04:25.920: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3976" to be "running and ready"
    Nov 15 17:04:25.949: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 27.692061ms
    Nov 15 17:04:25.949: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Nov 15 17:04:25.950: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Nov 15 17:04:25.964: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3976" to be "running and ready"
    Nov 15 17:04:25.980: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 15.804084ms
    Nov 15 17:04:25.980: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Nov 15 17:04:25.981: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 11/15/23 17:04:25.997
    Nov 15 17:04:26.018: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3976" to be "running"
    Nov 15 17:04:26.032: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.086749ms
    Nov 15 17:04:28.049: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.031389983s
    Nov 15 17:04:28.049: INFO: Pod "test-container-pod" satisfied condition "running"
    Nov 15 17:04:28.063: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Nov 15 17:04:28.064: INFO: Breadth first check of 172.30.191.99 on host 10.15.40.106...
    Nov 15 17:04:28.079: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.164.33:9080/dial?request=hostname&protocol=udp&host=172.30.191.99&port=8081&tries=1'] Namespace:pod-network-test-3976 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 17:04:28.079: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    Nov 15 17:04:28.081: INFO: ExecWithOptions: Clientset creation
    Nov 15 17:04:28.082: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3976/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.164.33%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.191.99%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Nov 15 17:04:28.419: INFO: Waiting for responses: map[]
    Nov 15 17:04:28.419: INFO: reached 172.30.191.99 after 0/1 tries
    Nov 15 17:04:28.419: INFO: Breadth first check of 172.30.205.246 on host 10.15.40.114...
    Nov 15 17:04:28.435: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.164.33:9080/dial?request=hostname&protocol=udp&host=172.30.205.246&port=8081&tries=1'] Namespace:pod-network-test-3976 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 17:04:28.435: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    Nov 15 17:04:28.437: INFO: ExecWithOptions: Clientset creation
    Nov 15 17:04:28.437: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3976/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.164.33%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.205.246%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Nov 15 17:04:28.711: INFO: Waiting for responses: map[]
    Nov 15 17:04:28.711: INFO: reached 172.30.205.246 after 0/1 tries
    Nov 15 17:04:28.711: INFO: Breadth first check of 172.30.164.44 on host 10.15.40.115...
    Nov 15 17:04:28.727: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.164.33:9080/dial?request=hostname&protocol=udp&host=172.30.164.44&port=8081&tries=1'] Namespace:pod-network-test-3976 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 15 17:04:28.727: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    Nov 15 17:04:28.728: INFO: ExecWithOptions: Clientset creation
    Nov 15 17:04:28.729: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3976/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.164.33%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.164.44%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Nov 15 17:04:29.095: INFO: Waiting for responses: map[]
    Nov 15 17:04:29.096: INFO: reached 172.30.164.44 after 0/1 tries
    Nov 15 17:04:29.096: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:04:29.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-3976" for this suite. 11/15/23 17:04:29.122
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:04:29.156
Nov 15 17:04:29.157: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename projected 11/15/23 17:04:29.158
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:04:29.212
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:04:29.229
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-778e69d2-3291-4610-bbd3-9fc8c3567461 11/15/23 17:04:29.248
STEP: Creating a pod to test consume secrets 11/15/23 17:04:29.275
Nov 15 17:04:29.309: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1cbf593a-4d9c-474c-8275-db4c9b082782" in namespace "projected-6941" to be "Succeeded or Failed"
Nov 15 17:04:29.327: INFO: Pod "pod-projected-secrets-1cbf593a-4d9c-474c-8275-db4c9b082782": Phase="Pending", Reason="", readiness=false. Elapsed: 18.0349ms
Nov 15 17:04:31.348: INFO: Pod "pod-projected-secrets-1cbf593a-4d9c-474c-8275-db4c9b082782": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038953354s
Nov 15 17:04:33.344: INFO: Pod "pod-projected-secrets-1cbf593a-4d9c-474c-8275-db4c9b082782": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035391553s
Nov 15 17:04:35.344: INFO: Pod "pod-projected-secrets-1cbf593a-4d9c-474c-8275-db4c9b082782": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035085773s
STEP: Saw pod success 11/15/23 17:04:35.344
Nov 15 17:04:35.345: INFO: Pod "pod-projected-secrets-1cbf593a-4d9c-474c-8275-db4c9b082782" satisfied condition "Succeeded or Failed"
Nov 15 17:04:35.361: INFO: Trying to get logs from node 10.15.40.115 pod pod-projected-secrets-1cbf593a-4d9c-474c-8275-db4c9b082782 container projected-secret-volume-test: <nil>
STEP: delete the pod 11/15/23 17:04:35.402
Nov 15 17:04:35.439: INFO: Waiting for pod pod-projected-secrets-1cbf593a-4d9c-474c-8275-db4c9b082782 to disappear
Nov 15 17:04:35.453: INFO: Pod pod-projected-secrets-1cbf593a-4d9c-474c-8275-db4c9b082782 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Nov 15 17:04:35.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6941" for this suite. 11/15/23 17:04:35.478
------------------------------
• [SLOW TEST] [6.348 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:04:29.156
    Nov 15 17:04:29.157: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename projected 11/15/23 17:04:29.158
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:04:29.212
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:04:29.229
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-778e69d2-3291-4610-bbd3-9fc8c3567461 11/15/23 17:04:29.248
    STEP: Creating a pod to test consume secrets 11/15/23 17:04:29.275
    Nov 15 17:04:29.309: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1cbf593a-4d9c-474c-8275-db4c9b082782" in namespace "projected-6941" to be "Succeeded or Failed"
    Nov 15 17:04:29.327: INFO: Pod "pod-projected-secrets-1cbf593a-4d9c-474c-8275-db4c9b082782": Phase="Pending", Reason="", readiness=false. Elapsed: 18.0349ms
    Nov 15 17:04:31.348: INFO: Pod "pod-projected-secrets-1cbf593a-4d9c-474c-8275-db4c9b082782": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038953354s
    Nov 15 17:04:33.344: INFO: Pod "pod-projected-secrets-1cbf593a-4d9c-474c-8275-db4c9b082782": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035391553s
    Nov 15 17:04:35.344: INFO: Pod "pod-projected-secrets-1cbf593a-4d9c-474c-8275-db4c9b082782": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035085773s
    STEP: Saw pod success 11/15/23 17:04:35.344
    Nov 15 17:04:35.345: INFO: Pod "pod-projected-secrets-1cbf593a-4d9c-474c-8275-db4c9b082782" satisfied condition "Succeeded or Failed"
    Nov 15 17:04:35.361: INFO: Trying to get logs from node 10.15.40.115 pod pod-projected-secrets-1cbf593a-4d9c-474c-8275-db4c9b082782 container projected-secret-volume-test: <nil>
    STEP: delete the pod 11/15/23 17:04:35.402
    Nov 15 17:04:35.439: INFO: Waiting for pod pod-projected-secrets-1cbf593a-4d9c-474c-8275-db4c9b082782 to disappear
    Nov 15 17:04:35.453: INFO: Pod pod-projected-secrets-1cbf593a-4d9c-474c-8275-db4c9b082782 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:04:35.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6941" for this suite. 11/15/23 17:04:35.478
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:04:35.508
Nov 15 17:04:35.508: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename replication-controller 11/15/23 17:04:35.51
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:04:35.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:04:35.578
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-nvzz6" 11/15/23 17:04:35.596
Nov 15 17:04:35.616: INFO: Get Replication Controller "e2e-rc-nvzz6" to confirm replicas
Nov 15 17:04:36.636: INFO: Get Replication Controller "e2e-rc-nvzz6" to confirm replicas
Nov 15 17:04:36.655: INFO: Found 1 replicas for "e2e-rc-nvzz6" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-nvzz6" 11/15/23 17:04:36.655
STEP: Updating a scale subresource 11/15/23 17:04:36.675
STEP: Verifying replicas where modified for replication controller "e2e-rc-nvzz6" 11/15/23 17:04:36.693
Nov 15 17:04:36.693: INFO: Get Replication Controller "e2e-rc-nvzz6" to confirm replicas
Nov 15 17:04:37.711: INFO: Get Replication Controller "e2e-rc-nvzz6" to confirm replicas
Nov 15 17:04:37.727: INFO: Found 2 replicas for "e2e-rc-nvzz6" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Nov 15 17:04:37.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-4199" for this suite. 11/15/23 17:04:37.753
------------------------------
• [2.270 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:04:35.508
    Nov 15 17:04:35.508: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename replication-controller 11/15/23 17:04:35.51
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:04:35.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:04:35.578
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-nvzz6" 11/15/23 17:04:35.596
    Nov 15 17:04:35.616: INFO: Get Replication Controller "e2e-rc-nvzz6" to confirm replicas
    Nov 15 17:04:36.636: INFO: Get Replication Controller "e2e-rc-nvzz6" to confirm replicas
    Nov 15 17:04:36.655: INFO: Found 1 replicas for "e2e-rc-nvzz6" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-nvzz6" 11/15/23 17:04:36.655
    STEP: Updating a scale subresource 11/15/23 17:04:36.675
    STEP: Verifying replicas where modified for replication controller "e2e-rc-nvzz6" 11/15/23 17:04:36.693
    Nov 15 17:04:36.693: INFO: Get Replication Controller "e2e-rc-nvzz6" to confirm replicas
    Nov 15 17:04:37.711: INFO: Get Replication Controller "e2e-rc-nvzz6" to confirm replicas
    Nov 15 17:04:37.727: INFO: Found 2 replicas for "e2e-rc-nvzz6" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:04:37.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-4199" for this suite. 11/15/23 17:04:37.753
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:04:37.78
Nov 15 17:04:37.780: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename downward-api 11/15/23 17:04:37.787
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:04:37.835
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:04:37.857
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 11/15/23 17:04:37.874
Nov 15 17:04:37.909: INFO: Waiting up to 5m0s for pod "downwardapi-volume-977b73a5-b86c-4085-be87-da6a4a26975a" in namespace "downward-api-5065" to be "Succeeded or Failed"
Nov 15 17:04:37.923: INFO: Pod "downwardapi-volume-977b73a5-b86c-4085-be87-da6a4a26975a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.880109ms
Nov 15 17:04:39.940: INFO: Pod "downwardapi-volume-977b73a5-b86c-4085-be87-da6a4a26975a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031044246s
Nov 15 17:04:41.943: INFO: Pod "downwardapi-volume-977b73a5-b86c-4085-be87-da6a4a26975a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033683013s
STEP: Saw pod success 11/15/23 17:04:41.943
Nov 15 17:04:41.943: INFO: Pod "downwardapi-volume-977b73a5-b86c-4085-be87-da6a4a26975a" satisfied condition "Succeeded or Failed"
Nov 15 17:04:41.958: INFO: Trying to get logs from node 10.15.40.115 pod downwardapi-volume-977b73a5-b86c-4085-be87-da6a4a26975a container client-container: <nil>
STEP: delete the pod 11/15/23 17:04:42
Nov 15 17:04:42.044: INFO: Waiting for pod downwardapi-volume-977b73a5-b86c-4085-be87-da6a4a26975a to disappear
Nov 15 17:04:42.062: INFO: Pod downwardapi-volume-977b73a5-b86c-4085-be87-da6a4a26975a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Nov 15 17:04:42.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5065" for this suite. 11/15/23 17:04:42.085
------------------------------
• [4.332 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:04:37.78
    Nov 15 17:04:37.780: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename downward-api 11/15/23 17:04:37.787
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:04:37.835
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:04:37.857
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 11/15/23 17:04:37.874
    Nov 15 17:04:37.909: INFO: Waiting up to 5m0s for pod "downwardapi-volume-977b73a5-b86c-4085-be87-da6a4a26975a" in namespace "downward-api-5065" to be "Succeeded or Failed"
    Nov 15 17:04:37.923: INFO: Pod "downwardapi-volume-977b73a5-b86c-4085-be87-da6a4a26975a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.880109ms
    Nov 15 17:04:39.940: INFO: Pod "downwardapi-volume-977b73a5-b86c-4085-be87-da6a4a26975a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031044246s
    Nov 15 17:04:41.943: INFO: Pod "downwardapi-volume-977b73a5-b86c-4085-be87-da6a4a26975a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033683013s
    STEP: Saw pod success 11/15/23 17:04:41.943
    Nov 15 17:04:41.943: INFO: Pod "downwardapi-volume-977b73a5-b86c-4085-be87-da6a4a26975a" satisfied condition "Succeeded or Failed"
    Nov 15 17:04:41.958: INFO: Trying to get logs from node 10.15.40.115 pod downwardapi-volume-977b73a5-b86c-4085-be87-da6a4a26975a container client-container: <nil>
    STEP: delete the pod 11/15/23 17:04:42
    Nov 15 17:04:42.044: INFO: Waiting for pod downwardapi-volume-977b73a5-b86c-4085-be87-da6a4a26975a to disappear
    Nov 15 17:04:42.062: INFO: Pod downwardapi-volume-977b73a5-b86c-4085-be87-da6a4a26975a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:04:42.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5065" for this suite. 11/15/23 17:04:42.085
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:04:42.127
Nov 15 17:04:42.127: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename projected 11/15/23 17:04:42.129
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:04:42.194
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:04:42.212
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 11/15/23 17:04:42.23
Nov 15 17:04:42.265: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a2580077-f4ca-49c5-9534-6788e706845f" in namespace "projected-6835" to be "Succeeded or Failed"
Nov 15 17:04:42.280: INFO: Pod "downwardapi-volume-a2580077-f4ca-49c5-9534-6788e706845f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.345575ms
Nov 15 17:04:44.298: INFO: Pod "downwardapi-volume-a2580077-f4ca-49c5-9534-6788e706845f": Phase="Running", Reason="", readiness=true. Elapsed: 2.032871166s
Nov 15 17:04:46.299: INFO: Pod "downwardapi-volume-a2580077-f4ca-49c5-9534-6788e706845f": Phase="Running", Reason="", readiness=false. Elapsed: 4.033624303s
Nov 15 17:04:48.297: INFO: Pod "downwardapi-volume-a2580077-f4ca-49c5-9534-6788e706845f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032175239s
STEP: Saw pod success 11/15/23 17:04:48.297
Nov 15 17:04:48.298: INFO: Pod "downwardapi-volume-a2580077-f4ca-49c5-9534-6788e706845f" satisfied condition "Succeeded or Failed"
Nov 15 17:04:48.313: INFO: Trying to get logs from node 10.15.40.115 pod downwardapi-volume-a2580077-f4ca-49c5-9534-6788e706845f container client-container: <nil>
STEP: delete the pod 11/15/23 17:04:48.352
Nov 15 17:04:48.392: INFO: Waiting for pod downwardapi-volume-a2580077-f4ca-49c5-9534-6788e706845f to disappear
Nov 15 17:04:48.407: INFO: Pod downwardapi-volume-a2580077-f4ca-49c5-9534-6788e706845f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Nov 15 17:04:48.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6835" for this suite. 11/15/23 17:04:48.431
------------------------------
• [SLOW TEST] [6.329 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:04:42.127
    Nov 15 17:04:42.127: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename projected 11/15/23 17:04:42.129
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:04:42.194
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:04:42.212
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 11/15/23 17:04:42.23
    Nov 15 17:04:42.265: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a2580077-f4ca-49c5-9534-6788e706845f" in namespace "projected-6835" to be "Succeeded or Failed"
    Nov 15 17:04:42.280: INFO: Pod "downwardapi-volume-a2580077-f4ca-49c5-9534-6788e706845f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.345575ms
    Nov 15 17:04:44.298: INFO: Pod "downwardapi-volume-a2580077-f4ca-49c5-9534-6788e706845f": Phase="Running", Reason="", readiness=true. Elapsed: 2.032871166s
    Nov 15 17:04:46.299: INFO: Pod "downwardapi-volume-a2580077-f4ca-49c5-9534-6788e706845f": Phase="Running", Reason="", readiness=false. Elapsed: 4.033624303s
    Nov 15 17:04:48.297: INFO: Pod "downwardapi-volume-a2580077-f4ca-49c5-9534-6788e706845f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032175239s
    STEP: Saw pod success 11/15/23 17:04:48.297
    Nov 15 17:04:48.298: INFO: Pod "downwardapi-volume-a2580077-f4ca-49c5-9534-6788e706845f" satisfied condition "Succeeded or Failed"
    Nov 15 17:04:48.313: INFO: Trying to get logs from node 10.15.40.115 pod downwardapi-volume-a2580077-f4ca-49c5-9534-6788e706845f container client-container: <nil>
    STEP: delete the pod 11/15/23 17:04:48.352
    Nov 15 17:04:48.392: INFO: Waiting for pod downwardapi-volume-a2580077-f4ca-49c5-9534-6788e706845f to disappear
    Nov 15 17:04:48.407: INFO: Pod downwardapi-volume-a2580077-f4ca-49c5-9534-6788e706845f no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:04:48.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6835" for this suite. 11/15/23 17:04:48.431
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:04:48.469
Nov 15 17:04:48.469: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename daemonsets 11/15/23 17:04:48.471
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:04:48.539
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:04:48.553
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385
Nov 15 17:04:48.661: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 11/15/23 17:04:48.683
Nov 15 17:04:48.720: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 17:04:48.720: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
Nov 15 17:04:49.761: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 17:04:49.761: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
Nov 15 17:04:50.760: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 15 17:04:50.760: INFO: Node 10.15.40.115 is running 0 daemon pod, expected 1
Nov 15 17:04:51.760: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 15 17:04:51.760: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 11/15/23 17:04:51.844
STEP: Check that daemon pods images are updated. 11/15/23 17:04:51.889
Nov 15 17:04:51.911: INFO: Wrong image for pod: daemon-set-4qlgv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Nov 15 17:04:51.911: INFO: Wrong image for pod: daemon-set-nw4jp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Nov 15 17:04:51.911: INFO: Wrong image for pod: daemon-set-vd54g. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Nov 15 17:04:52.957: INFO: Wrong image for pod: daemon-set-4qlgv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Nov 15 17:04:52.957: INFO: Wrong image for pod: daemon-set-nw4jp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Nov 15 17:04:53.951: INFO: Wrong image for pod: daemon-set-4qlgv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Nov 15 17:04:53.951: INFO: Pod daemon-set-fpqqv is not available
Nov 15 17:04:53.951: INFO: Wrong image for pod: daemon-set-nw4jp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Nov 15 17:04:54.951: INFO: Wrong image for pod: daemon-set-4qlgv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Nov 15 17:04:54.952: INFO: Pod daemon-set-fpqqv is not available
Nov 15 17:04:54.952: INFO: Wrong image for pod: daemon-set-nw4jp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Nov 15 17:04:55.951: INFO: Wrong image for pod: daemon-set-4qlgv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Nov 15 17:04:56.951: INFO: Wrong image for pod: daemon-set-4qlgv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Nov 15 17:04:57.951: INFO: Wrong image for pod: daemon-set-4qlgv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Nov 15 17:04:57.951: INFO: Pod daemon-set-nwwzs is not available
Nov 15 17:04:58.955: INFO: Wrong image for pod: daemon-set-4qlgv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Nov 15 17:05:00.952: INFO: Pod daemon-set-rw9mv is not available
STEP: Check that daemon pods are still running on every node of the cluster. 11/15/23 17:05:00.974
Nov 15 17:05:01.010: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 15 17:05:01.010: INFO: Node 10.15.40.114 is running 0 daemon pod, expected 1
Nov 15 17:05:02.057: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 15 17:05:02.057: INFO: Node 10.15.40.114 is running 0 daemon pod, expected 1
Nov 15 17:05:03.054: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 15 17:05:03.054: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 11/15/23 17:05:03.156
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3178, will wait for the garbage collector to delete the pods 11/15/23 17:05:03.156
Nov 15 17:05:03.259: INFO: Deleting DaemonSet.extensions daemon-set took: 32.525559ms
Nov 15 17:05:03.459: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.513547ms
Nov 15 17:05:05.877: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 17:05:05.877: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Nov 15 17:05:05.902: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"51397"},"items":null}

Nov 15 17:05:05.917: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"51397"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 17:05:05.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3178" for this suite. 11/15/23 17:05:06.015
------------------------------
• [SLOW TEST] [17.570 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:04:48.469
    Nov 15 17:04:48.469: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename daemonsets 11/15/23 17:04:48.471
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:04:48.539
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:04:48.553
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:385
    Nov 15 17:04:48.661: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 11/15/23 17:04:48.683
    Nov 15 17:04:48.720: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 17:04:48.720: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
    Nov 15 17:04:49.761: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 17:04:49.761: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
    Nov 15 17:04:50.760: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Nov 15 17:04:50.760: INFO: Node 10.15.40.115 is running 0 daemon pod, expected 1
    Nov 15 17:04:51.760: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Nov 15 17:04:51.760: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 11/15/23 17:04:51.844
    STEP: Check that daemon pods images are updated. 11/15/23 17:04:51.889
    Nov 15 17:04:51.911: INFO: Wrong image for pod: daemon-set-4qlgv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Nov 15 17:04:51.911: INFO: Wrong image for pod: daemon-set-nw4jp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Nov 15 17:04:51.911: INFO: Wrong image for pod: daemon-set-vd54g. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Nov 15 17:04:52.957: INFO: Wrong image for pod: daemon-set-4qlgv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Nov 15 17:04:52.957: INFO: Wrong image for pod: daemon-set-nw4jp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Nov 15 17:04:53.951: INFO: Wrong image for pod: daemon-set-4qlgv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Nov 15 17:04:53.951: INFO: Pod daemon-set-fpqqv is not available
    Nov 15 17:04:53.951: INFO: Wrong image for pod: daemon-set-nw4jp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Nov 15 17:04:54.951: INFO: Wrong image for pod: daemon-set-4qlgv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Nov 15 17:04:54.952: INFO: Pod daemon-set-fpqqv is not available
    Nov 15 17:04:54.952: INFO: Wrong image for pod: daemon-set-nw4jp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Nov 15 17:04:55.951: INFO: Wrong image for pod: daemon-set-4qlgv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Nov 15 17:04:56.951: INFO: Wrong image for pod: daemon-set-4qlgv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Nov 15 17:04:57.951: INFO: Wrong image for pod: daemon-set-4qlgv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Nov 15 17:04:57.951: INFO: Pod daemon-set-nwwzs is not available
    Nov 15 17:04:58.955: INFO: Wrong image for pod: daemon-set-4qlgv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Nov 15 17:05:00.952: INFO: Pod daemon-set-rw9mv is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 11/15/23 17:05:00.974
    Nov 15 17:05:01.010: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Nov 15 17:05:01.010: INFO: Node 10.15.40.114 is running 0 daemon pod, expected 1
    Nov 15 17:05:02.057: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Nov 15 17:05:02.057: INFO: Node 10.15.40.114 is running 0 daemon pod, expected 1
    Nov 15 17:05:03.054: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Nov 15 17:05:03.054: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 11/15/23 17:05:03.156
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3178, will wait for the garbage collector to delete the pods 11/15/23 17:05:03.156
    Nov 15 17:05:03.259: INFO: Deleting DaemonSet.extensions daemon-set took: 32.525559ms
    Nov 15 17:05:03.459: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.513547ms
    Nov 15 17:05:05.877: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 17:05:05.877: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Nov 15 17:05:05.902: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"51397"},"items":null}

    Nov 15 17:05:05.917: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"51397"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:05:05.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3178" for this suite. 11/15/23 17:05:06.015
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:05:06.047
Nov 15 17:05:06.047: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename container-probe 11/15/23 17:05:06.048
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:05:06.124
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:05:06.144
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-00788b30-da20-46a5-98cc-97d30fe3ac6c in namespace container-probe-2626 11/15/23 17:05:06.165
Nov 15 17:05:06.208: INFO: Waiting up to 5m0s for pod "liveness-00788b30-da20-46a5-98cc-97d30fe3ac6c" in namespace "container-probe-2626" to be "not pending"
Nov 15 17:05:06.223: INFO: Pod "liveness-00788b30-da20-46a5-98cc-97d30fe3ac6c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.87379ms
Nov 15 17:05:08.239: INFO: Pod "liveness-00788b30-da20-46a5-98cc-97d30fe3ac6c": Phase="Running", Reason="", readiness=true. Elapsed: 2.031395309s
Nov 15 17:05:08.240: INFO: Pod "liveness-00788b30-da20-46a5-98cc-97d30fe3ac6c" satisfied condition "not pending"
Nov 15 17:05:08.240: INFO: Started pod liveness-00788b30-da20-46a5-98cc-97d30fe3ac6c in namespace container-probe-2626
STEP: checking the pod's current state and verifying that restartCount is present 11/15/23 17:05:08.24
Nov 15 17:05:08.256: INFO: Initial restart count of pod liveness-00788b30-da20-46a5-98cc-97d30fe3ac6c is 0
Nov 15 17:05:28.452: INFO: Restart count of pod container-probe-2626/liveness-00788b30-da20-46a5-98cc-97d30fe3ac6c is now 1 (20.195761215s elapsed)
STEP: deleting the pod 11/15/23 17:05:28.452
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Nov 15 17:05:28.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2626" for this suite. 11/15/23 17:05:28.516
------------------------------
• [SLOW TEST] [22.496 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:05:06.047
    Nov 15 17:05:06.047: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename container-probe 11/15/23 17:05:06.048
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:05:06.124
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:05:06.144
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-00788b30-da20-46a5-98cc-97d30fe3ac6c in namespace container-probe-2626 11/15/23 17:05:06.165
    Nov 15 17:05:06.208: INFO: Waiting up to 5m0s for pod "liveness-00788b30-da20-46a5-98cc-97d30fe3ac6c" in namespace "container-probe-2626" to be "not pending"
    Nov 15 17:05:06.223: INFO: Pod "liveness-00788b30-da20-46a5-98cc-97d30fe3ac6c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.87379ms
    Nov 15 17:05:08.239: INFO: Pod "liveness-00788b30-da20-46a5-98cc-97d30fe3ac6c": Phase="Running", Reason="", readiness=true. Elapsed: 2.031395309s
    Nov 15 17:05:08.240: INFO: Pod "liveness-00788b30-da20-46a5-98cc-97d30fe3ac6c" satisfied condition "not pending"
    Nov 15 17:05:08.240: INFO: Started pod liveness-00788b30-da20-46a5-98cc-97d30fe3ac6c in namespace container-probe-2626
    STEP: checking the pod's current state and verifying that restartCount is present 11/15/23 17:05:08.24
    Nov 15 17:05:08.256: INFO: Initial restart count of pod liveness-00788b30-da20-46a5-98cc-97d30fe3ac6c is 0
    Nov 15 17:05:28.452: INFO: Restart count of pod container-probe-2626/liveness-00788b30-da20-46a5-98cc-97d30fe3ac6c is now 1 (20.195761215s elapsed)
    STEP: deleting the pod 11/15/23 17:05:28.452
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:05:28.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2626" for this suite. 11/15/23 17:05:28.516
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:05:28.549
Nov 15 17:05:28.549: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename runtimeclass 11/15/23 17:05:28.551
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:05:28.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:05:28.62
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Nov 15 17:05:28.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-7695" for this suite. 11/15/23 17:05:28.714
------------------------------
• [0.190 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:05:28.549
    Nov 15 17:05:28.549: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename runtimeclass 11/15/23 17:05:28.551
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:05:28.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:05:28.62
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:05:28.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-7695" for this suite. 11/15/23 17:05:28.714
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:05:28.745
Nov 15 17:05:28.745: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename projected 11/15/23 17:05:28.747
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:05:28.798
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:05:28.814
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-793d36fc-d336-45d8-bad1-ace6e1fe2997 11/15/23 17:05:28.833
STEP: Creating a pod to test consume secrets 11/15/23 17:05:28.856
Nov 15 17:05:28.899: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-06d2278d-9def-4ef6-9579-f88c3fe97899" in namespace "projected-7154" to be "Succeeded or Failed"
Nov 15 17:05:28.922: INFO: Pod "pod-projected-secrets-06d2278d-9def-4ef6-9579-f88c3fe97899": Phase="Pending", Reason="", readiness=false. Elapsed: 19.405195ms
Nov 15 17:05:30.939: INFO: Pod "pod-projected-secrets-06d2278d-9def-4ef6-9579-f88c3fe97899": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036426788s
Nov 15 17:05:32.939: INFO: Pod "pod-projected-secrets-06d2278d-9def-4ef6-9579-f88c3fe97899": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035829029s
Nov 15 17:05:34.946: INFO: Pod "pod-projected-secrets-06d2278d-9def-4ef6-9579-f88c3fe97899": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043508132s
STEP: Saw pod success 11/15/23 17:05:34.946
Nov 15 17:05:34.947: INFO: Pod "pod-projected-secrets-06d2278d-9def-4ef6-9579-f88c3fe97899" satisfied condition "Succeeded or Failed"
Nov 15 17:05:34.962: INFO: Trying to get logs from node 10.15.40.115 pod pod-projected-secrets-06d2278d-9def-4ef6-9579-f88c3fe97899 container projected-secret-volume-test: <nil>
STEP: delete the pod 11/15/23 17:05:35.007
Nov 15 17:05:35.043: INFO: Waiting for pod pod-projected-secrets-06d2278d-9def-4ef6-9579-f88c3fe97899 to disappear
Nov 15 17:05:35.058: INFO: Pod pod-projected-secrets-06d2278d-9def-4ef6-9579-f88c3fe97899 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Nov 15 17:05:35.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7154" for this suite. 11/15/23 17:05:35.083
------------------------------
• [SLOW TEST] [6.363 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:05:28.745
    Nov 15 17:05:28.745: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename projected 11/15/23 17:05:28.747
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:05:28.798
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:05:28.814
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-793d36fc-d336-45d8-bad1-ace6e1fe2997 11/15/23 17:05:28.833
    STEP: Creating a pod to test consume secrets 11/15/23 17:05:28.856
    Nov 15 17:05:28.899: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-06d2278d-9def-4ef6-9579-f88c3fe97899" in namespace "projected-7154" to be "Succeeded or Failed"
    Nov 15 17:05:28.922: INFO: Pod "pod-projected-secrets-06d2278d-9def-4ef6-9579-f88c3fe97899": Phase="Pending", Reason="", readiness=false. Elapsed: 19.405195ms
    Nov 15 17:05:30.939: INFO: Pod "pod-projected-secrets-06d2278d-9def-4ef6-9579-f88c3fe97899": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036426788s
    Nov 15 17:05:32.939: INFO: Pod "pod-projected-secrets-06d2278d-9def-4ef6-9579-f88c3fe97899": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035829029s
    Nov 15 17:05:34.946: INFO: Pod "pod-projected-secrets-06d2278d-9def-4ef6-9579-f88c3fe97899": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043508132s
    STEP: Saw pod success 11/15/23 17:05:34.946
    Nov 15 17:05:34.947: INFO: Pod "pod-projected-secrets-06d2278d-9def-4ef6-9579-f88c3fe97899" satisfied condition "Succeeded or Failed"
    Nov 15 17:05:34.962: INFO: Trying to get logs from node 10.15.40.115 pod pod-projected-secrets-06d2278d-9def-4ef6-9579-f88c3fe97899 container projected-secret-volume-test: <nil>
    STEP: delete the pod 11/15/23 17:05:35.007
    Nov 15 17:05:35.043: INFO: Waiting for pod pod-projected-secrets-06d2278d-9def-4ef6-9579-f88c3fe97899 to disappear
    Nov 15 17:05:35.058: INFO: Pod pod-projected-secrets-06d2278d-9def-4ef6-9579-f88c3fe97899 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:05:35.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7154" for this suite. 11/15/23 17:05:35.083
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:05:35.109
Nov 15 17:05:35.109: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename kubelet-test 11/15/23 17:05:35.112
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:05:35.159
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:05:35.172
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Nov 15 17:05:35.218: INFO: Waiting up to 5m0s for pod "busybox-scheduling-692142f7-9289-4812-9057-0d90f65651fc" in namespace "kubelet-test-3199" to be "running and ready"
Nov 15 17:05:35.232: INFO: Pod "busybox-scheduling-692142f7-9289-4812-9057-0d90f65651fc": Phase="Pending", Reason="", readiness=false. Elapsed: 14.626081ms
Nov 15 17:05:35.232: INFO: The phase of Pod busybox-scheduling-692142f7-9289-4812-9057-0d90f65651fc is Pending, waiting for it to be Running (with Ready = true)
Nov 15 17:05:37.256: INFO: Pod "busybox-scheduling-692142f7-9289-4812-9057-0d90f65651fc": Phase="Running", Reason="", readiness=true. Elapsed: 2.038124069s
Nov 15 17:05:37.256: INFO: The phase of Pod busybox-scheduling-692142f7-9289-4812-9057-0d90f65651fc is Running (Ready = true)
Nov 15 17:05:37.256: INFO: Pod "busybox-scheduling-692142f7-9289-4812-9057-0d90f65651fc" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Nov 15 17:05:37.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-3199" for this suite. 11/15/23 17:05:37.338
------------------------------
• [2.255 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:05:35.109
    Nov 15 17:05:35.109: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename kubelet-test 11/15/23 17:05:35.112
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:05:35.159
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:05:35.172
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Nov 15 17:05:35.218: INFO: Waiting up to 5m0s for pod "busybox-scheduling-692142f7-9289-4812-9057-0d90f65651fc" in namespace "kubelet-test-3199" to be "running and ready"
    Nov 15 17:05:35.232: INFO: Pod "busybox-scheduling-692142f7-9289-4812-9057-0d90f65651fc": Phase="Pending", Reason="", readiness=false. Elapsed: 14.626081ms
    Nov 15 17:05:35.232: INFO: The phase of Pod busybox-scheduling-692142f7-9289-4812-9057-0d90f65651fc is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 17:05:37.256: INFO: Pod "busybox-scheduling-692142f7-9289-4812-9057-0d90f65651fc": Phase="Running", Reason="", readiness=true. Elapsed: 2.038124069s
    Nov 15 17:05:37.256: INFO: The phase of Pod busybox-scheduling-692142f7-9289-4812-9057-0d90f65651fc is Running (Ready = true)
    Nov 15 17:05:37.256: INFO: Pod "busybox-scheduling-692142f7-9289-4812-9057-0d90f65651fc" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:05:37.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-3199" for this suite. 11/15/23 17:05:37.338
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:05:37.373
Nov 15 17:05:37.373: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename security-context 11/15/23 17:05:37.375
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:05:37.427
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:05:37.444
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 11/15/23 17:05:37.461
Nov 15 17:05:37.494: INFO: Waiting up to 5m0s for pod "security-context-e2b526ee-5f4a-4199-81ea-d895a9cb0ddd" in namespace "security-context-9859" to be "Succeeded or Failed"
Nov 15 17:05:37.509: INFO: Pod "security-context-e2b526ee-5f4a-4199-81ea-d895a9cb0ddd": Phase="Pending", Reason="", readiness=false. Elapsed: 15.144032ms
Nov 15 17:05:39.526: INFO: Pod "security-context-e2b526ee-5f4a-4199-81ea-d895a9cb0ddd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031917348s
Nov 15 17:05:41.529: INFO: Pod "security-context-e2b526ee-5f4a-4199-81ea-d895a9cb0ddd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034933249s
STEP: Saw pod success 11/15/23 17:05:41.529
Nov 15 17:05:41.530: INFO: Pod "security-context-e2b526ee-5f4a-4199-81ea-d895a9cb0ddd" satisfied condition "Succeeded or Failed"
Nov 15 17:05:41.548: INFO: Trying to get logs from node 10.15.40.115 pod security-context-e2b526ee-5f4a-4199-81ea-d895a9cb0ddd container test-container: <nil>
STEP: delete the pod 11/15/23 17:05:41.584
Nov 15 17:05:41.631: INFO: Waiting for pod security-context-e2b526ee-5f4a-4199-81ea-d895a9cb0ddd to disappear
Nov 15 17:05:41.672: INFO: Pod security-context-e2b526ee-5f4a-4199-81ea-d895a9cb0ddd no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Nov 15 17:05:41.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-9859" for this suite. 11/15/23 17:05:41.698
------------------------------
• [4.350 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:05:37.373
    Nov 15 17:05:37.373: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename security-context 11/15/23 17:05:37.375
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:05:37.427
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:05:37.444
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 11/15/23 17:05:37.461
    Nov 15 17:05:37.494: INFO: Waiting up to 5m0s for pod "security-context-e2b526ee-5f4a-4199-81ea-d895a9cb0ddd" in namespace "security-context-9859" to be "Succeeded or Failed"
    Nov 15 17:05:37.509: INFO: Pod "security-context-e2b526ee-5f4a-4199-81ea-d895a9cb0ddd": Phase="Pending", Reason="", readiness=false. Elapsed: 15.144032ms
    Nov 15 17:05:39.526: INFO: Pod "security-context-e2b526ee-5f4a-4199-81ea-d895a9cb0ddd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031917348s
    Nov 15 17:05:41.529: INFO: Pod "security-context-e2b526ee-5f4a-4199-81ea-d895a9cb0ddd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034933249s
    STEP: Saw pod success 11/15/23 17:05:41.529
    Nov 15 17:05:41.530: INFO: Pod "security-context-e2b526ee-5f4a-4199-81ea-d895a9cb0ddd" satisfied condition "Succeeded or Failed"
    Nov 15 17:05:41.548: INFO: Trying to get logs from node 10.15.40.115 pod security-context-e2b526ee-5f4a-4199-81ea-d895a9cb0ddd container test-container: <nil>
    STEP: delete the pod 11/15/23 17:05:41.584
    Nov 15 17:05:41.631: INFO: Waiting for pod security-context-e2b526ee-5f4a-4199-81ea-d895a9cb0ddd to disappear
    Nov 15 17:05:41.672: INFO: Pod security-context-e2b526ee-5f4a-4199-81ea-d895a9cb0ddd no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:05:41.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-9859" for this suite. 11/15/23 17:05:41.698
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:05:41.729
Nov 15 17:05:41.729: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename runtimeclass 11/15/23 17:05:41.731
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:05:41.79
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:05:41.804
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-4574-delete-me 11/15/23 17:05:41.84
STEP: Waiting for the RuntimeClass to disappear 11/15/23 17:05:41.872
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Nov 15 17:05:41.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-4574" for this suite. 11/15/23 17:05:41.948
------------------------------
• [0.244 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:05:41.729
    Nov 15 17:05:41.729: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename runtimeclass 11/15/23 17:05:41.731
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:05:41.79
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:05:41.804
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-4574-delete-me 11/15/23 17:05:41.84
    STEP: Waiting for the RuntimeClass to disappear 11/15/23 17:05:41.872
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:05:41.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-4574" for this suite. 11/15/23 17:05:41.948
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:05:41.974
Nov 15 17:05:41.974: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename taint-multiple-pods 11/15/23 17:05:41.978
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:05:42.052
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:05:42.067
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Nov 15 17:05:42.083: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 15 17:06:42.206: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Nov 15 17:06:42.222: INFO: Starting informer...
STEP: Starting pods... 11/15/23 17:06:42.223
Nov 15 17:06:42.503: INFO: Pod1 is running on 10.15.40.115. Tainting Node
Nov 15 17:06:42.740: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-1368" to be "running"
Nov 15 17:06:42.755: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.795908ms
Nov 15 17:06:44.772: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.031513551s
Nov 15 17:06:44.772: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Nov 15 17:06:44.772: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-1368" to be "running"
Nov 15 17:06:44.787: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 14.700306ms
Nov 15 17:06:44.787: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Nov 15 17:06:44.787: INFO: Pod2 is running on 10.15.40.115. Tainting Node
STEP: Trying to apply a taint on the Node 11/15/23 17:06:44.787
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 11/15/23 17:06:44.839
STEP: Waiting for Pod1 and Pod2 to be deleted 11/15/23 17:06:44.857
Nov 15 17:06:51.633: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Nov 15 17:07:11.748: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 11/15/23 17:07:11.797
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 17:07:11.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-1368" for this suite. 11/15/23 17:07:11.839
------------------------------
• [SLOW TEST] [89.891 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:05:41.974
    Nov 15 17:05:41.974: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename taint-multiple-pods 11/15/23 17:05:41.978
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:05:42.052
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:05:42.067
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Nov 15 17:05:42.083: INFO: Waiting up to 1m0s for all nodes to be ready
    Nov 15 17:06:42.206: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Nov 15 17:06:42.222: INFO: Starting informer...
    STEP: Starting pods... 11/15/23 17:06:42.223
    Nov 15 17:06:42.503: INFO: Pod1 is running on 10.15.40.115. Tainting Node
    Nov 15 17:06:42.740: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-1368" to be "running"
    Nov 15 17:06:42.755: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.795908ms
    Nov 15 17:06:44.772: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.031513551s
    Nov 15 17:06:44.772: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Nov 15 17:06:44.772: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-1368" to be "running"
    Nov 15 17:06:44.787: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 14.700306ms
    Nov 15 17:06:44.787: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Nov 15 17:06:44.787: INFO: Pod2 is running on 10.15.40.115. Tainting Node
    STEP: Trying to apply a taint on the Node 11/15/23 17:06:44.787
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 11/15/23 17:06:44.839
    STEP: Waiting for Pod1 and Pod2 to be deleted 11/15/23 17:06:44.857
    Nov 15 17:06:51.633: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Nov 15 17:07:11.748: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 11/15/23 17:07:11.797
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:07:11.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-1368" for this suite. 11/15/23 17:07:11.839
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:07:11.876
Nov 15 17:07:11.876: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename pods 11/15/23 17:07:11.878
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:07:11.937
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:07:11.951
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 11/15/23 17:07:11.969
STEP: submitting the pod to kubernetes 11/15/23 17:07:11.97
Nov 15 17:07:12.003: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-3a3fa349-6c0b-413b-8c55-19755dad04e2" in namespace "pods-9631" to be "running and ready"
Nov 15 17:07:12.017: INFO: Pod "pod-update-activedeadlineseconds-3a3fa349-6c0b-413b-8c55-19755dad04e2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.287619ms
Nov 15 17:07:12.017: INFO: The phase of Pod pod-update-activedeadlineseconds-3a3fa349-6c0b-413b-8c55-19755dad04e2 is Pending, waiting for it to be Running (with Ready = true)
Nov 15 17:07:14.035: INFO: Pod "pod-update-activedeadlineseconds-3a3fa349-6c0b-413b-8c55-19755dad04e2": Phase="Running", Reason="", readiness=true. Elapsed: 2.031643583s
Nov 15 17:07:14.035: INFO: The phase of Pod pod-update-activedeadlineseconds-3a3fa349-6c0b-413b-8c55-19755dad04e2 is Running (Ready = true)
Nov 15 17:07:14.035: INFO: Pod "pod-update-activedeadlineseconds-3a3fa349-6c0b-413b-8c55-19755dad04e2" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 11/15/23 17:07:14.049
STEP: updating the pod 11/15/23 17:07:14.064
Nov 15 17:07:14.618: INFO: Successfully updated pod "pod-update-activedeadlineseconds-3a3fa349-6c0b-413b-8c55-19755dad04e2"
Nov 15 17:07:14.618: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-3a3fa349-6c0b-413b-8c55-19755dad04e2" in namespace "pods-9631" to be "terminated with reason DeadlineExceeded"
Nov 15 17:07:14.632: INFO: Pod "pod-update-activedeadlineseconds-3a3fa349-6c0b-413b-8c55-19755dad04e2": Phase="Running", Reason="", readiness=true. Elapsed: 13.719597ms
Nov 15 17:07:16.649: INFO: Pod "pod-update-activedeadlineseconds-3a3fa349-6c0b-413b-8c55-19755dad04e2": Phase="Running", Reason="", readiness=true. Elapsed: 2.030584389s
Nov 15 17:07:18.650: INFO: Pod "pod-update-activedeadlineseconds-3a3fa349-6c0b-413b-8c55-19755dad04e2": Phase="Running", Reason="", readiness=false. Elapsed: 4.031617963s
Nov 15 17:07:20.655: INFO: Pod "pod-update-activedeadlineseconds-3a3fa349-6c0b-413b-8c55-19755dad04e2": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.037307096s
Nov 15 17:07:20.655: INFO: Pod "pod-update-activedeadlineseconds-3a3fa349-6c0b-413b-8c55-19755dad04e2" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Nov 15 17:07:20.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9631" for this suite. 11/15/23 17:07:20.681
------------------------------
• [SLOW TEST] [8.833 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:07:11.876
    Nov 15 17:07:11.876: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename pods 11/15/23 17:07:11.878
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:07:11.937
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:07:11.951
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 11/15/23 17:07:11.969
    STEP: submitting the pod to kubernetes 11/15/23 17:07:11.97
    Nov 15 17:07:12.003: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-3a3fa349-6c0b-413b-8c55-19755dad04e2" in namespace "pods-9631" to be "running and ready"
    Nov 15 17:07:12.017: INFO: Pod "pod-update-activedeadlineseconds-3a3fa349-6c0b-413b-8c55-19755dad04e2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.287619ms
    Nov 15 17:07:12.017: INFO: The phase of Pod pod-update-activedeadlineseconds-3a3fa349-6c0b-413b-8c55-19755dad04e2 is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 17:07:14.035: INFO: Pod "pod-update-activedeadlineseconds-3a3fa349-6c0b-413b-8c55-19755dad04e2": Phase="Running", Reason="", readiness=true. Elapsed: 2.031643583s
    Nov 15 17:07:14.035: INFO: The phase of Pod pod-update-activedeadlineseconds-3a3fa349-6c0b-413b-8c55-19755dad04e2 is Running (Ready = true)
    Nov 15 17:07:14.035: INFO: Pod "pod-update-activedeadlineseconds-3a3fa349-6c0b-413b-8c55-19755dad04e2" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 11/15/23 17:07:14.049
    STEP: updating the pod 11/15/23 17:07:14.064
    Nov 15 17:07:14.618: INFO: Successfully updated pod "pod-update-activedeadlineseconds-3a3fa349-6c0b-413b-8c55-19755dad04e2"
    Nov 15 17:07:14.618: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-3a3fa349-6c0b-413b-8c55-19755dad04e2" in namespace "pods-9631" to be "terminated with reason DeadlineExceeded"
    Nov 15 17:07:14.632: INFO: Pod "pod-update-activedeadlineseconds-3a3fa349-6c0b-413b-8c55-19755dad04e2": Phase="Running", Reason="", readiness=true. Elapsed: 13.719597ms
    Nov 15 17:07:16.649: INFO: Pod "pod-update-activedeadlineseconds-3a3fa349-6c0b-413b-8c55-19755dad04e2": Phase="Running", Reason="", readiness=true. Elapsed: 2.030584389s
    Nov 15 17:07:18.650: INFO: Pod "pod-update-activedeadlineseconds-3a3fa349-6c0b-413b-8c55-19755dad04e2": Phase="Running", Reason="", readiness=false. Elapsed: 4.031617963s
    Nov 15 17:07:20.655: INFO: Pod "pod-update-activedeadlineseconds-3a3fa349-6c0b-413b-8c55-19755dad04e2": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.037307096s
    Nov 15 17:07:20.655: INFO: Pod "pod-update-activedeadlineseconds-3a3fa349-6c0b-413b-8c55-19755dad04e2" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:07:20.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9631" for this suite. 11/15/23 17:07:20.681
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:07:20.718
Nov 15 17:07:20.718: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename replicaset 11/15/23 17:07:20.72
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:07:20.77
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:07:20.788
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 11/15/23 17:07:20.803
Nov 15 17:07:20.836: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov 15 17:07:25.854: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 11/15/23 17:07:25.854
STEP: getting scale subresource 11/15/23 17:07:25.854
STEP: updating a scale subresource 11/15/23 17:07:25.87
STEP: verifying the replicaset Spec.Replicas was modified 11/15/23 17:07:25.912
STEP: Patch a scale subresource 11/15/23 17:07:25.928
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Nov 15 17:07:25.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-2122" for this suite. 11/15/23 17:07:26.029
------------------------------
• [SLOW TEST] [5.336 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:07:20.718
    Nov 15 17:07:20.718: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename replicaset 11/15/23 17:07:20.72
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:07:20.77
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:07:20.788
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 11/15/23 17:07:20.803
    Nov 15 17:07:20.836: INFO: Pod name sample-pod: Found 0 pods out of 1
    Nov 15 17:07:25.854: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 11/15/23 17:07:25.854
    STEP: getting scale subresource 11/15/23 17:07:25.854
    STEP: updating a scale subresource 11/15/23 17:07:25.87
    STEP: verifying the replicaset Spec.Replicas was modified 11/15/23 17:07:25.912
    STEP: Patch a scale subresource 11/15/23 17:07:25.928
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:07:25.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-2122" for this suite. 11/15/23 17:07:26.029
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:07:26.055
Nov 15 17:07:26.056: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename pods 11/15/23 17:07:26.063
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:07:26.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:07:26.136
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 11/15/23 17:07:26.153
STEP: setting up watch 11/15/23 17:07:26.153
STEP: submitting the pod to kubernetes 11/15/23 17:07:26.271
STEP: verifying the pod is in kubernetes 11/15/23 17:07:26.317
STEP: verifying pod creation was observed 11/15/23 17:07:26.332
Nov 15 17:07:26.333: INFO: Waiting up to 5m0s for pod "pod-submit-remove-8f3f8690-38a6-4958-b8e7-90e8729827a9" in namespace "pods-4243" to be "running"
Nov 15 17:07:26.348: INFO: Pod "pod-submit-remove-8f3f8690-38a6-4958-b8e7-90e8729827a9": Phase="Pending", Reason="", readiness=false. Elapsed: 15.409462ms
Nov 15 17:07:28.365: INFO: Pod "pod-submit-remove-8f3f8690-38a6-4958-b8e7-90e8729827a9": Phase="Running", Reason="", readiness=true. Elapsed: 2.032490855s
Nov 15 17:07:28.365: INFO: Pod "pod-submit-remove-8f3f8690-38a6-4958-b8e7-90e8729827a9" satisfied condition "running"
STEP: deleting the pod gracefully 11/15/23 17:07:28.38
STEP: verifying pod deletion was observed 11/15/23 17:07:28.408
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Nov 15 17:07:30.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4243" for this suite. 11/15/23 17:07:30.906
------------------------------
• [4.877 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:07:26.055
    Nov 15 17:07:26.056: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename pods 11/15/23 17:07:26.063
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:07:26.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:07:26.136
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 11/15/23 17:07:26.153
    STEP: setting up watch 11/15/23 17:07:26.153
    STEP: submitting the pod to kubernetes 11/15/23 17:07:26.271
    STEP: verifying the pod is in kubernetes 11/15/23 17:07:26.317
    STEP: verifying pod creation was observed 11/15/23 17:07:26.332
    Nov 15 17:07:26.333: INFO: Waiting up to 5m0s for pod "pod-submit-remove-8f3f8690-38a6-4958-b8e7-90e8729827a9" in namespace "pods-4243" to be "running"
    Nov 15 17:07:26.348: INFO: Pod "pod-submit-remove-8f3f8690-38a6-4958-b8e7-90e8729827a9": Phase="Pending", Reason="", readiness=false. Elapsed: 15.409462ms
    Nov 15 17:07:28.365: INFO: Pod "pod-submit-remove-8f3f8690-38a6-4958-b8e7-90e8729827a9": Phase="Running", Reason="", readiness=true. Elapsed: 2.032490855s
    Nov 15 17:07:28.365: INFO: Pod "pod-submit-remove-8f3f8690-38a6-4958-b8e7-90e8729827a9" satisfied condition "running"
    STEP: deleting the pod gracefully 11/15/23 17:07:28.38
    STEP: verifying pod deletion was observed 11/15/23 17:07:28.408
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:07:30.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4243" for this suite. 11/15/23 17:07:30.906
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:07:30.938
Nov 15 17:07:30.938: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename daemonsets 11/15/23 17:07:30.942
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:07:30.997
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:07:31.013
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177
STEP: Creating simple DaemonSet "daemon-set" 11/15/23 17:07:31.123
STEP: Check that daemon pods launch on every node of the cluster. 11/15/23 17:07:31.154
Nov 15 17:07:31.196: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 17:07:31.196: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
Nov 15 17:07:32.261: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 17:07:32.261: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
Nov 15 17:07:33.239: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Nov 15 17:07:33.239: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
Nov 15 17:07:34.236: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 15 17:07:34.236: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 11/15/23 17:07:34.255
Nov 15 17:07:34.344: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 15 17:07:34.344: INFO: Node 10.15.40.115 is running 0 daemon pod, expected 1
Nov 15 17:07:35.384: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 15 17:07:35.384: INFO: Node 10.15.40.115 is running 0 daemon pod, expected 1
Nov 15 17:07:36.385: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 15 17:07:36.386: INFO: Node 10.15.40.115 is running 0 daemon pod, expected 1
Nov 15 17:07:37.399: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 15 17:07:37.400: INFO: Node 10.15.40.115 is running 0 daemon pod, expected 1
Nov 15 17:07:38.385: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 15 17:07:38.385: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 11/15/23 17:07:38.404
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-631, will wait for the garbage collector to delete the pods 11/15/23 17:07:38.404
Nov 15 17:07:38.507: INFO: Deleting DaemonSet.extensions daemon-set took: 32.366908ms
Nov 15 17:07:38.608: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.48234ms
Nov 15 17:07:41.026: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 15 17:07:41.026: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Nov 15 17:07:41.072: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"52178"},"items":null}

Nov 15 17:07:41.087: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"52178"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 17:07:41.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-631" for this suite. 11/15/23 17:07:41.177
------------------------------
• [SLOW TEST] [10.264 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:07:30.938
    Nov 15 17:07:30.938: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename daemonsets 11/15/23 17:07:30.942
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:07:30.997
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:07:31.013
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:177
    STEP: Creating simple DaemonSet "daemon-set" 11/15/23 17:07:31.123
    STEP: Check that daemon pods launch on every node of the cluster. 11/15/23 17:07:31.154
    Nov 15 17:07:31.196: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 17:07:31.196: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
    Nov 15 17:07:32.261: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 17:07:32.261: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
    Nov 15 17:07:33.239: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Nov 15 17:07:33.239: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
    Nov 15 17:07:34.236: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Nov 15 17:07:34.236: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 11/15/23 17:07:34.255
    Nov 15 17:07:34.344: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Nov 15 17:07:34.344: INFO: Node 10.15.40.115 is running 0 daemon pod, expected 1
    Nov 15 17:07:35.384: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Nov 15 17:07:35.384: INFO: Node 10.15.40.115 is running 0 daemon pod, expected 1
    Nov 15 17:07:36.385: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Nov 15 17:07:36.386: INFO: Node 10.15.40.115 is running 0 daemon pod, expected 1
    Nov 15 17:07:37.399: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Nov 15 17:07:37.400: INFO: Node 10.15.40.115 is running 0 daemon pod, expected 1
    Nov 15 17:07:38.385: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Nov 15 17:07:38.385: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 11/15/23 17:07:38.404
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-631, will wait for the garbage collector to delete the pods 11/15/23 17:07:38.404
    Nov 15 17:07:38.507: INFO: Deleting DaemonSet.extensions daemon-set took: 32.366908ms
    Nov 15 17:07:38.608: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.48234ms
    Nov 15 17:07:41.026: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 15 17:07:41.026: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Nov 15 17:07:41.072: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"52178"},"items":null}

    Nov 15 17:07:41.087: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"52178"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:07:41.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-631" for this suite. 11/15/23 17:07:41.177
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:07:41.213
Nov 15 17:07:41.213: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename resourcequota 11/15/23 17:07:41.215
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:07:41.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:07:41.297
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 11/15/23 17:07:41.313
STEP: Counting existing ResourceQuota 11/15/23 17:07:46.346
STEP: Creating a ResourceQuota 11/15/23 17:07:51.377
STEP: Ensuring resource quota status is calculated 11/15/23 17:07:51.398
STEP: Creating a Secret 11/15/23 17:07:53.417
STEP: Ensuring resource quota status captures secret creation 11/15/23 17:07:53.457
STEP: Deleting a secret 11/15/23 17:07:55.476
STEP: Ensuring resource quota status released usage 11/15/23 17:07:55.509
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Nov 15 17:07:57.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1965" for this suite. 11/15/23 17:07:57.55
------------------------------
• [SLOW TEST] [16.364 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:07:41.213
    Nov 15 17:07:41.213: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename resourcequota 11/15/23 17:07:41.215
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:07:41.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:07:41.297
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 11/15/23 17:07:41.313
    STEP: Counting existing ResourceQuota 11/15/23 17:07:46.346
    STEP: Creating a ResourceQuota 11/15/23 17:07:51.377
    STEP: Ensuring resource quota status is calculated 11/15/23 17:07:51.398
    STEP: Creating a Secret 11/15/23 17:07:53.417
    STEP: Ensuring resource quota status captures secret creation 11/15/23 17:07:53.457
    STEP: Deleting a secret 11/15/23 17:07:55.476
    STEP: Ensuring resource quota status released usage 11/15/23 17:07:55.509
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:07:57.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1965" for this suite. 11/15/23 17:07:57.55
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:07:57.587
Nov 15 17:07:57.587: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename services 11/15/23 17:07:57.588
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:07:57.645
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:07:57.661
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 11/15/23 17:07:57.679
Nov 15 17:07:57.680: INFO: Creating e2e-svc-a-2sdxw
Nov 15 17:07:57.722: INFO: Creating e2e-svc-b-bdlqm
Nov 15 17:07:57.758: INFO: Creating e2e-svc-c-fm54k
STEP: deleting service collection 11/15/23 17:07:57.826
Nov 15 17:07:57.947: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 15 17:07:57.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4170" for this suite. 11/15/23 17:07:57.968
------------------------------
• [0.406 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:07:57.587
    Nov 15 17:07:57.587: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename services 11/15/23 17:07:57.588
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:07:57.645
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:07:57.661
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 11/15/23 17:07:57.679
    Nov 15 17:07:57.680: INFO: Creating e2e-svc-a-2sdxw
    Nov 15 17:07:57.722: INFO: Creating e2e-svc-b-bdlqm
    Nov 15 17:07:57.758: INFO: Creating e2e-svc-c-fm54k
    STEP: deleting service collection 11/15/23 17:07:57.826
    Nov 15 17:07:57.947: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:07:57.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4170" for this suite. 11/15/23 17:07:57.968
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:07:57.996
Nov 15 17:07:57.997: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename emptydir 11/15/23 17:07:57.998
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:07:58.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:07:58.073
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 11/15/23 17:07:58.088
Nov 15 17:07:58.122: INFO: Waiting up to 5m0s for pod "pod-a78bed73-610b-48dc-9f46-9d7bcc8e8ecf" in namespace "emptydir-974" to be "Succeeded or Failed"
Nov 15 17:07:58.139: INFO: Pod "pod-a78bed73-610b-48dc-9f46-9d7bcc8e8ecf": Phase="Pending", Reason="", readiness=false. Elapsed: 16.292469ms
Nov 15 17:08:00.155: INFO: Pod "pod-a78bed73-610b-48dc-9f46-9d7bcc8e8ecf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032847367s
Nov 15 17:08:02.165: INFO: Pod "pod-a78bed73-610b-48dc-9f46-9d7bcc8e8ecf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042019132s
STEP: Saw pod success 11/15/23 17:08:02.165
Nov 15 17:08:02.165: INFO: Pod "pod-a78bed73-610b-48dc-9f46-9d7bcc8e8ecf" satisfied condition "Succeeded or Failed"
Nov 15 17:08:02.198: INFO: Trying to get logs from node 10.15.40.115 pod pod-a78bed73-610b-48dc-9f46-9d7bcc8e8ecf container test-container: <nil>
STEP: delete the pod 11/15/23 17:08:02.341
Nov 15 17:08:02.386: INFO: Waiting for pod pod-a78bed73-610b-48dc-9f46-9d7bcc8e8ecf to disappear
Nov 15 17:08:02.403: INFO: Pod pod-a78bed73-610b-48dc-9f46-9d7bcc8e8ecf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 15 17:08:02.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-974" for this suite. 11/15/23 17:08:02.429
------------------------------
• [4.459 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:07:57.996
    Nov 15 17:07:57.997: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename emptydir 11/15/23 17:07:57.998
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:07:58.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:07:58.073
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 11/15/23 17:07:58.088
    Nov 15 17:07:58.122: INFO: Waiting up to 5m0s for pod "pod-a78bed73-610b-48dc-9f46-9d7bcc8e8ecf" in namespace "emptydir-974" to be "Succeeded or Failed"
    Nov 15 17:07:58.139: INFO: Pod "pod-a78bed73-610b-48dc-9f46-9d7bcc8e8ecf": Phase="Pending", Reason="", readiness=false. Elapsed: 16.292469ms
    Nov 15 17:08:00.155: INFO: Pod "pod-a78bed73-610b-48dc-9f46-9d7bcc8e8ecf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032847367s
    Nov 15 17:08:02.165: INFO: Pod "pod-a78bed73-610b-48dc-9f46-9d7bcc8e8ecf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042019132s
    STEP: Saw pod success 11/15/23 17:08:02.165
    Nov 15 17:08:02.165: INFO: Pod "pod-a78bed73-610b-48dc-9f46-9d7bcc8e8ecf" satisfied condition "Succeeded or Failed"
    Nov 15 17:08:02.198: INFO: Trying to get logs from node 10.15.40.115 pod pod-a78bed73-610b-48dc-9f46-9d7bcc8e8ecf container test-container: <nil>
    STEP: delete the pod 11/15/23 17:08:02.341
    Nov 15 17:08:02.386: INFO: Waiting for pod pod-a78bed73-610b-48dc-9f46-9d7bcc8e8ecf to disappear
    Nov 15 17:08:02.403: INFO: Pod pod-a78bed73-610b-48dc-9f46-9d7bcc8e8ecf no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:08:02.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-974" for this suite. 11/15/23 17:08:02.429
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:08:02.461
Nov 15 17:08:02.461: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename pods 11/15/23 17:08:02.464
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:08:02.514
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:08:02.532
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 11/15/23 17:08:02.549
Nov 15 17:08:02.581: INFO: Waiting up to 5m0s for pod "pod-qq8jk" in namespace "pods-7941" to be "running"
Nov 15 17:08:02.596: INFO: Pod "pod-qq8jk": Phase="Pending", Reason="", readiness=false. Elapsed: 15.100634ms
Nov 15 17:08:04.636: INFO: Pod "pod-qq8jk": Phase="Running", Reason="", readiness=true. Elapsed: 2.055091139s
Nov 15 17:08:04.636: INFO: Pod "pod-qq8jk" satisfied condition "running"
STEP: patching /status 11/15/23 17:08:04.636
Nov 15 17:08:04.670: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Nov 15 17:08:04.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7941" for this suite. 11/15/23 17:08:04.716
------------------------------
• [2.281 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:08:02.461
    Nov 15 17:08:02.461: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename pods 11/15/23 17:08:02.464
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:08:02.514
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:08:02.532
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 11/15/23 17:08:02.549
    Nov 15 17:08:02.581: INFO: Waiting up to 5m0s for pod "pod-qq8jk" in namespace "pods-7941" to be "running"
    Nov 15 17:08:02.596: INFO: Pod "pod-qq8jk": Phase="Pending", Reason="", readiness=false. Elapsed: 15.100634ms
    Nov 15 17:08:04.636: INFO: Pod "pod-qq8jk": Phase="Running", Reason="", readiness=true. Elapsed: 2.055091139s
    Nov 15 17:08:04.636: INFO: Pod "pod-qq8jk" satisfied condition "running"
    STEP: patching /status 11/15/23 17:08:04.636
    Nov 15 17:08:04.670: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:08:04.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7941" for this suite. 11/15/23 17:08:04.716
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:08:04.744
Nov 15 17:08:04.744: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename deployment 11/15/23 17:08:04.748
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:08:04.805
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:08:04.821
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Nov 15 17:08:04.869: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Nov 15 17:08:09.887: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 11/15/23 17:08:09.887
Nov 15 17:08:09.887: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 11/15/23 17:08:09.934
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Nov 15 17:08:09.995: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4140  d9d8aff8-fcb7-4bb4-ad6a-67c4ad03b1d0 52346 1 2023-11-15 17:08:09 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-11-15 17:08:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004f2d768 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Nov 15 17:08:10.011: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Nov 15 17:08:10.011: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Nov 15 17:08:10.011: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-4140  551afe7e-a6ea-4d09-a89a-33475fcb8d86 52348 1 2023-11-15 17:08:04 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment d9d8aff8-fcb7-4bb4-ad6a-67c4ad03b1d0 0xc004f2daa7 0xc004f2daa8}] [] [{e2e.test Update apps/v1 2023-11-15 17:08:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 17:08:07 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-11-15 17:08:09 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"d9d8aff8-fcb7-4bb4-ad6a-67c4ad03b1d0\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004f2db68 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 15 17:08:10.030: INFO: Pod "test-cleanup-controller-flvmn" is available:
&Pod{ObjectMeta:{test-cleanup-controller-flvmn test-cleanup-controller- deployment-4140  9438350d-af53-4811-b535-701910c14ab7 52333 0 2023-11-15 17:08:04 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:b1413d14a321e548226eec216a168c6ffa3f83c521748fdd0dd2b6bf7d681932 cni.projectcalico.org/podIP:172.30.164.18/32 cni.projectcalico.org/podIPs:172.30.164.18/32] [{apps/v1 ReplicaSet test-cleanup-controller 551afe7e-a6ea-4d09-a89a-33475fcb8d86 0xc004774827 0xc004774828}] [] [{kube-controller-manager Update v1 2023-11-15 17:08:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"551afe7e-a6ea-4d09-a89a-33475fcb8d86\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 17:08:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 17:08:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.164.18\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hvqgt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hvqgt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:08:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:08:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:08:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:08:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:172.30.164.18,StartTime:2023-11-15 17:08:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 17:08:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ad484120daff3e849bfce97b01f9b011e6ca2af9e8005ec99500e3cfa5a037c8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.164.18,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Nov 15 17:08:10.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4140" for this suite. 11/15/23 17:08:10.057
------------------------------
• [SLOW TEST] [5.345 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:08:04.744
    Nov 15 17:08:04.744: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename deployment 11/15/23 17:08:04.748
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:08:04.805
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:08:04.821
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Nov 15 17:08:04.869: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Nov 15 17:08:09.887: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 11/15/23 17:08:09.887
    Nov 15 17:08:09.887: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 11/15/23 17:08:09.934
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Nov 15 17:08:09.995: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4140  d9d8aff8-fcb7-4bb4-ad6a-67c4ad03b1d0 52346 1 2023-11-15 17:08:09 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-11-15 17:08:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004f2d768 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Nov 15 17:08:10.011: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    Nov 15 17:08:10.011: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Nov 15 17:08:10.011: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-4140  551afe7e-a6ea-4d09-a89a-33475fcb8d86 52348 1 2023-11-15 17:08:04 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment d9d8aff8-fcb7-4bb4-ad6a-67c4ad03b1d0 0xc004f2daa7 0xc004f2daa8}] [] [{e2e.test Update apps/v1 2023-11-15 17:08:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 17:08:07 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-11-15 17:08:09 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"d9d8aff8-fcb7-4bb4-ad6a-67c4ad03b1d0\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004f2db68 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Nov 15 17:08:10.030: INFO: Pod "test-cleanup-controller-flvmn" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-flvmn test-cleanup-controller- deployment-4140  9438350d-af53-4811-b535-701910c14ab7 52333 0 2023-11-15 17:08:04 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:b1413d14a321e548226eec216a168c6ffa3f83c521748fdd0dd2b6bf7d681932 cni.projectcalico.org/podIP:172.30.164.18/32 cni.projectcalico.org/podIPs:172.30.164.18/32] [{apps/v1 ReplicaSet test-cleanup-controller 551afe7e-a6ea-4d09-a89a-33475fcb8d86 0xc004774827 0xc004774828}] [] [{kube-controller-manager Update v1 2023-11-15 17:08:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"551afe7e-a6ea-4d09-a89a-33475fcb8d86\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 17:08:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 17:08:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.164.18\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hvqgt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hvqgt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:08:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:08:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:08:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:08:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:172.30.164.18,StartTime:2023-11-15 17:08:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 17:08:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ad484120daff3e849bfce97b01f9b011e6ca2af9e8005ec99500e3cfa5a037c8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.164.18,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:08:10.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4140" for this suite. 11/15/23 17:08:10.057
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:08:10.092
Nov 15 17:08:10.092: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename conformance-tests 11/15/23 17:08:10.094
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:08:10.142
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:08:10.156
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 11/15/23 17:08:10.172
Nov 15 17:08:10.172: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Nov 15 17:08:10.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-7039" for this suite. 11/15/23 17:08:10.24
------------------------------
• [0.174 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:08:10.092
    Nov 15 17:08:10.092: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename conformance-tests 11/15/23 17:08:10.094
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:08:10.142
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:08:10.156
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 11/15/23 17:08:10.172
    Nov 15 17:08:10.172: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:08:10.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-7039" for this suite. 11/15/23 17:08:10.24
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:08:10.274
Nov 15 17:08:10.274: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename security-context-test 11/15/23 17:08:10.275
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:08:10.33
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:08:10.346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Nov 15 17:08:10.396: INFO: Waiting up to 5m0s for pod "busybox-user-65534-b47f2412-6eec-4a6f-bc35-9a86f0c70189" in namespace "security-context-test-3399" to be "Succeeded or Failed"
Nov 15 17:08:10.409: INFO: Pod "busybox-user-65534-b47f2412-6eec-4a6f-bc35-9a86f0c70189": Phase="Pending", Reason="", readiness=false. Elapsed: 13.710771ms
Nov 15 17:08:12.428: INFO: Pod "busybox-user-65534-b47f2412-6eec-4a6f-bc35-9a86f0c70189": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032330332s
Nov 15 17:08:14.425: INFO: Pod "busybox-user-65534-b47f2412-6eec-4a6f-bc35-9a86f0c70189": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029389455s
Nov 15 17:08:14.425: INFO: Pod "busybox-user-65534-b47f2412-6eec-4a6f-bc35-9a86f0c70189" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Nov 15 17:08:14.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-3399" for this suite. 11/15/23 17:08:14.462
------------------------------
• [4.213 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:08:10.274
    Nov 15 17:08:10.274: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename security-context-test 11/15/23 17:08:10.275
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:08:10.33
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:08:10.346
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Nov 15 17:08:10.396: INFO: Waiting up to 5m0s for pod "busybox-user-65534-b47f2412-6eec-4a6f-bc35-9a86f0c70189" in namespace "security-context-test-3399" to be "Succeeded or Failed"
    Nov 15 17:08:10.409: INFO: Pod "busybox-user-65534-b47f2412-6eec-4a6f-bc35-9a86f0c70189": Phase="Pending", Reason="", readiness=false. Elapsed: 13.710771ms
    Nov 15 17:08:12.428: INFO: Pod "busybox-user-65534-b47f2412-6eec-4a6f-bc35-9a86f0c70189": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032330332s
    Nov 15 17:08:14.425: INFO: Pod "busybox-user-65534-b47f2412-6eec-4a6f-bc35-9a86f0c70189": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029389455s
    Nov 15 17:08:14.425: INFO: Pod "busybox-user-65534-b47f2412-6eec-4a6f-bc35-9a86f0c70189" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:08:14.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-3399" for this suite. 11/15/23 17:08:14.462
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:08:14.49
Nov 15 17:08:14.490: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename disruption 11/15/23 17:08:14.493
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:08:14.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:08:14.556
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 11/15/23 17:08:14.593
STEP: Updating PodDisruptionBudget status 11/15/23 17:08:16.623
STEP: Waiting for all pods to be running 11/15/23 17:08:16.657
Nov 15 17:08:16.673: INFO: running pods: 0 < 1
STEP: locating a running pod 11/15/23 17:08:18.693
STEP: Waiting for the pdb to be processed 11/15/23 17:08:18.791
STEP: Patching PodDisruptionBudget status 11/15/23 17:08:18.825
STEP: Waiting for the pdb to be processed 11/15/23 17:08:18.864
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Nov 15 17:08:18.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-3812" for this suite. 11/15/23 17:08:18.906
------------------------------
• [4.447 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:08:14.49
    Nov 15 17:08:14.490: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename disruption 11/15/23 17:08:14.493
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:08:14.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:08:14.556
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 11/15/23 17:08:14.593
    STEP: Updating PodDisruptionBudget status 11/15/23 17:08:16.623
    STEP: Waiting for all pods to be running 11/15/23 17:08:16.657
    Nov 15 17:08:16.673: INFO: running pods: 0 < 1
    STEP: locating a running pod 11/15/23 17:08:18.693
    STEP: Waiting for the pdb to be processed 11/15/23 17:08:18.791
    STEP: Patching PodDisruptionBudget status 11/15/23 17:08:18.825
    STEP: Waiting for the pdb to be processed 11/15/23 17:08:18.864
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:08:18.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-3812" for this suite. 11/15/23 17:08:18.906
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:08:18.952
Nov 15 17:08:18.952: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename containers 11/15/23 17:08:18.953
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:08:19.001
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:08:19.015
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 11/15/23 17:08:19.028
Nov 15 17:08:19.060: INFO: Waiting up to 5m0s for pod "client-containers-b7894d93-28da-4159-830d-9794b2701f4d" in namespace "containers-7690" to be "Succeeded or Failed"
Nov 15 17:08:19.075: INFO: Pod "client-containers-b7894d93-28da-4159-830d-9794b2701f4d": Phase="Pending", Reason="", readiness=false. Elapsed: 15.063381ms
Nov 15 17:08:21.097: INFO: Pod "client-containers-b7894d93-28da-4159-830d-9794b2701f4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037522207s
Nov 15 17:08:23.096: INFO: Pod "client-containers-b7894d93-28da-4159-830d-9794b2701f4d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036409194s
Nov 15 17:08:25.091: INFO: Pod "client-containers-b7894d93-28da-4159-830d-9794b2701f4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031044672s
STEP: Saw pod success 11/15/23 17:08:25.091
Nov 15 17:08:25.092: INFO: Pod "client-containers-b7894d93-28da-4159-830d-9794b2701f4d" satisfied condition "Succeeded or Failed"
Nov 15 17:08:25.108: INFO: Trying to get logs from node 10.15.40.115 pod client-containers-b7894d93-28da-4159-830d-9794b2701f4d container agnhost-container: <nil>
STEP: delete the pod 11/15/23 17:08:25.161
Nov 15 17:08:25.199: INFO: Waiting for pod client-containers-b7894d93-28da-4159-830d-9794b2701f4d to disappear
Nov 15 17:08:25.213: INFO: Pod client-containers-b7894d93-28da-4159-830d-9794b2701f4d no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Nov 15 17:08:25.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-7690" for this suite. 11/15/23 17:08:25.235
------------------------------
• [SLOW TEST] [6.322 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:08:18.952
    Nov 15 17:08:18.952: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename containers 11/15/23 17:08:18.953
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:08:19.001
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:08:19.015
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 11/15/23 17:08:19.028
    Nov 15 17:08:19.060: INFO: Waiting up to 5m0s for pod "client-containers-b7894d93-28da-4159-830d-9794b2701f4d" in namespace "containers-7690" to be "Succeeded or Failed"
    Nov 15 17:08:19.075: INFO: Pod "client-containers-b7894d93-28da-4159-830d-9794b2701f4d": Phase="Pending", Reason="", readiness=false. Elapsed: 15.063381ms
    Nov 15 17:08:21.097: INFO: Pod "client-containers-b7894d93-28da-4159-830d-9794b2701f4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037522207s
    Nov 15 17:08:23.096: INFO: Pod "client-containers-b7894d93-28da-4159-830d-9794b2701f4d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036409194s
    Nov 15 17:08:25.091: INFO: Pod "client-containers-b7894d93-28da-4159-830d-9794b2701f4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031044672s
    STEP: Saw pod success 11/15/23 17:08:25.091
    Nov 15 17:08:25.092: INFO: Pod "client-containers-b7894d93-28da-4159-830d-9794b2701f4d" satisfied condition "Succeeded or Failed"
    Nov 15 17:08:25.108: INFO: Trying to get logs from node 10.15.40.115 pod client-containers-b7894d93-28da-4159-830d-9794b2701f4d container agnhost-container: <nil>
    STEP: delete the pod 11/15/23 17:08:25.161
    Nov 15 17:08:25.199: INFO: Waiting for pod client-containers-b7894d93-28da-4159-830d-9794b2701f4d to disappear
    Nov 15 17:08:25.213: INFO: Pod client-containers-b7894d93-28da-4159-830d-9794b2701f4d no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:08:25.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-7690" for this suite. 11/15/23 17:08:25.235
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:08:25.274
Nov 15 17:08:25.274: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename pods 11/15/23 17:08:25.276
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:08:25.328
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:08:25.354
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 11/15/23 17:08:25.371
Nov 15 17:08:25.408: INFO: Waiting up to 5m0s for pod "pod-hostip-bc6c6519-8296-4c97-98d6-f1e2e769a20d" in namespace "pods-7467" to be "running and ready"
Nov 15 17:08:25.447: INFO: Pod "pod-hostip-bc6c6519-8296-4c97-98d6-f1e2e769a20d": Phase="Pending", Reason="", readiness=false. Elapsed: 38.74021ms
Nov 15 17:08:25.447: INFO: The phase of Pod pod-hostip-bc6c6519-8296-4c97-98d6-f1e2e769a20d is Pending, waiting for it to be Running (with Ready = true)
Nov 15 17:08:27.464: INFO: Pod "pod-hostip-bc6c6519-8296-4c97-98d6-f1e2e769a20d": Phase="Running", Reason="", readiness=true. Elapsed: 2.055901446s
Nov 15 17:08:27.464: INFO: The phase of Pod pod-hostip-bc6c6519-8296-4c97-98d6-f1e2e769a20d is Running (Ready = true)
Nov 15 17:08:27.464: INFO: Pod "pod-hostip-bc6c6519-8296-4c97-98d6-f1e2e769a20d" satisfied condition "running and ready"
Nov 15 17:08:27.493: INFO: Pod pod-hostip-bc6c6519-8296-4c97-98d6-f1e2e769a20d has hostIP: 10.15.40.115
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Nov 15 17:08:27.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7467" for this suite. 11/15/23 17:08:27.526
------------------------------
• [2.285 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:08:25.274
    Nov 15 17:08:25.274: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename pods 11/15/23 17:08:25.276
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:08:25.328
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:08:25.354
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 11/15/23 17:08:25.371
    Nov 15 17:08:25.408: INFO: Waiting up to 5m0s for pod "pod-hostip-bc6c6519-8296-4c97-98d6-f1e2e769a20d" in namespace "pods-7467" to be "running and ready"
    Nov 15 17:08:25.447: INFO: Pod "pod-hostip-bc6c6519-8296-4c97-98d6-f1e2e769a20d": Phase="Pending", Reason="", readiness=false. Elapsed: 38.74021ms
    Nov 15 17:08:25.447: INFO: The phase of Pod pod-hostip-bc6c6519-8296-4c97-98d6-f1e2e769a20d is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 17:08:27.464: INFO: Pod "pod-hostip-bc6c6519-8296-4c97-98d6-f1e2e769a20d": Phase="Running", Reason="", readiness=true. Elapsed: 2.055901446s
    Nov 15 17:08:27.464: INFO: The phase of Pod pod-hostip-bc6c6519-8296-4c97-98d6-f1e2e769a20d is Running (Ready = true)
    Nov 15 17:08:27.464: INFO: Pod "pod-hostip-bc6c6519-8296-4c97-98d6-f1e2e769a20d" satisfied condition "running and ready"
    Nov 15 17:08:27.493: INFO: Pod pod-hostip-bc6c6519-8296-4c97-98d6-f1e2e769a20d has hostIP: 10.15.40.115
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:08:27.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7467" for this suite. 11/15/23 17:08:27.526
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:08:27.563
Nov 15 17:08:27.564: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename container-probe 11/15/23 17:08:27.567
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:08:27.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:08:27.634
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-df5fa883-5495-4e58-82f3-6df95ec57708 in namespace container-probe-2199 11/15/23 17:08:27.652
Nov 15 17:08:27.689: INFO: Waiting up to 5m0s for pod "liveness-df5fa883-5495-4e58-82f3-6df95ec57708" in namespace "container-probe-2199" to be "not pending"
Nov 15 17:08:27.703: INFO: Pod "liveness-df5fa883-5495-4e58-82f3-6df95ec57708": Phase="Pending", Reason="", readiness=false. Elapsed: 13.936742ms
Nov 15 17:08:29.719: INFO: Pod "liveness-df5fa883-5495-4e58-82f3-6df95ec57708": Phase="Running", Reason="", readiness=true. Elapsed: 2.029217711s
Nov 15 17:08:29.719: INFO: Pod "liveness-df5fa883-5495-4e58-82f3-6df95ec57708" satisfied condition "not pending"
Nov 15 17:08:29.719: INFO: Started pod liveness-df5fa883-5495-4e58-82f3-6df95ec57708 in namespace container-probe-2199
STEP: checking the pod's current state and verifying that restartCount is present 11/15/23 17:08:29.719
Nov 15 17:08:29.734: INFO: Initial restart count of pod liveness-df5fa883-5495-4e58-82f3-6df95ec57708 is 0
Nov 15 17:08:49.931: INFO: Restart count of pod container-probe-2199/liveness-df5fa883-5495-4e58-82f3-6df95ec57708 is now 1 (20.196413154s elapsed)
Nov 15 17:09:10.170: INFO: Restart count of pod container-probe-2199/liveness-df5fa883-5495-4e58-82f3-6df95ec57708 is now 2 (40.435433114s elapsed)
Nov 15 17:09:30.351: INFO: Restart count of pod container-probe-2199/liveness-df5fa883-5495-4e58-82f3-6df95ec57708 is now 3 (1m0.616768893s elapsed)
Nov 15 17:09:50.527: INFO: Restart count of pod container-probe-2199/liveness-df5fa883-5495-4e58-82f3-6df95ec57708 is now 4 (1m20.793149623s elapsed)
Nov 15 17:11:03.211: INFO: Restart count of pod container-probe-2199/liveness-df5fa883-5495-4e58-82f3-6df95ec57708 is now 5 (2m33.477036377s elapsed)
STEP: deleting the pod 11/15/23 17:11:03.211
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Nov 15 17:11:03.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2199" for this suite. 11/15/23 17:11:03.275
------------------------------
• [SLOW TEST] [155.737 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:08:27.563
    Nov 15 17:08:27.564: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename container-probe 11/15/23 17:08:27.567
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:08:27.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:08:27.634
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-df5fa883-5495-4e58-82f3-6df95ec57708 in namespace container-probe-2199 11/15/23 17:08:27.652
    Nov 15 17:08:27.689: INFO: Waiting up to 5m0s for pod "liveness-df5fa883-5495-4e58-82f3-6df95ec57708" in namespace "container-probe-2199" to be "not pending"
    Nov 15 17:08:27.703: INFO: Pod "liveness-df5fa883-5495-4e58-82f3-6df95ec57708": Phase="Pending", Reason="", readiness=false. Elapsed: 13.936742ms
    Nov 15 17:08:29.719: INFO: Pod "liveness-df5fa883-5495-4e58-82f3-6df95ec57708": Phase="Running", Reason="", readiness=true. Elapsed: 2.029217711s
    Nov 15 17:08:29.719: INFO: Pod "liveness-df5fa883-5495-4e58-82f3-6df95ec57708" satisfied condition "not pending"
    Nov 15 17:08:29.719: INFO: Started pod liveness-df5fa883-5495-4e58-82f3-6df95ec57708 in namespace container-probe-2199
    STEP: checking the pod's current state and verifying that restartCount is present 11/15/23 17:08:29.719
    Nov 15 17:08:29.734: INFO: Initial restart count of pod liveness-df5fa883-5495-4e58-82f3-6df95ec57708 is 0
    Nov 15 17:08:49.931: INFO: Restart count of pod container-probe-2199/liveness-df5fa883-5495-4e58-82f3-6df95ec57708 is now 1 (20.196413154s elapsed)
    Nov 15 17:09:10.170: INFO: Restart count of pod container-probe-2199/liveness-df5fa883-5495-4e58-82f3-6df95ec57708 is now 2 (40.435433114s elapsed)
    Nov 15 17:09:30.351: INFO: Restart count of pod container-probe-2199/liveness-df5fa883-5495-4e58-82f3-6df95ec57708 is now 3 (1m0.616768893s elapsed)
    Nov 15 17:09:50.527: INFO: Restart count of pod container-probe-2199/liveness-df5fa883-5495-4e58-82f3-6df95ec57708 is now 4 (1m20.793149623s elapsed)
    Nov 15 17:11:03.211: INFO: Restart count of pod container-probe-2199/liveness-df5fa883-5495-4e58-82f3-6df95ec57708 is now 5 (2m33.477036377s elapsed)
    STEP: deleting the pod 11/15/23 17:11:03.211
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:11:03.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2199" for this suite. 11/15/23 17:11:03.275
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:11:03.307
Nov 15 17:11:03.307: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename replication-controller 11/15/23 17:11:03.308
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:11:03.385
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:11:03.402
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 11/15/23 17:11:03.418
STEP: When the matched label of one of its pods change 11/15/23 17:11:03.447
Nov 15 17:11:03.462: INFO: Pod name pod-release: Found 0 pods out of 1
Nov 15 17:11:08.484: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 11/15/23 17:11:08.523
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Nov 15 17:11:09.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6424" for this suite. 11/15/23 17:11:09.581
------------------------------
• [SLOW TEST] [6.299 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:11:03.307
    Nov 15 17:11:03.307: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename replication-controller 11/15/23 17:11:03.308
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:11:03.385
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:11:03.402
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 11/15/23 17:11:03.418
    STEP: When the matched label of one of its pods change 11/15/23 17:11:03.447
    Nov 15 17:11:03.462: INFO: Pod name pod-release: Found 0 pods out of 1
    Nov 15 17:11:08.484: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 11/15/23 17:11:08.523
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:11:09.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6424" for this suite. 11/15/23 17:11:09.581
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:11:09.611
Nov 15 17:11:09.611: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename projected 11/15/23 17:11:09.613
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:11:09.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:11:09.685
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-29c212be-d217-463e-8d19-43f66be883b0 11/15/23 17:11:09.721
STEP: Creating the pod 11/15/23 17:11:09.741
Nov 15 17:11:09.776: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-567c78cb-a388-41d9-8075-e42328d6f64c" in namespace "projected-1216" to be "running and ready"
Nov 15 17:11:09.791: INFO: Pod "pod-projected-configmaps-567c78cb-a388-41d9-8075-e42328d6f64c": Phase="Pending", Reason="", readiness=false. Elapsed: 15.243976ms
Nov 15 17:11:09.791: INFO: The phase of Pod pod-projected-configmaps-567c78cb-a388-41d9-8075-e42328d6f64c is Pending, waiting for it to be Running (with Ready = true)
Nov 15 17:11:11.810: INFO: Pod "pod-projected-configmaps-567c78cb-a388-41d9-8075-e42328d6f64c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034099315s
Nov 15 17:11:11.810: INFO: The phase of Pod pod-projected-configmaps-567c78cb-a388-41d9-8075-e42328d6f64c is Pending, waiting for it to be Running (with Ready = true)
Nov 15 17:11:13.807: INFO: Pod "pod-projected-configmaps-567c78cb-a388-41d9-8075-e42328d6f64c": Phase="Running", Reason="", readiness=true. Elapsed: 4.031321177s
Nov 15 17:11:13.807: INFO: The phase of Pod pod-projected-configmaps-567c78cb-a388-41d9-8075-e42328d6f64c is Running (Ready = true)
Nov 15 17:11:13.807: INFO: Pod "pod-projected-configmaps-567c78cb-a388-41d9-8075-e42328d6f64c" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-29c212be-d217-463e-8d19-43f66be883b0 11/15/23 17:11:13.96
STEP: waiting to observe update in volume 11/15/23 17:11:13.981
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Nov 15 17:12:19.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1216" for this suite. 11/15/23 17:12:19.673
------------------------------
• [SLOW TEST] [70.089 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:11:09.611
    Nov 15 17:11:09.611: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename projected 11/15/23 17:11:09.613
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:11:09.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:11:09.685
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-29c212be-d217-463e-8d19-43f66be883b0 11/15/23 17:11:09.721
    STEP: Creating the pod 11/15/23 17:11:09.741
    Nov 15 17:11:09.776: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-567c78cb-a388-41d9-8075-e42328d6f64c" in namespace "projected-1216" to be "running and ready"
    Nov 15 17:11:09.791: INFO: Pod "pod-projected-configmaps-567c78cb-a388-41d9-8075-e42328d6f64c": Phase="Pending", Reason="", readiness=false. Elapsed: 15.243976ms
    Nov 15 17:11:09.791: INFO: The phase of Pod pod-projected-configmaps-567c78cb-a388-41d9-8075-e42328d6f64c is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 17:11:11.810: INFO: Pod "pod-projected-configmaps-567c78cb-a388-41d9-8075-e42328d6f64c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034099315s
    Nov 15 17:11:11.810: INFO: The phase of Pod pod-projected-configmaps-567c78cb-a388-41d9-8075-e42328d6f64c is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 17:11:13.807: INFO: Pod "pod-projected-configmaps-567c78cb-a388-41d9-8075-e42328d6f64c": Phase="Running", Reason="", readiness=true. Elapsed: 4.031321177s
    Nov 15 17:11:13.807: INFO: The phase of Pod pod-projected-configmaps-567c78cb-a388-41d9-8075-e42328d6f64c is Running (Ready = true)
    Nov 15 17:11:13.807: INFO: Pod "pod-projected-configmaps-567c78cb-a388-41d9-8075-e42328d6f64c" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-29c212be-d217-463e-8d19-43f66be883b0 11/15/23 17:11:13.96
    STEP: waiting to observe update in volume 11/15/23 17:11:13.981
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:12:19.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1216" for this suite. 11/15/23 17:12:19.673
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:12:19.718
Nov 15 17:12:19.718: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename projected 11/15/23 17:12:19.719
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:12:19.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:12:19.788
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-d211e78f-3fdb-43ea-b989-af2f0c20dd66 11/15/23 17:12:19.804
STEP: Creating secret with name secret-projected-all-test-volume-4357e660-1819-459d-8744-4636cd29694a 11/15/23 17:12:19.825
STEP: Creating a pod to test Check all projections for projected volume plugin 11/15/23 17:12:19.847
Nov 15 17:12:19.882: INFO: Waiting up to 5m0s for pod "projected-volume-6a1df312-1062-4273-8ca9-b0f245336f0d" in namespace "projected-8347" to be "Succeeded or Failed"
Nov 15 17:12:19.898: INFO: Pod "projected-volume-6a1df312-1062-4273-8ca9-b0f245336f0d": Phase="Pending", Reason="", readiness=false. Elapsed: 15.742885ms
Nov 15 17:12:21.921: INFO: Pod "projected-volume-6a1df312-1062-4273-8ca9-b0f245336f0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039407818s
Nov 15 17:12:23.914: INFO: Pod "projected-volume-6a1df312-1062-4273-8ca9-b0f245336f0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032134521s
STEP: Saw pod success 11/15/23 17:12:23.914
Nov 15 17:12:23.914: INFO: Pod "projected-volume-6a1df312-1062-4273-8ca9-b0f245336f0d" satisfied condition "Succeeded or Failed"
Nov 15 17:12:23.930: INFO: Trying to get logs from node 10.15.40.115 pod projected-volume-6a1df312-1062-4273-8ca9-b0f245336f0d container projected-all-volume-test: <nil>
STEP: delete the pod 11/15/23 17:12:23.991
Nov 15 17:12:24.034: INFO: Waiting for pod projected-volume-6a1df312-1062-4273-8ca9-b0f245336f0d to disappear
Nov 15 17:12:24.049: INFO: Pod projected-volume-6a1df312-1062-4273-8ca9-b0f245336f0d no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Nov 15 17:12:24.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8347" for this suite. 11/15/23 17:12:24.074
------------------------------
• [4.381 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:12:19.718
    Nov 15 17:12:19.718: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename projected 11/15/23 17:12:19.719
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:12:19.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:12:19.788
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-d211e78f-3fdb-43ea-b989-af2f0c20dd66 11/15/23 17:12:19.804
    STEP: Creating secret with name secret-projected-all-test-volume-4357e660-1819-459d-8744-4636cd29694a 11/15/23 17:12:19.825
    STEP: Creating a pod to test Check all projections for projected volume plugin 11/15/23 17:12:19.847
    Nov 15 17:12:19.882: INFO: Waiting up to 5m0s for pod "projected-volume-6a1df312-1062-4273-8ca9-b0f245336f0d" in namespace "projected-8347" to be "Succeeded or Failed"
    Nov 15 17:12:19.898: INFO: Pod "projected-volume-6a1df312-1062-4273-8ca9-b0f245336f0d": Phase="Pending", Reason="", readiness=false. Elapsed: 15.742885ms
    Nov 15 17:12:21.921: INFO: Pod "projected-volume-6a1df312-1062-4273-8ca9-b0f245336f0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039407818s
    Nov 15 17:12:23.914: INFO: Pod "projected-volume-6a1df312-1062-4273-8ca9-b0f245336f0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032134521s
    STEP: Saw pod success 11/15/23 17:12:23.914
    Nov 15 17:12:23.914: INFO: Pod "projected-volume-6a1df312-1062-4273-8ca9-b0f245336f0d" satisfied condition "Succeeded or Failed"
    Nov 15 17:12:23.930: INFO: Trying to get logs from node 10.15.40.115 pod projected-volume-6a1df312-1062-4273-8ca9-b0f245336f0d container projected-all-volume-test: <nil>
    STEP: delete the pod 11/15/23 17:12:23.991
    Nov 15 17:12:24.034: INFO: Waiting for pod projected-volume-6a1df312-1062-4273-8ca9-b0f245336f0d to disappear
    Nov 15 17:12:24.049: INFO: Pod projected-volume-6a1df312-1062-4273-8ca9-b0f245336f0d no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:12:24.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8347" for this suite. 11/15/23 17:12:24.074
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:12:24.105
Nov 15 17:12:24.105: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename container-lifecycle-hook 11/15/23 17:12:24.107
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:12:24.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:12:24.179
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 11/15/23 17:12:24.22
Nov 15 17:12:24.254: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4095" to be "running and ready"
Nov 15 17:12:24.269: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 14.522456ms
Nov 15 17:12:24.269: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 15 17:12:26.284: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029883025s
Nov 15 17:12:26.284: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 15 17:12:28.287: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.033128344s
Nov 15 17:12:28.287: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Nov 15 17:12:28.287: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 11/15/23 17:12:28.303
Nov 15 17:12:28.321: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-4095" to be "running and ready"
Nov 15 17:12:28.338: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 16.388347ms
Nov 15 17:12:28.338: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 15 17:12:30.353: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032109666s
Nov 15 17:12:30.353: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 15 17:12:32.357: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.035503456s
Nov 15 17:12:32.357: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Nov 15 17:12:32.357: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 11/15/23 17:12:32.372
Nov 15 17:12:32.397: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 15 17:12:32.414: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 15 17:12:34.415: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 15 17:12:34.432: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 11/15/23 17:12:34.432
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Nov 15 17:12:34.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-4095" for this suite. 11/15/23 17:12:34.526
------------------------------
• [SLOW TEST] [10.449 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:12:24.105
    Nov 15 17:12:24.105: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename container-lifecycle-hook 11/15/23 17:12:24.107
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:12:24.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:12:24.179
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 11/15/23 17:12:24.22
    Nov 15 17:12:24.254: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4095" to be "running and ready"
    Nov 15 17:12:24.269: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 14.522456ms
    Nov 15 17:12:24.269: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 17:12:26.284: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029883025s
    Nov 15 17:12:26.284: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 17:12:28.287: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.033128344s
    Nov 15 17:12:28.287: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Nov 15 17:12:28.287: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 11/15/23 17:12:28.303
    Nov 15 17:12:28.321: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-4095" to be "running and ready"
    Nov 15 17:12:28.338: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 16.388347ms
    Nov 15 17:12:28.338: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 17:12:30.353: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032109666s
    Nov 15 17:12:30.353: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Nov 15 17:12:32.357: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.035503456s
    Nov 15 17:12:32.357: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Nov 15 17:12:32.357: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 11/15/23 17:12:32.372
    Nov 15 17:12:32.397: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Nov 15 17:12:32.414: INFO: Pod pod-with-prestop-exec-hook still exists
    Nov 15 17:12:34.415: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Nov 15 17:12:34.432: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 11/15/23 17:12:34.432
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:12:34.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-4095" for this suite. 11/15/23 17:12:34.526
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:12:34.571
Nov 15 17:12:34.571: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename dns 11/15/23 17:12:34.572
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:12:34.645
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:12:34.662
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 11/15/23 17:12:34.682
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8861.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8861.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 11/15/23 17:12:34.702
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8861.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8861.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 11/15/23 17:12:34.702
STEP: creating a pod to probe DNS 11/15/23 17:12:34.702
STEP: submitting the pod to kubernetes 11/15/23 17:12:34.704
Nov 15 17:12:34.760: INFO: Waiting up to 15m0s for pod "dns-test-cefb669f-66de-4aab-9e2e-47d0614dfad2" in namespace "dns-8861" to be "running"
Nov 15 17:12:34.776: INFO: Pod "dns-test-cefb669f-66de-4aab-9e2e-47d0614dfad2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.045803ms
Nov 15 17:12:36.793: INFO: Pod "dns-test-cefb669f-66de-4aab-9e2e-47d0614dfad2": Phase="Running", Reason="", readiness=true. Elapsed: 2.032501625s
Nov 15 17:12:36.793: INFO: Pod "dns-test-cefb669f-66de-4aab-9e2e-47d0614dfad2" satisfied condition "running"
STEP: retrieving the pod 11/15/23 17:12:36.793
STEP: looking for the results for each expected name from probers 11/15/23 17:12:36.808
Nov 15 17:12:36.954: INFO: DNS probes using dns-8861/dns-test-cefb669f-66de-4aab-9e2e-47d0614dfad2 succeeded

STEP: deleting the pod 11/15/23 17:12:36.954
STEP: deleting the test headless service 11/15/23 17:12:37.004
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Nov 15 17:12:37.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8861" for this suite. 11/15/23 17:12:37.078
------------------------------
• [2.534 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:12:34.571
    Nov 15 17:12:34.571: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename dns 11/15/23 17:12:34.572
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:12:34.645
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:12:34.662
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 11/15/23 17:12:34.682
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8861.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8861.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     11/15/23 17:12:34.702
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8861.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8861.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     11/15/23 17:12:34.702
    STEP: creating a pod to probe DNS 11/15/23 17:12:34.702
    STEP: submitting the pod to kubernetes 11/15/23 17:12:34.704
    Nov 15 17:12:34.760: INFO: Waiting up to 15m0s for pod "dns-test-cefb669f-66de-4aab-9e2e-47d0614dfad2" in namespace "dns-8861" to be "running"
    Nov 15 17:12:34.776: INFO: Pod "dns-test-cefb669f-66de-4aab-9e2e-47d0614dfad2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.045803ms
    Nov 15 17:12:36.793: INFO: Pod "dns-test-cefb669f-66de-4aab-9e2e-47d0614dfad2": Phase="Running", Reason="", readiness=true. Elapsed: 2.032501625s
    Nov 15 17:12:36.793: INFO: Pod "dns-test-cefb669f-66de-4aab-9e2e-47d0614dfad2" satisfied condition "running"
    STEP: retrieving the pod 11/15/23 17:12:36.793
    STEP: looking for the results for each expected name from probers 11/15/23 17:12:36.808
    Nov 15 17:12:36.954: INFO: DNS probes using dns-8861/dns-test-cefb669f-66de-4aab-9e2e-47d0614dfad2 succeeded

    STEP: deleting the pod 11/15/23 17:12:36.954
    STEP: deleting the test headless service 11/15/23 17:12:37.004
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:12:37.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8861" for this suite. 11/15/23 17:12:37.078
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:12:37.107
Nov 15 17:12:37.107: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename emptydir 11/15/23 17:12:37.109
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:12:37.163
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:12:37.178
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 11/15/23 17:12:37.194
Nov 15 17:12:37.230: INFO: Waiting up to 5m0s for pod "pod-e97640ec-5ee6-4fe3-bc0e-e7100f07af34" in namespace "emptydir-7547" to be "Succeeded or Failed"
Nov 15 17:12:37.250: INFO: Pod "pod-e97640ec-5ee6-4fe3-bc0e-e7100f07af34": Phase="Pending", Reason="", readiness=false. Elapsed: 20.332266ms
Nov 15 17:12:39.266: INFO: Pod "pod-e97640ec-5ee6-4fe3-bc0e-e7100f07af34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036481752s
Nov 15 17:12:41.265: INFO: Pod "pod-e97640ec-5ee6-4fe3-bc0e-e7100f07af34": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03532333s
Nov 15 17:12:43.270: INFO: Pod "pod-e97640ec-5ee6-4fe3-bc0e-e7100f07af34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040694314s
STEP: Saw pod success 11/15/23 17:12:43.27
Nov 15 17:12:43.271: INFO: Pod "pod-e97640ec-5ee6-4fe3-bc0e-e7100f07af34" satisfied condition "Succeeded or Failed"
Nov 15 17:12:43.286: INFO: Trying to get logs from node 10.15.40.115 pod pod-e97640ec-5ee6-4fe3-bc0e-e7100f07af34 container test-container: <nil>
STEP: delete the pod 11/15/23 17:12:43.332
Nov 15 17:12:43.401: INFO: Waiting for pod pod-e97640ec-5ee6-4fe3-bc0e-e7100f07af34 to disappear
Nov 15 17:12:43.415: INFO: Pod pod-e97640ec-5ee6-4fe3-bc0e-e7100f07af34 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 15 17:12:43.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7547" for this suite. 11/15/23 17:12:43.44
------------------------------
• [SLOW TEST] [6.358 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:12:37.107
    Nov 15 17:12:37.107: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename emptydir 11/15/23 17:12:37.109
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:12:37.163
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:12:37.178
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 11/15/23 17:12:37.194
    Nov 15 17:12:37.230: INFO: Waiting up to 5m0s for pod "pod-e97640ec-5ee6-4fe3-bc0e-e7100f07af34" in namespace "emptydir-7547" to be "Succeeded or Failed"
    Nov 15 17:12:37.250: INFO: Pod "pod-e97640ec-5ee6-4fe3-bc0e-e7100f07af34": Phase="Pending", Reason="", readiness=false. Elapsed: 20.332266ms
    Nov 15 17:12:39.266: INFO: Pod "pod-e97640ec-5ee6-4fe3-bc0e-e7100f07af34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036481752s
    Nov 15 17:12:41.265: INFO: Pod "pod-e97640ec-5ee6-4fe3-bc0e-e7100f07af34": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03532333s
    Nov 15 17:12:43.270: INFO: Pod "pod-e97640ec-5ee6-4fe3-bc0e-e7100f07af34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040694314s
    STEP: Saw pod success 11/15/23 17:12:43.27
    Nov 15 17:12:43.271: INFO: Pod "pod-e97640ec-5ee6-4fe3-bc0e-e7100f07af34" satisfied condition "Succeeded or Failed"
    Nov 15 17:12:43.286: INFO: Trying to get logs from node 10.15.40.115 pod pod-e97640ec-5ee6-4fe3-bc0e-e7100f07af34 container test-container: <nil>
    STEP: delete the pod 11/15/23 17:12:43.332
    Nov 15 17:12:43.401: INFO: Waiting for pod pod-e97640ec-5ee6-4fe3-bc0e-e7100f07af34 to disappear
    Nov 15 17:12:43.415: INFO: Pod pod-e97640ec-5ee6-4fe3-bc0e-e7100f07af34 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:12:43.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7547" for this suite. 11/15/23 17:12:43.44
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:12:43.469
Nov 15 17:12:43.469: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename projected 11/15/23 17:12:43.471
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:12:43.519
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:12:43.534
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-8e97e24b-1f3e-4538-9c30-eb79b16a9483 11/15/23 17:12:43.544
STEP: Creating a pod to test consume configMaps 11/15/23 17:12:43.57
Nov 15 17:12:43.603: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ae232faf-50cb-472b-b876-fc084d748d72" in namespace "projected-735" to be "Succeeded or Failed"
Nov 15 17:12:43.617: INFO: Pod "pod-projected-configmaps-ae232faf-50cb-472b-b876-fc084d748d72": Phase="Pending", Reason="", readiness=false. Elapsed: 14.037692ms
Nov 15 17:12:45.633: INFO: Pod "pod-projected-configmaps-ae232faf-50cb-472b-b876-fc084d748d72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02961081s
Nov 15 17:12:47.633: INFO: Pod "pod-projected-configmaps-ae232faf-50cb-472b-b876-fc084d748d72": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029954524s
Nov 15 17:12:49.635: INFO: Pod "pod-projected-configmaps-ae232faf-50cb-472b-b876-fc084d748d72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032333568s
STEP: Saw pod success 11/15/23 17:12:49.635
Nov 15 17:12:49.636: INFO: Pod "pod-projected-configmaps-ae232faf-50cb-472b-b876-fc084d748d72" satisfied condition "Succeeded or Failed"
Nov 15 17:12:49.652: INFO: Trying to get logs from node 10.15.40.115 pod pod-projected-configmaps-ae232faf-50cb-472b-b876-fc084d748d72 container agnhost-container: <nil>
STEP: delete the pod 11/15/23 17:12:49.696
Nov 15 17:12:49.731: INFO: Waiting for pod pod-projected-configmaps-ae232faf-50cb-472b-b876-fc084d748d72 to disappear
Nov 15 17:12:49.746: INFO: Pod pod-projected-configmaps-ae232faf-50cb-472b-b876-fc084d748d72 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Nov 15 17:12:49.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-735" for this suite. 11/15/23 17:12:49.77
------------------------------
• [SLOW TEST] [6.329 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:12:43.469
    Nov 15 17:12:43.469: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename projected 11/15/23 17:12:43.471
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:12:43.519
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:12:43.534
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-8e97e24b-1f3e-4538-9c30-eb79b16a9483 11/15/23 17:12:43.544
    STEP: Creating a pod to test consume configMaps 11/15/23 17:12:43.57
    Nov 15 17:12:43.603: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ae232faf-50cb-472b-b876-fc084d748d72" in namespace "projected-735" to be "Succeeded or Failed"
    Nov 15 17:12:43.617: INFO: Pod "pod-projected-configmaps-ae232faf-50cb-472b-b876-fc084d748d72": Phase="Pending", Reason="", readiness=false. Elapsed: 14.037692ms
    Nov 15 17:12:45.633: INFO: Pod "pod-projected-configmaps-ae232faf-50cb-472b-b876-fc084d748d72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02961081s
    Nov 15 17:12:47.633: INFO: Pod "pod-projected-configmaps-ae232faf-50cb-472b-b876-fc084d748d72": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029954524s
    Nov 15 17:12:49.635: INFO: Pod "pod-projected-configmaps-ae232faf-50cb-472b-b876-fc084d748d72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032333568s
    STEP: Saw pod success 11/15/23 17:12:49.635
    Nov 15 17:12:49.636: INFO: Pod "pod-projected-configmaps-ae232faf-50cb-472b-b876-fc084d748d72" satisfied condition "Succeeded or Failed"
    Nov 15 17:12:49.652: INFO: Trying to get logs from node 10.15.40.115 pod pod-projected-configmaps-ae232faf-50cb-472b-b876-fc084d748d72 container agnhost-container: <nil>
    STEP: delete the pod 11/15/23 17:12:49.696
    Nov 15 17:12:49.731: INFO: Waiting for pod pod-projected-configmaps-ae232faf-50cb-472b-b876-fc084d748d72 to disappear
    Nov 15 17:12:49.746: INFO: Pod pod-projected-configmaps-ae232faf-50cb-472b-b876-fc084d748d72 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:12:49.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-735" for this suite. 11/15/23 17:12:49.77
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:12:49.8
Nov 15 17:12:49.801: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename kubelet-test 11/15/23 17:12:49.803
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:12:49.859
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:12:49.872
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 11/15/23 17:12:49.927
Nov 15 17:12:49.927: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases0445a80b-136e-4b8e-bf67-1f671177d7a8" in namespace "kubelet-test-9297" to be "completed"
Nov 15 17:12:49.943: INFO: Pod "agnhost-host-aliases0445a80b-136e-4b8e-bf67-1f671177d7a8": Phase="Pending", Reason="", readiness=false. Elapsed: 15.315487ms
Nov 15 17:12:51.959: INFO: Pod "agnhost-host-aliases0445a80b-136e-4b8e-bf67-1f671177d7a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032025006s
Nov 15 17:12:53.966: INFO: Pod "agnhost-host-aliases0445a80b-136e-4b8e-bf67-1f671177d7a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038954091s
Nov 15 17:12:53.966: INFO: Pod "agnhost-host-aliases0445a80b-136e-4b8e-bf67-1f671177d7a8" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Nov 15 17:12:54.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-9297" for this suite. 11/15/23 17:12:54.033
------------------------------
• [4.263 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:12:49.8
    Nov 15 17:12:49.801: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename kubelet-test 11/15/23 17:12:49.803
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:12:49.859
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:12:49.872
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 11/15/23 17:12:49.927
    Nov 15 17:12:49.927: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases0445a80b-136e-4b8e-bf67-1f671177d7a8" in namespace "kubelet-test-9297" to be "completed"
    Nov 15 17:12:49.943: INFO: Pod "agnhost-host-aliases0445a80b-136e-4b8e-bf67-1f671177d7a8": Phase="Pending", Reason="", readiness=false. Elapsed: 15.315487ms
    Nov 15 17:12:51.959: INFO: Pod "agnhost-host-aliases0445a80b-136e-4b8e-bf67-1f671177d7a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032025006s
    Nov 15 17:12:53.966: INFO: Pod "agnhost-host-aliases0445a80b-136e-4b8e-bf67-1f671177d7a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038954091s
    Nov 15 17:12:53.966: INFO: Pod "agnhost-host-aliases0445a80b-136e-4b8e-bf67-1f671177d7a8" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:12:54.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-9297" for this suite. 11/15/23 17:12:54.033
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:12:54.066
Nov 15 17:12:54.066: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename container-runtime 11/15/23 17:12:54.069
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:12:54.117
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:12:54.132
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 11/15/23 17:12:54.2
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 11/15/23 17:13:12.514
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 11/15/23 17:13:12.53
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 11/15/23 17:13:12.563
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 11/15/23 17:13:12.563
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 11/15/23 17:13:12.644
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 11/15/23 17:13:16.727
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 11/15/23 17:13:18.776
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 11/15/23 17:13:18.806
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 11/15/23 17:13:18.807
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 11/15/23 17:13:18.883
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 11/15/23 17:13:19.912
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 11/15/23 17:13:23.999
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 11/15/23 17:13:24.029
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 11/15/23 17:13:24.029
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Nov 15 17:13:24.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-9820" for this suite. 11/15/23 17:13:24.17
------------------------------
• [SLOW TEST] [30.131 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:12:54.066
    Nov 15 17:12:54.066: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename container-runtime 11/15/23 17:12:54.069
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:12:54.117
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:12:54.132
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 11/15/23 17:12:54.2
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 11/15/23 17:13:12.514
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 11/15/23 17:13:12.53
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 11/15/23 17:13:12.563
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 11/15/23 17:13:12.563
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 11/15/23 17:13:12.644
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 11/15/23 17:13:16.727
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 11/15/23 17:13:18.776
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 11/15/23 17:13:18.806
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 11/15/23 17:13:18.807
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 11/15/23 17:13:18.883
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 11/15/23 17:13:19.912
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 11/15/23 17:13:23.999
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 11/15/23 17:13:24.029
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 11/15/23 17:13:24.029
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:13:24.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-9820" for this suite. 11/15/23 17:13:24.17
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:13:24.202
Nov 15 17:13:24.202: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename namespaces 11/15/23 17:13:24.205
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:13:24.259
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:13:24.273
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 11/15/23 17:13:24.288
STEP: patching the Namespace 11/15/23 17:13:24.339
STEP: get the Namespace and ensuring it has the label 11/15/23 17:13:24.358
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 17:13:24.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-6353" for this suite. 11/15/23 17:13:24.393
STEP: Destroying namespace "nspatchtest-815eeb80-7b64-4c9e-ba27-b4cebadc4acc-8964" for this suite. 11/15/23 17:13:24.416
------------------------------
• [0.238 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:13:24.202
    Nov 15 17:13:24.202: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename namespaces 11/15/23 17:13:24.205
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:13:24.259
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:13:24.273
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 11/15/23 17:13:24.288
    STEP: patching the Namespace 11/15/23 17:13:24.339
    STEP: get the Namespace and ensuring it has the label 11/15/23 17:13:24.358
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:13:24.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-6353" for this suite. 11/15/23 17:13:24.393
    STEP: Destroying namespace "nspatchtest-815eeb80-7b64-4c9e-ba27-b4cebadc4acc-8964" for this suite. 11/15/23 17:13:24.416
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:13:24.443
Nov 15 17:13:24.443: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename crd-webhook 11/15/23 17:13:24.445
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:13:24.5
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:13:24.515
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 11/15/23 17:13:24.547
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 11/15/23 17:13:24.907
STEP: Deploying the custom resource conversion webhook pod 11/15/23 17:13:24.943
STEP: Wait for the deployment to be ready 11/15/23 17:13:24.993
Nov 15 17:13:25.036: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Nov 15 17:13:27.092: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 17, 13, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 17, 13, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 17, 13, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 17, 13, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 11/15/23 17:13:29.111
STEP: Verifying the service has paired with the endpoint 11/15/23 17:13:29.152
Nov 15 17:13:30.154: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Nov 15 17:13:30.170: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Creating a v1 custom resource 11/15/23 17:13:33.255
STEP: Create a v2 custom resource 11/15/23 17:13:33.326
STEP: List CRs in v1 11/15/23 17:13:33.508
STEP: List CRs in v2 11/15/23 17:13:33.543
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 17:13:34.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-6946" for this suite. 11/15/23 17:13:34.334
------------------------------
• [SLOW TEST] [9.916 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:13:24.443
    Nov 15 17:13:24.443: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename crd-webhook 11/15/23 17:13:24.445
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:13:24.5
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:13:24.515
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 11/15/23 17:13:24.547
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 11/15/23 17:13:24.907
    STEP: Deploying the custom resource conversion webhook pod 11/15/23 17:13:24.943
    STEP: Wait for the deployment to be ready 11/15/23 17:13:24.993
    Nov 15 17:13:25.036: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    Nov 15 17:13:27.092: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 17, 13, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 17, 13, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 17, 13, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 17, 13, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 11/15/23 17:13:29.111
    STEP: Verifying the service has paired with the endpoint 11/15/23 17:13:29.152
    Nov 15 17:13:30.154: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Nov 15 17:13:30.170: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Creating a v1 custom resource 11/15/23 17:13:33.255
    STEP: Create a v2 custom resource 11/15/23 17:13:33.326
    STEP: List CRs in v1 11/15/23 17:13:33.508
    STEP: List CRs in v2 11/15/23 17:13:33.543
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:13:34.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-6946" for this suite. 11/15/23 17:13:34.334
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:13:34.36
Nov 15 17:13:34.360: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename projected 11/15/23 17:13:34.361
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:13:34.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:13:34.429
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-26b264ab-a87e-42d7-a401-bf2cdda8f511 11/15/23 17:13:34.443
STEP: Creating a pod to test consume configMaps 11/15/23 17:13:34.462
Nov 15 17:13:34.496: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fe522d30-27c8-4fcb-b2d1-0acd2cad3a0e" in namespace "projected-6303" to be "Succeeded or Failed"
Nov 15 17:13:34.514: INFO: Pod "pod-projected-configmaps-fe522d30-27c8-4fcb-b2d1-0acd2cad3a0e": Phase="Pending", Reason="", readiness=false. Elapsed: 17.496097ms
Nov 15 17:13:36.531: INFO: Pod "pod-projected-configmaps-fe522d30-27c8-4fcb-b2d1-0acd2cad3a0e": Phase="Running", Reason="", readiness=true. Elapsed: 2.034300127s
Nov 15 17:13:38.533: INFO: Pod "pod-projected-configmaps-fe522d30-27c8-4fcb-b2d1-0acd2cad3a0e": Phase="Running", Reason="", readiness=false. Elapsed: 4.036510386s
Nov 15 17:13:40.530: INFO: Pod "pod-projected-configmaps-fe522d30-27c8-4fcb-b2d1-0acd2cad3a0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033119206s
STEP: Saw pod success 11/15/23 17:13:40.53
Nov 15 17:13:40.530: INFO: Pod "pod-projected-configmaps-fe522d30-27c8-4fcb-b2d1-0acd2cad3a0e" satisfied condition "Succeeded or Failed"
Nov 15 17:13:40.544: INFO: Trying to get logs from node 10.15.40.115 pod pod-projected-configmaps-fe522d30-27c8-4fcb-b2d1-0acd2cad3a0e container agnhost-container: <nil>
STEP: delete the pod 11/15/23 17:13:40.6
Nov 15 17:13:40.636: INFO: Waiting for pod pod-projected-configmaps-fe522d30-27c8-4fcb-b2d1-0acd2cad3a0e to disappear
Nov 15 17:13:40.651: INFO: Pod pod-projected-configmaps-fe522d30-27c8-4fcb-b2d1-0acd2cad3a0e no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Nov 15 17:13:40.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6303" for this suite. 11/15/23 17:13:40.677
------------------------------
• [SLOW TEST] [6.342 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:13:34.36
    Nov 15 17:13:34.360: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename projected 11/15/23 17:13:34.361
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:13:34.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:13:34.429
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-26b264ab-a87e-42d7-a401-bf2cdda8f511 11/15/23 17:13:34.443
    STEP: Creating a pod to test consume configMaps 11/15/23 17:13:34.462
    Nov 15 17:13:34.496: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fe522d30-27c8-4fcb-b2d1-0acd2cad3a0e" in namespace "projected-6303" to be "Succeeded or Failed"
    Nov 15 17:13:34.514: INFO: Pod "pod-projected-configmaps-fe522d30-27c8-4fcb-b2d1-0acd2cad3a0e": Phase="Pending", Reason="", readiness=false. Elapsed: 17.496097ms
    Nov 15 17:13:36.531: INFO: Pod "pod-projected-configmaps-fe522d30-27c8-4fcb-b2d1-0acd2cad3a0e": Phase="Running", Reason="", readiness=true. Elapsed: 2.034300127s
    Nov 15 17:13:38.533: INFO: Pod "pod-projected-configmaps-fe522d30-27c8-4fcb-b2d1-0acd2cad3a0e": Phase="Running", Reason="", readiness=false. Elapsed: 4.036510386s
    Nov 15 17:13:40.530: INFO: Pod "pod-projected-configmaps-fe522d30-27c8-4fcb-b2d1-0acd2cad3a0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033119206s
    STEP: Saw pod success 11/15/23 17:13:40.53
    Nov 15 17:13:40.530: INFO: Pod "pod-projected-configmaps-fe522d30-27c8-4fcb-b2d1-0acd2cad3a0e" satisfied condition "Succeeded or Failed"
    Nov 15 17:13:40.544: INFO: Trying to get logs from node 10.15.40.115 pod pod-projected-configmaps-fe522d30-27c8-4fcb-b2d1-0acd2cad3a0e container agnhost-container: <nil>
    STEP: delete the pod 11/15/23 17:13:40.6
    Nov 15 17:13:40.636: INFO: Waiting for pod pod-projected-configmaps-fe522d30-27c8-4fcb-b2d1-0acd2cad3a0e to disappear
    Nov 15 17:13:40.651: INFO: Pod pod-projected-configmaps-fe522d30-27c8-4fcb-b2d1-0acd2cad3a0e no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:13:40.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6303" for this suite. 11/15/23 17:13:40.677
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:13:40.708
Nov 15 17:13:40.708: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename controllerrevisions 11/15/23 17:13:40.711
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:13:40.764
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:13:40.777
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-llmlh-daemon-set" 11/15/23 17:13:40.882
STEP: Check that daemon pods launch on every node of the cluster. 11/15/23 17:13:40.904
Nov 15 17:13:40.941: INFO: Number of nodes with available pods controlled by daemonset e2e-llmlh-daemon-set: 0
Nov 15 17:13:40.941: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
Nov 15 17:13:41.984: INFO: Number of nodes with available pods controlled by daemonset e2e-llmlh-daemon-set: 0
Nov 15 17:13:41.984: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
Nov 15 17:13:42.985: INFO: Number of nodes with available pods controlled by daemonset e2e-llmlh-daemon-set: 0
Nov 15 17:13:42.985: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
Nov 15 17:13:43.987: INFO: Number of nodes with available pods controlled by daemonset e2e-llmlh-daemon-set: 3
Nov 15 17:13:43.987: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-llmlh-daemon-set
STEP: Confirm DaemonSet "e2e-llmlh-daemon-set" successfully created with "daemonset-name=e2e-llmlh-daemon-set" label 11/15/23 17:13:44.007
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-llmlh-daemon-set" 11/15/23 17:13:44.055
Nov 15 17:13:44.081: INFO: Located ControllerRevision: "e2e-llmlh-daemon-set-7bdd75d68d"
STEP: Patching ControllerRevision "e2e-llmlh-daemon-set-7bdd75d68d" 11/15/23 17:13:44.1
Nov 15 17:13:44.128: INFO: e2e-llmlh-daemon-set-7bdd75d68d has been patched
STEP: Create a new ControllerRevision 11/15/23 17:13:44.129
Nov 15 17:13:44.150: INFO: Created ControllerRevision: e2e-llmlh-daemon-set-7d955df466
STEP: Confirm that there are two ControllerRevisions 11/15/23 17:13:44.15
Nov 15 17:13:44.151: INFO: Requesting list of ControllerRevisions to confirm quantity
Nov 15 17:13:44.170: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-llmlh-daemon-set-7bdd75d68d" 11/15/23 17:13:44.17
STEP: Confirm that there is only one ControllerRevision 11/15/23 17:13:44.204
Nov 15 17:13:44.205: INFO: Requesting list of ControllerRevisions to confirm quantity
Nov 15 17:13:44.223: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-llmlh-daemon-set-7d955df466" 11/15/23 17:13:44.242
Nov 15 17:13:44.286: INFO: e2e-llmlh-daemon-set-7d955df466 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 11/15/23 17:13:44.286
W1115 17:13:44.314329      22 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 11/15/23 17:13:44.314
Nov 15 17:13:44.315: INFO: Requesting list of ControllerRevisions to confirm quantity
Nov 15 17:13:45.335: INFO: Requesting list of ControllerRevisions to confirm quantity
Nov 15 17:13:45.359: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-llmlh-daemon-set-7d955df466=updated" 11/15/23 17:13:45.36
STEP: Confirm that there is only one ControllerRevision 11/15/23 17:13:45.415
Nov 15 17:13:45.415: INFO: Requesting list of ControllerRevisions to confirm quantity
Nov 15 17:13:45.432: INFO: Found 1 ControllerRevisions
Nov 15 17:13:45.450: INFO: ControllerRevision "e2e-llmlh-daemon-set-6c6c4874c5" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-llmlh-daemon-set" 11/15/23 17:13:45.469
STEP: deleting DaemonSet.extensions e2e-llmlh-daemon-set in namespace controllerrevisions-6218, will wait for the garbage collector to delete the pods 11/15/23 17:13:45.469
Nov 15 17:13:45.570: INFO: Deleting DaemonSet.extensions e2e-llmlh-daemon-set took: 32.740754ms
Nov 15 17:13:45.671: INFO: Terminating DaemonSet.extensions e2e-llmlh-daemon-set pods took: 101.226648ms
Nov 15 17:13:47.588: INFO: Number of nodes with available pods controlled by daemonset e2e-llmlh-daemon-set: 0
Nov 15 17:13:47.588: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-llmlh-daemon-set
Nov 15 17:13:47.605: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"53641"},"items":null}

Nov 15 17:13:47.620: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"53641"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 15 17:13:47.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-6218" for this suite. 11/15/23 17:13:47.717
------------------------------
• [SLOW TEST] [7.033 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:13:40.708
    Nov 15 17:13:40.708: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename controllerrevisions 11/15/23 17:13:40.711
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:13:40.764
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:13:40.777
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-llmlh-daemon-set" 11/15/23 17:13:40.882
    STEP: Check that daemon pods launch on every node of the cluster. 11/15/23 17:13:40.904
    Nov 15 17:13:40.941: INFO: Number of nodes with available pods controlled by daemonset e2e-llmlh-daemon-set: 0
    Nov 15 17:13:40.941: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
    Nov 15 17:13:41.984: INFO: Number of nodes with available pods controlled by daemonset e2e-llmlh-daemon-set: 0
    Nov 15 17:13:41.984: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
    Nov 15 17:13:42.985: INFO: Number of nodes with available pods controlled by daemonset e2e-llmlh-daemon-set: 0
    Nov 15 17:13:42.985: INFO: Node 10.15.40.106 is running 0 daemon pod, expected 1
    Nov 15 17:13:43.987: INFO: Number of nodes with available pods controlled by daemonset e2e-llmlh-daemon-set: 3
    Nov 15 17:13:43.987: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-llmlh-daemon-set
    STEP: Confirm DaemonSet "e2e-llmlh-daemon-set" successfully created with "daemonset-name=e2e-llmlh-daemon-set" label 11/15/23 17:13:44.007
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-llmlh-daemon-set" 11/15/23 17:13:44.055
    Nov 15 17:13:44.081: INFO: Located ControllerRevision: "e2e-llmlh-daemon-set-7bdd75d68d"
    STEP: Patching ControllerRevision "e2e-llmlh-daemon-set-7bdd75d68d" 11/15/23 17:13:44.1
    Nov 15 17:13:44.128: INFO: e2e-llmlh-daemon-set-7bdd75d68d has been patched
    STEP: Create a new ControllerRevision 11/15/23 17:13:44.129
    Nov 15 17:13:44.150: INFO: Created ControllerRevision: e2e-llmlh-daemon-set-7d955df466
    STEP: Confirm that there are two ControllerRevisions 11/15/23 17:13:44.15
    Nov 15 17:13:44.151: INFO: Requesting list of ControllerRevisions to confirm quantity
    Nov 15 17:13:44.170: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-llmlh-daemon-set-7bdd75d68d" 11/15/23 17:13:44.17
    STEP: Confirm that there is only one ControllerRevision 11/15/23 17:13:44.204
    Nov 15 17:13:44.205: INFO: Requesting list of ControllerRevisions to confirm quantity
    Nov 15 17:13:44.223: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-llmlh-daemon-set-7d955df466" 11/15/23 17:13:44.242
    Nov 15 17:13:44.286: INFO: e2e-llmlh-daemon-set-7d955df466 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 11/15/23 17:13:44.286
    W1115 17:13:44.314329      22 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 11/15/23 17:13:44.314
    Nov 15 17:13:44.315: INFO: Requesting list of ControllerRevisions to confirm quantity
    Nov 15 17:13:45.335: INFO: Requesting list of ControllerRevisions to confirm quantity
    Nov 15 17:13:45.359: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-llmlh-daemon-set-7d955df466=updated" 11/15/23 17:13:45.36
    STEP: Confirm that there is only one ControllerRevision 11/15/23 17:13:45.415
    Nov 15 17:13:45.415: INFO: Requesting list of ControllerRevisions to confirm quantity
    Nov 15 17:13:45.432: INFO: Found 1 ControllerRevisions
    Nov 15 17:13:45.450: INFO: ControllerRevision "e2e-llmlh-daemon-set-6c6c4874c5" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-llmlh-daemon-set" 11/15/23 17:13:45.469
    STEP: deleting DaemonSet.extensions e2e-llmlh-daemon-set in namespace controllerrevisions-6218, will wait for the garbage collector to delete the pods 11/15/23 17:13:45.469
    Nov 15 17:13:45.570: INFO: Deleting DaemonSet.extensions e2e-llmlh-daemon-set took: 32.740754ms
    Nov 15 17:13:45.671: INFO: Terminating DaemonSet.extensions e2e-llmlh-daemon-set pods took: 101.226648ms
    Nov 15 17:13:47.588: INFO: Number of nodes with available pods controlled by daemonset e2e-llmlh-daemon-set: 0
    Nov 15 17:13:47.588: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-llmlh-daemon-set
    Nov 15 17:13:47.605: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"53641"},"items":null}

    Nov 15 17:13:47.620: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"53641"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:13:47.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-6218" for this suite. 11/15/23 17:13:47.717
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:13:47.758
Nov 15 17:13:47.758: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename deployment 11/15/23 17:13:47.76
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:13:47.814
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:13:47.832
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Nov 15 17:13:47.851: INFO: Creating deployment "webserver-deployment"
Nov 15 17:13:47.875: INFO: Waiting for observed generation 1
Nov 15 17:13:49.915: INFO: Waiting for all required pods to come up
Nov 15 17:13:49.940: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 11/15/23 17:13:49.941
Nov 15 17:13:49.941: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-5snc7" in namespace "deployment-5147" to be "running"
Nov 15 17:13:49.941: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-2ltt2" in namespace "deployment-5147" to be "running"
Nov 15 17:13:49.941: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-dqtb2" in namespace "deployment-5147" to be "running"
Nov 15 17:13:49.941: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-4npvr" in namespace "deployment-5147" to be "running"
Nov 15 17:13:49.941: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-6zns8" in namespace "deployment-5147" to be "running"
Nov 15 17:13:49.941: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-hqm97" in namespace "deployment-5147" to be "running"
Nov 15 17:13:49.942: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-fs45z" in namespace "deployment-5147" to be "running"
Nov 15 17:13:49.942: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-lt5zb" in namespace "deployment-5147" to be "running"
Nov 15 17:13:49.941: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-rtxlx" in namespace "deployment-5147" to be "running"
Nov 15 17:13:49.956: INFO: Pod "webserver-deployment-7f5969cbc7-4npvr": Phase="Pending", Reason="", readiness=false. Elapsed: 15.130221ms
Nov 15 17:13:49.962: INFO: Pod "webserver-deployment-7f5969cbc7-dqtb2": Phase="Pending", Reason="", readiness=false. Elapsed: 21.172852ms
Nov 15 17:13:49.963: INFO: Pod "webserver-deployment-7f5969cbc7-rtxlx": Phase="Pending", Reason="", readiness=false. Elapsed: 20.435111ms
Nov 15 17:13:49.963: INFO: Pod "webserver-deployment-7f5969cbc7-5snc7": Phase="Pending", Reason="", readiness=false. Elapsed: 22.158626ms
Nov 15 17:13:49.965: INFO: Pod "webserver-deployment-7f5969cbc7-fs45z": Phase="Pending", Reason="", readiness=false. Elapsed: 23.480609ms
Nov 15 17:13:49.965: INFO: Pod "webserver-deployment-7f5969cbc7-6zns8": Phase="Pending", Reason="", readiness=false. Elapsed: 23.741462ms
Nov 15 17:13:49.965: INFO: Pod "webserver-deployment-7f5969cbc7-2ltt2": Phase="Pending", Reason="", readiness=false. Elapsed: 24.157396ms
Nov 15 17:13:49.965: INFO: Pod "webserver-deployment-7f5969cbc7-hqm97": Phase="Pending", Reason="", readiness=false. Elapsed: 23.925794ms
Nov 15 17:13:49.966: INFO: Pod "webserver-deployment-7f5969cbc7-lt5zb": Phase="Pending", Reason="", readiness=false. Elapsed: 24.742638ms
Nov 15 17:13:51.975: INFO: Pod "webserver-deployment-7f5969cbc7-4npvr": Phase="Running", Reason="", readiness=true. Elapsed: 2.033998869s
Nov 15 17:13:51.976: INFO: Pod "webserver-deployment-7f5969cbc7-4npvr" satisfied condition "running"
Nov 15 17:13:51.981: INFO: Pod "webserver-deployment-7f5969cbc7-dqtb2": Phase="Running", Reason="", readiness=true. Elapsed: 2.039652564s
Nov 15 17:13:51.981: INFO: Pod "webserver-deployment-7f5969cbc7-dqtb2" satisfied condition "running"
Nov 15 17:13:51.984: INFO: Pod "webserver-deployment-7f5969cbc7-fs45z": Phase="Running", Reason="", readiness=true. Elapsed: 2.041988814s
Nov 15 17:13:51.984: INFO: Pod "webserver-deployment-7f5969cbc7-fs45z" satisfied condition "running"
Nov 15 17:13:51.986: INFO: Pod "webserver-deployment-7f5969cbc7-hqm97": Phase="Running", Reason="", readiness=true. Elapsed: 2.044070941s
Nov 15 17:13:51.986: INFO: Pod "webserver-deployment-7f5969cbc7-hqm97" satisfied condition "running"
Nov 15 17:13:51.986: INFO: Pod "webserver-deployment-7f5969cbc7-rtxlx": Phase="Running", Reason="", readiness=true. Elapsed: 2.042936192s
Nov 15 17:13:51.986: INFO: Pod "webserver-deployment-7f5969cbc7-rtxlx" satisfied condition "running"
Nov 15 17:13:51.986: INFO: Pod "webserver-deployment-7f5969cbc7-2ltt2": Phase="Running", Reason="", readiness=true. Elapsed: 2.045064183s
Nov 15 17:13:51.986: INFO: Pod "webserver-deployment-7f5969cbc7-2ltt2" satisfied condition "running"
Nov 15 17:13:51.989: INFO: Pod "webserver-deployment-7f5969cbc7-lt5zb": Phase="Running", Reason="", readiness=true. Elapsed: 2.046922305s
Nov 15 17:13:51.989: INFO: Pod "webserver-deployment-7f5969cbc7-lt5zb" satisfied condition "running"
Nov 15 17:13:51.990: INFO: Pod "webserver-deployment-7f5969cbc7-6zns8": Phase="Running", Reason="", readiness=true. Elapsed: 2.048239479s
Nov 15 17:13:51.990: INFO: Pod "webserver-deployment-7f5969cbc7-6zns8" satisfied condition "running"
Nov 15 17:13:51.991: INFO: Pod "webserver-deployment-7f5969cbc7-5snc7": Phase="Running", Reason="", readiness=true. Elapsed: 2.049581554s
Nov 15 17:13:51.991: INFO: Pod "webserver-deployment-7f5969cbc7-5snc7" satisfied condition "running"
Nov 15 17:13:51.991: INFO: Waiting for deployment "webserver-deployment" to complete
Nov 15 17:13:52.026: INFO: Updating deployment "webserver-deployment" with a non-existent image
Nov 15 17:13:52.075: INFO: Updating deployment webserver-deployment
Nov 15 17:13:52.075: INFO: Waiting for observed generation 2
Nov 15 17:13:54.113: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Nov 15 17:13:54.131: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Nov 15 17:13:54.145: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Nov 15 17:13:54.197: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Nov 15 17:13:54.198: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Nov 15 17:13:54.213: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Nov 15 17:13:54.243: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Nov 15 17:13:54.243: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Nov 15 17:13:54.286: INFO: Updating deployment webserver-deployment
Nov 15 17:13:54.286: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Nov 15 17:13:54.315: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Nov 15 17:13:56.365: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Nov 15 17:13:56.400: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-5147  da7e4a75-fb1f-4e93-ba29-949c93624ee8 54109 3 2023-11-15 17:13:47 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 17:13:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0027b1dd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:9,UnavailableReplicas:24,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-11-15 17:13:54 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-11-15 17:13:56 +0000 UTC,LastTransitionTime:2023-11-15 17:13:47 +0000 UTC,},},ReadyReplicas:9,CollisionCount:nil,},}

Nov 15 17:13:56.414: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-5147  e3b68dae-8121-4ff3-aad1-2b86189d3e09 53967 3 2023-11-15 17:13:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment da7e4a75-fb1f-4e93-ba29-949c93624ee8 0xc00538af57 0xc00538af58}] [] [{kube-controller-manager Update apps/v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"da7e4a75-fb1f-4e93-ba29-949c93624ee8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00538b008 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 15 17:13:56.414: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Nov 15 17:13:56.414: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-5147  874c63cf-fcb8-44c8-81cc-22dcee9ca51f 54108 3 2023-11-15 17:13:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment da7e4a75-fb1f-4e93-ba29-949c93624ee8 0xc00538ae67 0xc00538ae68}] [] [{kube-controller-manager Update apps/v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"da7e4a75-fb1f-4e93-ba29-949c93624ee8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 17:13:56 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00538aef8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:9,AvailableReplicas:9,Conditions:[]ReplicaSetCondition{},},}
Nov 15 17:13:56.449: INFO: Pod "webserver-deployment-7f5969cbc7-292zm" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-292zm webserver-deployment-7f5969cbc7- deployment-5147  74b56f4d-8172-42a9-ad5a-22e5adeefee1 54047 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:dd66cf3667564e54d91068eceee910bf974b484c0d35837e9cd83f2c4f0ee761 cni.projectcalico.org/podIP:172.30.191.76/32 cni.projectcalico.org/podIPs:172.30.191.76/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc00538b537 0xc00538b538}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 17:13:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cvtsf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cvtsf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.106,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.106,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 17:13:56.449: INFO: Pod "webserver-deployment-7f5969cbc7-4npvr" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-4npvr webserver-deployment-7f5969cbc7- deployment-5147  88d70992-367c-4cda-865d-abe180d6c40d 53795 0 2023-11-15 17:13:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:826b1da600c75ceed2110278ac698bf6d950bc2456c6c61e01c703d0abf03043 cni.projectcalico.org/podIP:172.30.164.36/32 cni.projectcalico.org/podIPs:172.30.164.36/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc00538b747 0xc00538b748}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 17:13:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 17:13:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.164.36\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bqrq9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bqrq9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:172.30.164.36,StartTime:2023-11-15 17:13:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 17:13:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c0a120f2f6efd4a1c00a5b7b1318d5c1c286697fa16dc8f97fb085f6afba44aa,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.164.36,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 17:13:56.450: INFO: Pod "webserver-deployment-7f5969cbc7-4rp6h" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-4rp6h webserver-deployment-7f5969cbc7- deployment-5147  1169e5f4-c244-4205-8c17-c67477d5aea3 54044 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:03c4a6c84cf352c8706332d15df458b09bacf47b919e75457cadfe9e889154e4 cni.projectcalico.org/podIP:172.30.205.218/32 cni.projectcalico.org/podIPs:172.30.205.218/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc00538b957 0xc00538b958}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 17:13:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ph95c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ph95c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.114,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 17:13:56.450: INFO: Pod "webserver-deployment-7f5969cbc7-5snc7" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5snc7 webserver-deployment-7f5969cbc7- deployment-5147  6da5120a-9998-468f-8d1e-2682d56c9042 53776 0 2023-11-15 17:13:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:bf24f3b67f673f9f4ee3703bd38b13939f38914eceea178df596e0455ebada4e cni.projectcalico.org/podIP:172.30.164.56/32 cni.projectcalico.org/podIPs:172.30.164.56/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc00538bb67 0xc00538bb68}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 17:13:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 17:13:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.164.56\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5pg5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5pg5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:172.30.164.56,StartTime:2023-11-15 17:13:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 17:13:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0e30fca4e6a3c1d7c143045670069faecb57a3ce990cc637b3b59e72fd84889d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.164.56,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 17:13:56.450: INFO: Pod "webserver-deployment-7f5969cbc7-6jdzs" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6jdzs webserver-deployment-7f5969cbc7- deployment-5147  4e53ee77-1171-45d6-8909-80e1243bd4df 53737 0 2023-11-15 17:13:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c2678c957157736d663fccf27920b716800ce3bee4f82a14641e4a1e46c68170 cni.projectcalico.org/podIP:172.30.191.127/32 cni.projectcalico.org/podIPs:172.30.191.127/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc00538bd77 0xc00538bd78}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 17:13:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 17:13:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.191.127\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hrrvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hrrvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.106,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.106,PodIP:172.30.191.127,StartTime:2023-11-15 17:13:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 17:13:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://8481306076c9030d0a1e7593458c2d4e84e3bbda7e7d6a64a76201ac975838e0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.191.127,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 17:13:56.451: INFO: Pod "webserver-deployment-7f5969cbc7-6pg9c" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6pg9c webserver-deployment-7f5969cbc7- deployment-5147  fe043dbc-11fa-4c08-9699-de298b53b65b 54066 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:5f068488d48e34ca31a4e21d732e4b79282a03693a5e7d7c6366a621fd8596d7 cni.projectcalico.org/podIP:172.30.191.126/32 cni.projectcalico.org/podIPs:172.30.191.126/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc00538bf87 0xc00538bf88}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 17:13:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4djqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4djqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.106,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.106,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 17:13:56.451: INFO: Pod "webserver-deployment-7f5969cbc7-6zns8" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6zns8 webserver-deployment-7f5969cbc7- deployment-5147  9615564f-123d-46fd-ab81-025bc5d04a22 53785 0 2023-11-15 17:13:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:b1b7b3391c3065fb374bd31ac2770d50d7409a537800c40a004e089244a3ef7c cni.projectcalico.org/podIP:172.30.205.249/32 cni.projectcalico.org/podIPs:172.30.205.249/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc000f1e3f7 0xc000f1e3f8}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 17:13:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 17:13:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.205.249\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xl7s4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xl7s4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.114,PodIP:172.30.205.249,StartTime:2023-11-15 17:13:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 17:13:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6876419f879c0b1b8cba758c7e77b0a644a3fad90cbcd152f59d9d8f508ec1fc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.205.249,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 17:13:56.451: INFO: Pod "webserver-deployment-7f5969cbc7-7g9nd" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7g9nd webserver-deployment-7f5969cbc7- deployment-5147  b5af8387-5fc2-44ca-ba19-64a6b33e212a 54029 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:5907d333570f6c670238c2934dd429a82f6bd35500094c9390300ae5a65f44b6 cni.projectcalico.org/podIP:172.30.205.209/32 cni.projectcalico.org/podIPs:172.30.205.209/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc000f1e8e7 0xc000f1e8e8}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 17:13:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8jzb4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8jzb4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.114,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 17:13:56.452: INFO: Pod "webserver-deployment-7f5969cbc7-87hcr" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-87hcr webserver-deployment-7f5969cbc7- deployment-5147  c5229312-f97c-4c1e-ae9a-545dc5a05274 53952 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc000f1ed57 0xc000f1ed58}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v8b96,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v8b96,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 17:13:56.452: INFO: Pod "webserver-deployment-7f5969cbc7-dqtb2" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dqtb2 webserver-deployment-7f5969cbc7- deployment-5147  84a96962-dcc9-4ba2-b010-eff588d0b34d 53791 0 2023-11-15 17:13:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:24ed3adb7fc21d8cfeec68b1628dfb30b0460693956c999396a3ba0f09998a71 cni.projectcalico.org/podIP:172.30.205.248/32 cni.projectcalico.org/podIPs:172.30.205.248/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc000f1ef27 0xc000f1ef28}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 17:13:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 17:13:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.205.248\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pm6nk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pm6nk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.114,PodIP:172.30.205.248,StartTime:2023-11-15 17:13:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 17:13:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2719a3036bf75dc07192bc5df4ea8c00fc6d3447d579a01865470fd07e9aba1f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.205.248,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 17:13:56.452: INFO: Pod "webserver-deployment-7f5969cbc7-flkwx" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-flkwx webserver-deployment-7f5969cbc7- deployment-5147  25ea989d-f7f5-4c20-8b82-1aef18039448 54059 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:e15740e4f785771d465381ff176781ec06f8484eae753e01b0829064f03718dd cni.projectcalico.org/podIP:172.30.164.42/32 cni.projectcalico.org/podIPs:172.30.164.42/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc000f1f157 0xc000f1f158}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 17:13:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7r7xm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7r7xm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 17:13:56.453: INFO: Pod "webserver-deployment-7f5969cbc7-gnrst" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-gnrst webserver-deployment-7f5969cbc7- deployment-5147  7d7a216f-9fa0-4fff-92fb-54a76a0135da 54015 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc000f1f367 0xc000f1f368}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-574pm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-574pm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 17:13:56.453: INFO: Pod "webserver-deployment-7f5969cbc7-hp8th" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hp8th webserver-deployment-7f5969cbc7- deployment-5147  f7a4360a-e6eb-41f7-a369-7b706592594d 54077 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:d288a5f7e0272e1e5785d691e4f63baf57761760b2d710af77056919603f1076 cni.projectcalico.org/podIP:172.30.164.4/32 cni.projectcalico.org/podIPs:172.30.164.4/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc000f1fbe7 0xc000f1fbe8}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 17:13:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v8vbd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v8vbd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 17:13:56.453: INFO: Pod "webserver-deployment-7f5969cbc7-hqm97" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hqm97 webserver-deployment-7f5969cbc7- deployment-5147  b026a262-75f0-4599-bbec-a3855d047e6b 53780 0 2023-11-15 17:13:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:2602f8782ed4fac3ab92f2a550ce9e3b6e33d9564f8e348aee0e6a3242a96b09 cni.projectcalico.org/podIP:172.30.191.79/32 cni.projectcalico.org/podIPs:172.30.191.79/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc004aaa0b7 0xc004aaa0b8}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 17:13:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 17:13:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.191.79\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ltq4g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ltq4g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.106,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.106,PodIP:172.30.191.79,StartTime:2023-11-15 17:13:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 17:13:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7b231e297ab561ec7aa0da823b0ff7b94464be4a110aaa258adb4af17e6646fb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.191.79,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 17:13:56.453: INFO: Pod "webserver-deployment-7f5969cbc7-hrs64" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hrs64 webserver-deployment-7f5969cbc7- deployment-5147  370787ea-82bc-44ab-8093-770ba3982c3c 54116 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:9a5106be5d0c6f243b08378aaa46852d1003b940b9db78942f288c8b30c7e37b cni.projectcalico.org/podIP:172.30.191.118/32 cni.projectcalico.org/podIPs:172.30.191.118/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc004aaa2c7 0xc004aaa2c8}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 17:13:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j5w5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j5w5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.106,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.106,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 17:13:56.454: INFO: Pod "webserver-deployment-7f5969cbc7-hsftt" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hsftt webserver-deployment-7f5969cbc7- deployment-5147  20a1cadc-5589-4f0b-94c1-772694cb175f 54104 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:2f8326e30e6c8005b326a3c1d3964c3f3e9a4bb2cda90d9dcb1f0ba48d8de81f cni.projectcalico.org/podIP:172.30.164.62/32 cni.projectcalico.org/podIPs:172.30.164.62/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc004aaa4d7 0xc004aaa4d8}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 17:13:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 17:13:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.164.62\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-85trf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-85trf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:172.30.164.62,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 17:13:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2aa79b2601ff0e4e14ab94145338eeba03f257d071273cdf7d43b1c52699f0f9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.164.62,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 17:13:56.454: INFO: Pod "webserver-deployment-7f5969cbc7-lt5zb" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-lt5zb webserver-deployment-7f5969cbc7- deployment-5147  0036be3d-b51b-4529-97db-732bd36b03df 53786 0 2023-11-15 17:13:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:954af8e5886ab859aaa7a8987707c93a7941051e4f4d6dafda3d63ae3bade2ff cni.projectcalico.org/podIP:172.30.191.89/32 cni.projectcalico.org/podIPs:172.30.191.89/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc004aaa707 0xc004aaa708}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 17:13:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 17:13:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.191.89\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dfsbx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dfsbx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.106,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.106,PodIP:172.30.191.89,StartTime:2023-11-15 17:13:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 17:13:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://99153443ac5f1fb038448acc01afe54154bd6d9745437ee4222c994d629b8f02,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.191.89,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 17:13:56.455: INFO: Pod "webserver-deployment-7f5969cbc7-mvwn6" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mvwn6 webserver-deployment-7f5969cbc7- deployment-5147  c723454e-4eb6-44f0-9b07-9e0938b5b6df 54090 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:f46b5a6990807848a3476100a206f939ae613e0ac967a0e6e3e71c99efd8b62a cni.projectcalico.org/podIP:172.30.205.219/32 cni.projectcalico.org/podIPs:172.30.205.219/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc004aaa917 0xc004aaa918}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 17:13:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ng6jn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ng6jn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.114,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 17:13:56.456: INFO: Pod "webserver-deployment-7f5969cbc7-qxg5c" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qxg5c webserver-deployment-7f5969cbc7- deployment-5147  88ce5662-827e-4ed5-8ebf-d2bd9c78ca3d 54012 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc004aaab07 0xc004aaab08}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ddgm2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ddgm2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 17:13:56.457: INFO: Pod "webserver-deployment-7f5969cbc7-rtxlx" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rtxlx webserver-deployment-7f5969cbc7- deployment-5147  998d4d49-cac7-48bb-b5e8-178324a08cef 53779 0 2023-11-15 17:13:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:2793054dc9999de1aa3b9fcdc97365c06be08e5c460b3359012c40e3b1b29138 cni.projectcalico.org/podIP:172.30.205.207/32 cni.projectcalico.org/podIPs:172.30.205.207/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc004aaace7 0xc004aaace8}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 17:13:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 17:13:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.205.207\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9chcv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9chcv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.114,PodIP:172.30.205.207,StartTime:2023-11-15 17:13:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 17:13:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://997a51ea1c5405cf6e8d11f08c28a50542dfaeea539cebc03cbbedc096a93042,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.205.207,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 17:13:56.458: INFO: Pod "webserver-deployment-d9f79cb5-2d9zs" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-2d9zs webserver-deployment-d9f79cb5- deployment-5147  b153e7db-c593-4d0e-9cc9-fd47158cc7ae 54042 0 2023-11-15 17:13:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:3e3fa3aa2f6834ff9de55e80054d45d775ff34d168f594809b40a45fb49fd58f cni.projectcalico.org/podIP:172.30.191.120/32 cni.projectcalico.org/podIPs:172.30.191.120/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e3b68dae-8121-4ff3-aad1-2b86189d3e09 0xc004aaaf07 0xc004aaaf08}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3b68dae-8121-4ff3-aad1-2b86189d3e09\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 17:13:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 17:13:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.191.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6rsf2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6rsf2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.106,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.106,PodIP:172.30.191.120,StartTime:2023-11-15 17:13:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.191.120,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 17:13:56.458: INFO: Pod "webserver-deployment-d9f79cb5-4q4kq" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-4q4kq webserver-deployment-d9f79cb5- deployment-5147  5fbf8518-db2f-48b0-9fd5-0d218f4945d7 54031 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:cc2ac21236cf028556dbc7da616416f805c44d970f79104095d324e0b8d36db7 cni.projectcalico.org/podIP:172.30.191.84/32 cni.projectcalico.org/podIPs:172.30.191.84/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e3b68dae-8121-4ff3-aad1-2b86189d3e09 0xc004aab167 0xc004aab168}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3b68dae-8121-4ff3-aad1-2b86189d3e09\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 17:13:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sxd66,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sxd66,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.106,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.106,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 17:13:56.458: INFO: Pod "webserver-deployment-d9f79cb5-6g25z" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-6g25z webserver-deployment-d9f79cb5- deployment-5147  7724a3f0-65ae-4a17-8eea-51535efa8985 54095 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:3bcdf221021bca44538deedf3904cf4374c2b587876789d600cfad957c548e9d cni.projectcalico.org/podIP:172.30.164.12/32 cni.projectcalico.org/podIPs:172.30.164.12/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e3b68dae-8121-4ff3-aad1-2b86189d3e09 0xc004aab397 0xc004aab398}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3b68dae-8121-4ff3-aad1-2b86189d3e09\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 17:13:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vwnpd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vwnpd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 17:13:56.459: INFO: Pod "webserver-deployment-d9f79cb5-bfmf7" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-bfmf7 webserver-deployment-d9f79cb5- deployment-5147  4bbd1473-74ce-431a-9e88-a3d24c1c24dc 54009 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e3b68dae-8121-4ff3-aad1-2b86189d3e09 0xc004aab5c7 0xc004aab5c8}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3b68dae-8121-4ff3-aad1-2b86189d3e09\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ql5nr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ql5nr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.114,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 17:13:56.459: INFO: Pod "webserver-deployment-d9f79cb5-d9hq2" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-d9hq2 webserver-deployment-d9f79cb5- deployment-5147  10207612-d940-4557-9e18-734691d13950 54085 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:6bb5a1eacd4e86a8cc41ae1fa2b29669004b04fb501237be1909b8b70d6ce915 cni.projectcalico.org/podIP:172.30.191.116/32 cni.projectcalico.org/podIPs:172.30.191.116/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e3b68dae-8121-4ff3-aad1-2b86189d3e09 0xc004aab7b7 0xc004aab7b8}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3b68dae-8121-4ff3-aad1-2b86189d3e09\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 17:13:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c7hg8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c7hg8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.106,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.106,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 17:13:56.459: INFO: Pod "webserver-deployment-d9f79cb5-gw5n6" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-gw5n6 webserver-deployment-d9f79cb5- deployment-5147  220dcb14-97e1-4076-b8a3-11be2696d297 54004 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e3b68dae-8121-4ff3-aad1-2b86189d3e09 0xc004aab9c7 0xc004aab9c8}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3b68dae-8121-4ff3-aad1-2b86189d3e09\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d9kc5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d9kc5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 17:13:56.459: INFO: Pod "webserver-deployment-d9f79cb5-j648k" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-j648k webserver-deployment-d9f79cb5- deployment-5147  0ea9704a-7da8-4759-80b9-5c76f0044a61 54068 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:7e795f1cb78bcd1a56b745a0fd651cdcd750972c638a4d7f8306f76a44aea516 cni.projectcalico.org/podIP:172.30.205.241/32 cni.projectcalico.org/podIPs:172.30.205.241/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e3b68dae-8121-4ff3-aad1-2b86189d3e09 0xc004aabbc7 0xc004aabbc8}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3b68dae-8121-4ff3-aad1-2b86189d3e09\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 17:13:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-87smw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-87smw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.114,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 17:13:56.460: INFO: Pod "webserver-deployment-d9f79cb5-klpwv" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-klpwv webserver-deployment-d9f79cb5- deployment-5147  08fe591a-4168-4ccd-a7b6-6c8a7030b6e4 54112 0 2023-11-15 17:13:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:b942d0aaec3b008803a0b67a1a732699c2b073d2f250641430b8f4936c8648ef cni.projectcalico.org/podIP:172.30.164.60/32 cni.projectcalico.org/podIPs:172.30.164.60/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e3b68dae-8121-4ff3-aad1-2b86189d3e09 0xc004aabdf7 0xc004aabdf8}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3b68dae-8121-4ff3-aad1-2b86189d3e09\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 17:13:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 17:13:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.164.60\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rc7r5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rc7r5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:172.30.164.60,StartTime:2023-11-15 17:13:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.164.60,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 17:13:56.460: INFO: Pod "webserver-deployment-d9f79cb5-mpgxb" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-mpgxb webserver-deployment-d9f79cb5- deployment-5147  b7f2d7f4-0194-487b-b32e-3c63f981c979 54100 0 2023-11-15 17:13:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:001351b4b5e6100e92676f853d054f223b07d7de33ed5e9fa947f81bf54b7830 cni.projectcalico.org/podIP:172.30.164.63/32 cni.projectcalico.org/podIPs:172.30.164.63/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e3b68dae-8121-4ff3-aad1-2b86189d3e09 0xc003364057 0xc003364058}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3b68dae-8121-4ff3-aad1-2b86189d3e09\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 17:13:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 17:13:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.164.63\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4pc5f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4pc5f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:172.30.164.63,StartTime:2023-11-15 17:13:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.164.63,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 17:13:56.460: INFO: Pod "webserver-deployment-d9f79cb5-pngst" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-pngst webserver-deployment-d9f79cb5- deployment-5147  169c93db-dd2f-4a86-bc2a-609891d114e6 54040 0 2023-11-15 17:13:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:aa966e02eb4c9a84a93afb0d1e1885b9040d577873b0289b12249eea06f311e6 cni.projectcalico.org/podIP:172.30.205.198/32 cni.projectcalico.org/podIPs:172.30.205.198/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e3b68dae-8121-4ff3-aad1-2b86189d3e09 0xc0033642b7 0xc0033642b8}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3b68dae-8121-4ff3-aad1-2b86189d3e09\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 17:13:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 17:13:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.205.198\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9qjcp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9qjcp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.114,PodIP:172.30.205.198,StartTime:2023-11-15 17:13:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.205.198,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 17:13:56.460: INFO: Pod "webserver-deployment-d9f79cb5-sr8sz" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-sr8sz webserver-deployment-d9f79cb5- deployment-5147  6ac99e48-742a-4c93-87bb-056d83dd35d5 54098 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:40f471c62ed6cc476821ae4045fcd63f21fec6c2c3849e46485988c068620243 cni.projectcalico.org/podIP:172.30.205.195/32 cni.projectcalico.org/podIPs:172.30.205.195/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e3b68dae-8121-4ff3-aad1-2b86189d3e09 0xc003364507 0xc003364508}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3b68dae-8121-4ff3-aad1-2b86189d3e09\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 17:13:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2wskc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2wskc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.114,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 17:13:56.461: INFO: Pod "webserver-deployment-d9f79cb5-tlf28" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-tlf28 webserver-deployment-d9f79cb5- deployment-5147  5d394e6d-e8f5-4d60-8465-118a48f16f8f 54016 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e3b68dae-8121-4ff3-aad1-2b86189d3e09 0xc003364717 0xc003364718}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3b68dae-8121-4ff3-aad1-2b86189d3e09\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g7vpq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g7vpq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 15 17:13:56.461: INFO: Pod "webserver-deployment-d9f79cb5-w92q6" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-w92q6 webserver-deployment-d9f79cb5- deployment-5147  a00e00d3-6e07-49dc-ba1a-b5e6cf490e5b 54048 0 2023-11-15 17:13:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:b1b7e9d2d63d3dc095813c59f67e30959c009d6fd30f0c335e1b3e41f8c393a7 cni.projectcalico.org/podIP:172.30.191.105/32 cni.projectcalico.org/podIPs:172.30.191.105/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e3b68dae-8121-4ff3-aad1-2b86189d3e09 0xc003364917 0xc003364918}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3b68dae-8121-4ff3-aad1-2b86189d3e09\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 17:13:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 17:13:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.191.105\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vz26w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vz26w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.106,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.106,PodIP:172.30.191.105,StartTime:2023-11-15 17:13:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.191.105,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Nov 15 17:13:56.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5147" for this suite. 11/15/23 17:13:56.493
------------------------------
• [SLOW TEST] [8.765 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:13:47.758
    Nov 15 17:13:47.758: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename deployment 11/15/23 17:13:47.76
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:13:47.814
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:13:47.832
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Nov 15 17:13:47.851: INFO: Creating deployment "webserver-deployment"
    Nov 15 17:13:47.875: INFO: Waiting for observed generation 1
    Nov 15 17:13:49.915: INFO: Waiting for all required pods to come up
    Nov 15 17:13:49.940: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 11/15/23 17:13:49.941
    Nov 15 17:13:49.941: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-5snc7" in namespace "deployment-5147" to be "running"
    Nov 15 17:13:49.941: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-2ltt2" in namespace "deployment-5147" to be "running"
    Nov 15 17:13:49.941: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-dqtb2" in namespace "deployment-5147" to be "running"
    Nov 15 17:13:49.941: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-4npvr" in namespace "deployment-5147" to be "running"
    Nov 15 17:13:49.941: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-6zns8" in namespace "deployment-5147" to be "running"
    Nov 15 17:13:49.941: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-hqm97" in namespace "deployment-5147" to be "running"
    Nov 15 17:13:49.942: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-fs45z" in namespace "deployment-5147" to be "running"
    Nov 15 17:13:49.942: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-lt5zb" in namespace "deployment-5147" to be "running"
    Nov 15 17:13:49.941: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-rtxlx" in namespace "deployment-5147" to be "running"
    Nov 15 17:13:49.956: INFO: Pod "webserver-deployment-7f5969cbc7-4npvr": Phase="Pending", Reason="", readiness=false. Elapsed: 15.130221ms
    Nov 15 17:13:49.962: INFO: Pod "webserver-deployment-7f5969cbc7-dqtb2": Phase="Pending", Reason="", readiness=false. Elapsed: 21.172852ms
    Nov 15 17:13:49.963: INFO: Pod "webserver-deployment-7f5969cbc7-rtxlx": Phase="Pending", Reason="", readiness=false. Elapsed: 20.435111ms
    Nov 15 17:13:49.963: INFO: Pod "webserver-deployment-7f5969cbc7-5snc7": Phase="Pending", Reason="", readiness=false. Elapsed: 22.158626ms
    Nov 15 17:13:49.965: INFO: Pod "webserver-deployment-7f5969cbc7-fs45z": Phase="Pending", Reason="", readiness=false. Elapsed: 23.480609ms
    Nov 15 17:13:49.965: INFO: Pod "webserver-deployment-7f5969cbc7-6zns8": Phase="Pending", Reason="", readiness=false. Elapsed: 23.741462ms
    Nov 15 17:13:49.965: INFO: Pod "webserver-deployment-7f5969cbc7-2ltt2": Phase="Pending", Reason="", readiness=false. Elapsed: 24.157396ms
    Nov 15 17:13:49.965: INFO: Pod "webserver-deployment-7f5969cbc7-hqm97": Phase="Pending", Reason="", readiness=false. Elapsed: 23.925794ms
    Nov 15 17:13:49.966: INFO: Pod "webserver-deployment-7f5969cbc7-lt5zb": Phase="Pending", Reason="", readiness=false. Elapsed: 24.742638ms
    Nov 15 17:13:51.975: INFO: Pod "webserver-deployment-7f5969cbc7-4npvr": Phase="Running", Reason="", readiness=true. Elapsed: 2.033998869s
    Nov 15 17:13:51.976: INFO: Pod "webserver-deployment-7f5969cbc7-4npvr" satisfied condition "running"
    Nov 15 17:13:51.981: INFO: Pod "webserver-deployment-7f5969cbc7-dqtb2": Phase="Running", Reason="", readiness=true. Elapsed: 2.039652564s
    Nov 15 17:13:51.981: INFO: Pod "webserver-deployment-7f5969cbc7-dqtb2" satisfied condition "running"
    Nov 15 17:13:51.984: INFO: Pod "webserver-deployment-7f5969cbc7-fs45z": Phase="Running", Reason="", readiness=true. Elapsed: 2.041988814s
    Nov 15 17:13:51.984: INFO: Pod "webserver-deployment-7f5969cbc7-fs45z" satisfied condition "running"
    Nov 15 17:13:51.986: INFO: Pod "webserver-deployment-7f5969cbc7-hqm97": Phase="Running", Reason="", readiness=true. Elapsed: 2.044070941s
    Nov 15 17:13:51.986: INFO: Pod "webserver-deployment-7f5969cbc7-hqm97" satisfied condition "running"
    Nov 15 17:13:51.986: INFO: Pod "webserver-deployment-7f5969cbc7-rtxlx": Phase="Running", Reason="", readiness=true. Elapsed: 2.042936192s
    Nov 15 17:13:51.986: INFO: Pod "webserver-deployment-7f5969cbc7-rtxlx" satisfied condition "running"
    Nov 15 17:13:51.986: INFO: Pod "webserver-deployment-7f5969cbc7-2ltt2": Phase="Running", Reason="", readiness=true. Elapsed: 2.045064183s
    Nov 15 17:13:51.986: INFO: Pod "webserver-deployment-7f5969cbc7-2ltt2" satisfied condition "running"
    Nov 15 17:13:51.989: INFO: Pod "webserver-deployment-7f5969cbc7-lt5zb": Phase="Running", Reason="", readiness=true. Elapsed: 2.046922305s
    Nov 15 17:13:51.989: INFO: Pod "webserver-deployment-7f5969cbc7-lt5zb" satisfied condition "running"
    Nov 15 17:13:51.990: INFO: Pod "webserver-deployment-7f5969cbc7-6zns8": Phase="Running", Reason="", readiness=true. Elapsed: 2.048239479s
    Nov 15 17:13:51.990: INFO: Pod "webserver-deployment-7f5969cbc7-6zns8" satisfied condition "running"
    Nov 15 17:13:51.991: INFO: Pod "webserver-deployment-7f5969cbc7-5snc7": Phase="Running", Reason="", readiness=true. Elapsed: 2.049581554s
    Nov 15 17:13:51.991: INFO: Pod "webserver-deployment-7f5969cbc7-5snc7" satisfied condition "running"
    Nov 15 17:13:51.991: INFO: Waiting for deployment "webserver-deployment" to complete
    Nov 15 17:13:52.026: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Nov 15 17:13:52.075: INFO: Updating deployment webserver-deployment
    Nov 15 17:13:52.075: INFO: Waiting for observed generation 2
    Nov 15 17:13:54.113: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Nov 15 17:13:54.131: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Nov 15 17:13:54.145: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Nov 15 17:13:54.197: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Nov 15 17:13:54.198: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Nov 15 17:13:54.213: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Nov 15 17:13:54.243: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Nov 15 17:13:54.243: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Nov 15 17:13:54.286: INFO: Updating deployment webserver-deployment
    Nov 15 17:13:54.286: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Nov 15 17:13:54.315: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Nov 15 17:13:56.365: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Nov 15 17:13:56.400: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-5147  da7e4a75-fb1f-4e93-ba29-949c93624ee8 54109 3 2023-11-15 17:13:47 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 17:13:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0027b1dd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:9,UnavailableReplicas:24,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-11-15 17:13:54 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-11-15 17:13:56 +0000 UTC,LastTransitionTime:2023-11-15 17:13:47 +0000 UTC,},},ReadyReplicas:9,CollisionCount:nil,},}

    Nov 15 17:13:56.414: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-5147  e3b68dae-8121-4ff3-aad1-2b86189d3e09 53967 3 2023-11-15 17:13:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment da7e4a75-fb1f-4e93-ba29-949c93624ee8 0xc00538af57 0xc00538af58}] [] [{kube-controller-manager Update apps/v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"da7e4a75-fb1f-4e93-ba29-949c93624ee8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00538b008 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Nov 15 17:13:56.414: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Nov 15 17:13:56.414: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-5147  874c63cf-fcb8-44c8-81cc-22dcee9ca51f 54108 3 2023-11-15 17:13:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment da7e4a75-fb1f-4e93-ba29-949c93624ee8 0xc00538ae67 0xc00538ae68}] [] [{kube-controller-manager Update apps/v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"da7e4a75-fb1f-4e93-ba29-949c93624ee8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-15 17:13:56 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00538aef8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:9,AvailableReplicas:9,Conditions:[]ReplicaSetCondition{},},}
    Nov 15 17:13:56.449: INFO: Pod "webserver-deployment-7f5969cbc7-292zm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-292zm webserver-deployment-7f5969cbc7- deployment-5147  74b56f4d-8172-42a9-ad5a-22e5adeefee1 54047 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:dd66cf3667564e54d91068eceee910bf974b484c0d35837e9cd83f2c4f0ee761 cni.projectcalico.org/podIP:172.30.191.76/32 cni.projectcalico.org/podIPs:172.30.191.76/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc00538b537 0xc00538b538}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 17:13:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cvtsf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cvtsf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.106,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.106,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 17:13:56.449: INFO: Pod "webserver-deployment-7f5969cbc7-4npvr" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-4npvr webserver-deployment-7f5969cbc7- deployment-5147  88d70992-367c-4cda-865d-abe180d6c40d 53795 0 2023-11-15 17:13:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:826b1da600c75ceed2110278ac698bf6d950bc2456c6c61e01c703d0abf03043 cni.projectcalico.org/podIP:172.30.164.36/32 cni.projectcalico.org/podIPs:172.30.164.36/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc00538b747 0xc00538b748}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 17:13:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 17:13:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.164.36\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bqrq9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bqrq9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:172.30.164.36,StartTime:2023-11-15 17:13:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 17:13:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c0a120f2f6efd4a1c00a5b7b1318d5c1c286697fa16dc8f97fb085f6afba44aa,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.164.36,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 17:13:56.450: INFO: Pod "webserver-deployment-7f5969cbc7-4rp6h" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-4rp6h webserver-deployment-7f5969cbc7- deployment-5147  1169e5f4-c244-4205-8c17-c67477d5aea3 54044 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:03c4a6c84cf352c8706332d15df458b09bacf47b919e75457cadfe9e889154e4 cni.projectcalico.org/podIP:172.30.205.218/32 cni.projectcalico.org/podIPs:172.30.205.218/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc00538b957 0xc00538b958}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 17:13:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ph95c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ph95c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.114,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 17:13:56.450: INFO: Pod "webserver-deployment-7f5969cbc7-5snc7" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5snc7 webserver-deployment-7f5969cbc7- deployment-5147  6da5120a-9998-468f-8d1e-2682d56c9042 53776 0 2023-11-15 17:13:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:bf24f3b67f673f9f4ee3703bd38b13939f38914eceea178df596e0455ebada4e cni.projectcalico.org/podIP:172.30.164.56/32 cni.projectcalico.org/podIPs:172.30.164.56/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc00538bb67 0xc00538bb68}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 17:13:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 17:13:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.164.56\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5pg5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5pg5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:172.30.164.56,StartTime:2023-11-15 17:13:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 17:13:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0e30fca4e6a3c1d7c143045670069faecb57a3ce990cc637b3b59e72fd84889d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.164.56,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 17:13:56.450: INFO: Pod "webserver-deployment-7f5969cbc7-6jdzs" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6jdzs webserver-deployment-7f5969cbc7- deployment-5147  4e53ee77-1171-45d6-8909-80e1243bd4df 53737 0 2023-11-15 17:13:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c2678c957157736d663fccf27920b716800ce3bee4f82a14641e4a1e46c68170 cni.projectcalico.org/podIP:172.30.191.127/32 cni.projectcalico.org/podIPs:172.30.191.127/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc00538bd77 0xc00538bd78}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 17:13:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 17:13:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.191.127\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hrrvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hrrvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.106,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.106,PodIP:172.30.191.127,StartTime:2023-11-15 17:13:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 17:13:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://8481306076c9030d0a1e7593458c2d4e84e3bbda7e7d6a64a76201ac975838e0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.191.127,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 17:13:56.451: INFO: Pod "webserver-deployment-7f5969cbc7-6pg9c" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6pg9c webserver-deployment-7f5969cbc7- deployment-5147  fe043dbc-11fa-4c08-9699-de298b53b65b 54066 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:5f068488d48e34ca31a4e21d732e4b79282a03693a5e7d7c6366a621fd8596d7 cni.projectcalico.org/podIP:172.30.191.126/32 cni.projectcalico.org/podIPs:172.30.191.126/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc00538bf87 0xc00538bf88}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 17:13:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4djqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4djqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.106,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.106,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 17:13:56.451: INFO: Pod "webserver-deployment-7f5969cbc7-6zns8" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6zns8 webserver-deployment-7f5969cbc7- deployment-5147  9615564f-123d-46fd-ab81-025bc5d04a22 53785 0 2023-11-15 17:13:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:b1b7b3391c3065fb374bd31ac2770d50d7409a537800c40a004e089244a3ef7c cni.projectcalico.org/podIP:172.30.205.249/32 cni.projectcalico.org/podIPs:172.30.205.249/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc000f1e3f7 0xc000f1e3f8}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 17:13:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 17:13:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.205.249\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xl7s4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xl7s4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.114,PodIP:172.30.205.249,StartTime:2023-11-15 17:13:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 17:13:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6876419f879c0b1b8cba758c7e77b0a644a3fad90cbcd152f59d9d8f508ec1fc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.205.249,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 17:13:56.451: INFO: Pod "webserver-deployment-7f5969cbc7-7g9nd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7g9nd webserver-deployment-7f5969cbc7- deployment-5147  b5af8387-5fc2-44ca-ba19-64a6b33e212a 54029 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:5907d333570f6c670238c2934dd429a82f6bd35500094c9390300ae5a65f44b6 cni.projectcalico.org/podIP:172.30.205.209/32 cni.projectcalico.org/podIPs:172.30.205.209/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc000f1e8e7 0xc000f1e8e8}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 17:13:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8jzb4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8jzb4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.114,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 17:13:56.452: INFO: Pod "webserver-deployment-7f5969cbc7-87hcr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-87hcr webserver-deployment-7f5969cbc7- deployment-5147  c5229312-f97c-4c1e-ae9a-545dc5a05274 53952 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc000f1ed57 0xc000f1ed58}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v8b96,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v8b96,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 17:13:56.452: INFO: Pod "webserver-deployment-7f5969cbc7-dqtb2" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dqtb2 webserver-deployment-7f5969cbc7- deployment-5147  84a96962-dcc9-4ba2-b010-eff588d0b34d 53791 0 2023-11-15 17:13:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:24ed3adb7fc21d8cfeec68b1628dfb30b0460693956c999396a3ba0f09998a71 cni.projectcalico.org/podIP:172.30.205.248/32 cni.projectcalico.org/podIPs:172.30.205.248/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc000f1ef27 0xc000f1ef28}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 17:13:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 17:13:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.205.248\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pm6nk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pm6nk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.114,PodIP:172.30.205.248,StartTime:2023-11-15 17:13:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 17:13:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2719a3036bf75dc07192bc5df4ea8c00fc6d3447d579a01865470fd07e9aba1f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.205.248,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 17:13:56.452: INFO: Pod "webserver-deployment-7f5969cbc7-flkwx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-flkwx webserver-deployment-7f5969cbc7- deployment-5147  25ea989d-f7f5-4c20-8b82-1aef18039448 54059 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:e15740e4f785771d465381ff176781ec06f8484eae753e01b0829064f03718dd cni.projectcalico.org/podIP:172.30.164.42/32 cni.projectcalico.org/podIPs:172.30.164.42/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc000f1f157 0xc000f1f158}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 17:13:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7r7xm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7r7xm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 17:13:56.453: INFO: Pod "webserver-deployment-7f5969cbc7-gnrst" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-gnrst webserver-deployment-7f5969cbc7- deployment-5147  7d7a216f-9fa0-4fff-92fb-54a76a0135da 54015 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc000f1f367 0xc000f1f368}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-574pm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-574pm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 17:13:56.453: INFO: Pod "webserver-deployment-7f5969cbc7-hp8th" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hp8th webserver-deployment-7f5969cbc7- deployment-5147  f7a4360a-e6eb-41f7-a369-7b706592594d 54077 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:d288a5f7e0272e1e5785d691e4f63baf57761760b2d710af77056919603f1076 cni.projectcalico.org/podIP:172.30.164.4/32 cni.projectcalico.org/podIPs:172.30.164.4/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc000f1fbe7 0xc000f1fbe8}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 17:13:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v8vbd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v8vbd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 17:13:56.453: INFO: Pod "webserver-deployment-7f5969cbc7-hqm97" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hqm97 webserver-deployment-7f5969cbc7- deployment-5147  b026a262-75f0-4599-bbec-a3855d047e6b 53780 0 2023-11-15 17:13:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:2602f8782ed4fac3ab92f2a550ce9e3b6e33d9564f8e348aee0e6a3242a96b09 cni.projectcalico.org/podIP:172.30.191.79/32 cni.projectcalico.org/podIPs:172.30.191.79/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc004aaa0b7 0xc004aaa0b8}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 17:13:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 17:13:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.191.79\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ltq4g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ltq4g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.106,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.106,PodIP:172.30.191.79,StartTime:2023-11-15 17:13:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 17:13:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7b231e297ab561ec7aa0da823b0ff7b94464be4a110aaa258adb4af17e6646fb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.191.79,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 17:13:56.453: INFO: Pod "webserver-deployment-7f5969cbc7-hrs64" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hrs64 webserver-deployment-7f5969cbc7- deployment-5147  370787ea-82bc-44ab-8093-770ba3982c3c 54116 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:9a5106be5d0c6f243b08378aaa46852d1003b940b9db78942f288c8b30c7e37b cni.projectcalico.org/podIP:172.30.191.118/32 cni.projectcalico.org/podIPs:172.30.191.118/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc004aaa2c7 0xc004aaa2c8}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 17:13:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j5w5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j5w5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.106,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.106,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 17:13:56.454: INFO: Pod "webserver-deployment-7f5969cbc7-hsftt" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hsftt webserver-deployment-7f5969cbc7- deployment-5147  20a1cadc-5589-4f0b-94c1-772694cb175f 54104 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:2f8326e30e6c8005b326a3c1d3964c3f3e9a4bb2cda90d9dcb1f0ba48d8de81f cni.projectcalico.org/podIP:172.30.164.62/32 cni.projectcalico.org/podIPs:172.30.164.62/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc004aaa4d7 0xc004aaa4d8}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 17:13:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 17:13:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.164.62\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-85trf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-85trf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:172.30.164.62,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 17:13:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2aa79b2601ff0e4e14ab94145338eeba03f257d071273cdf7d43b1c52699f0f9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.164.62,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 17:13:56.454: INFO: Pod "webserver-deployment-7f5969cbc7-lt5zb" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-lt5zb webserver-deployment-7f5969cbc7- deployment-5147  0036be3d-b51b-4529-97db-732bd36b03df 53786 0 2023-11-15 17:13:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:954af8e5886ab859aaa7a8987707c93a7941051e4f4d6dafda3d63ae3bade2ff cni.projectcalico.org/podIP:172.30.191.89/32 cni.projectcalico.org/podIPs:172.30.191.89/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc004aaa707 0xc004aaa708}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 17:13:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 17:13:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.191.89\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dfsbx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dfsbx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.106,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.106,PodIP:172.30.191.89,StartTime:2023-11-15 17:13:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 17:13:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://99153443ac5f1fb038448acc01afe54154bd6d9745437ee4222c994d629b8f02,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.191.89,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 17:13:56.455: INFO: Pod "webserver-deployment-7f5969cbc7-mvwn6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mvwn6 webserver-deployment-7f5969cbc7- deployment-5147  c723454e-4eb6-44f0-9b07-9e0938b5b6df 54090 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:f46b5a6990807848a3476100a206f939ae613e0ac967a0e6e3e71c99efd8b62a cni.projectcalico.org/podIP:172.30.205.219/32 cni.projectcalico.org/podIPs:172.30.205.219/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc004aaa917 0xc004aaa918}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 17:13:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ng6jn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ng6jn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.114,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 17:13:56.456: INFO: Pod "webserver-deployment-7f5969cbc7-qxg5c" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qxg5c webserver-deployment-7f5969cbc7- deployment-5147  88ce5662-827e-4ed5-8ebf-d2bd9c78ca3d 54012 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc004aaab07 0xc004aaab08}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ddgm2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ddgm2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 17:13:56.457: INFO: Pod "webserver-deployment-7f5969cbc7-rtxlx" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rtxlx webserver-deployment-7f5969cbc7- deployment-5147  998d4d49-cac7-48bb-b5e8-178324a08cef 53779 0 2023-11-15 17:13:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:2793054dc9999de1aa3b9fcdc97365c06be08e5c460b3359012c40e3b1b29138 cni.projectcalico.org/podIP:172.30.205.207/32 cni.projectcalico.org/podIPs:172.30.205.207/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 874c63cf-fcb8-44c8-81cc-22dcee9ca51f 0xc004aaace7 0xc004aaace8}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"874c63cf-fcb8-44c8-81cc-22dcee9ca51f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 17:13:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 17:13:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.205.207\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9chcv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9chcv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.114,PodIP:172.30.205.207,StartTime:2023-11-15 17:13:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-15 17:13:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://997a51ea1c5405cf6e8d11f08c28a50542dfaeea539cebc03cbbedc096a93042,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.205.207,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 17:13:56.458: INFO: Pod "webserver-deployment-d9f79cb5-2d9zs" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-2d9zs webserver-deployment-d9f79cb5- deployment-5147  b153e7db-c593-4d0e-9cc9-fd47158cc7ae 54042 0 2023-11-15 17:13:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:3e3fa3aa2f6834ff9de55e80054d45d775ff34d168f594809b40a45fb49fd58f cni.projectcalico.org/podIP:172.30.191.120/32 cni.projectcalico.org/podIPs:172.30.191.120/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e3b68dae-8121-4ff3-aad1-2b86189d3e09 0xc004aaaf07 0xc004aaaf08}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3b68dae-8121-4ff3-aad1-2b86189d3e09\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 17:13:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 17:13:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.191.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6rsf2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6rsf2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.106,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.106,PodIP:172.30.191.120,StartTime:2023-11-15 17:13:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.191.120,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 17:13:56.458: INFO: Pod "webserver-deployment-d9f79cb5-4q4kq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-4q4kq webserver-deployment-d9f79cb5- deployment-5147  5fbf8518-db2f-48b0-9fd5-0d218f4945d7 54031 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:cc2ac21236cf028556dbc7da616416f805c44d970f79104095d324e0b8d36db7 cni.projectcalico.org/podIP:172.30.191.84/32 cni.projectcalico.org/podIPs:172.30.191.84/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e3b68dae-8121-4ff3-aad1-2b86189d3e09 0xc004aab167 0xc004aab168}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3b68dae-8121-4ff3-aad1-2b86189d3e09\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 17:13:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sxd66,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sxd66,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.106,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.106,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 17:13:56.458: INFO: Pod "webserver-deployment-d9f79cb5-6g25z" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-6g25z webserver-deployment-d9f79cb5- deployment-5147  7724a3f0-65ae-4a17-8eea-51535efa8985 54095 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:3bcdf221021bca44538deedf3904cf4374c2b587876789d600cfad957c548e9d cni.projectcalico.org/podIP:172.30.164.12/32 cni.projectcalico.org/podIPs:172.30.164.12/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e3b68dae-8121-4ff3-aad1-2b86189d3e09 0xc004aab397 0xc004aab398}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3b68dae-8121-4ff3-aad1-2b86189d3e09\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 17:13:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vwnpd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vwnpd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 17:13:56.459: INFO: Pod "webserver-deployment-d9f79cb5-bfmf7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-bfmf7 webserver-deployment-d9f79cb5- deployment-5147  4bbd1473-74ce-431a-9e88-a3d24c1c24dc 54009 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e3b68dae-8121-4ff3-aad1-2b86189d3e09 0xc004aab5c7 0xc004aab5c8}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3b68dae-8121-4ff3-aad1-2b86189d3e09\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ql5nr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ql5nr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.114,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 17:13:56.459: INFO: Pod "webserver-deployment-d9f79cb5-d9hq2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-d9hq2 webserver-deployment-d9f79cb5- deployment-5147  10207612-d940-4557-9e18-734691d13950 54085 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:6bb5a1eacd4e86a8cc41ae1fa2b29669004b04fb501237be1909b8b70d6ce915 cni.projectcalico.org/podIP:172.30.191.116/32 cni.projectcalico.org/podIPs:172.30.191.116/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e3b68dae-8121-4ff3-aad1-2b86189d3e09 0xc004aab7b7 0xc004aab7b8}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3b68dae-8121-4ff3-aad1-2b86189d3e09\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 17:13:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c7hg8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c7hg8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.106,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.106,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 17:13:56.459: INFO: Pod "webserver-deployment-d9f79cb5-gw5n6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-gw5n6 webserver-deployment-d9f79cb5- deployment-5147  220dcb14-97e1-4076-b8a3-11be2696d297 54004 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e3b68dae-8121-4ff3-aad1-2b86189d3e09 0xc004aab9c7 0xc004aab9c8}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3b68dae-8121-4ff3-aad1-2b86189d3e09\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d9kc5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d9kc5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 17:13:56.459: INFO: Pod "webserver-deployment-d9f79cb5-j648k" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-j648k webserver-deployment-d9f79cb5- deployment-5147  0ea9704a-7da8-4759-80b9-5c76f0044a61 54068 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:7e795f1cb78bcd1a56b745a0fd651cdcd750972c638a4d7f8306f76a44aea516 cni.projectcalico.org/podIP:172.30.205.241/32 cni.projectcalico.org/podIPs:172.30.205.241/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e3b68dae-8121-4ff3-aad1-2b86189d3e09 0xc004aabbc7 0xc004aabbc8}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3b68dae-8121-4ff3-aad1-2b86189d3e09\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 17:13:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-87smw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-87smw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.114,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 17:13:56.460: INFO: Pod "webserver-deployment-d9f79cb5-klpwv" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-klpwv webserver-deployment-d9f79cb5- deployment-5147  08fe591a-4168-4ccd-a7b6-6c8a7030b6e4 54112 0 2023-11-15 17:13:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:b942d0aaec3b008803a0b67a1a732699c2b073d2f250641430b8f4936c8648ef cni.projectcalico.org/podIP:172.30.164.60/32 cni.projectcalico.org/podIPs:172.30.164.60/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e3b68dae-8121-4ff3-aad1-2b86189d3e09 0xc004aabdf7 0xc004aabdf8}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3b68dae-8121-4ff3-aad1-2b86189d3e09\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 17:13:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 17:13:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.164.60\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rc7r5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rc7r5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:172.30.164.60,StartTime:2023-11-15 17:13:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.164.60,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 17:13:56.460: INFO: Pod "webserver-deployment-d9f79cb5-mpgxb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-mpgxb webserver-deployment-d9f79cb5- deployment-5147  b7f2d7f4-0194-487b-b32e-3c63f981c979 54100 0 2023-11-15 17:13:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:001351b4b5e6100e92676f853d054f223b07d7de33ed5e9fa947f81bf54b7830 cni.projectcalico.org/podIP:172.30.164.63/32 cni.projectcalico.org/podIPs:172.30.164.63/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e3b68dae-8121-4ff3-aad1-2b86189d3e09 0xc003364057 0xc003364058}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3b68dae-8121-4ff3-aad1-2b86189d3e09\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 17:13:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 17:13:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.164.63\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4pc5f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4pc5f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:172.30.164.63,StartTime:2023-11-15 17:13:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.164.63,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 17:13:56.460: INFO: Pod "webserver-deployment-d9f79cb5-pngst" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-pngst webserver-deployment-d9f79cb5- deployment-5147  169c93db-dd2f-4a86-bc2a-609891d114e6 54040 0 2023-11-15 17:13:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:aa966e02eb4c9a84a93afb0d1e1885b9040d577873b0289b12249eea06f311e6 cni.projectcalico.org/podIP:172.30.205.198/32 cni.projectcalico.org/podIPs:172.30.205.198/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e3b68dae-8121-4ff3-aad1-2b86189d3e09 0xc0033642b7 0xc0033642b8}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3b68dae-8121-4ff3-aad1-2b86189d3e09\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 17:13:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 17:13:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.205.198\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9qjcp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9qjcp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.114,PodIP:172.30.205.198,StartTime:2023-11-15 17:13:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.205.198,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 17:13:56.460: INFO: Pod "webserver-deployment-d9f79cb5-sr8sz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-sr8sz webserver-deployment-d9f79cb5- deployment-5147  6ac99e48-742a-4c93-87bb-056d83dd35d5 54098 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:40f471c62ed6cc476821ae4045fcd63f21fec6c2c3849e46485988c068620243 cni.projectcalico.org/podIP:172.30.205.195/32 cni.projectcalico.org/podIPs:172.30.205.195/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e3b68dae-8121-4ff3-aad1-2b86189d3e09 0xc003364507 0xc003364508}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3b68dae-8121-4ff3-aad1-2b86189d3e09\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-11-15 17:13:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2wskc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2wskc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.114,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.114,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 17:13:56.461: INFO: Pod "webserver-deployment-d9f79cb5-tlf28" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-tlf28 webserver-deployment-d9f79cb5- deployment-5147  5d394e6d-e8f5-4d60-8465-118a48f16f8f 54016 0 2023-11-15 17:13:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e3b68dae-8121-4ff3-aad1-2b86189d3e09 0xc003364717 0xc003364718}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3b68dae-8121-4ff3-aad1-2b86189d3e09\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-15 17:13:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g7vpq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g7vpq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.115,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.115,PodIP:,StartTime:2023-11-15 17:13:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 15 17:13:56.461: INFO: Pod "webserver-deployment-d9f79cb5-w92q6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-w92q6 webserver-deployment-d9f79cb5- deployment-5147  a00e00d3-6e07-49dc-ba1a-b5e6cf490e5b 54048 0 2023-11-15 17:13:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:b1b7e9d2d63d3dc095813c59f67e30959c009d6fd30f0c335e1b3e41f8c393a7 cni.projectcalico.org/podIP:172.30.191.105/32 cni.projectcalico.org/podIPs:172.30.191.105/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e3b68dae-8121-4ff3-aad1-2b86189d3e09 0xc003364917 0xc003364918}] [] [{kube-controller-manager Update v1 2023-11-15 17:13:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3b68dae-8121-4ff3-aad1-2b86189d3e09\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-11-15 17:13:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-11-15 17:13:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.191.105\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vz26w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vz26w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.40.106,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-15 17:13:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.40.106,PodIP:172.30.191.105,StartTime:2023-11-15 17:13:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.191.105,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:13:56.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5147" for this suite. 11/15/23 17:13:56.493
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:13:56.54
Nov 15 17:13:56.540: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename webhook 11/15/23 17:13:56.542
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:13:56.598
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:13:56.608
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/15/23 17:13:56.677
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 17:13:57.321
STEP: Deploying the webhook pod 11/15/23 17:13:57.351
STEP: Wait for the deployment to be ready 11/15/23 17:13:57.406
Nov 15 17:13:57.446: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 15 17:13:59.501: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 17, 13, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 17, 13, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 17, 13, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 17, 13, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 11/15/23 17:14:01.52
STEP: Verifying the service has paired with the endpoint 11/15/23 17:14:01.588
Nov 15 17:14:02.589: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 11/15/23 17:14:02.607
STEP: Updating a mutating webhook configuration's rules to not include the create operation 11/15/23 17:14:02.775
STEP: Creating a configMap that should not be mutated 11/15/23 17:14:02.808
STEP: Patching a mutating webhook configuration's rules to include the create operation 11/15/23 17:14:02.902
STEP: Creating a configMap that should be mutated 11/15/23 17:14:02.935
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 15 17:14:03.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9495" for this suite. 11/15/23 17:14:03.388
STEP: Destroying namespace "webhook-9495-markers" for this suite. 11/15/23 17:14:03.415
------------------------------
• [SLOW TEST] [6.900 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:13:56.54
    Nov 15 17:13:56.540: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename webhook 11/15/23 17:13:56.542
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:13:56.598
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:13:56.608
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/15/23 17:13:56.677
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/15/23 17:13:57.321
    STEP: Deploying the webhook pod 11/15/23 17:13:57.351
    STEP: Wait for the deployment to be ready 11/15/23 17:13:57.406
    Nov 15 17:13:57.446: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Nov 15 17:13:59.501: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 15, 17, 13, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 17, 13, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 15, 17, 13, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 15, 17, 13, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 11/15/23 17:14:01.52
    STEP: Verifying the service has paired with the endpoint 11/15/23 17:14:01.588
    Nov 15 17:14:02.589: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 11/15/23 17:14:02.607
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 11/15/23 17:14:02.775
    STEP: Creating a configMap that should not be mutated 11/15/23 17:14:02.808
    STEP: Patching a mutating webhook configuration's rules to include the create operation 11/15/23 17:14:02.902
    STEP: Creating a configMap that should be mutated 11/15/23 17:14:02.935
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:14:03.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9495" for this suite. 11/15/23 17:14:03.388
    STEP: Destroying namespace "webhook-9495-markers" for this suite. 11/15/23 17:14:03.415
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:14:03.481
Nov 15 17:14:03.481: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename replication-controller 11/15/23 17:14:03.482
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:14:03.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:14:03.58
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Nov 15 17:14:03.598: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 11/15/23 17:14:03.653
STEP: Checking rc "condition-test" has the desired failure condition set 11/15/23 17:14:03.675
STEP: Scaling down rc "condition-test" to satisfy pod quota 11/15/23 17:14:04.724
Nov 15 17:14:04.802: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 11/15/23 17:14:04.802
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Nov 15 17:14:04.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-8676" for this suite. 11/15/23 17:14:04.852
------------------------------
• [1.400 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:14:03.481
    Nov 15 17:14:03.481: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename replication-controller 11/15/23 17:14:03.482
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:14:03.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:14:03.58
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Nov 15 17:14:03.598: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 11/15/23 17:14:03.653
    STEP: Checking rc "condition-test" has the desired failure condition set 11/15/23 17:14:03.675
    STEP: Scaling down rc "condition-test" to satisfy pod quota 11/15/23 17:14:04.724
    Nov 15 17:14:04.802: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 11/15/23 17:14:04.802
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:14:04.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-8676" for this suite. 11/15/23 17:14:04.852
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:14:04.89
Nov 15 17:14:04.891: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename downward-api 11/15/23 17:14:04.896
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:14:04.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:14:04.971
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 11/15/23 17:14:04.997
Nov 15 17:14:05.037: INFO: Waiting up to 5m0s for pod "downward-api-0140021a-bd44-4b59-a107-7111429dbf74" in namespace "downward-api-1343" to be "Succeeded or Failed"
Nov 15 17:14:05.054: INFO: Pod "downward-api-0140021a-bd44-4b59-a107-7111429dbf74": Phase="Pending", Reason="", readiness=false. Elapsed: 17.59905ms
Nov 15 17:14:07.078: INFO: Pod "downward-api-0140021a-bd44-4b59-a107-7111429dbf74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041487989s
Nov 15 17:14:09.073: INFO: Pod "downward-api-0140021a-bd44-4b59-a107-7111429dbf74": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035856351s
Nov 15 17:14:11.072: INFO: Pod "downward-api-0140021a-bd44-4b59-a107-7111429dbf74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035036835s
STEP: Saw pod success 11/15/23 17:14:11.072
Nov 15 17:14:11.073: INFO: Pod "downward-api-0140021a-bd44-4b59-a107-7111429dbf74" satisfied condition "Succeeded or Failed"
Nov 15 17:14:11.090: INFO: Trying to get logs from node 10.15.40.115 pod downward-api-0140021a-bd44-4b59-a107-7111429dbf74 container dapi-container: <nil>
STEP: delete the pod 11/15/23 17:14:11.137
Nov 15 17:14:11.194: INFO: Waiting for pod downward-api-0140021a-bd44-4b59-a107-7111429dbf74 to disappear
Nov 15 17:14:11.210: INFO: Pod downward-api-0140021a-bd44-4b59-a107-7111429dbf74 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Nov 15 17:14:11.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1343" for this suite. 11/15/23 17:14:11.237
------------------------------
• [SLOW TEST] [6.372 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:14:04.89
    Nov 15 17:14:04.891: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename downward-api 11/15/23 17:14:04.896
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:14:04.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:14:04.971
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 11/15/23 17:14:04.997
    Nov 15 17:14:05.037: INFO: Waiting up to 5m0s for pod "downward-api-0140021a-bd44-4b59-a107-7111429dbf74" in namespace "downward-api-1343" to be "Succeeded or Failed"
    Nov 15 17:14:05.054: INFO: Pod "downward-api-0140021a-bd44-4b59-a107-7111429dbf74": Phase="Pending", Reason="", readiness=false. Elapsed: 17.59905ms
    Nov 15 17:14:07.078: INFO: Pod "downward-api-0140021a-bd44-4b59-a107-7111429dbf74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041487989s
    Nov 15 17:14:09.073: INFO: Pod "downward-api-0140021a-bd44-4b59-a107-7111429dbf74": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035856351s
    Nov 15 17:14:11.072: INFO: Pod "downward-api-0140021a-bd44-4b59-a107-7111429dbf74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035036835s
    STEP: Saw pod success 11/15/23 17:14:11.072
    Nov 15 17:14:11.073: INFO: Pod "downward-api-0140021a-bd44-4b59-a107-7111429dbf74" satisfied condition "Succeeded or Failed"
    Nov 15 17:14:11.090: INFO: Trying to get logs from node 10.15.40.115 pod downward-api-0140021a-bd44-4b59-a107-7111429dbf74 container dapi-container: <nil>
    STEP: delete the pod 11/15/23 17:14:11.137
    Nov 15 17:14:11.194: INFO: Waiting for pod downward-api-0140021a-bd44-4b59-a107-7111429dbf74 to disappear
    Nov 15 17:14:11.210: INFO: Pod downward-api-0140021a-bd44-4b59-a107-7111429dbf74 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:14:11.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1343" for this suite. 11/15/23 17:14:11.237
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/15/23 17:14:11.264
Nov 15 17:14:11.264: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
STEP: Building a namespace api object, basename downward-api 11/15/23 17:14:11.268
STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:14:11.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:14:11.335
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 11/15/23 17:14:11.353
Nov 15 17:14:11.388: INFO: Waiting up to 5m0s for pod "downwardapi-volume-25cb7eb8-f11d-492f-aa08-06a9c6f4fe6b" in namespace "downward-api-2680" to be "Succeeded or Failed"
Nov 15 17:14:11.403: INFO: Pod "downwardapi-volume-25cb7eb8-f11d-492f-aa08-06a9c6f4fe6b": Phase="Pending", Reason="", readiness=false. Elapsed: 15.552979ms
Nov 15 17:14:13.420: INFO: Pod "downwardapi-volume-25cb7eb8-f11d-492f-aa08-06a9c6f4fe6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032397897s
Nov 15 17:14:15.420: INFO: Pod "downwardapi-volume-25cb7eb8-f11d-492f-aa08-06a9c6f4fe6b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032191379s
Nov 15 17:14:17.424: INFO: Pod "downwardapi-volume-25cb7eb8-f11d-492f-aa08-06a9c6f4fe6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036157073s
STEP: Saw pod success 11/15/23 17:14:17.424
Nov 15 17:14:17.424: INFO: Pod "downwardapi-volume-25cb7eb8-f11d-492f-aa08-06a9c6f4fe6b" satisfied condition "Succeeded or Failed"
Nov 15 17:14:17.439: INFO: Trying to get logs from node 10.15.40.115 pod downwardapi-volume-25cb7eb8-f11d-492f-aa08-06a9c6f4fe6b container client-container: <nil>
STEP: delete the pod 11/15/23 17:14:17.506
Nov 15 17:14:17.542: INFO: Waiting for pod downwardapi-volume-25cb7eb8-f11d-492f-aa08-06a9c6f4fe6b to disappear
Nov 15 17:14:17.556: INFO: Pod downwardapi-volume-25cb7eb8-f11d-492f-aa08-06a9c6f4fe6b no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Nov 15 17:14:17.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2680" for this suite. 11/15/23 17:14:17.584
------------------------------
• [SLOW TEST] [6.346 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/15/23 17:14:11.264
    Nov 15 17:14:11.264: INFO: >>> kubeConfig: /tmp/kubeconfig-831742819
    STEP: Building a namespace api object, basename downward-api 11/15/23 17:14:11.268
    STEP: Waiting for a default service account to be provisioned in namespace 11/15/23 17:14:11.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/15/23 17:14:11.335
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 11/15/23 17:14:11.353
    Nov 15 17:14:11.388: INFO: Waiting up to 5m0s for pod "downwardapi-volume-25cb7eb8-f11d-492f-aa08-06a9c6f4fe6b" in namespace "downward-api-2680" to be "Succeeded or Failed"
    Nov 15 17:14:11.403: INFO: Pod "downwardapi-volume-25cb7eb8-f11d-492f-aa08-06a9c6f4fe6b": Phase="Pending", Reason="", readiness=false. Elapsed: 15.552979ms
    Nov 15 17:14:13.420: INFO: Pod "downwardapi-volume-25cb7eb8-f11d-492f-aa08-06a9c6f4fe6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032397897s
    Nov 15 17:14:15.420: INFO: Pod "downwardapi-volume-25cb7eb8-f11d-492f-aa08-06a9c6f4fe6b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032191379s
    Nov 15 17:14:17.424: INFO: Pod "downwardapi-volume-25cb7eb8-f11d-492f-aa08-06a9c6f4fe6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036157073s
    STEP: Saw pod success 11/15/23 17:14:17.424
    Nov 15 17:14:17.424: INFO: Pod "downwardapi-volume-25cb7eb8-f11d-492f-aa08-06a9c6f4fe6b" satisfied condition "Succeeded or Failed"
    Nov 15 17:14:17.439: INFO: Trying to get logs from node 10.15.40.115 pod downwardapi-volume-25cb7eb8-f11d-492f-aa08-06a9c6f4fe6b container client-container: <nil>
    STEP: delete the pod 11/15/23 17:14:17.506
    Nov 15 17:14:17.542: INFO: Waiting for pod downwardapi-volume-25cb7eb8-f11d-492f-aa08-06a9c6f4fe6b to disappear
    Nov 15 17:14:17.556: INFO: Pod downwardapi-volume-25cb7eb8-f11d-492f-aa08-06a9c6f4fe6b no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Nov 15 17:14:17.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2680" for this suite. 11/15/23 17:14:17.584
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Nov 15 17:14:17.618: INFO: Running AfterSuite actions on node 1
Nov 15 17:14:17.618: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.001 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Nov 15 17:14:17.618: INFO: Running AfterSuite actions on node 1
    Nov 15 17:14:17.618: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.093 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 6552.562 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h49m13.026214198s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

